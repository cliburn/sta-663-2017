<doc id="9627" url="https://en.wikipedia.org/wiki?curid=9627" title="Elizabeth Barrett Browning">
Elizabeth Barrett Browning

Elizabeth Barrett Browning (née Moulton-Barrett, ; 6 March 1806 – 29 June 1861) was one of the most prominent English poets of the Victorian era, popular in Britain and the United States during her lifetime.
Born in County Durham, the eldest of 12 children, Elizabeth Barrett wrote poetry from about the age of six. Her mother's collection of her poems forms one of the largest collections extant of juvenilia by any English writer. At 15 she became ill, suffering intense head and spinal pain for the rest of her life. Later in life she also developed lung problems, possibly tuberculosis. She took laudanum for the pain from an early age, which is likely to have contributed to her frail health.
In the 1830s Elizabeth was introduced to literary society through her cousin, John Kenyon. Her first adult collection of poems was published in 1838 and she wrote prolifically between 1841 and 1844, producing poetry, translation and prose. She campaigned for the abolition of slavery and her work helped influence reform in the child labour legislation. Her prolific output made her a rival to Tennyson as a candidate for poet laureate on the death of Wordsworth.
Elizabeth's volume "Poems" (1844) brought her great success, attracting the admiration of the writer Robert Browning. Their correspondence, courtship and marriage were carried out in secret, for fear of her father's disapproval. Following the wedding she was indeed disinherited by her father. The couple moved to Italy in 1846, where she would live for the rest of her life. They had one son, Robert Barrett Browning, whom they called Pen. She died in Florence in 1861. A collection of her last poems was published by her husband shortly after her death.
Elizabeth's work had a major influence on prominent writers of the day, including the American poets Edgar Allan Poe and Emily Dickinson. She is remembered for such poems as "How Do I Love Thee?" (Sonnet 43, 1845) and "Aurora Leigh "(1856).
Life and career.
Family background.
Some of Elizabeth Barrett's family had lived in Jamaica since 1655. Their wealth derived mainly from Edward Barrett (1734–1798), owner of in Cinnamon Hill, Cornwall, Cambridge, and Oxford estates in northern Jamaica. Elizabeth's maternal grandfather owned sugar plantations, mills, glassworks and ships that traded between Jamaica and Newcastle. Biographer Julia Markus states the poet "believed that she had African blood through her grandfather Charles Moulton", but there is no evidence of this - although other branches of her family had African blood through relationships between plantation owners and slaves. What the family believed to be their genealogy in relation to Jamaica is unclear.
The family wished to hand down their name, stipulating that Barrett should always be held as a surname. In some cases inheritance was given on condition that the name was used by the beneficiary; the English gentry and "squirearchy" had long encouraged this sort of name changing. Given this strong tradition, Elizabeth used "Elizabeth Barrett Moulton Barrett" on legal documents and before she was married often signed herself 'Elizabeth Barrett Barrett' or 'EBB' (initials which she was able to keep after her wedding).
Elizabeth's father chose to raise his family in England while his business enterprises remained in Jamaica. The fortune of Elizabeth's mother's line, the Graham Clarke family, also derived in part from slave labour, and was considerable.
Early life.
Elizabeth Barrett Moulton-Barrett was born on 6 March 1806, in Coxhoe Hall, between the villages of Coxhoe and Kelloe in County Durham, England. Her parents were Edward Barrett Moulton Barrett and Mary Graham Clarke; Elizabeth was the eldest of 12 children (eight boys and four girls). All lived to adulthood except for one girl, who died at the age of three, when Elizabeth was eight.
The children all had nicknames: Elizabeth was "Ba". She rode her pony, went for family walks and picnics, socialised with other county families, and participated in home theatrical productions. But unlike her siblings, she immersed herself in books as often as she could get away from the social rituals of her family.
She was baptized in 1809 at Kelloe parish church, although she had already been baptised by a family friend in her first week of life.
In 1809, the family moved to Hope End, a estate near the Malvern Hills in Ledbury, Herefordshire. Her father converted the Georgian house into stables and built a new mansion of opulent Turkish design, which his wife described as something from the "Arabian Nights Entertainments".
The interior's brass balustrades, mahogany doors inlaid with mother-of-pearl, and finely carved fireplaces were eventually complemented by lavish landscaping: ponds, grottos, kiosks, an ice house, a hothouse, and a subterranean passage from house to gardens. Her time at Hope End would inspire her in later life to write her most ambitious work, "Aurora Leigh" (1857), which went through over twenty editions by 1900, but none between 1905 and 1978.
She was educated at home and tutored by Daniel McSwiney with her oldest brother. She began writing verses at the age of four. During the Hope End period, she was an intensely studious, precocious child. She claimed that at the age of six she was reading novels, at eight entranced by Pope's translations of Homer, studying Greek at ten, and at twelve writing her own Homeric epic, "".
In 1820 Mr Barrett privately published "The Battle of Marathon", an epic-style poem, though all copies remained within the family. Her mother compiled the child's poetry into collections of "Poems by Elizabeth B. Barrett". Her father called her the "Poet Laureate of Hope End" and encouraged her work. The result is one of the largest collections of juvenilia of any English writer.
Mary Russell Mitford described the young Elizabeth at this time, as having "a slight, delicate figure, with a shower of dark curls falling on each side of a most expressive face; large, tender eyes, richly fringed by dark eyelashes, and a smile like a sunbeam."
At about this time, Elizabeth began to battle with illness, which the medical science of the time was unable to diagnose. All three sisters came down with the syndrome although it lasted only with Elizabeth. She had intense head and spinal pain with loss of mobility. Various biographies link this to a riding accident at the time (she fell while trying to dismount a horse), but there is no evidence to support the link. Sent to recover at the Gloucester spa, she was treated — in the absence of symptoms supporting another diagnosis — for a spinal problem. Though this illness continued for the rest of her life, it is believed to be unrelated to the lung disease which she developed in 1837.
She began to take opiates for the pain, laudanum (an opium concoction) followed by morphine, then commonly prescribed. She would become dependent on them for much of her adulthood; the use from an early age may well have contributed to her frail health. Biographers such as Alethea Hayter have suggested this may also have contributed to the wild vividness of her imagination and the poetry that it produced.
By 1821 she had read Mary Wollstonecraft's "Vindication of the Rights of Woman" (1792), and become a passionate supporter of Wollstonecraft's ideas. The child's intellectual fascination with the classics and metaphysics was reflected in a religious intensity which she later described as "not the deep persuasion of the mild Christian but the wild visions of an enthusiast." The Barretts attended services at the nearest Dissenting chapel, and Edward was active in Bible and Missionary societies.
Elizabeth's mother died in 1828, and is buried at St Michael's Church, Ledbury, next to her daughter Mary. Sarah Graham-Clarke, Elizabeth's aunt, helped to care for the children, and she had clashes with Elizabeth's strong will. In 1831 Elizabeth's grandmother, Elizabeth Moulton, died.
Following lawsuits and the abolition of slavery Mr Barrett incurred great financial and investment losses that forced him to sell Hope End. Although the family was never poor, the place was seized and put up for sale to satisfy creditors. Always secret in his financial dealings, he would not discuss his situation and the family was haunted by the idea that they might have to move to Jamaica. In 1838, some years after the sale of Hope End, the family settled at 50 Wimpole Street.
During 1837–38 the poet was struck with illness again, with symptoms today suggesting tuberculous ulceration of the lungs. That same year, at her physician's insistence, she moved from London to Torquay, on the Devonshire coast. Two tragedies then struck. In February 1840 her brother Samuel died of a fever in Jamaica. Then her favourite brother Edward ("Bro") was drowned in a sailing accident in Torquay in July. This had a serious effect on her already fragile health. She felt guilty as her father had disapproved of Edward's trip to Torquay. She wrote to Mitford, "That was a very near escape from madness, absolute hopeless madness". The family returned to Wimpole Street in 1841.
Success.
At Wimpole Street Barrett Browning spent most of her time in her upstairs room. Her health began to improve, though she saw few people other than her immediate family. One of those was Kenyon, a wealthy friend of the family and patron of the arts. She received comfort from a spaniel named Flush, a gift from Mary Mitford. (Virginia Woolf later fictionalised the life of the dog, making him the protagonist of her 1933 novel "").
Between 1841 and 1844 Barrett Browning was prolific in poetry, translation and prose. The poem "The Cry of the Children", published in 1842 in "Blackwoods", condemned child labour and helped bring about child-labour reforms by raising support for Lord Shaftesbury's Ten Hours Bill (1844). At about the same time, she contributed critical prose pieces to Richard Henry Horne's "A New Spirit of the Age".
In 1844 she published two volumes of "Poems", which included "A Drama of Exile", "A Vision of Poets", and "Lady Geraldine's Courtship" and two substantial critical essays for 1842 issues of "The Athenaeum". "Since she was not burdened with any domestic duties expected of her sisters, Barrett Browning could now devote herself entirely to the life of the mind, cultivating an enormous correspondence, reading widely". Her prolific output made her a rival to Tennyson as a candidate for poet laureate in 1850 on the death of Wordsworth.
A Royal Society of Arts blue plaque now commemorates Elizabeth at 50 Wimpole Street.
Robert Browning and Italy.
Her 1844 volume "Poems" made her one of the most popular writers in the country, and inspired Robert Browning to write to her. He wrote, "I love your verses with all my heart, dear Miss Barrett," praising their "fresh strange music, the affluent language, the exquisite pathos and true new brave thought."
Kenyon arranged for Browning to meet Elizabeth on 20 May 1845, in her rooms, and so began one of the most famous courtships in literature. Elizabeth had already produced a large amount of work, but Browning had a great influence on her subsequent writing, as did she on his: two of Barrett’s most famous pieces were written after she met Browning, "Sonnets from the Portuguese" and "Aurora Leigh." Robert's "Men and Women" is also a product of that time.
Some critics, however, see him as an undermining influence: "Until her relationship with Robert Browning began in 1845, Barrett's willingness to engage in public discourse about social issues and about aesthetic issues in poetry, which had been so strong in her youth, gradually diminished, as did her physical health. As an intellectual presence and a physical being, she was becoming a shadow of herself."
The courtship and marriage between Robert Browning and Elizabeth were carried out secretly, as she knew her father would disapprove. After a private marriage at St Marylebone Parish Church, they honeymooned in Paris before moving to Italy, in September 1846, which became their home almost continuously until her death. Elizabeth's loyal nurse, Wilson, who witnessed the marriage, accompanied the couple to Italy.
Mr Barrett disinherited Elizabeth, as he did each of his children who married. Elizabeth had foreseen her father's anger but had not anticipated her brothers' rejection. As Elizabeth had some money of her own, the couple were reasonably comfortable in Italy. The Brownings were well respected, and even famous. Elizabeth grew stronger and in 1849, at the age of 43, between four miscarriages, she gave birth to a son, Robert Wiedeman Barrett Browning, whom they called Pen. Their son later married, but had no legitimate children.
At her husband's insistence, Elizabeth's second edition of "Poems" included her love sonnets; as a result, her popularity increased (as well as critical regard), and her artistic position was confirmed.
The couple came to know a wide circle of artists and writers including William Makepeace Thackeray, sculptor Harriet Hosmer (who, she wrote, seemed to be the "perfectly emancipated female") and Harriet Beecher Stowe. In 1849 she met Margaret Fuller, and the female French novelist George Sand in 1852, whom she had long admired. Among her intimate friends in Florence was the writer Isa Blagden, whom she encouraged to write novels. They met Alfred Tennyson in Paris, and John Forster, Samuel Rogers and the Carlyles in London, later befriending Charles Kingsley and John Ruskin.
Decline and death.
After the death of an old friend, G. B. Hunter, and then of her father, Barrett Browning's health started to deteriorate. The Brownings moved from Florence to Siena, residing at the "Villa Alberti". Engrossed in Italian politics, she issued a small volume of political poems titled "Poems before Congress" (1860) "most of which were written to express her sympathy with the Italian cause after the outbreak of fighting in 1859". They caused a furore in England, and the conservative magazines "Blackwood's" and the "Saturday Review" labelled her a fanatic. She dedicated this book to her husband. Her last work was "A Musical Instrument", published posthumously.
Barrett Browning's sister Henrietta died in November 1860. The couple spent the winter of 1860–61 in Rome where Barrett Browning's health further deteriorated and they returned to Florence in early June 1861. She became gradually weaker, using morphine to ease her pain. She died on 29 June 1861 in her husband's arms. Browning said that she died "smilingly, happily, and with a face like a girl's... Her last word was... 'Beautiful'". She was buried in the Protestant English Cemetery of Florence. "On Monday July 1 the shops in the area around Casa Guidi were closed, while Elizabeth was mourned with unusual demonstrations." The nature of her illness is still unclear. Some modern scientists speculate her illness may have been hypokalemic periodic paralysis, a genetic disorder that causes weakness and many of the other symptoms she described.
Publications.
Barrett Browning's first known poem was written at the age of six or eight, "On the Cruelty of Forcement to Man". The manuscript, which protests against impressment, is currently in the Berg Collection of the New York Public Library; the exact date is controversial because the "2" in the date 1812 is written over something else that is scratched out.
Her first independent publication was "Stanzas Excited by Reflections on the Present State of Greece" in "The New Monthly Magazine" of May 1821; followed two months later by "Thoughts Awakened by Contemplating a Piece of the Palm which Grows on the Summit of the Acropolis at Athens".
Her first collection of poems, "An Essay on Mind, with Other Poems," was published in 1826 and reflected her passion for Byron and Greek politics. Its publication drew the attention of a blind scholar of the Greek language, Hugh Stuart Boyd, and of another Greek scholar, Uvedale Price, with whom she maintained sustained correspondence. Among other neighbours was Mrs James Martin from Colwall, with whom she also corresponded throughout her life. Later, at Boyd's suggestion, she translated Aeschylus' "Prometheus Bound" (published in 1833; retranslated in 1850). During their friendship Barrett studied Greek literature, including Homer, Pindar and Aristophanes.
Elizabeth opposed slavery and published two poems highlighting the barbarity of slavers and her support for the abolitionist cause: "The Runaway Slave at Pilgrim's Point"; and "A Curse for a Nation". In "Runaway" she describes a slave woman who is whipped, raped, and made pregnant as she curses the slavers. Elizabeth declared herself glad that the slaves were "virtually free" when the Emancipation Act abolishing slavery in British colonies was passed in 1833, despite the fact that her father believed that Abolitionism would ruin his business.
The date of publication of these poems is in dispute, but her position on slavery in the poems is clear and may have led to a rift between Elizabeth and her father. She wrote to John Ruskin in 1855 "I belong to a family of West Indian slaveholders, and if I believed in curses, I should be afraid". After the Jamaican slave uprising of 1831–32, her father and uncle continued to treat the slaves humanely.
In London, John Kenyon, a distant cousin, introduced Elizabeth to literary figures including William Wordsworth, Mary Russell Mitford, Samuel Taylor Coleridge, Alfred Tennyson and Thomas Carlyle. Elizabeth continued to write, contributing "The Romaunt of Margaret", "The Romaunt of the Page", "The Poet's Vow" and other pieces to various periodicals. She corresponded with other writers, including Mary Russell Mitford, who would become a close friend and who would support Elizabeth's literary ambitions.
In 1838 "The Seraphim and Other Poems" appeared, the first volume of Elizabeth's mature poetry to appear under her own name.
"Sonnets from the Portuguese" was published in 1850. There is debate about the origin of the title. Some say it refers to the series of sonnets of the 16th-century Portuguese poet Luís de Camões. However, "my little Portuguese" was a pet name that Browning had adopted for Elizabeth and this may have some connection.
The verse-novel "Aurora Leigh," her most ambitious and perhaps the most popular of her longer poems, appeared in 1856. It is the story of a female writer making her way in life, balancing work and love, and based on Elizabeth's own experiences. The "North American Review" praised Elizabeth's poem: "Mrs. Browning's poems are, in all respects, the utterance of a woman — of a woman of great learning, rich experience, and powerful genius, uniting to her woman's nature the strength which is sometimes thought peculiar to a man."
Spiritual influence.
Much of Barrett Browning's work carries a religious theme. She had read and studied such works as Milton's "Paradise Lost" and Dante's "Inferno". She says in her writing, "We want the sense of the saturation of Christ's blood upon the souls of our poets, that it may cry through them in answer to the ceaseless wail of the Sphinx of our humanity, expounding agony into renovation. Something of this has been perceived in art when its glory was at the fullest. Something of a yearning after this may be seen among the Greek Christian poets, something which would have been much with a stronger faculty". She believed that "Christ's religion is essentially poetry — poetry glorified". She explored the religious aspect in many of her poems, especially in her early work, such as the sonnets.
She was interested in theological debate, had learned Hebrew and read the Hebrew Bible. Her seminal "Aurora Leigh", for example, features religious imagery and allusion to the apocalypse. The critic Cynthia Scheinberg notes that female characters in "Aurora Leigh" and her earlier work "The Virgin Mary to the Child Jesus" allude to the female character Miriam from the Hebrew Bible. These allusions to Miriam in both poems mirror the way in which Barrett Browning herself drew from Jewish history, while distancing herself from it, in order to achieve a public appearance of a Christian woman poet of the Victorian Age.
Barrett Browning Institute.
In 1892, Ledbury, Herefordshire, held a design competition to build an Institute in honour of Barrett Browning. Brightwen Binyon beat 44 other designs. It was based on the timber-framed Market House, which was opposite the site. It was completed in 1896. However, Nikolaus Pevsner was not impressed by its style. In 1938, it became a public library. It has been Grade II-listed since 2007.
Critical reception.
Barrett Browning was widely popular in the United Kingdom and America during her lifetime. Edgar Allan Poe was inspired by her poem "Lady Geraldine's Courtship" and specifically borrowed the poem's metre for his poem "The Raven". Poe had reviewed Barrett Browning's work in the January 1845 issue of the "Broadway Journal", saying that "her poetic inspiration is the highest — we can conceive of nothing more august. Her sense of Art is pure in itself." In return, she praised "The Raven", and Poe dedicated his 1845 collection "The Raven and Other Poems" to her, referring to her as "the noblest of her sex".
Barrett Browning's poetry greatly influenced Emily Dickinson, who admired her as a woman of achievement. Her popularity in the United States and Britain was further advanced by her stands against social injustice, including slavery in the United States, injustice toward Italian citizens by foreign rulers, and child labour.
Lilian Whiting published a biography of Barrett Browning (1899) which describes her as "the most philosophical poet" and depicts her life as "a Gospel of applied Christianity". To Whiting, the term "art for art's sake" did not apply to Barrett Browning's work, as each poem, distinctively purposeful, was borne of a more "honest vision". In this critical analysis, Whiting portrays Barrett Browning as a poet who uses knowledge of Classical literature with an "intuitive gift of spiritual divination". In "Elizabeth Barrett Browning", Angela Leighton suggests that the portrayal of Barrett Browning as the "pious iconography of womanhood" has distracted us from her poetic achievements. Leighton cites the 1931 play by Rudolf Besier, "The Barretts of Wimpole Street", as evidence that 20th-century literary criticism of Barrett Browning's work has suffered more as a result of her popularity than poetic ineptitude. The play was popularized by actress Katharine Cornell, for whom it became a signature role. It was an enormous success, both artistically and commercially, and was revived several times and adapted twice into movies.
Throughout the 20th century, literary criticism of Barrett Browning's poetry remained sparse until her poems were discovered by the women's movement. She once described herself as being inclined to reject several women's rights principles, suggesting in letters to Mary Russell Mitford and her husband that she believed that there was an inferiority of intellect in women. In "Aurora Leigh", however, she created a strong and independent woman who embraces both work and love. Leighton writes that because Elizabeth participates in the literary world, where voice and diction are dominated by perceived masculine superiority, she "is defined only in mysterious opposition to everything that distinguishes the male subject who writes..." A five-volume scholarly edition of her works was published in 2010, the first in over a century.

</doc>
<doc id="9628" url="https://en.wikipedia.org/wiki?curid=9628" title="Enlil">
Enlil

Enlil (nlin), (EN = Lord + LÍL = Wind, "Lord (of the) Storm") is the god of breath, wind, loft and breadth (height and distance). It was the name of a chief deity listed and written about in Sumerian religion, and later in Akkadian (Assyrian and Babylonian), Hittite, Canaanite and other Mesopotamian clay and stone tablets. The name is perhaps pronounced and sometimes rendered in translations as "Ellil" in later Akkadian, Hittite, and Canaanite literature. In later Akkadian, Enlil is the son of Anshar and Kishar.
Origins.
The myth of Enlil and Ninlil discusses when Enlil was a young god, he was banished from Ekur in Nippur, home of the gods, to Kur, the underworld for seducing a goddess named Ninlil. Ninlil followed him to the underworld where she bore his first child, the moon god Sin (Sumerian Nanna/Suen). After fathering three more underworld-deities (substitutes for Sin), Enlil was allowed to return to the Ekur.
Enlil was known as the inventor of the mattock (a key agricultural pick, hoe, ax or digging tool of the Sumerians) and helped plants to grow.
Cosmological role.
Enlil, along with Anu/An, Enki and Ninhursag were gods of the Sumerians.
By his wife Ninlil or Sud, Enlil was father of the moon god Nanna/Suen (in Akkadian, Sin) and of Ninurta (also called Ningirsu). Enlil is the father of Nisaba the goddess of grain, of Pabilsag who is sometimes equated with Ninurta, and sometimes of Enbilulu. By Ereshkigal Enlil was father of Namtar.
In one myth, Enlil gives advice to his son, the god Ninurta, advising him on a strategy to slay the demon Asag. This advice is relayed to Ninurta by way of Sharur, his enchanted talking mace, which had been sent by Ninurta to the realm of the gods to seek counsel from Enlil directly.
Cultural histories.
Enlil is associated with the ancient city of Nippur, sometimes referred to as the cult city of Enlil. His temple was named Ekur, "House of the Mountain." Such was the sanctity acquired by this edifice that Babylonian and Assyrian rulers, down to the latest days, vied with one another to embellish and restore Enlil's seat of worship. Eventually, the name Ekur became the designation of a temple in general.
Grouped around the main sanctuary, there arose temples and chapels to the gods and goddesses who formed his court, so that Ekur became the name for an entire sacred precinct in the city of Nippur. The name "mountain house" suggests a lofty structure and was perhaps the designation originally of the staged tower at Nippur, built in imitation of a mountain, with the sacred shrine of the god on the top.
Enlil was also known as the god of weather. According to the Sumerians, Enlil requested the creation of a slave race, but then got tired of their noise and tried to kill them by sending a flood. A mortal known as Utnapishtim survived the flood through the help of another god, Ea, and he was made immortal by Enlil after Enlil's initial fury had subsided.
As Enlil was the only god who could reach An, the god of heaven, he held sway over the other gods who were assigned tasks by his agent and would travel to Nippur to draw in his power. He is thus seen as the model for kingship. Enlil was assimilated to the north "Pole of the Ecliptic". His sacred number name was 50.
At a very early period prior to 3000 BC, Nippur had become the centre of a political district of considerable extent. Inscriptions found at Nippur, where extensive excavations were carried on during 1888–1900 by John P. Peters and John Henry Haynes, under the auspices of the University of Pennsylvania, show that Enlil was the head of an extensive pantheon. Among the titles accorded to him are "king of lands", "king of heaven and earth", and "father of the gods".

</doc>
<doc id="9630" url="https://en.wikipedia.org/wiki?curid=9630" title="Ecology">
Ecology

Ecology (from , "house", or "environment"; -λογία, "study of") is the scientific analysis and study of interactions among organisms and their environment. It is an interdisciplinary field that includes biology, geography and Earth science. Ecology includes the study of interactions organisms have with each other, other organisms, and with abiotic components of their environment. Topics of interest to ecologists include the diversity, distribution, amount (biomass), and number (population) of particular organisms, as well as cooperation and competition between organisms, both within and among ecosystems. Ecosystems are composed of dynamically interacting parts including organisms, the communities they make up, and the non-living components of their environment. Ecosystem processes, such as primary production, pedogenesis, nutrient cycling, and various niche construction activities, regulate the flux of energy and matter through an environment. These processes are sustained by organisms with specific life history traits, and the variety of organisms is called biodiversity. Biodiversity, which refers to the varieties of species, genes, and ecosystems, enhances certain ecosystem services.
Ecology is not synonymous with environment, environmentalism, natural history, or environmental science. It is closely related to evolutionary biology, genetics, and ethology. An important focus for ecologists is to improve the understanding of how biodiversity affects ecological function. Ecologists seek to explain:
Ecology is a human science as well. There are many practical applications of ecology in conservation biology, wetland management, natural resource management (agroecology, agriculture, forestry, agroforestry, fisheries), city planning (urban ecology), community health, economics, basic and applied science, and human social interaction (human ecology). For example, the "Circles of Sustainability" approach treats ecology as more than the environment 'out there'. It is not treated as separate from humans. Organisms (including humans) and resources compose ecosystems which, in turn, maintain biophysical feedback mechanisms that moderate processes acting on living (biotic) and non-living (abiotic) components of the planet. Ecosystems sustain life-supporting functions and produce natural capital like biomass production (food, fuel, fiber and medicine), the regulation of climate, global biogeochemical cycles, water filtration, soil formation, erosion control, flood protection and many other natural features of scientific, historical, economic, or intrinsic value.
The word "ecology" ("Ökologie") was coined in 1866 by the German scientist Ernst Haeckel (1834–1919). Ecological thought is derivative of established currents in philosophy, particularly from ethics and politics. Ancient Greek philosophers such as Hippocrates and Aristotle laid the foundations of ecology in their studies on natural history. Modern ecology became a much more rigorous science in the late 19th century. Evolutionary concepts relating to adaptation and natural selection became the cornerstones of modern ecological theory.
Integrative levels, scope, and scale of organization.
The scope of ecology contains a wide array of interacting levels of organization spanning micro-level (e.g., cells) to a planetary scale (e.g., biosphere) phenomena. Ecosystems, for example, contain abiotic resources and interacting life forms (i.e., individual organisms that aggregate into populations which aggregate into distinct ecological communities). Ecosystems are dynamic, they do not always follow a linear successional path, but they are always changing, sometimes rapidly and sometimes so slowly that it can take thousands of years for ecological processes to bring about certain successional stages of a forest. An ecosystem's area can vary greatly, from tiny to vast. A single tree is of little consequence to the classification of a forest ecosystem, but critically relevant to organisms living in and on it. Several generations of an aphid population can exist over the lifespan of a single leaf. Each of those aphids, in turn, support diverse bacterial communities. The nature of connections in ecological communities cannot be explained by knowing the details of each species in isolation, because the emergent pattern is neither revealed nor predicted until the ecosystem is studied as an integrated whole. Some ecological principles, however, do exhibit collective properties where the sum of the components explain the properties of the whole, such as birth rates of a population being equal to the sum of individual births over a designated time frame.
Hierarchical ecology.
The scale of ecological dynamics can operate like a closed system, such as aphids migrating on a single tree, while at the same time remain open with regard to broader scale influences, such as atmosphere or climate. Hence, ecologists classify ecosystems hierarchically by analyzing data collected from finer scale units, such as vegetation associations, climate, and soil types, and integrate this information to identify emergent patterns of uniform organization and processes that operate on local to regional, landscape, and chronological scales.
To structure the study of ecology into a conceptually manageable framework, the biological world is organized into a nested hierarchy, ranging in scale from genes, to cells, to tissues, to organs, to organisms, to species, to populations, to communities, to ecosystems, to biomes, and up to the level of the biosphere. This framework forms a panarchy and exhibits non-linear behaviors; this means that "effect and cause are disproportionate, so that small changes to critical variables, such as the number of nitrogen fixers, can lead to disproportionate, perhaps irreversible, changes in the system properties."
Biodiversity.
Biodiversity (an abbreviation of "biological diversity") describes the diversity of life from genes to ecosystems and spans every level of biological organization. The term has several interpretations, and there are many ways to index, measure, characterize, and represent its complex organization. Biodiversity includes species diversity, ecosystem diversity, and genetic diversity and scientists are interested in the way that this diversity affects the complex ecological processes operating at and among these respective levels. Biodiversity plays an important role in ecosystem services which by definition maintain and improve human quality of life. Preventing species extinctions is one way to preserve biodiversity and that goal rests on techniques that preserve genetic diversity, habitat and the ability for species to migrate. Conservation priorities and management techniques require different approaches and considerations to address the full ecological scope of biodiversity. Natural capital that supports populations is critical for maintaining ecosystem services and species migration (e.g., riverine fish runs and avian insect control) has been implicated as one mechanism by which those service losses are experienced. An understanding of biodiversity has practical applications for species and ecosystem-level conservation planners as they make management recommendations to consulting firms, governments, and industry.
Habitat.
The habitat of a species describes the environment over which a species is known to occur and the type of community that is formed as a result. More specifically, "habitats can be defined as regions in environmental space that are composed of multiple dimensions, each representing a biotic or abiotic environmental variable; that is, any component or characteristic of the environment related directly (e.g. forage biomass and quality) or indirectly (e.g. elevation) to the use of a location by the animal." For example, a habitat might be an aquatic or terrestrial environment that can be further categorized as a montane or alpine ecosystem. Habitat shifts provide important evidence of competition in nature where one population changes relative to the habitats that most other individuals of the species occupy. For example, one population of a species of tropical lizards ("Tropidurus hispidus") has a flattened body relative to the main populations that live in open savanna. The population that lives in an isolated rock outcrop hides in crevasses where its flattened body offers a selective advantage. Habitat shifts also occur in the developmental life history of amphibians and in insects that transition from aquatic to terrestrial habitats. Biotope and habitat are sometimes used interchangeably, but the former applies to a community's environment, whereas the latter applies to a species' environment.
Additionally, some species are ecosystem engineers, altering the environment within a localized region. For instance, beavers manage water levels by building dams which improves their habitat in a landscape.
Niche.
Definitions of the niche date back to 1917, but G. Evelyn Hutchinson made conceptual advances in 1957 by introducing a widely adopted definition: "the set of biotic and abiotic conditions in which a species is able to persist and maintain stable population sizes." The ecological niche is a central concept in the ecology of organisms and is sub-divided into the "fundamental" and the "realized" niche. The fundamental niche is the set of environmental conditions under which a species is able to persist. The realized niche is the set of environmental plus ecological conditions under which a species persists. The Hutchinsonian niche is defined more technically as a "Euclidean hyperspace whose "dimensions" are defined as environmental variables and whose "size" is a function of the number of values that the environmental values may assume for which an organism has "positive fitness"."
Biogeographical patterns and range distributions are explained or predicted through knowledge of a species' traits and niche requirements. Species have functional traits that are uniquely adapted to the ecological niche. A trait is a measurable property, phenotype, or characteristic of an organism that may influence its survival. Genes play an important role in the interplay of development and environmental expression of traits. Resident species evolve traits that are fitted to the selection pressures of their local environment. This tends to afford them a competitive advantage and discourages similarly adapted species from having an overlapping geographic range. The competitive exclusion principle states that two species cannot coexist indefinitely by living off the same limiting resource; one will always outcompete the other. When similarly adapted species overlap geographically, closer inspection reveals subtle ecological differences in their habitat or dietary requirements. Some models and empirical studies, however, suggest that disturbances can stabilize the coevolution and shared niche occupancy of similar species inhabiting species-rich communities. The habitat plus the niche is called the ecotope, which is defined as the full range of environmental and biological variables affecting an entire species.
Niche construction.
Organisms are subject to environmental pressures, but they also modify their habitats. The regulatory feedback between organisms and their environment can affect conditions from local (e.g., a beaver pond) to global scales, over time and even after death, such as decaying logs or silica skeleton deposits from marine organisms. The process and concept of ecosystem engineering is related to niche construction, but the former relates only to the physical modifications of the habitat whereas the latter also considers the evolutionary implications of physical changes to the environment and the feedback this causes on the process of natural selection. Ecosystem engineers are defined as: "organisms that directly or indirectly modulate the availability of resources to other species, by causing physical state changes in biotic or abiotic materials. In so doing they modify, maintain and create habitats."
The ecosystem engineering concept has stimulated a new appreciation for the influence that organisms have on the ecosystem and evolutionary process. The term "niche construction" is more often used in reference to the under-appreciated feedback mechanisms of natural selection imparting forces on the abiotic niche. An example of natural selection through ecosystem engineering occurs in the nests of social insects, including ants, bees, wasps, and termites. There is an emergent homeostasis or homeorhesis in the structure of the nest that regulates, maintains and defends the physiology of the entire colony. Termite mounds, for example, maintain a constant internal temperature through the design of air-conditioning chimneys. The structure of the nests themselves are subject to the forces of natural selection. Moreover, a nest can survive over successive generations, so that progeny inherit both genetic material and a legacy niche that was constructed before their time.
Biome.
Biomes are larger units of organization that categorize regions of the Earth's ecosystems, mainly according to the structure and composition of vegetation. There are different methods to define the continental boundaries of biomes dominated by different functional types of vegetative communities that are limited in distribution by climate, precipitation, weather and other environmental variables. Biomes include tropical rainforest, temperate broadleaf and mixed forest, temperate deciduous forest, taiga, tundra, hot desert, and polar desert. Other researchers have recently categorized other biomes, such as the human and oceanic microbiomes. To a microbe, the human body is a habitat and a landscape. Microbiomes were discovered largely through advances in molecular genetics, which have revealed a hidden richness of microbial diversity on the planet. The oceanic microbiome plays a significant role in the ecological biogeochemistry of the planet's oceans.
Biosphere.
The largest scale of ecological organization is the biosphere: the total sum of ecosystems on the planet. Ecological relationships regulate the flux of energy, nutrients, and climate all the way up to the planetary scale. For example, the dynamic history of the planetary atmosphere's CO and O composition has been affected by the biogenic flux of gases coming from respiration and photosynthesis, with levels fluctuating over time in relation to the ecology and evolution of plants and animals. Ecological theory has also been used to explain self-emergent regulatory phenomena at the planetary scale: for example, the Gaia hypothesis is an example of holism applied in ecological theory. The Gaia hypothesis states that there is an emergent feedback loop generated by the metabolism of living organisms that maintains the core temperature of the Earth and atmospheric conditions within a narrow self-regulating range of tolerance.
Individual ecology.
Understanding traits of individual organisms helps explain patterns and processes at other levels of organization including populations, communities and ecosystems. Several areas of ecology of evolution that focus on such traits are life history theory, ecophysiology, metabolic theory of ecology, and Ethology. Examples of such traits include features of an organisms life cycle such as age to maturity, life span, or metabolic costs of reproduction. Other traits may be related to structure, such as the spines of a cactus or dorsal spines of a bluegill sunfish, or behaviors such as courtship displays or pair bonding. Other traits include emergent properties that are the result at least in part of interactions with the surrounding environment such as growth rate, resource uptake rate, or winter deciduous vs. drought deciduous trees and shrubs.
One set of characteristics relate to body size and temperature. The metabolic theory of ecology provides a predictive qualitative set of relationships between an organism’s body size and temperature and metabolic processes. In general, smaller, warmer organisms have higher metabolic rates and this results in a variety of predictions regarding individual somatic growth rates, reproduction and population growth rates, population size, and resource uptake rates.
The traits of organisms are subject to change through acclimation, development, and evolution. For this reason, individuals form a shared focus for ecology and for evolutionary ecology.
Population ecology.
Population ecology studies the dynamics of specie populations and how these populations interact with the wider environment. A population consists of individuals of the same species that live, interact and migrate through the same niche and habitat.
A primary law of population ecology is the Malthusian growth model which states, "a population will grow (or decline) exponentially as long as the environment experienced by all individuals in the population remains constant." Simplified population models usually start with four variables: death, birth, immigration, and emigration.
An example of an introductory population model describes a closed population, such as on an island, where immigration and emigration does not take place. Hypotheses are evaluated with reference to a null hypothesis which states that random processes create the observed data. In these island models, the rate of population change is described by:
where "N" is the total number of individuals in the population, "b" and "d" are the per capita rates of birth and death respectively, and "r" is the per capita rate of population change.
Using these modelling techniques, Malthus' population principle of growth was later transformed into a model known as the logistic equation:
where "N" is the number of individuals measured as biomass density, "a" is the maximum per-capita rate of change, and "K" is the carrying capacity of the population. The formula states that the rate of change in population size ("dN/dT") is equal to growth ("aN") that is limited by carrying capacity (1 – "N"/"K").
Population ecology builds upon these introductory models to further understand demographic processes in real study populations. Commonly used types of data include life history, fecundity, and survivorship, and these are analysed using mathematical techniques such as matrix algebra. The information is used for managing wildlife stocks and setting harvest quotas. In cases where basic models are insufficient, ecologists may adopt different kinds of statistical methods, such as the Akaike information criterion, or use models that can become mathematically complex as "several competing hypotheses are simultaneously confronted with the data."
Metapopulations and migration.
The concept of metapopulations was defined in 1969 as "a population of populations which go extinct locally and recolonize". Metapopulation ecology is another statistical approach that is often used in conservation research. Metapopulation models simplify the landscape into patches of varying levels of quality, and metapopulations are linked by the migratory behaviours of organisms. Animal migration is set apart from other kinds of movement because it involves the seasonal departure and return of individuals from a habitat. Migration is also a population-level phenomenon, as with the migration routes followed by plants as they occupied northern post-glacial environments. Plant ecologists use pollen records that accumulate and stratify in wetlands to reconstruct the timing of plant migration and dispersal relative to historic and contemporary climates. These migration routes involved an expansion of the range as plant populations expanded from one area to another. There is a larger taxonomy of movement, such as commuting, foraging, territorial behaviour, stasis, and ranging. Dispersal is usually distinguished from migration because it involves the one way permanent movement of individuals from their birth population into another population.
In metapopulation terminology, migrating individuals are classed as emigrants (when they leave a region) or immigrants (when they enter a region), and sites are classed either as sources or sinks. A site is a generic term that refers to places where ecologists sample populations, such as ponds or defined sampling areas in a forest. Source patches are productive sites that generate a seasonal supply of juveniles that migrate to other patch locations. Sink patches are unproductive sites that only receive migrants; the population at the site will disappear unless rescued by an adjacent source patch or environmental conditions become more favourable. Metapopulation models examine patch dynamics over time to answer potential questions about spatial and demographic ecology. The ecology of metapopulations is a dynamic process of extinction and colonization. Small patches of lower quality (i.e., sinks) are maintained or rescued by a seasonal influx of new immigrants. A dynamic metapopulation structure evolves from year to year, where some patches are sinks in dry years and are sources when conditions are more favourable. Ecologists use a mixture of computer models and field studies to explain metapopulation structure.
Community ecology.
Community ecology is the study of the interactions among a collections of species that inhabit the same geographic area. Community ecologists study the determinants of patterns and processes for two or more interacting species. Research in community ecology might measure species diversity in grasslands in relation to soil fertility. It might also include the analysis of predator-prey dynamics, competition among similar plant species, or mutualistic interactions between crabs and corals.
Ecosystem ecology.
Ecosystems may be habitats within biomes that form an integrated whole and a dynamically responsive system having both physical and biological complexes. Ecosystem ecology is the science of determining the fluxes of materials (e.g. carbon, phosphorus) between different pools (e.g., tree biomass, soil organic material). Ecosystem ecologist attempt to determine the underlying causes of these fluxes. Research in ecosystem ecology might measure primary production (g C/m^2) in a wetland in relation to decomposition and consumption rates (g C/m^2/y). This requires an understanding of the community connections between plants (i.e., primary producers) and the decomposers (e.g., fungi and bacteria),
The underlying concept of ecosystem can be traced back to 1864 in the published work of George Perkins Marsh ("Man and Nature"). Within an ecosystem, organisms are linked to the physical and biological components of their environment to which they are adapted. Ecosystems are complex adaptive systems where the interaction of life processes form self-organizing patterns across different scales of time and space. Ecosystems are broadly categorized as terrestrial, freshwater, atmospheric, or marine. Differences stem from the nature of the unique physical environments that shapes the biodiversity within each. A more recent addition to ecosystem ecology are technoecosystems, which are affected by or primarily the result of human activity.
Food webs.
A food web is the archetypal ecological network. Plants capture solar energy and use it to synthesize simple sugars during photosynthesis. As plants grow, they accumulate nutrients and are eaten by grazing herbivores, and the energy is transferred through a chain of organisms by consumption. The simplified linear feeding pathways that move from a basal trophic species to a top consumer is called the food chain. The larger interlocking pattern of food chains in an ecological community creates a complex food web. Food webs are a type of concept map or a heuristic device that is used to illustrate and study pathways of energy and material flows.
Food webs are often limited relative to the real world. Complete empirical measurements are generally restricted to a specific habitat, such as a cave or a pond, and principles gleaned from food web microcosm studies are extrapolated to larger systems. Feeding relations require extensive investigations into the gut contents of organisms, which can be difficult to decipher, or stable isotopes can be used to trace the flow of nutrient diets and energy through a food web. Despite these limitations, food webs remain a valuable tool in understanding community ecosystems.
Food webs exhibit principles of ecological emergence through the nature of trophic relationships: some species have many weak feeding links (e.g., omnivores) while some are more specialized with fewer stronger feeding links (e.g., primary predators). Theoretical and empirical studies identify non-random emergent patterns of few strong and many weak linkages that explain how ecological communities remain stable over time. Food webs are composed of subgroups where members in a community are linked by strong interactions, and the weak interactions occur between these subgroups. This increases food web stability. Step by step lines or relations are drawn until a web of life is illustrated.
Trophic levels.
A trophic level (from Greek "troph", τροφή, trophē, meaning "food" or "feeding") is "a group of organisms acquiring a considerable majority of its energy from the adjacent level nearer the abiotic source." Links in food webs primarily connect feeding relations or trophism among species. Biodiversity within ecosystems can be organized into trophic pyramids, in which the vertical dimension represents feeding relations that become further removed from the base of the food chain up toward top predators, and the horizontal dimension represents the abundance or biomass at each level. When the relative abundance or biomass of each species is sorted into its respective trophic level, they naturally sort into a 'pyramid of numbers'.
Species are broadly categorized as autotrophs (or primary producers), heterotrophs (or consumers), and Detritivores (or decomposers). Autotrophs are organisms that produce their own food (production is greater than respiration) by photosynthesis or chemosynthesis. Heterotrophs are organisms that must feed on others for nourishment and energy (respiration exceeds production). Heterotrophs can be further sub-divided into different functional groups, including primary consumers (strict herbivores), secondary consumers (carnivorous predators that feed exclusively on herbivores) and tertiary consumers (predators that feed on a mix of herbivores and predators). Omnivores do not fit neatly into a functional category because they eat both plant and animal tissues. It has been suggested that omnivores have a greater functional influence as predators, because compared to herbivores they are relatively inefficient at grazing.
Trophic levels are part of the holistic or complex systems view of ecosystems. Each trophic level contains unrelated species that are grouped together because they share common ecological functions, giving a macroscopic view of the system. While the notion of trophic levels provides insight into energy flow and top-down control within food webs, it is troubled by the prevalence of omnivory in real ecosystems. This has led some ecologists to "reiterate that the notion that species clearly aggregate into discrete, homogeneous trophic levels is fiction." Nonetheless, recent studies have shown that real trophic levels do exist, but "above the herbivore trophic level, food webs are better characterized as a tangled web of omnivores."
Keystone species.
A keystone species is a species that is connected to a disproportionately large number of other species in the food-web. Keystone species have lower levels of biomass in the trophic pyramid relative to the importance of their role. The many connections that a keystone species holds means that it maintains the organization and structure of entire communities. The loss of a keystone species results in a range of dramatic cascading effects that alters trophic dynamics, other food web connections, and can cause the extinction of other species.
Sea otters ("Enhydra lutris") are commonly cited as an example of a keystone species because they limit the density of sea urchins that feed on kelp. If sea otters are removed from the system, the urchins graze until the kelp beds disappear and this has a dramatic effect on community structure. Hunting of sea otters, for example, is thought to have led indirectly to the extinction of the Steller's sea cow ("Hydrodamalis gigas"). While the keystone species concept has been used extensively as a conservation tool, it has been criticized for being poorly defined from an operational stance. It is difficult to experimentally determine what species may hold a keystone role in each ecosystem. Furthermore, food web theory suggests that keystone species may not be common, so it is unclear how generally the keystone species model can be applied.
Ecological complexity.
Complexity is understood as a large computational effort needed to piece together numerous interacting parts exceeding the iterative memory capacity of the human mind. Global patterns of biological diversity are complex. This biocomplexity stems from the interplay among ecological processes that operate and influence patterns at different scales that grade into each other, such as transitional areas or ecotones spanning landscapes. Complexity stems from the interplay among levels of biological organization as energy and matter is integrated into larger units that superimpose onto the smaller parts. "What were wholes on one level become parts on a higher one." Small scale patterns do not necessarily explain large scale phenomena, otherwise captured in the expression (coined by Aristotle) 'the sum is greater than the parts'.
"Complexity in ecology is of at least six distinct types: spatial, temporal, structural, process, behavioral, and geometric." From these principles, ecologists have identified emergent and self-organizing phenomena that operate at different environmental scales of influence, ranging from molecular to planetary, and these require different explanations at each integrative level. Ecological complexity relates to the dynamic resilience of ecosystems that transition to multiple shifting steady-states directed by random fluctuations of history. Long-term ecological studies provide important track records to better understand the complexity and resilience of ecosystems over longer temporal and broader spatial scales. These studies are managed by the International Long Term Ecological Network (LTER). The longest experiment in existence is the Park Grass Experiment, which was initiated in 1856. Another example is the Hubbard Brook study, which has been in operation since 1960.
Holism.
Holism remains a critical part of the theoretical foundation in contemporary ecological studies. Holism addresses the biological organization of life that self-organizes into layers of emergent whole systems that function according to nonreducible properties. This means that higher order patterns of a whole functional system, such as an ecosystem, cannot be predicted or understood by a simple summation of the parts. "New properties emerge because the components interact, not because the basic nature of the components is changed."
Ecological studies are necessarily holistic as opposed to reductionistic. Holism has three scientific meanings or uses that identify with ecology: 1) the mechanistic complexity of ecosystems, 2) the practical description of patterns in quantitative reductionist terms where correlations may be identified but nothing is understood about the causal relations without reference to the whole system, which leads to 3) a metaphysical hierarchy whereby the causal relations of larger systems are understood without reference to the smaller parts. Scientific holism differs from mysticism that has appropriated the same term. An example of metaphysical holism is identified in the trend of increased exterior thickness in shells of different species. The reason for a thickness increase can be understood through reference to principles of natural selection via predation without need to reference or understand the biomolecular properties of the exterior shells.
Relation to evolution.
Ecology and evolution are considered sister disciplines of the life sciences. Natural selection, life history, development, adaptation, populations, and inheritance are examples of concepts that thread equally into ecological and evolutionary theory. Morphological, behavioural and genetic traits, for example, can be mapped onto evolutionary trees to study the historical development of a species in relation to their functions and roles in different ecological circumstances. In this framework, the analytical tools of ecologists and evolutionists overlap as they organize, classify and investigate life through common systematic principals, such as phylogenetics or the Linnaean system of taxonomy. The two disciplines often appear together, such as in the title of the journal "Trends in Ecology and Evolution". There is no sharp boundary separating ecology from evolution and they differ more in their areas of applied focus. Both disciplines discover and explain emergent and unique properties and processes operating across different spatial or temporal scales of organization. While the boundary between ecology and evolution is not always clear, ecologists study the abiotic and biotic factors that influence evolutionary processes, and evolution can be rapid, occurring on ecological timescales as short as one generation.
Behavioural ecology.
All organisms can exhibit behaviours. Even plants express complex behaviour, including memory and communication. Behavioural ecology is the study of an organism's behaviour in its environment and its ecological and evolutionary implications. Ethology is the study of observable movement or behaviour in animals. This could include investigations of motile sperm of plants, mobile phytoplankton, zooplankton swimming toward the female egg, the cultivation of fungi by weevils, the mating dance of a salamander, or social gatherings of amoeba.
Adaptation is the central unifying concept in behavioural ecology. Behaviours can be recorded as traits and inherited in much the same way that eye and hair colour can. Behaviours can evolve by means of natural selection as adaptive traits conferring functional utilities that increases reproductive fitness.
Predator-prey interactions are an introductory concept into food-web studies as well as behavioural ecology. Prey species can exhibit different kinds of behavioural adaptations to predators, such as avoid, flee or defend. Many prey species are faced with multiple predators that differ in the degree of danger posed. To be adapted to their environment and face predatory threats, organisms must balance their energy budgets as they invest in different aspects of their life history, such as growth, feeding, mating, socializing, or modifying their habitat. Hypotheses posited in behavioural ecology are generally based on adaptive principles of conservation, optimization or efficiency. For example, "he threat-sensitive predator avoidance hypothesis predicts that prey should assess the degree of threat posed by different predators and match their behaviour according to current levels of risk" or "he optimal flight initiation distance occurs where expected postencounter fitness is maximized, which depends on the prey's initial fitness, benefits obtainable by not fleeing, energetic escape costs, and expected fitness loss due to predation risk."
Elaborate sexual displays and posturing are encountered in the behavioural ecology of animals. The birds-of-paradise, for example, sing and display elaborate ornaments during courtship. These displays serve a dual purpose of signalling healthy or well-adapted individuals and desirable genes. The displays are driven by sexual selection as an advertisement of quality of traits among suitors.
Cognitive ecology.
Cognitive ecology integrates theory and observations from evolutionary ecology and neurobiology, primarily cognitive science, in order to understand the effect that animal interaction with their habitat has on their cognitive systems and how those systems restrict behavior within an ecological and evolutionary framework. "Until recently, however, cognitive scientists have not paid sufficient attention to the fundamental fact that cognitive traits evolved under particular natural settings. With consideration of the selection pressure on cognition, cognitive ecology can contribute intellectual coherence to the multidisciplinary study of cognition." As a study involving the 'coupling' or interactions between organism and environment, cognitive ecology is closely related to enactivism, a field based upon the view that "...we must see the organism and environment as bound together in reciprocal specification and selection...".
Social ecology.
Social ecological behaviours are notable in the social insects, slime moulds, social spiders, human society, and naked mole-rats where eusocialism has evolved. Social behaviours include reciprocally beneficial behaviours among kin and nest mates and evolve from kin and group selection. Kin selection explains altruism through genetic relationships, whereby an altruistic behaviour leading to death is rewarded by the survival of genetic copies distributed among surviving relatives. The social insects, including ants, bees and wasps are most famously studied for this type of relationship because the male drones are clones that share the same genetic make-up as every other male in the colony. In contrast, group selectionists find examples of altruism among non-genetic relatives and explain this through selection acting on the group, whereby it becomes selectively advantageous for groups if their members express altruistic behaviours to one another. Groups with predominantly altruistic members beat groups with predominantly selfish members.
Coevolution.
Ecological interactions can be classified broadly into a host and an associate relationship. A host is any entity that harbours another that is called the associate. Relationships within a species that are mutually or reciprocally beneficial are called mutualisms. Examples of mutualism include fungus-growing ants employing agricultural symbiosis, bacteria living in the guts of insects and other organisms, the fig wasp and yucca moth pollination complex, lichens with fungi and photosynthetic algae, and corals with photosynthetic algae. If there is a physical connection between host and associate, the relationship is called symbiosis. Approximately 60% of all plants, for example, have a symbiotic relationship with arbuscular mycorrhizal fungi living in their roots forming an exchange network of carbohydrates for mineral nutrients.
Indirect mutualisms occur where the organisms live apart. For example, trees living in the equatorial regions of the planet supply oxygen into the atmosphere that sustains species living in distant polar regions of the planet. This relationship is called commensalism because many others receive the benefits of clean air at no cost or harm to trees supplying the oxygen. If the associate benefits while the host suffers, the relationship is called parasitism. Although parasites impose a cost to their host (e.g., via damage to their reproductive organs or propagules, denying the services of a beneficial partner), their net effect on host fitness is not necessarily negative and, thus, becomes difficult to forecast. Coevolution is also driven by competition among species or among members of the same species under the banner of reciprocal antagonism, such as grasses competing for growth space. The Red Queen Hypothesis, for example, posits that parasites track down and specialize on the locally common genetic defence systems of its host that drives the evolution of sexual reproduction to diversify the genetic constituency of populations responding to the antagonistic pressure.
Biogeography.
Biogeography (an amalgamation of "biology" and "geography") is the comparative study of the geographic distribution of organisms and the corresponding evolution of their traits in space and time. The "Journal of Biogeography" was established in 1974. Biogeography and ecology share many of their disciplinary roots. For example, the theory of island biogeography, published by the mathematician Robert MacArthur and ecologist Edward O. Wilson in 1967 is considered one of the fundamentals of ecological theory.
Biogeography has a long history in the natural sciences concerning the spatial distribution of plants and animals. Ecology and evolution provide the explanatory context for biogeographical studies. Biogeographical patterns result from ecological processes that influence range distributions, such as migration and dispersal. and from historical processes that split populations or species into different areas. The biogeographic processes that result in the natural splitting of species explains much of the modern distribution of the Earth's biota. The splitting of lineages in a species is called vicariance biogeography and it is a sub-discipline of biogeography. There are also practical applications in the field of biogeography concerning ecological systems and processes. For example, the range and distribution of biodiversity and invasive species responding to climate change is a serious concern and active area of research in the context of global warming.
r/K-Selection theory.
A population ecology concept is r/K selection theory, one of the first predictive models in ecology used to explain life-history evolution. The premise behind the r/K selection model is that natural selection pressures change according to population density. For example, when an island is first colonized, density of individuals is low. The initial increase in population size is not limited by competition, leaving an abundance of available resources for rapid population growth. These early phases of population growth experience "density-independent" forces of natural selection, which is called "r"-selection. As the population becomes more crowded, it approaches the island's carrying capacity, thus forcing individuals to compete more heavily for fewer available resources. Under crowded conditions, the population experiences density-dependent forces of natural selection, called "K"-selection.
In the "r/K"-selection model, the first variable "r" is the intrinsic rate of natural increase in population size and the second variable "K" is the carrying capacity of a population. Different species evolve different life-history strategies spanning a continuum between these two selective forces. An "r"-selected species is one that has high birth rates, low levels of parental investment, and high rates of mortality before individuals reach maturity. Evolution favours high rates of fecundity in "r"-selected species. Many kinds of insects and invasive species exhibit "r"-selected characteristics. In contrast, a "K"-selected species has low rates of fecundity, high levels of parental investment in the young, and low rates of mortality as individuals mature. Humans and elephants are examples of species exhibiting "K"-selected characteristics, including longevity and efficiency in the conversion of more resources into fewer offspring.
Molecular ecology.
The important relationship between ecology and genetic inheritance predates modern techniques for molecular analysis. Molecular ecological research became more feasible with the development of rapid and accessible genetic technologies, such as the polymerase chain reaction (PCR). The rise of molecular technologies and influx of research questions into this new ecological field resulted in the publication "Molecular Ecology" in 1992. Molecular ecology uses various analytical techniques to study genes in an evolutionary and ecological context. In 1994, John Avise also played a leading role in this area of science with the publication of his book, "Molecular Markers, Natural History and Evolution". Newer technologies opened a wave of genetic analysis into organisms once difficult to study from an ecological or evolutionary standpoint, such as bacteria, fungi and nematodes. Molecular ecology engendered a new research paradigm for investigating ecological questions considered otherwise intractable. Molecular investigations revealed previously obscured details in the tiny intricacies of nature and improved resolution into probing questions about behavioural and biogeographical ecology. For example, molecular ecology revealed promiscuous sexual behaviour and multiple male partners in tree swallows previously thought to be socially monogamous. In a biogeographical context, the marriage between genetics, ecology and evolution resulted in a new sub-discipline called phylogeography.
Human ecology.
Ecology is as much a biological science as it is a human science. Human ecology is an interdisciplinary investigation into the ecology of our species. "Human ecology may be defined: (1) from a bio-ecological standpoint as the study of man as the ecological dominant in plant and animal communities and systems; (2) from a bio-ecological standpoint as simply another animal affecting and being affected by his physical environment; and (3) as a human being, somehow different from animal life in general, interacting with physical and modified environments in a distinctive and creative way. A truly interdisciplinary human ecology will most likely address itself to all three." The term was formally introduced in 1921, but many sociologists, geographers, psychologists, and other disciplines were interested in human relations to natural systems centuries prior, especially in the late 19th century.
The ecological complexities human beings are facing through the technological transformation of the planetary biome has brought on the Anthropocene. The unique set of circumstances has generated the need for a new unifying science called coupled human and natural systems that builds upon, but moves beyond the field of human ecology. Ecosystems tie into human societies through the critical and all encompassing life-supporting functions they sustain. In recognition of these functions and the incapability of traditional economic valuation methods to see the value in ecosystems, there has been a surge of interest in social-natural capital, which provides the means to put a value on the stock and use of information and materials stemming from ecosystem goods and services. Ecosystems produce, regulate, maintain, and supply services of critical necessity and beneficial to human health (cognitive and physiological), economies, and they even provide an information or reference function as a living library giving opportunities for science and cognitive development in children engaged in the complexity of the natural world. Ecosystems relate importantly to human ecology as they are the ultimate base foundation of global economics as every commodity and the capacity for exchange ultimately stems from the ecosystems on Earth.
Restoration and management.
Ecology is an employed science of restoration, repairing disturbed sites through human intervention, in natural resource management, and in environmental impact assessments. Edward O. Wilson predicted in 1992 that the 21st century "will be the era of restoration in ecology". Ecological science has boomed in the industrial investment of restoring ecosystems and their processes in abandoned sites after disturbance. Natural resource managers, in forestry, for example, employ ecologists to develop, adapt, and implement ecosystem based methods into the planning, operation, and restoration phases of land-use. Ecological science is used in the methods of sustainable harvesting, disease and fire outbreak management, in fisheries stock management, for integrating land-use with protected areas and communities, and conservation in complex geo-political landscapes.
Relation to the environment.
The environment of ecosystems includes both physical parameters and biotic attributes. It is dynamically interlinked, and contains resources for organisms at any time throughout their life cycle. Like "ecology," the term "environment" has different conceptual meanings and overlaps with the concept of "nature." Environment "... includes the physical world, the social world of human relations and the built world of human creation." The physical environment is external to the level of biological organization under investigation, including abiotic factors such as temperature, radiation, light, chemistry, climate and geology. The biotic environment includes genes, cells, organisms, members of the same species (conspecifics) and other species that share a habitat.
The distinction between external and internal environments, however, is an abstraction parsing life and environment into units or facts that are inseparable in reality. There is an interpenetration of cause and effect between the environment and life. The laws of thermodynamics, for example, apply to ecology by means of its physical state. With an understanding of metabolic and thermodynamic principles, a complete accounting of energy and material flow can be traced through an ecosystem. In this way, the environmental and ecological relations are studied through reference to conceptually manageable and isolated material parts. After the effective environmental components are understood through reference to their causes, however, they conceptually link back together as an integrated whole, or "holocoenotic" system as it was once called. This is known as the dialectical approach to ecology. The dialectical approach examines the parts, but integrates the organism and the environment into a dynamic whole (or umwelt). Change in one ecological or environmental factor can concurrently affect the dynamic state of an entire ecosystem.
Disturbance and resilience.
Ecosystems are regularly confronted with natural environmental variations and disturbances over time and geographic space. A disturbance is any process that removes biomass from a community, such as a fire, flood, drought, or predation. Disturbances occur over vastly different ranges in terms of magnitudes as well as distances and time periods, and are both the cause and product of natural fluctuations in death rates, species assemblages, and biomass densities within an ecological community. These disturbances create places of renewal where new directions emerge from the patchwork of natural experimentation and opportunity. Ecological resilience is a cornerstone theory in ecosystem management. Biodiversity fuels the resilience of ecosystems acting as a kind of regenerative insurance.
Metabolism and the early atmosphere.
The Earth was formed approximately 4.5 billion years ago. As it cooled and a crust and oceans formed, its atmosphere transformed from being dominated by hydrogen to one composed mostly of methane and ammonia. Over the next billion years, the metabolic activity of life transformed the atmosphere into a mixture of carbon dioxide, nitrogen, and water vapor. These gases changed the way that light from the sun hit the Earth's surface and greenhouse effects trapped heat. There were untapped sources of free energy within the mixture of reducing and oxidizing gasses that set the stage for primitive ecosystems to evolve and, in turn, the atmosphere also evolved.
Throughout history, the Earth's atmosphere and biogeochemical cycles have been in a dynamic equilibrium with planetary ecosystems. The history is characterized by periods of significant transformation followed by millions of years of stability. The evolution of the earliest organisms, likely anaerobic methanogen microbes, started the process by converting atmospheric hydrogen into methane (4H + CO → CH + 2HO). Anoxygenic photosynthesis reduced hydrogen concentrations and increased atmospheric methane, by converting hydrogen sulfide into water or other sulfur compounds (for example, 2HS + CO + h"v" → CHO + HO + 2S). Early forms of fermentation also increased levels of atmospheric methane. The transition to an oxygen-dominant atmosphere (the "Great Oxidation") did not begin until approximately 2.4–2.3 billion years ago, but photosynthetic processes started 0.3 to 1 billion years prior.
Radiation: heat, temperature and light.
The biology of life operates within a certain range of temperatures. Heat is a form of energy that regulates temperature. Heat affects growth rates, activity, behaviour and primary production. Temperature is largely dependent on the incidence of solar radiation. The latitudinal and longitudinal spatial variation of temperature greatly affects climates and consequently the distribution of biodiversity and levels of primary production in different ecosystems or biomes across the planet. Heat and temperature relate importantly to metabolic activity. Poikilotherms, for example, have a body temperature that is largely regulated and dependent on the temperature of the external environment. In contrast, homeotherms regulate their internal body temperature by expending metabolic energy.
There is a relationship between light, primary production, and ecological energy budgets. Sunlight is the primary input of energy into the planet's ecosystems. Light is composed of electromagnetic energy of different wavelengths. Radiant energy from the sun generates heat, provides photons of light measured as active energy in the chemical reactions of life, and also acts as a catalyst for genetic mutation. Plants, algae, and some bacteria absorb light and assimilate the energy through photosynthesis. Organisms capable of assimilating energy by photosynthesis or through inorganic fixation of HS are autotrophs. Autotrophs — responsible for primary production — assimilate light energy which becomes metabolically stored as potential energy in the form of biochemical enthalpic bonds.
Physical environments.
Water.
Diffusion of carbon dioxide and oxygen is approximately 10,000 times slower in water than in air. When soils are flooded, they quickly lose oxygen, becoming hypoxic (an environment with O concentration below 2 mg/liter) and eventually completely anoxic where anaerobic bacteria thrive among the roots. Water also influences the intensity and spectral composition of light as it reflects off the water surface and submerged particles. Aquatic plants exhibit a wide variety of morphological and physiological adaptations that allow them to survive, compete and diversify in these environments. For example, their roots and stems contain large air spaces (aerenchyma) that regulate the efficient transportation of gases (for example, CO and O) used in respiration and photosynthesis. Salt water plants (halophytes) have additional specialized adaptations, such as the development of special organs for shedding salt and osmoregulating their internal salt (NaCl) concentrations, to live in estuarine, brackish, or oceanic environments. Anaerobic soil microorganisms in aquatic environments use nitrate, manganese ions, ferric ions, sulfate, carbon dioxide and some organic compounds; other microorganisms are facultative anaerobes and use oxygen during respiration when the soil becomes drier. The activity of soil microorganisms and the chemistry of the water reduces the oxidation-reduction potentials of the water. Carbon dioxide, for example, is reduced to methane (CH) by methanogenic bacteria. The physiology of fish is also specially adapted to compensate for environmental salt levels through osmoregulation. Their gills form electrochemical gradients that mediate salt excretion in salt water and uptake in fresh water.
Gravity.
The shape and energy of the land is significantly affected by gravitational forces. On a large scale, the distribution of gravitational forces on the earth is uneven and influences the shape and movement of tectonic plates as well as influencing geomorphic processes such as orogeny and erosion. These forces govern many of the geophysical properties and distributions of ecological biomes across the Earth. On the organismal scale, gravitational forces provide directional cues for plant and fungal growth (gravitropism), orientation cues for animal migrations, and influence the biomechanics and size of animals. Ecological traits, such as allocation of biomass in trees during growth are subject to mechanical failure as gravitational forces influence the position and structure of branches and leaves. The cardiovascular systems of animals are functionally adapted to overcome pressure and gravitational forces that change according to the features of organisms (e.g., height, size, shape), their behaviour (e.g., diving, running, flying), and the habitat occupied (e.g., water, hot deserts, cold tundra).
Pressure.
Climatic and osmotic pressure places physiological constraints on organisms, especially those that fly and respire at high altitudes, or dive to deep ocean depths. These constraints influence vertical limits of ecosystems in the biosphere, as organisms are physiologically sensitive and adapted to atmospheric and osmotic water pressure differences. For example, oxygen levels decrease with decreasing pressure and are a limiting factor for life at higher altitudes. Water transportation by plants is another important ecophysiological process affected by osmotic pressure gradients. Water pressure in the depths of oceans requires that organisms adapt to these conditions. For example, diving animals such as whales, dolphins and seals are specially adapted to deal with changes in sound due to water pressure differences. Differences between hagfish species provide another example of adaptation to deep-sea pressure through specialized protein adaptations.
Wind and turbulence.
Turbulent forces in air and water affect the environment and ecosystem distribution, form and dynamics. On a planetary scale, ecosystems are affected by circulation patterns in the global trade winds. Wind power and the turbulent forces it creates can influence heat, nutrient, and biochemical profiles of ecosystems. For example, wind running over the surface of a lake creates turbulence, mixing the water column and influencing the environmental profile to create thermally layered zones, affecting how fish, algae, and other parts of the aquatic ecosystem are structured. Wind speed and turbulence also influence evapotranspiration rates and energy budgets in plants and animals. Wind speed, temperature and moisture content can vary as winds travel across different land features and elevations. For example, the westerlies come into contact with the coastal and interior mountains of western North America to produce a rain shadow on the leeward side of the mountain. The air expands and moisture condenses as the winds increase in elevation; this is called orographic lift and can cause precipitation. This environmental process produces spatial divisions in biodiversity, as species adapted to wetter conditions are range-restricted to the coastal mountain valleys and unable to migrate across the xeric ecosystems (e.g., of the Columbia Basin in western North America) to intermix with sister lineages that are segregated to the interior mountain systems.
Fire.
Plants convert carbon dioxide into biomass and emit oxygen into the atmosphere. By approximately 350 million years ago (the end of the Devonian period), photosynthesis had brought the concentration of atmospheric oxygen above 17%, which allowed combustion to occur. Fire releases CO and converts fuel into ash and tar. Fire is a significant ecological parameter that raises many issues pertaining to its control and suppression. While the issue of fire in relation to ecology and plants has been recognized for a long time, Charles Cooper brought attention to the issue of forest fires in relation to the ecology of forest fire suppression and management in the 1960s.
Native North Americans were among the first to influence fire regimes by controlling their spread near their homes or by lighting fires to stimulate the production of herbaceous foods and basketry materials. Fire creates a heterogeneous ecosystem age and canopy structure, and the altered soil nutrient supply and cleared canopy structure opens new ecological niches for seedling establishment. Most ecosystems are adapted to natural fire cycles. Plants, for example, are equipped with a variety of adaptations to deal with forest fires. Some species (e.g., "Pinus halepensis") cannot germinate until after their seeds have lived through a fire or been exposed to certain compounds from smoke. Environmentally triggered germination of seeds is called serotiny. Fire plays a major role in the persistence and resilience of ecosystems.
Soils.
Soil is the living top layer of mineral and organic dirt that covers the surface of the planet. It is the chief organizing centre of most ecosystem functions, and it is of critical importance in agricultural science and ecology. The decomposition of dead organic matter (for example, leaves on the forest floor), results in soils containing minerals and nutrients that feed into plant production. The whole of the planet's soil ecosystems is called the pedosphere where a large biomass of the Earth's biodiversity organizes into trophic levels. Invertebrates that feed and shred larger leaves, for example, create smaller bits for smaller organisms in the feeding chain. Collectively, these organisms are the detritivores that regulate soil formation. Tree roots, fungi, bacteria, worms, ants, beetles, centipedes, spiders, mammals, birds, reptiles, amphibians and other less familiar creatures all work to create the trophic web of life in soil ecosystems. Soils form composite phenotypes where inorganic matter is enveloped into the physiology of a whole community. As organisms feed and migrate through soils they physically displace materials, an ecological process called bioturbation. This aerates soils and stimulates heterotrophic growth and production. Soil microorganisms are influenced by and feed back into the trophic dynamics of the ecosystem. No single axis of causality can be discerned to segregate the biological from geomorphological systems in soils. Paleoecological studies of soils places the origin for bioturbation to a time before the Cambrian period. Other events, such as the evolution of trees and the colonization of land in the Devonian period played a significant role in the early development of ecological trophism in soils.
Biogeochemistry and climate.
Ecologists study and measure nutrient budgets to understand how these materials are regulated, flow, and recycled through the environment. This research has led to an understanding that there is global feedback between ecosystems and the physical parameters of this planet, including minerals, soil, pH, ions, water and atmospheric gases. Six major elements (hydrogen, carbon, nitrogen, oxygen, sulfur, and phosphorus; H, C, N, O, S, and P) form the constitution of all biological macromolecules and feed into the Earth's geochemical processes. From the smallest scale of biology, the combined effect of billions upon billions of ecological processes amplify and ultimately regulate the biogeochemical cycles of the Earth. Understanding the relations and cycles mediated between these elements and their ecological pathways has significant bearing toward understanding global biogeochemistry.
The ecology of global carbon budgets gives one example of the linkage between biodiversity and biogeochemistry. It is estimated that the Earth's oceans hold 40,000 gigatonnes (Gt) of carbon, that vegetation and soil hold 2070 Gt, and that fossil fuel emissions are 6.3 Gt carbon per year. There have been major restructurings in these global carbon budgets during the Earth's history, regulated to a large extent by the ecology of the land. For example, through the early-mid Eocene volcanic outgassing, the oxidation of methane stored in wetlands, and seafloor gases increased atmospheric CO (carbon dioxide) concentrations to levels as high as 3500 ppm.
In the Oligocene, from 25 to 32 million years ago, there was another significant restructuring of the global carbon cycle as grasses evolved a new mechanism of photosynthesis, C photosynthesis, and expanded their ranges. This new pathway evolved in response to the drop in atmospheric CO concentrations below 550 ppm. The relative abundance and distribution of biodiversity alters the dynamics between organisms and their environment such that ecosystems can be both cause and effect in relation to climate change. Human-driven modifications to the planet's ecosystems (e.g., disturbance, biodiversity loss, agriculture) contributes to rising atmospheric greenhouse gas levels. Transformation of the global carbon cycle in the next century is projected to raise planetary temperatures, lead to more extreme fluctuations in weather, alter species distributions, and increase extinction rates. The effect of global warming is already being registered in melting glaciers, melting mountain ice caps, and rising sea levels. Consequently, species distributions are changing along waterfronts and in continental areas where migration patterns and breeding grounds are tracking the prevailing shifts in climate. Large sections of permafrost are also melting to create a new mosaic of flooded areas having increased rates of soil decomposition activity that raises methane (CH) emissions. There is concern over increases in atmospheric methane in the context of the global carbon cycle, because methane is a greenhouse gas that is 23 times more effective at absorbing long-wave radiation than CO on a 100-year time scale. Hence, there is a relationship between global warming, decomposition and respiration in soils and wetlands producing significant climate feedbacks and globally altered biogeochemical cycles.
History.
Early beginnings.
Ecology has a complex origin, due in large part to its interdisciplinary nature. Ancient Greek philosophers such as Hippocrates and Aristotle were among the first to record observations on natural history. However, they viewed life in terms of essentialism, where species were conceptualized as static unchanging things while varieties were seen as aberrations of an idealized type. This contrasts against the modern understanding of ecological theory where varieties are viewed as the real phenomena of interest and having a role in the origins of adaptations by means of natural selection. Early conceptions of ecology, such as a balance and regulation in nature can be traced to Herodotus (died "c". 425 BC), who described one of the earliest accounts of mutualism in his observation of "natural dentistry". Basking Nile crocodiles, he noted, would open their mouths to give sandpipers safe access to pluck leeches out, giving nutrition to the sandpiper and oral hygiene for the crocodile. Aristotle was an early influence on the philosophical development of ecology. He and his student Theophrastus made extensive observations on plant and animal migrations, biogeography, physiology, and on their behaviour, giving an early analogue to the modern concept of an ecological niche.
Ecological concepts such as food chains, population regulation, and productivity were first developed in the 1700s, through the published works of microscopist Antoni van Leeuwenhoek (1632–1723) and botanist Richard Bradley (1688?–1732). Biogeographer Alexander von Humboldt (1769–1859) was an early pioneer in ecological thinking and was among the first to recognize ecological gradients, where species are replaced or altered in form along environmental gradients, such as a cline forming along a rise in elevation. Humboldt drew inspiration from Isaac Newton as he developed a form of "terrestrial physics." In Newtonian fashion, he brought a scientific exactitude for measurement into natural history and even alluded to concepts that are the foundation of a modern ecological law on species-to-area relationships. Natural historians, such as Humboldt, James Hutton and Jean-Baptiste Lamarck (among others) laid the foundations of the modern ecological sciences. The term "ecology" () is of a more recent origin and was first coined by the German biologist Ernst Haeckel in his book "Generelle Morphologie der Organismen" (1866). Haeckel was a zoologist, artist, writer, and later in life a professor of comparative anatomy.
Opinions differ on who was the founder of modern ecological theory. Some mark Haeckel's definition as the beginning; others say it was Eugenius Warming with the writing of Oecology of Plants: An Introduction to the Study of Plant Communities (1895), or Carl Linnaeus' principles on the economy of nature that matured in the early 18th century. Linnaeus founded an early branch of ecology that he called the economy of nature. His works influenced Charles Darwin, who adopted Linnaeus' phrase on the "economy or polity of nature" in "The Origin of Species". Linnaeus was the first to frame the balance of nature as a testable hypothesis. Haeckel, who admired Darwin's work, defined ecology in reference to the economy of nature, which has led some to question whether ecology and the economy of nature are synonymous.
From Aristotle until Darwin, the natural world was predominantly considered static and unchanging. Prior to "The Origin of Species", there was little appreciation or understanding of the dynamic and reciprocal relations between organisms, their adaptations, and the environment. An exception is the 1789 publication "Natural History of Selborne" by Gilbert White (1720–1793), considered by some to be one of the earliest texts on ecology. While Charles Darwin is mainly noted for his treatise on evolution, he was one of the founders of soil ecology, and he made note of the first ecological experiment in "The Origin of Species". Evolutionary theory changed the way that researchers approached the ecological sciences.
Since 1900.
Modern ecology is a young science that first attracted substantial scientific attention toward the end of the 19th century (around the same time that evolutionary studies were gaining scientific interest). Notable scientist Ellen Swallow Richards may have first introduced the term "oekology" (which eventually morphed into home economics) in the U.S. as early 1892.
In the early 20th century, ecology transitioned from a more descriptive form of natural history to a more analytical form of "scientific natural history". Frederic Clements published the first American ecology book in 1905, presenting the idea of plant communities as a superorganism. This publication launched a debate between ecological holism and individualism that lasted until the 1970s. Clements' superorganism concept proposed that ecosystems progress through regular and determined stages of seral development that are analogous to the developmental stages of an organism. The Clementsian paradigm was challenged by Henry Gleason, who stated that ecological communities develop from the unique and coincidental association of individual organisms. This perceptual shift placed the focus back onto the life histories of individual organisms and how this relates to the development of community associations.
The Clementsian superorganism theory was an overextended application of an idealistic form of holism. The term "holism" was coined in 1926 by Jan Christiaan Smuts, a South African general and polarizing historical figure who was inspired by Clements' superorganism concept. Around the same time, Charles Elton pioneered the concept of food chains in his classical book "Animal Ecology". Elton defined ecological relations using concepts of food chains, food cycles, and food size, and described numerical relations among different functional groups and their relative abundance. Elton's 'food cycle' was replaced by 'food web' in a subsequent ecological text. Alfred J. Lotka brought in many theoretical concepts applying thermodynamic principles to ecology.
In 1942, Raymond Lindeman wrote a landmark paper on the trophic dynamics of ecology, which was published posthumously after initially being rejected for its theoretical emphasis. Trophic dynamics became the foundation for much of the work to follow on energy and material flow through ecosystems. Robert MacArthur advanced mathematical theory, predictions and tests in ecology in the 1950s, which inspired a resurgent school of theoretical mathematical ecologists. Ecology also has developed through contributions from other nations, including Russia's Vladimir Vernadsky and his founding of the biosphere concept in the 1920s and Japan's Kinji Imanishi and his concepts of harmony in nature and habitat segregation in the 1950s. Scientific recognition of contributions to ecology from non-English-speaking cultures is hampered by language and translation barriers.
Ecology surged in popular and scientific interest during the 1960–1970s environmental movement. There are strong historical and scientific ties between ecology, environmental management, and protection. The historical emphasis and poetic naturalistic writings advocating the protection of wild places by notable ecologists in the history of conservation biology, such as Aldo Leopold and Arthur Tansley, have been seen as far removed from urban centres where, it is claimed, the concentration of pollution and environmental degradation is located. Palamar (2008) notes an overshadowing by mainstream environmentalism of pioneering women in the early 1900s who fought for urban health ecology (then called euthenics) and brought about changes in environmental legislation. Women such as Ellen Swallow Richards and Julia Lathrop, among others, were precursors to the more popularized environmental movements after the 1950s.
In 1962, marine biologist and ecologist Rachel Carson's book "Silent Spring" helped to mobilize the environmental movement by alerting the public to toxic pesticides, such as DDT, bioaccumulating in the environment. Carson used ecological science to link the release of environmental toxins to human and ecosystem health. Since then, ecologists have worked to bridge their understanding of the degradation of the planet's ecosystems with environmental politics, law, restoration, and natural resources management.

</doc>
<doc id="9631" url="https://en.wikipedia.org/wiki?curid=9631" title="Glossary of country dance terms">
Glossary of country dance terms

An alphabetic list of modern Country dance terminology;
Active Couple - for long-ways sets with more than one couple dancing, the active couple is the couple doing the more complicated movement during any given portion of the dance. For duple dances, that is every other couple, and for triple dances, every third couple is the active couple. The term is applicable to triplet dances, where typically the active couple is the only couple that is active. In the seventeenth and eighteenth centuries, only the active couple—the "1st couple"—initiated the action, other couples supporting their movements and joining in as needed, until they also took their turn as leading couples.
Arm right (or left) - couples link right (or left) arms and move forward in a circle, returning to their starting positions.
Back to back - facing another person, move forward "passing" right shoulders and "fall back" to place passing left. May also start by passing left and falling back right. Called a do si do in other dance forms (and "dos-à-dos" in France).
Balance back - a "single" backward.
Both hands - two dancers face each other and give hands right to left and left to right.
Cast - turn outward and dance up or down outside the set, as directed. The instruction "cast off" is frequently synonymous with "cast down".
Changes (starting right or left) - like the "circular hey", but dancers give hands as they pass (handing hey). The number of changes is given first (e.g. two changes, three changes, etc.). 
Chassé - slipping step to right or left as directed.
Circular hey - dancers face partners or along the line and "pass" right and left alternating a stated number of changes. Usually done without hands, the circular hey may also be done by more than two couples facing alternately and moving in opposite directions - usually to their original places. This name for the figure was invented by Cecil Sharp and does not appear in sources pre-1900.Nonetheless, some early country dances calling for heys have been interpreted in modern times using circular heys. In early dances, where the hey is called a "double hey", it works to interpret this as an oval hey, like the modern circular hey but adapted to the straight sides of a longways formation.
Clockwise - in a ring, move to one's left. In a "turn single" turn to the right.
Contrary - your contrary is not your partner. In Playford's original notation, this term meant the same thing that "Corner" (or sometimes "Opposite") means today.
Corner - in a two-couple set, the dancer diagonally opposite, i.e., the first man and the second woman, first woman and second man.
Counter-clockwise - the opposite of clockwise - in a ring, move right. In a "turn single", turn to the left.
Cross hands - face and give left to left and right to right.
Cross over or Pass - change places with another dancer moving forward and passing by the right shoulder, unless otherwise directed.
Cross and go below - cross as above and go outside below one couple, ending improper.
Double - four steps forward or back, closing the feet on the 4th step (see "Single" below).
Fall (back) - dance backwards.
Figure of 8 - a weaving figure in which dancers pass between two standing people and move around them in a figure 8 pattern. A Full Figure of 8 returns the dancer to original position; a Half Figure of 8 leaves the dancer on the opposite side of the set from original position. In doing this figure, the man lets his partner pass in front of him in some communities; others prefer the rule of "the dancer coming from the left-hand side has right of way". A double figure of 8 involves four dancers tracing a whole figure of eight around the (now unoccupied) positions of the other couple; half the dancers typically start going around the outside first.
Forward - "lead" or move in the direction you are facing.
Gypsy - two dancers move around each other in a circular path while facing each other.
Hands across - right or left hands are given to "corners", and dancers move in the direction they face.
Hands three, four etc.. - the designated number of dancers form a ring and move around in the direction indicated, usually first to the left and back to the right.
Hey - a weaving figure in which two groups of dancers move in single file and in opposite directions (see "circular hey" and "straight hey").
Honour - couples step forward and right, close, shift weight, and curtsey or bow, then repeat to their left. In the time of Playford's original manual, a woman's curtsey was similar to the modern one, but a man's honour (or reverence) kept the upper body upright and involved sliding the left leg forward while bending the right knee.
Lead - couples join inside hands and walk up or down the set.
"Mad Robin" figure - a back to back with your neighbor while maintaining eye-contact with your partner across the set. Men take one step forward and then slide to the right passing in front of their neighbour, then step backwards and slide left behind their neighbour. Conversely women take one step backwards and then slide to the left passing behind of their neighbour, then step forwards and slide right in front of their neighbour. In some versions, the dancer who is going outside the set at the moment casts out to begin that motion. The term "Mad Robin" comes from the name of a dance which has the move and was adopted into contra dancing (as a move for all four dancers, unlike the original English dance, where only one couple does it a time) before being readmitted as an all-four figure into English in modern dances.
Neighbour - the person you are standing beside, but not your partner.
Opposite - the person you are facing.
Poussette - two dancers face, give both hands and change places as a couple with two adjacent dancers. One pair moves a "double" toward one wall, the other toward the other wall. In this half-poussette, couples pass around each other diagonally. To complete the poussette, move in the opposite direction. Dancers end in their original places. In a similar movement, the Draw Poussette, the dancing pairs move on a U-shaped track with one dancer of the pair always moving forwards.
Proper - with the man on the left and the woman on the right, from the perspective of someone facing the music. Improper is the opposite.
Right & left - like the "circular hey", but dancers give hands as they pass (handing hey).
Set - a dancer steps right, closes with left foot and shifts weight to it, then steps back to the right foot (right-together-step); then repeats the process mirror-image (left-together-step). In some areas it is done starting to the left. It may be done in place or advancing. Often followed by a turn single.
Siding - two dancers, partners by default if not otherwise specified, go forward in four counts to meet side by side, then back in four counts to where they started the figure. As depicted by Feuillet, this is done right side by right side the first time, left by left the second time.
Single - two steps in any direction closing feet on the second step (the second step tends to be interpreted as a closing action in which weight usually stays on the same foot as before, consistent with descriptions from Renaissance sources).
Slipping circle (left or right) - dancers take hands in a circle (facing in) and chassé left or right.
Hands across or Star - some number of dancers (usually four) join right or left hands in the middle of their circle (facing either CW or CCW). The dancers circle in the direction they face.
Straight hey for four - dancers face alternately, the two in the middle facing out. Dancers pass right shoulders on either end and weave to the end opposite. If the last pass at the end is by the right. the dancer turns right and reenters the line by the same shoulder; vice versa if the last pass was to the left. Dancers end in their original places.
Straight hey for three - the first dancer faces the other two and "passes" right shoulders with the second dancer, left shoulder with the third - the other dancers moving and passing the indicated shoulder. On making the last pass, each dancer makes a whole turn on the end, bearing right if the last pass was by the right shoulder or left if last pass was by the left, and reenters the figure returning to place. Each dancer describes a figure of eight pattern.
Swing - a "turn" with two hands, but moving faster and making more than one revolution.
Turn - face, give "both hands", and make a complete circular, clockwise turn to place.
Turn by right or left - dancers join right (or left) hands and turn around, separate, and "fall" to places.
Turn single - dancers turn around in four steps. 'Turn single right shoulder' is a clockwise turn; 'turn single left shoulder' is a counterclockwise turn.
Up a double and back - common combination in which dancers (usually having linked hands in a line) advance a double and then retire another double.

</doc>
<doc id="9632" url="https://en.wikipedia.org/wiki?curid=9632" title="Ecosystem">
Ecosystem

An ecosystem is a community of living organisms called producers, consumers, and decomposers. These biotic and abiotic components are regarded as linked together through nutrient cycles and energy flows. The relationship between the abiotic components and the biotic components of the ecosystem is termed 'holocoenosis'. As ecosystems are defined by the network of interactions among organisms, and between organisms and their environment, they can be of any size but usually encompass specific, limited spaces (although some scientists say that the entire planet is an ecosystem, which is probably true).
Energy, water, nitrogen and soil minerals are other essential abiotic components of an ecosystem. The energy that flows through ecosystems is obtained primarily from the sun. It generally enters the system through photosynthesis, a process that also captures carbon from the atmosphere. By feeding on plants and on one another, animals play an important role in the movement of matter and energy through the system. They also influence the quantity of plant and microbial biomass present. By breaking down dead organic matter, decomposers release carbon back to the atmosphere and facilitate nutrient cycling by converting nutrients stored in dead biomass back to a form that can be readily used by plants and other microbes.
Ecosystems are controlled both by external and internal factors. External factors such as climate, the parent material that forms the soil, and topography control the overall structure of an ecosystem and the way things work within it, but are not themselves influenced by the ecosystem. Other external factors include time and potential biota. Ecosystems are dynamic entities - invariably, they are subject to periodic disturbances and are in the process of recovering from some past disturbance. Ecosystems in similar environments that are located in different parts of the world can have very different characteristics simply because they contain different species. The introduction of non-native species can cause substantial shifts in ecosystem function. Internal factors not only control ecosystem processes but are also controlled by them and are often subject to feedback loops. While the resource inputs are generally controlled by external processes like climate and parent material, the availability of these resources within the ecosystem is controlled by internal factors like decomposition, root competition or shading. Other internal factors include disturbance, succession and the types of species present. Although humans exist and operate within ecosystems, their cumulative effects are large enough to influence external factors like climate.
Biodiversity affects ecosystem function, as do the processes of disturbance and succession. Ecosystems provide a variety of goods and services upon which people depend; the principles of ecosystem management suggest that rather than managing individual species, natural resources should be managed at the level of the ecosystem itself. Classifying ecosystems into ecologically homogeneous units is an important step towards effective ecosystem management, but there is no single, agreed-upon way to do this.
History and development.
The term "ecosystem" was first used in a publication by British ecologist Arthur Tansley. Tansley devised the concept to draw attention to the importance of transfers of materials between organisms and their environment. He later refined the term, describing it as "The whole system, ... including not only the organism-complex, but also the whole complex of physical factors forming what we call the environment". Tansley regarded ecosystems not simply as natural units, but as mental isolates. Tansley later defined the spatial extent of ecosystems using the term ecotope.
G. Evelyn Hutchinson, a pioneering limnologist who was a contemporary of Tansley's, combined Charles Elton's ideas about trophic ecology with those of Russian geochemist Vladimir Vernadsky to suggest that mineral nutrient availability in a lake limited algal production which would, in turn, limit the abundance of animals that feed on algae. Raymond Lindeman took these ideas one step further to suggest that the flow of energy through a lake was the primary driver of the ecosystem. Hutchinson's students, brothers Howard T. Odum and Eugene P. Odum, further developed a "systems approach" to the study of ecosystems, allowing them to study the flow of energy and material through ecological systems.
Ecosystem processes.
Energy and carbon enter ecosystems through photosynthesis, are incorporated into living tissue, transferred to other organisms that feed on the living and dead plant matter, and eventually released through respiration. Most mineral nutrients, on the other hand, are recycled within ecosystems.
Ecosystems are controlled both by external and internal factors. External factors, also called state factors, control the overall structure of an ecosystem and the way things work within it, but are not themselves influenced by the ecosystem. The most important of these is climate. Climate determines the biome in which the ecosystem is embedded. Rainfall patterns and temperature seasonality determine the amount of water available to the ecosystem and the supply of energy available (by influencing photosynthesis). Parent material, the underlying geological material that gives rise to soils, determines the nature of the soils present, and influences the supply of mineral nutrients. Topography also controls ecosystem processes by affecting things like microclimate, soil development and the movement of water through a system. This may be the difference between the ecosystem present in wetland situated in a small depression on the landscape, and one present on an adjacent steep hillside.
Other external factors that play an important role in ecosystem functioning include time and potential biota. Ecosystems are dynamic entities—invariably, they are subject to periodic disturbances and are in the process of recovering from some past disturbance. Time plays a role in the development of soil from bare rock and the recovery of a community from disturbance. Similarly, the set of organisms that can potentially be present in an area can also have a major impact on ecosystems. Ecosystems in similar environments that are located in different parts of the world can end up doing things very differently simply because they have different pools of species present. The introduction of non-native species can cause substantial shifts in ecosystem function.
Unlike external factors, internal factors in ecosystems not only control ecosystem processes, but are also controlled by them. Consequently, they are often subject to feedback loops. While the resource inputs are generally controlled by external processes like climate and parent material, the availability of these resources within the ecosystem is controlled by internal factors like decomposition, root competition or shading. Other factors like disturbance, succession or the types of species present are also internal factors. Human activities are important in almost all ecosystems. Although humans exist and operate within ecosystems, their cumulative effects are large enough to influence external factors like climate.
Primary production.
Primary production is the production of organic matter from inorganic carbon sources. Overwhelmingly, this occurs through photosynthesis. The energy incorporated through this process supports life on earth, while the carbon makes up much of the organic matter in living and dead biomass, soil carbon and fossil fuels. It also drives the carbon cycle, which influences global climate via the greenhouse effect.
Through the process of photosynthesis, plants capture energy from light and use it to combine carbon dioxide and water to produce carbohydrates and oxygen. The photosynthesis carried out by all the plants in an ecosystem is called the gross primary production (GPP). About 48–60% of the GPP is consumed in plant respiration. The remainder, that portion of GPP that is not used up by respiration, is known as the net primary production (NPP). Total photosynthesis is limited by a range of environmental factors. These include the amount of light available, the amount of leaf area a plant has to capture light (shading by other plants is a major limitation of photosynthesis), rate at which carbon dioxide can be supplied to the chloroplasts to support photosynthesis, the availability of water, and the availability of suitable temperatures for carrying out photosynthesis.
Energy flow.
The carbon and energy incorporated into plant tissues (net primary production) is either consumed by animals while the plant is alive, or it remains uneaten when the plant tissue dies and becomes detritus. In terrestrial ecosystems, roughly 90% of the NPP ends up being broken down by decomposers. The remainder is either consumed by animals while still alive and enters the plant-based trophic system, or it is consumed after it has died, and enters the detritus-based trophic system. In aquatic systems, the proportion of plant biomass that gets consumed by herbivores is much higher.
In trophic systems photosynthetic organisms are the primary producers. The organisms that consume their tissues are called primary consumers or secondary producers—herbivores. Organisms which feed on microbes (bacteria and fungi) are termed microbivores. Animals that feed on primary consumers—carnivores—are secondary consumers. Each of these constitutes a trophic level. The sequence of consumption—from plant to herbivore, to carnivore—forms a food chain. Real systems are much more complex than this—organisms will generally feed on more than one form of food, and may feed at more than one trophic level. Carnivores may capture some prey which are part of a plant-based trophic system and others that are part of a detritus-based trophic system (a bird that feeds both on herbivorous grasshoppers and earthworms, which consume detritus). Real systems, with all these complexities, form food webs rather than food chains.
Decomposition.
The carbon and nutrients in dead organic matter are broken down by a group of processes known as decomposition. This releases nutrients that can then be re-used for plant and microbial production, and returns carbon dioxide to the atmosphere (or water) where it can be used for photosynthesis. In the absence of decomposition, dead organic matter would accumulate in an ecosystem and nutrients and atmospheric carbon dioxide would be depleted. Approximately 90% of terrestrial NPP goes directly from plant to decomposer.
Decomposition processes can be separated into three categories—leaching, fragmentation and chemical alteration of dead material. As water moves through dead organic matter, it dissolves and carries with it the water-soluble components. These are then taken up by organisms in the soil, react with mineral soil, or are transported beyond the confines of the ecosystem (and are considered "lost" to it). Newly shed leaves and newly dead animals have high concentrations of water-soluble components, and include sugars, amino acids and mineral nutrients. Leaching is more important in wet environments, and much less important in dry ones.
Fragmentation processes break organic material into smaller pieces, exposing new surfaces for colonization by microbes. Freshly shed leaf litter may be inaccessible due to an outer layer of cuticle or bark, and cell contents are protected by a cell wall. Newly dead animals may be covered by an exoskeleton. Fragmentation processes, which break through these protective layers, accelerate the rate of microbial decomposition. Animals fragment detritus as they hunt for food, as does passage through the gut. Freeze-thaw cycles and cycles of wetting and drying also fragment dead material.
The chemical alteration of dead organic matter is primarily achieved through bacterial and fungal action. Fungal hyphae produce enzymes which can break through the tough outer structures surrounding dead plant material. They also produce enzymes which break down lignin, which allows to them access to both cell contents and to the nitrogen in the lignin. Fungi can transfer carbon and nitrogen through their hyphal networks and thus, unlike bacteria, are not dependent solely on locally available resources.
Decomposition rates vary among ecosystems. The rate of decomposition is governed by three sets of factors—the physical environment (temperature, moisture and soil properties), the quantity and quality of the dead material available to decomposers, and the nature of the microbial community itself. Temperature controls the rate of microbial respiration; the higher the temperature, the faster microbial decomposition occurs. It also affects soil moisture, which slows microbial growth and reduces leaching. Freeze-thaw cycles also affect decomposition—freezing temperatures kill soil microorganisms, which allows leaching to play a more important role in moving nutrients around. This can be especially important as the soil thaws in the Spring, creating a pulse of nutrients which become available.
Decomposition rates are low under very wet or very dry conditions. Decomposition rates are highest in wet, moist conditions with adequate levels of oxygen. Wet soils tend to become deficient in oxygen (this is especially true in wetlands), which slows microbial growth. In dry soils, decomposition slows as well, but bacteria continue to grow (albeit at a slower rate) even after soils become too dry to support plant growth. When the rains return and soils become wet, the osmotic gradient between the bacterial cells and the soil water causes the cells to gain water quickly. Under these conditions, many bacterial cells burst, releasing a pulse of nutrients. Decomposition rates also tend to be slower in acidic soils. Soils which are rich in clay minerals tend to have lower decomposition rates, and thus, higher levels of organic matter. The smaller particles of clay result in a larger surface area that can hold water. The higher the water content of a soil, the lower the oxygen content and consequently, the lower the rate of decomposition. Clay minerals also bind particles of organic material to their surface, making them less accessibly to microbes. Soil disturbance like tilling increase decomposition by increasing the amount of oxygen in the soil and by exposing new organic matter to soil microbes.
The quality and quantity of the material available to decomposers is another major factor that influences the rate of decomposition. Substances like sugars and amino acids decompose readily and are considered "labile". Cellulose and hemicellulose, which are broken down more slowly, are "moderately labile". Compounds which are more resistant to decay, like lignin or cutin, are considered "recalcitrant". Litter with a higher proportion of labile compounds decomposes much more rapidly than does litter with a higher proportion of recalcitrant material. Consequently, dead animals decompose more rapidly than dead leaves, which themselves decompose more rapidly than fallen branches. As organic material in the soil ages, its quality decreases. The more labile compounds decompose quickly, leaving an increasing proportion of recalcitrant material. Microbial cell walls also contain recalcitrant materials like chitin, and these also accumulate as the microbes die, further reducing the quality of older soil organic matter.
Nutrient cycling.
Ecosystems continually exchange energy and carbon with the wider environment; mineral nutrients, on the other hand, are mostly cycled back and forth between plants, animals, microbes and the soil. Most nitrogen enters ecosystems through biological nitrogen fixation, is deposited through precipitation, dust, gases or is applied as fertilizer. Since most terrestrial ecosystems are nitrogen-limited, nitrogen cycling is an important control on ecosystem production.
Until modern times, nitrogen fixation was the major source of nitrogen for ecosystems. Nitrogen fixing bacteria either live symbiotically with plants, or live freely in the soil. The energetic cost is high for plants which support nitrogen-fixing symbionts—as much as 25% of GPP when measured in controlled conditions. Many members of the legume plant family support nitrogen-fixing symbionts. Some cyanobacteria are also capable of nitrogen fixation. These are phototrophs, which carry out photosynthesis. Like other nitrogen-fixing bacteria, they can either be free-living or have symbiotic relationships with plants. Other sources of nitrogen include acid deposition produced through the combustion of fossil fuels, ammonia gas which evaporates from agricultural fields which have had fertilizers applied to them, and dust. Anthropogenic nitrogen inputs account for about 80% of all nitrogen fluxes in ecosystems.
When plant tissues are shed or are eaten, the nitrogen in those tissues becomes available to animals and microbes. Microbial decomposition releases nitrogen compounds from dead organic matter in the soil, where plants, fungi and bacteria compete for it. Some soil bacteria use organic nitrogen-containing compounds as a source of carbon, and release ammonium ions into the soil. This process is known as nitrogen mineralization. Others convert ammonium to nitrite and nitrate ions, a process known as nitrification. Nitric oxide and nitrous oxide are also produced during nitrification. Under nitrogen-rich and oxygen-poor conditions, nitrates and nitrites are converted to nitrogen gas, a process known as denitrification.
Other important nutrients include phosphorus, sulfur, calcium, potassium, magnesium and manganese. Phosphorus enters ecosystems through weathering. As ecosystems age this supply diminishes, making phosphorus-limitation more common in older landscapes (especially in the tropics). Calcium and sulfur are also produced by weathering, but acid deposition is an important source of sulfur in many ecosystems. Although magnesium and manganese are produced by weathering, exchanges between soil organic matter and living cells account for a significant portion of ecosystem fluxes. Potassium is primarily cycled between living cells and soil organic matter.
Function and biodiversity.
Ecosystem processes are broad generalizations that actually take place through the actions of individual organisms. The nature of the organisms—the species, functional groups and trophic levels to which they belong—dictates the sorts of actions these individuals are capable of carrying out, and the relative efficiency with which they do so. Thus, ecosystem processes are driven by the number of species in an ecosystem, the exact nature of each individual species, and the relative abundance organisms within these species. Biodiversity plays an important role in ecosystem functioning.
Ecological theory suggests that in order to coexist, species must have some level of limiting similarity—they must be different from one another in some fundamental way, otherwise one species would competitively exclude the other. Despite this, the cumulative effect of additional species in an ecosystem is not linear—additional species may enhance nitrogen retention, for example, but beyond some level of species richness, additional species may have little additive effect. The addition (or loss) of species which are ecologically similar to those already present in an ecosystem tends to only have a small effect on ecosystem function. Ecologically distinct species, on the other hand, have a much larger effect. Similarly, dominant species have a large impact on ecosystem function, while rare species tend to have a small effect. Keystone species tend to have an effect on ecosystem function that is disproportionate to their abundance in an ecosystem.
Ecosystem goods and services.
Ecosystems provide a variety of goods and services upon which people depend. Ecosystem goods include the "tangible, material products" of ecosystem processes—food, construction material, medicinal plants—in addition to less tangible items like tourism and recreation, and genes from wild plants and animals that can be used to improve domestic species. Ecosystem services, on the other hand, are generally "improvements in the condition or location of things of value". These include things like the maintenance of hydrological cycles, cleaning air and water, the maintenance of oxygen in the atmosphere, crop pollination and even things like beauty, inspiration and opportunities for research. While ecosystem goods have traditionally been recognized as being the basis for things of economic value, ecosystem services tend to be taken for granted. While Gretchen Daily's original definition distinguished between ecosystem goods and ecosystem services, Robert Costanza and colleagues' later work and that of the Millennium Ecosystem Assessment lumped all of these together as ecosystem services.
Ecosystem management.
When natural resource management is applied to whole ecosystems, rather than single species, it is termed ecosystem management. A variety of definitions exist: F. Stuart Chapin and coauthors define it as "the application of ecological science to resource management to promote long-term sustainability of ecosystems and the delivery of essential ecosystem goods and services", while Norman Christensen and coauthors defined it as "management driven by explicit goals, executed by policies, protocols, and practices, and made adaptable by monitoring and research based on our best understanding of the ecological interactions and processes necessary to sustain ecosystem structure and function" and Peter Brussard and colleagues defined it as "managing areas at various scales in such a way that ecosystem services and biological resources are preserved while appropriate human use and options for livelihood are sustained".
Although definitions of ecosystem management abound, there is a common set of principles which underlie these definitions. A fundamental principle is the long-term sustainability of the production of goods and services by the ecosystem; "intergenerational sustainability a precondition for management, not an afterthought". It also requires clear goals with respect to future trajectories and behaviors of the system being managed. Other important requirements include a sound ecological understanding of the system, including connectedness, ecological dynamics and the context in which the system is embedded. Other important principles include an understanding of the role of humans as components of the ecosystems and the use of adaptive management. While ecosystem management can be used as part of a plan for wilderness conservation, it can also be used in intensively managed ecosystems (see, for example, agroecosystem and close to nature forestry).
Ecosystem dynamics.
Ecosystems are dynamic entities—invariably, they are subject to periodic disturbances and are in the process of recovering from some past disturbance. When an ecosystem is subject to some sort of perturbation, it responds by moving away from its initial state. The tendency of a system to remain close to its equilibrium state, despite that disturbance, is termed its resistance. On the other hand, the speed with which it returns to its initial state after disturbance is called its resilience.
From one year to another, ecosystems experience variation in their biotic and abiotic environments. A drought, an especially cold winter and a pest outbreak all constitute short-term variability in environmental conditions. Animal populations vary from year to year, building up during resource-rich periods and crashing as they overshoot their food supply. These changes play out in changes in NPP, decomposition rates, and other ecosystem processes. Longer-term changes also shape ecosystem processes—the forests of eastern North America still show legacies of cultivation which ceased 200 years ago, while methane production in eastern Siberian lakes is controlled by organic matter which accumulated during the Pleistocene.
Disturbance also plays an important role in ecological processes. F. Stuart Chapin and coauthors define disturbance as "a relatively discrete event in time and space that alters the structure of populations, communities and ecosystems and causes changes in resources availability or the physical environment". This can range from tree falls and insect outbreaks to hurricanes and wildfires to volcanic eruptions and can cause large changes in plant, animal and microbe populations, as well soil organic matter content. Disturbance is followed by succession, a "directional change in ecosystem structure and functioning resulting from biotically driven changes in resources supply."
The frequency and severity of disturbance determines the way it impacts ecosystem function. Major disturbance like a volcanic eruption or glacial advance and retreat leave behind soils that lack plants, animals or organic matter. Ecosystems that experience disturbances that undergo primary succession. Less severe disturbance like forest fires, hurricanes or cultivation result in secondary succession. More severe disturbance and more frequent disturbance result in longer recovery times. Ecosystems recover more quickly from less severe disturbance events.
The early stages of primary succession are dominated by species with small propagules (seed and spores) which can be dispersed long distances. The early colonizers—often algae, cyanobacteria and lichens—stabilize the substrate. Nitrogen supplies are limited in new soils, and nitrogen-fixing species tend to play an important role early in primary succession. Unlike in primary succession, the species that dominate secondary succession, are usually present from the start of the process, often in the soil seed bank. In some systems the successional pathways are fairly consistent, and thus, are easy to predict. In others, there are many possible pathways—for example, the introduced nitrogen-fixing legume, "Myrica faya", alter successional trajectories in Hawaiian forests.
The theoretical ecologist Robert Ulanowicz has used information theory tools to describe the structure of ecosystems, emphasizing mutual information (correlations) in studied systems. Drawing on this methodology and prior observations of complex ecosystems, Ulanowicz depicts approaches to determining the stress levels on ecosystems and predicting system reactions to defined types of alteration in their settings (such as increased or reduced energy flow, and eutrophication.
Ecosystem ecology.
Ecosystem ecology studies "the flow of energy and materials through organisms and the physical environment". It seeks to understand the processes which govern the stocks of material and energy in ecosystems, and the flow of matter and energy through them. The study of ecosystems can cover 10 orders of magnitude, from the surface layers of rocks to the surface of the planet.
There is no single definition of what constitutes an ecosystem. German ecologist Ernst-Detlef Schulze and coauthors defined an ecosystem as an area which is "uniform regarding the biological turnover, and contains all the fluxes above and below the ground area under consideration." They explicitly reject Gene Likens' use of entire river catchments as "too wide a demarcation" to be a single ecosystem, given the level of heterogeneity within such an area. Other authors have suggested that an ecosystem can encompass a much larger area, even the whole planet. Schulze and coauthors also rejected the idea that a single rotting log could be studied as an ecosystem because the size of the flows between the log and its surroundings are too large, relative to the proportion cycles within the log. Philosopher of science Mark Sagoff considers the failure to define "the kind of object it studies" to be an obstacle to the development of theory in ecosystem ecology.
Ecosystems can be studied through a variety of approaches—theoretical studies, studies monitoring specific ecosystems over long periods of time, those that look at differences between ecosystems to elucidate how they work and direct manipulative experimentation. Studies can be carried out at a variety of scales, from and mesocosms which serve as simplified representations of ecosystems, through whole-ecosystem studies. American ecologist Stephen R. Carpenter has argued that microcosm experiments can be "irrelevant and diversionary" if they are not carried out in conjunction with field studies carried out at the ecosystem scale, because microcosm experiments often fail to accurately predict ecosystem-level dynamics.
The Hubbard Brook Ecosystem Study, established in the White Mountains, New Hampshire in 1963, was the first successful attempt to study an entire watershed as an ecosystem. The study used stream chemistry as a means of monitoring ecosystem properties, and developed a detailed biogeochemical model of the ecosystem. Long-term research at the site led to the discovery of acid rain in North America in 1972, and was able to document the consequent depletion of soil cations (especially calcium) over the next several decades.
Classification.
Classifying ecosystems into ecologically homogeneous units is an important step towards effective ecosystem management. A variety of systems exist, based on vegetation cover, remote sensing, and bioclimatic classification systems. American geographer Robert Bailey defines a hierarchy of ecosystem units ranging from microecosystems (individual homogeneous sites, on the order of in area), through mesoecosystems (landscape mosaics, on the order of ) to macroecosystems (ecoregions, on the order of ).
Bailey outlined five different methods for identifying ecosystems: "gestalt" ("a whole that is not derived through considerable of its parts"), in which regions are recognized and boundaries drawn intuitively; a map overlay system where different layers like geology, landforms and soil types are overlain to identify ecosystems; multivariate clustering of site attributes; digital image processing of remotely sensed data grouping areas based on their appearance or other spectral properties; or by a "controlling factors method" where a subset of factors (like soils, climate, vegetation physiognomy or the distribution of plant or animal species) are selected from a large array of possible ones are used to delineate ecosystems. In contrast with Bailey's methodology, Puerto Rico ecologist Ariel Lugo and coauthors identified ten characteristics of an effective classification system: that it be based on georeferenced, quantitative data; that it should minimize subjectivity and explicitly identify criteria and assumptions; that it should be structured around the factors that drive ecosystem processes; that it should reflect the hierarchical nature of ecosystems; that it should be flexible enough to conform to the various scales at which ecosystem management operates; that it should be tied to reliable measures of climate so that it can "anticipat global climate change; that it be applicable worldwide; that it should be validated against independent data; that it take into account the sometimes complex relationship between climate, vegetation and ecosystem functioning; and that it should be able to adapt and improve as new data become available".
Anthropogenic threats.
As human populations and per capita consumption grow, so do the resource demands imposed on ecosystems and the impacts of the human ecological footprint. Natural resources are not invulnerable and infinitely available. The environmental impacts of anthropogenic actions, which are processes or materials derived from human activities, are becoming more apparent—air and water quality are increasingly compromised, oceans are being overfished, pests and diseases are extending beyond their historical boundaries, and deforestation is exacerbating flooding downstream. It has been reported that approximately 40–50% of Earth's ice-free land surface has been heavily transformed or degraded by anthropogenic activities, 66% of marine fisheries are either overexploited or at their limit, atmospheric CO has increased more than 30% since the advent of industrialization, and nearly 25% of Earth's bird species have gone extinct in the last two thousand years. Society is increasingly becoming aware that ecosystem services are not only limited, but also that they are threatened by human activities. The need to better consider long-term ecosystem health and its role in enabling human habitation and economic activity is urgent. To help inform decision-makers, many ecosystem services are being assigned economic values, often based on the cost of replacement with anthropogenic alternatives. The ongoing challenge of prescribing economic value to nature, for example through biodiversity banking, is prompting transdisciplinary shifts in how we recognize and manage the environment, social responsibility, business opportunities, and our future as a species.

</doc>
<doc id="9633" url="https://en.wikipedia.org/wiki?curid=9633" title="E (mathematical constant)">
E (mathematical constant)

The number is an important mathematical constant that is the base of the natural logarithm. It is approximately equal to 2.71828, and is the limit of as approaches infinity, an expression that arises in the study of compound interest. It can also be calculated as the sum of the infinite series
The constant can be defined in many ways. For example, can be defined as the unique positive number such that the graph of the function has unit slope at . The function is called the exponential function, and its inverse is the natural logarithm, or logarithm to base . The natural logarithm of a positive number can also be defined directly as the area under the curve between and , in which case is the number whose natural logarithm is 1. There are alternative characterizations.
Sometimes called Euler's number after the Swiss mathematician Leonhard Euler, is not to be confused with , the Euler–Mascheroni constant, sometimes called simply "Euler's constant". The number is also known as Napier's constant, but Euler's choice of the symbol is said to have been retained in his honor. The constant was discovered by the Swiss mathematician Jacob Bernoulli while studying compound interest.
The number is of eminent importance in mathematics, alongside 0, 1, and . All five of these numbers play important and recurring roles across mathematics, and are the five constants appearing in one formulation of Euler's identity. Like the constant , is irrational: it is not a ratio of integers. Also like , is transcendental: it is not a root of "any" non-zero polynomial with rational coefficients. The numerical value of truncated to 50 decimal places is
History.
The first references to the constant were published in 1618 in the table of an appendix of a work on logarithms by John Napier. However, this did not contain the constant itself, but simply a list of logarithms calculated from the constant. It is assumed that the table was written by William Oughtred. The discovery of the constant itself is credited to Jacob Bernoulli, who attempted to find the value of the following expression (which is in fact ):
The first known use of the constant, represented by the letter , was in correspondence from Gottfried Leibniz to Christiaan Huygens in 1690 and 1691. Leonhard Euler introduced the letter as the base for natural logarithms, writing in a letter to Christian Goldbach of 25 November 1731. Euler started to use the letter for the constant in 1727 or 1728, in an unpublished paper on explosive forces in cannons, and the first appearance of in a publication was Euler's "Mechanica" (1736). While in the subsequent years some researchers used the letter , was more common and eventually became the standard.
Applications.
Compound interest.
Jacob Bernoulli discovered this constant in 1683 by studying a question about compound interest:
If the interest is credited twice in the year, the interest rate for each 6 months will be 50%, so the initial $1 is multiplied by 1.5 twice, yielding $1.00×1.5 = $2.25 at the end of the year. Compounding quarterly yields $1.00×1.25 = $2.4414..., and compounding monthly yields $1.00×(1+1/12) = $2.613035... If there are compounding intervals, the interest for each interval will be and the value at the end of the year will be $1.00×.
Bernoulli noticed that this sequence approaches a limit (the force of interest) with larger and, thus, smaller compounding intervals. Compounding weekly () yields $2.692597..., while compounding daily () yields $2.714567..., just two cents more. The limit as grows large is the number that came to be known as ; with "continuous" compounding, the account value will reach $2.7182818... More generally, an account that starts at $1 and offers an annual interest rate of will, after years, yield dollars with continuous compounding. (Here is the decimal equivalent of the rate of interest expressed as a percent, so for 5% interest, )
Bernoulli trials.
The number itself also has applications to probability theory, where it arises in a way not obviously related to exponential growth. Suppose that a gambler plays a slot machine that pays out with a probability of one in and plays it times. Then, for large (such as a million) the probability that the gambler will lose every bet is (approximately) . For it is already approximately 1/2.79.
This is an example of a Bernoulli trials process. Each time the gambler plays the slots, there is a one in one million chance of winning. Playing one million times is modelled by the binomial distribution, which is closely related to the binomial theorem. The probability of winning times out of a million trials is;
In particular, the probability of winning zero times () is
This is very close to the following limit for :
Derangements.
Another application of , also discovered in part by Jacob Bernoulli along with Pierre Raymond de Montmort is in the problem of derangements, also known as the "hat check problem": guests are invited to a party, and at the door each guest checks his hat with the butler who then places them into boxes, each labelled with the name of one guest. But the butler does not know the identities of the guests, and so he puts the hats into boxes selected at random. The problem of de Montmort is to find the probability that "none" of the hats gets put into the right box. The answer is:
As the number of guests tends to infinity, approaches . Furthermore, the number of ways the hats can be placed into the boxes so that none of the hats are in the right box is rounded to the nearest integer, for every positive .
Asymptotics.
The number occurs naturally in connection with many problems involving asymptotics. A prominent example is Stirling's formula for the asymptotics of the factorial function, in which both the numbers and enter:
A particular consequence of this is
Standard normal distribution.
The simplest case of a normal distribution is known as the "standard normal distribution", described by this probability density function:
The factor formula_10 in this expression ensures that the total area under the curve "ϕ"("x") is equal to one. The in the exponent ensures that the distribution has unit variance (and therefore also unit standard deviation). This function is symmetric around , where it attains its maximum value formula_10; and has inflection points at +1 and −1.
in calculus.
The principal motivation for introducing the number , particularly in calculus, is to perform differential and integral calculus with exponential functions and logarithms. A general exponential function has derivative given as the limit:
The limit on the far right is independent of the variable : it depends only on the base . When the base is , this limit is equal to 1, and so is symbolically defined by the equation:
Consequently, the exponential function with base is particularly suited to doing calculus. Choosing , as opposed to some other number, as the base of the exponential function makes calculations involving the derivative much simpler.
Another motivation comes from considering the base- logarithm. Considering the definition of the derivative of as the limit:
where the substitution was made in the last step. The last limit appearing in this calculation is again an undetermined limit that depends only on the base , and if that base is , the limit is equal to 1. So symbolically,
The logarithm in this special base is called the natural logarithm and is represented as ; it behaves well under differentiation since there is no undetermined limit to carry through the calculations.
There are thus two ways in which to select a special number . One way is to set the derivative of the exponential function to , and solve for . The other way is to set the derivative of the base logarithm to and solve for . In each case, one arrives at a convenient choice of base for doing calculus. In fact, these two solutions for are actually "the same", the number .
Alternative characterizations.
Other characterizations of are also possible: one is as the limit of a sequence, another is as the sum of an infinite series, and still others rely on integral calculus. So far, the following two (equivalent) properties have been introduced:
The following three characterizations can be proven equivalent:
Properties.
Calculus.
As in the motivation, the exponential function is important in part because it is the unique nontrivial function (up to multiplication by a constant) which is its own derivative
and therefore its own antiderivative as well:
Inequalities.
The number is the unique real number such that
for all positive "x".
Also, we have the inequality
for all real "x", with equality if and only if . Furthermore, is the unique base of the exponential for which the inquality holds for all "x".
Exponential-like functions.
Steiner's problem asks to find the global maximum for the function
This maximum occurs precisely at . For proof, the inequality formula_27, from above, evaluated at formula_28 and simplifying gives formula_29. So formula_30 for all positive "x".
Similarly, is where the global minimum occurs for the function
defined for positive . More generally, for the function
the global maximum for positive occurs at for any ; and the global minimum occurs at for any .
The infinite tetration
converges if and only if (or approximately between 0.0660 and 1.4447), due to a theorem of Leonhard Euler.
Number theory.
The real number is irrational. Euler proved this by showing that its simple continued fraction expansion is infinite. (See also Fourier's proof that is irrational.)
Furthermore, by the Lindemann–Weierstrass theorem, is transcendental, meaning that it is not a solution of any non-constant polynomial equation with rational coefficients. It was the first number to be proved transcendental without having been specifically constructed for this purpose (compare with Liouville number); the proof was given by Charles Hermite in 1873.
It is conjectured that is normal, meaning that when is expressed in any base the possible digits in that base are uniformly distributed (occur with equal probability in any sequence of given length).
Complex numbers.
The exponential function may be written as a Taylor series
Because this series keeps many important properties for even when is complex, it is commonly used to extend the definition of to the complex numbers. This, with the Taylor series for sin and cos, allows one to derive Euler's formula:
which holds for all . The special case with is Euler's identity:
from which it follows that, in the principal branch of the logarithm,
Furthermore, using the laws for exponentiation,
which is de Moivre's formula.
The expression
is sometimes referred to as .
Differential equations.
The general function
is the solution to the differential equation:
Representations.
The number can be represented as a real number in a variety of ways: as an infinite series, an infinite product, a continued fraction, or a limit of a sequence. The chief among these representations, particularly in introductory calculus courses is the limit
given above, as well as the series
given by evaluating the above power series for at .
Less common is the continued fraction .
which written out looks like
This continued fraction for converges three times as quickly:
which written out looks like
Many other series, sequence, continued fraction, and infinite product representations of have been developed.
Stochastic representations.
In addition to exact analytical expressions for representation of , there are stochastic techniques for estimating . One such approach begins with an infinite sequence of independent random variables , ..., drawn from the uniform distribution on , . Let be the least number such that the sum of the first observations exceeds 1:
Then the expected value of is : .
Known digits.
The number of known digits of has increased substantially during the last decades. This is due both to the increased performance of computers and to algorithmic improvements.
Since that time, the proliferation of modern high-speed desktop computers has made it possible for amateurs to compute billions of digits of "e".
In computer culture.
In contemporary internet culture, individuals and organizations frequently pay homage to the number .
For instance, in the IPO filing for Google in 2004, rather than a typical round-number amount of money, the company announced its intention to raise $2,718,281,828, which is billion dollars rounded to the nearest dollar. Google was also responsible for a billboard that appeared in the heart of Silicon Valley, and later in Cambridge, Massachusetts; Seattle, Washington; and Austin, Texas. It read "{first 10-digit prime found in consecutive digits of }.com". Solving this problem and visiting the advertised (now defunct) web site led to an even more difficult problem to solve, which in turn led to Google Labs where the visitor was invited to submit a resume. The first 10-digit prime in is 7427466391, which starts at the 99th digit.
In another instance, the computer scientist Donald Knuth let the version numbers of his program Metafont approach . The versions are 2, 2.7, 2.71, 2.718, and so forth.

</doc>
<doc id="9637" url="https://en.wikipedia.org/wiki?curid=9637" title="Euler–Maclaurin formula">
Euler–Maclaurin formula

In mathematics, the Euler–Maclaurin formula provides a powerful connection between integrals (see calculus) and sums. It can be used to approximate integrals by finite sums, or conversely to evaluate finite sums and infinite series using integrals and the machinery of calculus. For example, many asymptotic expansions are derived from the formula, and Faulhaber's formula for the sum of powers is an immediate consequence.
The formula was discovered independently by Leonhard Euler and Colin Maclaurin around 1735 (and later generalized as Darboux's formula). Euler needed it to compute slowly converging infinite series while Maclaurin used it to calculate integrals.
The formula.
If and are natural numbers and is a complex or real valued continuous function for real numbers in the interval formula_1, then the integral
can be approximated by the sum (or vice versa)
(see trapezoidal rule). The Euler–Maclaurin formula provides expressions for the difference between the sum and the integral in terms of the higher derivatives evaluated at the end points of the interval, that is to say when and .
Explicitly, for a natural number formula_4 and a function formula_5 that is formula_4 times continuously differentiable in the interval formula_7, we have
where is the th Bernoulli number, with 
, 
, 
, 
,
The formula is often written with the subscript taking only even values, since the odd Bernoulli numbers are zero except for "B", in which case we have
The Bernoulli polynomials and periodic function.
The formula is derived below using repeated integration by parts applied to successive intervals formula_10 for integers formula_11. The derivation uses the periodic Bernoulli functions, formula_12 which are defined in terms of the Bernoulli polynomials formula_13 for formula_14.
the Bernoulli polynomials may be defined recursively by
and the periodic Bernoulli functions are defined as
where denotes the largest integer that
is not greater than so that always lies in the interval .
It can be shown that formula_17 for all formula_18 so that except for formula_19, all the periodic Bernoulli functions are continuous. The functions formula_12 are sometimes written as formula_21.
The remainder term.
The remainder term can be written as
or equivalently, integrating by parts, assuming "ƒ" is differentiable again and recalling that all odd Bernoulli numbers (but the first one) are zero:
When "n" > 0, it can be shown that
where denotes the Riemann zeta function; one approach to prove this inequality is to obtain the Fourier series for the polynomials . The bound is achieved for even when is zero. The term may be omitted for odd but the proof in this case is more complex (see Lehmer:)
Using this inequality, the size of the remainder term can be estimated using
Applicable formula.
We can use the formula as a means of approximating a finite integral, with the following simple formula:
where is the number of points in the interval of integration from formula_27 to formula_28 and is the distance between points so that formula_29. Note the series above is usually not convergent; indeed, often the terms will increase rapidly after a number of iterations. Thus, attention generally needs to be paid to the remainder term.
This may be viewed as an extension of the trapezoid rule by the inclusion of correction terms.
Applications.
The Basel problem.
The Basel problem asks to determine the sum
Euler computed this sum to 20 decimal places with only a few terms of the Euler–Maclaurin formula in 1735. This probably convinced him that the sum equals π / 6, which he proved in the same year. Parseval's identity for the Fourier series of "f"("x") = "x" gives the same result.
Sums involving a polynomial.
If "f" is a polynomial and "p" is big enough, then the remainder term vanishes. For instance, if "f"("x") = "x", we can choose "p" = 2 to obtain after simplification
(see Faulhaber's formula).
Numerical integration.
The Euler–Maclaurin formula is also used for detailed error analysis in numerical quadrature. It explains the superior performance of the trapezoidal rule on smooth periodic functions and is used in certain extrapolation methods. Clenshaw–Curtis quadrature is essentially a change of variables to cast an arbitrary integral in terms of integrals of periodic functions where the Euler–Maclaurin approach is very accurate (in that particular case the Euler–Maclaurin formula takes the form of a discrete cosine transform). This technique is known as a periodizing transformation.
Asymptotic expansion of sums.
In the context of computing asymptotic expansions of sums and series, usually the most useful form of the Euler–Maclaurin formula is
where "a" and "b" are integers. Often the expansion remains valid even after taking the limits formula_33 or formula_34, or both. In many cases the integral on the right-hand side can be evaluated in closed form in terms of elementary functions even though the sum on the left-hand side cannot. Then all the terms in the asymptotic series can be expressed in terms of elementary functions. For example,
Here the left-hand side is equal to formula_36, namely the first-order polygamma function defined through formula_37; the gamma function formula_38 is equal to formula_39 if formula_40 is a positive integer. This results in an asymptotic expansion for formula_36. That expansion, in turn, serves as the starting point for one of the derivations of precise error estimates for Stirling's approximation of the factorial function.
Proofs.
Derivation by mathematical induction.
We follow the argument given in Apostol.
The Bernoulli polynomials and the periodic Bernoulli functions for were introduced above.
The first several Bernoulli polynomials are
The values are the Bernoulli numbers. Notice that for we have
For ,
The functions agree with the Bernoulli polynomials on the interval and are periodic with period 1. Furthermore, except when , they are also continuous. Thus,
Let be an integer, and consider the integral
where
Integrating by parts, we get
Summing the above from "k" = 0 to "k" = "n" − 1, we get
Adding ("f"("n") - "f"(0))/2 to both sides and rearranging, we have
The last two terms therefore give the error when the integral is taken to approximate the sum.
Next, consider
where
Integrating by parts again, we get
Then summing from "k" = 0 to "k" = "n" − 1, and then replacing the last integral in (1) with what we have thus shown to be equal to it, we have
By now the reader will have guessed that this process can be iterated. In this way we get a proof of the Euler–Maclaurin summation formula which can be formalized by mathematical induction, in which the induction step relies on integration by parts and on the identities for periodic Bernoulli functions.

</doc>
<doc id="9638" url="https://en.wikipedia.org/wiki?curid=9638" title="Epimenides paradox">
Epimenides paradox

The Epimenides paradox reveals a problem with self-reference in logic.
It is named after the Cretan philosopher Epimenides of Knossos (alive circa 600 BC) who is credited with the original statement.
A typical description of the problem is given in the book "Gödel, Escher, Bach", by Douglas Hofstadter
A paradox of self-reference arises when one considers whether it is possible for Epimenides to have spoken the truth.
Logical paradox.
Thomas Fowler (1869) states the paradox as follows: "Epimenides the Cretan says, 'that all the Cretans are liars,' but Epimenides is himself a Cretan; therefore he is himself a liar. But if he be a liar, what he says is untrue, and consequently the Cretans are veracious; but Epimenides is a Cretan, and therefore what he says is true; saying the Cretans are liars, Epimenides is himself a liar, and what he says is untrue. Thus we may go on alternately proving that Epimenides and the Cretans are truthful and untruthful."
The Epimenides paradox in this form can however be solved. A paradox only results when the statement is assumed to be true. Namely, if the statement "all Cretans are liars" (stated by Epimenides, himself a Cretan) is true, then Epimenides, being a Cretan, would be a liar; making the assumption that liars only make false statements, the statement should be false. So assuming the statement is true leads us to conclude that the statement is false and cannot be accepted, therefore it must be true, continuing in a self-referential paradox.
However, if we assume the statement is false and that Epimenides is lying about all Cretans being liars, then there must exist at least one Cretan who is honest. This does not lead to contradiction, since it is not required that this Cretan be Epimenides, meaning that Epimenides can say the false statement that all Cretans are Liars while knowing at least one honest Cretan and lying about this particular Cretan. Hence, from the assumption that the statement is false it does not follow that the statement is true. So we can avoid a paradox as seeing the statement "all Cretans are liars" as a false statement, which is made by a lying Cretan, Epimenides. The mistake made by Thomas Fowler (and many other people) above is to think that the negation of "all Cretans are liars" is "all Cretans are honest" (a paradox) when in fact the negation is "there exists a Cretan who is honest", or "not all Cretans are liars". The Epimenides paradox can be slightly modified as to not allow the kind of solution described above, as it was in the first paradox of Eubulides but instead leading to a non-avoidable self-contradiction. Paradoxical versions of the Epimenides problem are closely related to a class of more difficult logical problems, including the liar paradox, Socratic paradox, and the Burali-Forti paradox, all of which have self-reference in common with Epimenides. Indeed, the Epimenides paradox is usually classified as a variation on the liar paradox, and sometimes the two are not distinguished. The study of self-reference led to important developments in logic and mathematics in the twentieth century.
To psychology, scepticism and law.
The Epimenides paradox is the same principle as psychologists and sceptics using arguments from psychology claiming humans to be unreliable. The paradox comes from the fact that the psychologists and sceptics are human themselves, meaning that they state themselves to be unreliable. This means that psychology is not a science.
The same goes for any statements that laws are necessary due to human nature, since the laws are written and enforced by human beings and so if that was the case the alleged human flaws would have been built into the laws and exaggerated by the enforcement into something worse than nothing.
Origin of the phrase.
Epimenides was a 6th-century BC philosopher and religious prophet who, against the general sentiment of Crete, proposed that Zeus was immortal, as in the following poem:
Denying the immortality of Zeus, then, was the lie of the Cretans.
The phrase "Cretans, always liars" was quoted by the poet Callimachus in his "Hymn to Zeus", with the same theological intent as Epimenides:
Emergence as a logical contradiction.
The logical inconsistency of a Cretan asserting all Cretans are always liars may not have occurred to Epimenides, nor to Callimachus, who both used the phrase to emphasize their point, without irony, perhaps meaning that all Cretans lie routinely, but not exclusively.
In the 1st century AD, the quote is mentioned by Paul of Tarsus as the words of a man who was insubordinate, an empty talker and a deceiver, yet spoke truly in this statement.
Clement of Alexandria, in the late 2nd century AD, fails to indicate that the concept of logical paradox is an issue:
During the early 4th century, Saint Augustine restates the closely related liar paradox in "Against the Academicians" (III.13.29), but without mentioning Epimenides.
In the Middle Ages, many forms of the liar paradox were studied under the heading of insolubilia, but these were not explicitly associated with Epimenides.
Finally, in 1740, the second volume of Pierre Bayle's "Dictionnaire Historique et Critique" explicitly connects Epimenides with the paradox, though Bayle labels the paradox a "sophisme".
References by other authors.
All of the works of Epimenides are now lost, and known only through quotations by other authors. The quotation from the "Cretica" of Epimenides is given by R.N. Longenecker, "Acts of the Apostles", in volume 9 of "The Expositor's Bible Commentary", Frank E. Gaebelein, editor (Grand Rapids, Michigan: Zondervan Corporation, 1976–1984), page 476. Longenecker in turn cites M.D. Gibson, "Horae Semiticae X" (Cambridge: Cambridge University Press, 1913), page 40, "in Syriac". Longenecker states the following in a footnote:
An oblique reference to Epimenides in the context of logic appears in "The Logical Calculus" by W. E. Johnson, "Mind" (New Series), volume 1, number 2 (April, 1892), pages 235–250. Johnson writes in a footnote,
The Epimenides paradox appears explicitly in "Mathematical Logic as Based on the Theory of Types", by Bertrand Russell, in the "American Journal of Mathematics", volume 30, number 3 (July, 1908), pages 222–262, which opens with the following:
In that article, Russell uses the Epimenides paradox as the point of departure for discussions of other problems, including the Burali-Forti paradox and the paradox now called Russell's paradox. Since Russell, the Epimenides paradox has been referenced repeatedly in logic. Typical of these references is "Gödel, Escher, Bach" by Douglas Hofstadter, which accords the paradox a prominent place in a discussion of self-reference.

</doc>
<doc id="9640" url="https://en.wikipedia.org/wiki?curid=9640" title="Engine">
Engine

An engine or motor is a machine designed to convert one form of energy into mechanical energy. Heat engines, including internal combustion engines and external combustion engines (such as steam engines), burn a fuel to create heat, which then creates a force. Electric motors convert electrical energy into mechanical motion; pneumatic motors use compressed air and others—such as clockwork motors in wind-up toys—use elastic energy. In biological systems, molecular motors, like myosins in muscles, use chemical energy to create forces and eventually motion.
Terminology.
The word "engine" derives from Old French "engin", from the Latin "ingenium"–the root of the word "ingenious". Pre-industrial weapons of war, such as catapults, trebuchets and battering rams, were called "siege engines", and knowledge of how to construct them was often treated as a military secret. The word "gin", as in "cotton gin", is short for "engine". Most mechanical devices invented during the industrial revolution were described as engines—the steam engine being a notable example.
In modern usage, the term "engine" typically describes devices, like steam engines and internal combustion engines, that burn or otherwise consume fuel to perform mechanical work by exerting a torque or linear force (usually in the form of thrust). Examples of engines which exert a torque include the familiar automobile gasoline and diesel engines, as well as turboshafts. Examples of engines which produce thrust include turbofans and rockets.
When the internal combustion engine was invented, the term "motor" was initially used to distinguish it from the steam engine—which was in wide use at the time, powering locomotives and other vehicles such as steam rollers. "Motor" and "engine" later came to be used interchangeably in casual discourse. However, technically, the two words have different meanings. An "engine" is a device that burns or otherwise consumes fuel, changing its chemical composition, whereas a motor is a device driven by electricity, air, or hydraulic pressure, which does not change the chemical composition of its energy source. However, rocketry uses the term rocket motor, even though they consume fuel.
A heat engine may also serve as a "prime mover"—a component that transforms the flow or changes in pressure of a fluid into mechanical energy. An automobile powered by an internal combustion engine may make use of various motors and pumps, but ultimately all such devices derive their power from the engine. Another way of looking at it is that a motor receives power from an external source, and then converts it into mechanical energy, while an engine creates power from pressure (derived directly from the explosive force of combustion or other chemical reaction, or secondarily from the action of some such force on other substances such as air, water, or steam).
Devices converting heat energy into motion are commonly referred to simply as "engines".
History.
Antiquity.
Simple machines, such as the club and oar (examples of the lever), are prehistoric. More complex engines using human power, animal power, water power, wind power and even steam power date back to antiquity. Human power was focused by the use of simple engines, such as the capstan, windlass or treadmill, and with ropes, pulleys, and block and tackle arrangements; this power was transmitted usually with the forces multiplied and the speed reduced. These were used in cranes and aboard ships in Ancient Greece, as well as in mines, water pumps and siege engines in Ancient Rome. The writers of those times, including Vitruvius, Frontinus and Pliny the Elder, treat these engines as commonplace, so their invention may be more ancient. By the 1st century AD, cattle and horses were used in mills, driving machines similar to those powered by humans in earlier times.
According to Strabo, a water powered mill was built in Kaberia of the kingdom of Mithridates during the 1st century BC. Use of water wheels in mills spread throughout the Roman Empire over the next few centuries. Some were quite complex, with aqueducts, dams, and sluices to maintain and channel the water, along with systems of gears, or toothed-wheels made of wood and metal to regulate the speed of rotation. More sophisticated small devices, such as the Antikythera Mechanism used complex trains of gears and dials to act as calendars or predict astronomical events. In a poem by Ausonius in the 4th century AD, he mentions a stone-cutting saw powered by water. Hero of Alexandria is credited with many such wind and steam powered machines in the 1st century AD, including the Aeolipile and the vending machine, often these machines were associated with worship, such as animated altars and automated temple doors.
Medieval.
Medieval Muslim engineers employed gears in mills and water-raising machines, and used dams as a source of water power to provide additional power to watermills and water-raising machines. In the medieval Islamic world, such advances made it possible to mechanize many industrial tasks previously carried out by manual labour.
In 1206, al-Jazari employed a crank-conrod system for two of his water-raising machines. A rudimentary steam turbine device was described by Taqi al-Din in 1551 and by Giovanni Branca in 1629.
In the 13th century, the solid rocket motor was invented in China. Driven by gunpowder, this, the simplest form of internal combustion engine was unable to deliver sustained power, but was useful for propelling weaponry at high speeds towards enemies in battle and for fireworks. After invention, this innovation spread throughout Europe.
Industrial Revolution.
The Watt steam engine was the first type of steam engine to make use of steam at a pressure just above atmospheric to drive the piston helped by a partial vacuum. Improving on the design of the 1712 Newcomen steam engine, the Watt steam engine, developed sporadically from 1763 to 1775, was a great step in the development of the steam engine. Offering a dramatic increase in fuel efficiency, James Watt's design became synonymous with steam engines, due in no small part to his business partner, Matthew Boulton. It enabled rapid development of efficient semi-automated factories on a previously unimaginable scale in places where waterpower was not available. Later development led to steam locomotives and great expansion of railway transportation.
As for internal combustion piston engines, these were tested in France in 1807 by de Rivaz and independently, by the Niépce brothers. They were theoretically advanced by Carnot in 1824. In 1853-57 Eugenio Barsanti and Felice Matteucci invented and patented an engine using the free-piston principle that was possibly the first 4-cycle engine. The Otto cycle in 1877 was capable of giving a far higher power to weight ratio than steam engines and worked much better for many transportation applications such as cars and aircraft.
Automobiles.
The first commercially successful automobile, created by Karl Benz, added to the interest in light and powerful engines. The lightweight petrol internal combustion engine, operating on a four-stroke Otto cycle, has been the most successful for light automobiles, while the more efficient Diesel engine is used for trucks and buses. However, in recent years, turbo Diesel engines have become increasingly popular, especially outside of the United States, even for quite small cars.
Horizontally opposed pistons.
In 1896, Karl Benz was granted a patent for his design of the first engine with horizontally opposed pistons. His design created an engine in which the corresponding pistons move in horizontal cylinders and reach top dead center simultaneously, thus automatically balancing each other with respect to their individual momentum. Engines of this design are often referred to as flat engines because of their shape and lower profile. They are or were used in the Volkswagen Beetle, some Porsche and Subaru cars, many BMW and Honda motorcycles, and aircraft engines (for propeller driven aircraft).
Advancement.
Continuance of the use of the internal combustion engine for automobiles is partly due to the improvement of engine control systems (onboard computers providing engine management processes, and electronically controlled fuel injection). Forced air induction by turbocharging and supercharging have increased power outputs and engine efficiencies. Similar changes have been applied to smaller diesel engines giving them almost the same power characteristics as petrol engines. This is especially evident with the popularity of smaller diesel engine propelled cars in Europe. Larger diesel engines are still often used in trucks and heavy machinery, although they require special machining not available in most factories. Diesel engines produce lower hydrocarbon and emissions, but greater particulate and pollution, than gasoline engines. Diesel engines are also 40% more fuel efficient than comparable gasoline engines.
Increasing power.
The first half of the 20th century saw a trend to increasing engine power, particularly in the American models. Design changes incorporated all known methods of raising engine capacity, including increasing the pressure in the cylinders to improve efficiency, increasing the size of the engine, and increasing the rate at which the engine produces work. The higher forces and pressures created by these changes created engine vibration and size problems that led to stiffer, more compact engines with V and opposed cylinder layouts replacing longer straight-line arrangements.
Combustion efficiency.
The design principles favoured in Europe, because of economic and other restraints such as smaller and twistier roads, leant toward smaller cars and corresponding to the design principles that concentrated on increasing the combustion efficiency of smaller engines. This produced more economical engines with earlier four-cylinder designs rated at 40 horsepower (30 kW) and six-cylinder designs rated as low as 80 horsepower (60 kW), compared with the large volume V-8 American engines with power ratings in the range from 250 to 350 hp, some even over 400 hp (190 to 260 kW).
Engine configuration.
Earlier automobile engine development produced a much larger range of engines than is in common use today. Engines have ranged from 1- to 16-cylinder designs with corresponding differences in overall size, weight, engine displacement, and cylinder bores. Four cylinders and power ratings from 19 to 120 hp (14 to 90 kW) were followed in a majority of the models. Several three-cylinder, two-stroke-cycle models were built while most engines had straight or in-line cylinders. There were several V-type models and horizontally opposed two- and four-cylinder makes too. Overhead camshafts were frequently employed. The smaller engines were commonly air-cooled and located at the rear of the vehicle; compression ratios were relatively low. The 1970s and 1980s saw an increased interest in improved fuel economy, which caused a return to smaller V-6 and four-cylinder layouts, with as many as five valves per cylinder to improve efficiency. The Bugatti Veyron 16.4 operates with a W16 engine, meaning that two V8 cylinder layouts are positioned next to each other to create the W shape sharing the same crankshaft.
The largest internal combustion engine ever built is the Wärtsilä-Sulzer RTA96-C, a 14-cylinder, 2-stroke turbocharged diesel engine that was designed to power the "Emma Mærsk", the largest container ship in the world. This engine weighs 2,300 tons, and when running at 102 RPM produces 109,000 bhp (80,080 kW) consuming some 13.7 tons of fuel each hour.
Types.
An engine can be put into a category according to two criteria: the form of energy it accepts in order to create motion, and the type of motion it outputs.
Heat engine.
Combustion engine.
Combustion engines are heat engines driven by the heat of a combustion process.
Internal combustion engine.
The internal combustion engine is an engine in which the combustion of a fuel (generally, fossil fuel) occurs with an oxidizer (usually air) in a combustion chamber. In an internal combustion engine the expansion of the high temperature and high pressure gases, which are produced by the combustion, directly applies force to components of the engine, such as the pistons or turbine blades or a nozzle, and by moving it over a distance, generates useful mechanical energy.
External combustion engine.
An external combustion engine (EC engine) is a heat engine where an internal working fluid is heated by combustion of an external source, through the engine wall or a heat exchanger. The fluid then, by expanding and acting on the mechanism of the engine produces motion and usable work. The fluid is then cooled, compressed and reused (closed cycle), or (less commonly) dumped, and cool fluid pulled in (open cycle air engine).
"Combustion" refers to burning fuel with an oxidizer, to supply the heat. Engines of similar (or even identical) configuration and operation may use a supply of heat from other sources such as nuclear, solar, geothermal or exothermic reactions not involving combustion; but are not then strictly classed as external combustion engines, but as external thermal engines.
The working fluid can be a gas as in a Stirling engine, or steam as in a steam engine or an organic liquid such as n-pentane in an Organic Rankine cycle. The fluid can be of any composition; gas is by far the most common, although even single-phase liquid is sometimes used. In the case of the steam engine, the fluid changes phases between liquid and gas.
Air-breathing combustion engines.
Air-breathing combustion engines are combustion engines that use the oxygen in atmospheric air to oxidise ('burn') the fuel, rather than carrying an oxidiser, as in a rocket. Theoretically, this should result in a better specific impulse than for rocket engines.
A continuous stream of air flows through the air-breathing engine. This air is compressed, mixed with fuel, ignited and expelled as the exhaust gas.
Typical air-breathing engines include:
Environmental effects.
The operation of engines typically has a negative impact upon air quality and ambient sound levels. There has been a growing emphasis on the pollution producing features of automotive power systems. This has created new interest in alternate power sources and internal-combustion engine refinements. Though a few limited-production battery-powered electric vehicles have appeared, they have not proved competitive owing to costs and operating characteristics. In the 21st century the diesel engine has been increasing in popularity with automobile owners. However, the gasoline engine and the Diesel engine, with their new emission-control devices to improve emission performance, have not yet been significantly challenged. A number of manufacturers have introduced hybrid engines, mainly involving a small gasoline engine coupled with an electric motor and with a large battery bank, but these too have yet to make much of an inroad into the market shares of gasoline and Diesel engines.
Air quality.
Exhaust from a spark ignition engine consists of the following: nitrogen 70 to 75% (by volume), water vapor 10 to 12%, carbon dioxide 10 to 13.5%, hydrogen 0.5 to 2%, oxygen 0.2 to 2%, carbon monoxide: 0.1 to 6%, unburnt hydrocarbons and partial oxidation products (e.g. aldehydes) 0.5 to 1%, nitrogen monoxide 0.01 to 0.4%, nitrous oxide <100 ppm, sulfur dioxide 15 to 60 ppm, traces of other compounds such as fuel additives and lubricants, also halogen and metallic compounds, and other particles. Carbon monoxide is highly toxic, and can cause carbon monoxide poisoning, so it is important to avoid any build-up of the gas in a confined space. Catalytic converters can reduce toxic emissions, but not completely eliminate them. Also, resulting greenhouse gas emissions, chiefly carbon dioxide, from the widespread use of engines in the modern industrialized world is contributing to the global greenhouse effect – a primary concern regarding global warming.
Non-combusting heat engines.
Some engines convert heat from noncombustive processes into mechanical work, for example a nuclear power plant uses the heat from the nuclear reaction to produce steam and drive a steam engine, or a gas turbine in a rocket engine may be driven by decomposing hydrogen peroxide. Apart from the different energy source, the engine is often engineered much the same as an internal or external combustion engine. Another group of noncombustive engines includes thermoacoustic heat engines (sometimes called "TA engines") which are thermoacoustic devices which use high-amplitude sound waves to pump heat from one place to another, or conversely use a heat difference to induce high-amplitude sound waves. In general, thermoacoustic engines can be divided into standing wave and travelling wave devices.
Non-thermal chemically powered motor.
Non-thermal motors usually are powered by a chemical reaction, but are not heat engines. Examples include:
Electric motor.
An electric motor uses electrical energy to produce mechanical energy, usually through the interaction of magnetic fields and current-carrying conductors. The reverse process, producing electrical energy from mechanical energy, is accomplished by a generator or dynamo. Traction motors used on vehicles often perform both tasks. Electric motors can be run as generators and vice versa, although this is not always practical.
Electric motors are ubiquitous, being found in applications as diverse as industrial fans, blowers and pumps, machine tools, household appliances, power tools, and disk drives. They may be powered by direct current (for example a battery powered portable device or motor vehicle), or by alternating current from a central electrical distribution grid. The smallest motors may be found in electric wristwatches. Medium-size motors of highly standardized dimensions and characteristics provide convenient mechanical power for industrial uses. The very largest electric motors are used for propulsion of large ships, and for such purposes as pipeline compressors, with ratings in the thousands of kilowatts. Electric motors may be classified by the source of electric power, by their internal construction, and by their application.
The physical principle of production of mechanical force by the interactions of an electric current and a magnetic field was known as early as 1821. Electric motors of increasing efficiency were constructed throughout the 19th century, but commercial exploitation of electric motors on a large scale required efficient electrical generators and electrical distribution networks.
To reduce the electric energy consumption from motors and their associated carbon footprints, various regulatory authorities in many countries have introduced and implemented legislation to encourage the manufacture and use of higher efficiency electric motors. A well-designed motor can convert over 90% of its input energy into useful power for decades. When the efficiency of a motor is raised by even a few percentage points, the savings, in kilowatt hours (and therefore in cost), are enormous. The electrical energy efficiency of a typical industrial induction motor can be improved by: 1) reducing the electrical losses in the stator windings (e.g., by increasing the cross-sectional area of the conductor, improving the winding technique, and using materials with higher electrical conductivities, such as copper), 2) reducing the electrical losses in the rotor coil or casting (e.g., by using materials with higher electrical conductivities, such as copper), 3) reducing magnetic losses by using better quality magnetic steel, 4) improving the aerodynamics of motors to reduce mechanical windage losses, 5) improving bearings to reduce friction losses, and 6) minimizing manufacturing tolerances. "For further discussion on this subject, see Premium efficiency and Copper in energy efficient motors.)"
By convention, "electric engine" refers to a railroad electric locomotive, rather than an electric motor.
Physically powered motor.
Some motors are powered by potential or kinetic energy, for example some funiculars, gravity plane and ropeway conveyors have used the energy from moving water or rocks, and some clocks have a weight that falls under gravity. Other forms of potential energy include compressed gases (such as pneumatic motors), springs (clockwork motors) and elastic bands.
Historic military siege engines included large catapults, trebuchets, and (to some extent) battering rams were powered by potential energy.
Pneumatic motor.
A pneumatic motor is a machine that converts potential energy in the form of compressed air into mechanical work. Pneumatic motors generally convert the compressed air to mechanical work though either linear or rotary motion. Linear motion can come from either a diaphragm or piston actuator, while rotary motion is supplied by either a vane type air motor or piston air motor. Pneumatic motors have found widespread success in the hand-held tool industry and continual attempts are being made to expand their use to the transportation industry. However, pneumatic motors must overcome efficiency deficiencies before being seen as a viable option in the transportation industry.
Hydraulic motor.
A hydraulic motor is one that derives its power from a pressurized fluid. This type of engine can be used to move heavy loads or produce motion.
Performance.
Engine speed.
In the case of engines outputting shaft power, engine speed is measured in revolutions per minute (RPM). Engines may be classified as low-speed, medium-speed or high-speed but these terms are inexact and depend on the type of engine being described. Generally, diesel engines operate at lower speed compared to gasoline engines. Electric motors and turboshafts are capable of very high speeds. In the case of engines producing thrust, it is rather inaccurate to talk of an 'engine speed' since what is moving is not the engine, but the working medium that the engine is accelerating; in this case one talks of an exhaust velocity, which is exactly the I outside of a gravitational field and therefore makes one jump straight to a discussion of efficiency; see the article on specific impulse for more information.
Thrust.
Thrust is the force arising from the interaction between two masses which exert equal but opposite forces on each other due to their speed. The force can be measured either in newtons (N, SI units) or in pounds-thrust (lb, imperial units).
Torque.
Torque is the force being exerted on a theoretical lever connected to the output shaft of an engine. This is expressed by the formula:
where is the length of the lever, is the force applied on it, and is the vector cross product.
Torque is measured typically either in newton-metres (N·m, SI units) or in foot-pounds (ft·lb, imperial units).
Power.
Power is the amount of work being done, or energy being produced, per unit of time. This is expressed by the formula:
With a quick demonstration, it can be shown that:
This formula with linear forces and speeds can be used equally well for both engines outputting thrust and engines exerting torque.
When considering propulsive engines, typically only the raw force of the core mass flow is considered, leading to such engines having their 'power' rated in any of the units discussed above for forces.
If the engine in question outputs its power on a shaft, then:
This is the reason why any engine outputting its power on a rotating shaft always informs, along with its rated power, the rotational speed at which that rated power is developed.
Typically, among engines driving a rotating shaft, combustion engines have their power rated in horsepower (hp), while electric engines have their power rated in watts (W, not to be confused with the mathematical symbol for work) or multiples thereof.
Efficiency.
Depending on the type of engine employed, different rates of efficiency are attained.
For heat engines, efficiency cannot be greater than the Carnot efficiency.
Sound levels.
In the case of sound levels, engine operation is of greatest impact with respect to mobile sources such as automobiles and trucks. Engine noise is a particularly large component of mobile source noise for vehicles operating at lower speeds, where aerodynamic and tire noise is less significant. Generally speaking, petrol and diesel engines emit less noise than turboshafts of equivalent power output; electric motors very often emit less noise than their fossil fuel-powered equivalents. Thrust-outputting engines, such as turbofans, turbojets and rockets emit the greatest amount of noise because their method of producing thrust is directly related to the production of sound.
Various methods have been devised to reduce noise. Petrol and diesel engines are fitted with mufflers (silencers); newer turbofans often have outsized fans (the so-called high-bypass technology) in order to reduce the proportion of noisy, hot exhaust from the integrated turboshaft in the exhaust stream, and hushkits exist for older, low-bypass turbofans. No known methods exist for reducing the noise output of rockets without a corresponding reduction in thrust.
Engines by use.
Particularly notable kinds of engines include:

</doc>
<doc id="9643" url="https://en.wikipedia.org/wiki?curid=9643" title="Economic and monetary union">
Economic and monetary union

An economic and monetary union is a type of trade bloc which is composed of an economic union (common market and customs union) with a monetary union. It is to be distinguished from a mere monetary union (e.g. the Latin Monetary Union in the 19th century), which does not involve a common market. This is the sixth stage of economic integration. EMU is established through a currency-related trade pact. An intermediate step between pure EMU and a complete economic integration is the fiscal union.
Additionally the autonomous and dependent territories, such as some of the EU member state special territories, are sometimes treated as separate customs territory from their mainland state or have varying arrangements of formal or de facto customs union, common market and currency union (or combinations thereof) with the mainland and in regards to third countries through the trade pacts signed by the mainland state.

</doc>
<doc id="9644" url="https://en.wikipedia.org/wiki?curid=9644" title="European Environment Agency">
European Environment Agency

The European Environment Agency (EEA) is the agency of the European Union (EU) that provides independent information on the environment, thereby helping those involved in developing, adopting, implementing and evaluating environmental policy, as well as informing the general public. The agency is governed by a management board composed of representatives of the governments of its 33 member states, a European Commission representative and two scientists appointed by the European Parliament, assisted by a committee of scientists.
The EEA was established by the European Economic Community (EEC) Regulation 1210/1990 (amended by EEC Regulation 933/1999 and EC Regulation 401/2009) and became operational in 1994. It is headquartered in Copenhagen, Denmark.
The current Executive Director of the agency is Professor Hans Bruyninckx, who has been appointed for a five-year term. He is the successor of Professor Jacqueline McGlade.
The member states of the union are members; however the Council Regulation establishing it provided that other states may become members of it by means of agreements concluded between them and the EU. 
It was the first EU body to open its membership to the 13 candidate countries (pre-2004 enlargement).
The EEA has 33 member countries and six cooperating countries. The European environment information and observation network (Eionet) is a partnership network of the EEA and the countries. The EEA is responsible for developing the network and coordinating its activities. To do so, the EEA works closely together with national focal points, typically national environment agencies or environment ministries. They are responsible for coordinating national networks involving many institutions (about 350 in all). 
The 33 member countries include the 28 European Union Member States together with Iceland, Liechtenstein, Norway, Switzerland and Turkey.
The six Balkans countries are cooperating countries: Albania, Bosnia and Herzegovina, the Republic of Macedonia, Montenegro, Serbia as well as Kosovo under the UN Security Council Resolution 1244/99. These cooperation activities are integrated into Eionet and are supported by the European Union under the Instrument for Pre-Accession Assistance.
The EEA is an active member of the EPA Network.
Member countries.
The 33 member countries include the 28 European Union member states together with Iceland, Liechtenstein, Norway, Switzerland and Turkey. The six Western Balkan countries are cooperating countries: Albania, Bosnia and Herzegovina, the Republic of Macedonia, Montenegro, Serbia as well as Kosovo under the UN Security Council Resolution 1244/99.
European environment information and observation network.
The European environment information and observation network (Eionet) is a partnership network of the EEA and its member and cooperating countries. The EEA is responsible for developing the network and coordinating its activities. To do this, the EEA works closely together with the National Focal Points (NFPs), typically national environment agencies or environment ministries in the member countries.
The NFPs are responsible for coordinating networks of the National Reference Centres (NRCs), bringing altogether around 1000 experts from over 350 national institutions and other bodies dealing with environmental information.
Apart from the NFPs and NRCs, Eionet currently covers six European Topic Centres (ETCs) in the areas of air and climate change, biological diversity, climate change impacts, vulnerability and adaptation, water, land use and spatial information and analysis and sustainable consumption and production.
Annual discharge process.
On February 2012, the European Parliament's Committee on Budgetary Control published a draft report identifying potential areas of concern in the use of funds and influence for the 2010 budget. The EEA's Executive Director refuted allegations of irregularities in a public hearing 
Members of the European Parliament (MEPs) voted on the report on 27 March 2012 and commended the cooperation between the Agency and NGOs working in the environmental area. On 23 October 2012, the European Parliament voted and granted the discharge to the European Environment Agency for its 2010 budget.
On 17 April 2013, the European Parliament (MEPs) voted and granted the discharge to the European Environment Agency for its 2011 budget.
International cooperation.
In addition to its 33 members and six Balkan cooperating countries, the EEA also cooperates and fosters partnerships with its neighbours and other countries and regions, mostly in the context of the European Neighbourhood Policy:
Additionally the EEA cooperates with multiple international organizations and the corresponding agencies of the following countries:
Official languages.
The 26 official languages used by the EEA are: Bulgarian, Czech, Croatian, Danish, German, Greek, English, Spanish, Estonian, Finnish, French, Hungarian, Icelandic, Italian, Lithuanian, Latvian, Malti, Dutch, Norwegian, Polish, Portuguese, Romanian, Slovak, Slovene, Swedish and Turkish.

</doc>
<doc id="9645" url="https://en.wikipedia.org/wiki?curid=9645" title="EV">
EV

EV and similar forms may refer to:

</doc>
<doc id="9646" url="https://en.wikipedia.org/wiki?curid=9646" title="Erlang (programming language)">
Erlang (programming language)

Erlang ( ) is a general-purpose, concurrent, garbage-collected programming language and runtime system. The sequential subset of Erlang is almost a functional language (excluding certain built-in functions), with eager evaluation, single assignment, and dynamic typing. It was originally designed by Ericsson to support distributed, fault-tolerant, soft real-time, highly available, non-stop applications. It supports hot swapping, thus code can be changed without stopping a system.
While threads require external library support in most languages, Erlang provides language-level features for creating and managing processes with the aim of simplifying concurrent programming. Though all concurrency is explicit in Erlang, processes communicate using message passing instead of shared variables, which removes the need for explicit locks (a locking scheme is still used internally by the VM).
The first version was developed by Joe Armstrong, Robert Virding and Mike Williams in 1986. It was originally a proprietary language within Ericsson, but was released as open source in 1998. Erlang, along with OTP, a collection of middleware and libraries in Erlang, are now supported and maintained by the OTP product unit at Ericsson and widely referred to as Erlang/OTP.
History.
The name "Erlang", attributed to Bjarne Däcker, has been presumed by those working on the telephony switches (for whom the language was designed) to be a reference to Danish mathematician and engineer Agner Krarup Erlang or the ubiquitous use of the unit named for him, and (initially at least) simultaneously as a syllabic abbreviation of "Ericsson Language".
Erlang was designed with the aim of improving the development of telephony applications. The initial version of Erlang was implemented in Prolog and was influenced by the programming language PLEX used in earlier Ericsson exchanges. By 1988 Erlang had proven that it was suitable for prototyping telephone exchanges, but the Prolog interpreter was far too slow. One group within Ericsson estimated that it would need to be 40 times faster in order to be suitable for production use. In 1992 work began on the BEAM virtual machine which compiles Erlang to C using a mix of natively compiled code and threaded code to strike a balance between performance and disk space. According to Armstrong, the language went from lab product to real applications following the collapse of the next-generation AXE exchange named "AXE-N" in 1995. As a result, Erlang was chosen for the next ATM exchange "AXD".
In 1998 Ericsson announced the AXD301 switch, containing over a million lines of Erlang and reported to achieve a high availability of nine "9"s. Shortly thereafter, Ericsson Radio Systems banned the in-house use of Erlang for new products, citing a preference for non-proprietary languages. The ban caused Armstrong and others to leave Ericsson. The implementation was open-sourced at the end of the year. Ericsson eventually lifted the ban; it re-hired Armstrong in 2004.
In 2006, native symmetric multiprocessing support was added to the runtime system and virtual machine.
Erlang has now been adopted by companies worldwide, including Nortel and T-Mobile. Erlang is used in Ericsson’s support nodes, and in GPRS, 3G and LTE mobile networks worldwide.
As Tim Bray, director of Web Technologies at Sun Microsystems, expressed in his keynote at OSCON in July 2008:
Functional programming examples.
An Erlang function that uses recursion to count to ten:
A factorial algorithm implemented in Erlang:
A Fibonacci algorithm implemented in Erlang (Note: This is only for demonstrating the Erlang syntax. This algorithm is rather slow.):
A sorting algorithm (similar to quicksort):
The above example recursively invokes the function codice_1 until nothing remains to be sorted. The expression codice_2 is a list comprehension, meaning "Construct a list of elements codice_3 such that codice_3 is a member of codice_5, and codice_3 is less than codice_7." codice_8 is the list concatenation operator.
A comparison function can be used for more complicated structures for the sake of readability.
The following code would sort lists according to length:
Here again, a codice_7 is taken from the first parameter given to codice_10 and the rest of codice_11 is named codice_5. Note that the expression
is no different in form from
(in the previous example) except for the use of a comparison function in the last part, saying "Construct a list of elements codice_13 such that codice_13 is a member of codice_5, and codice_16 is true", with codice_16 being defined earlier as
Note also that the anonymous function is named codice_16 in the parameter list of the second definition of codice_1 so that it can be referenced by that name within that function. It is not named in the first definition of codice_1, which deals with the base case of an empty list and thus has no need of this function, let alone a name for it.
Data types.
Erlang has eight primitive data types:
And three compound data types:
Two forms of syntactic sugar are provided:
Erlang has no method of defining classes, although there are external libraries available.
Concurrency and distribution orientation.
Erlang's main strength is support for concurrency. It has a small but powerful set of primitives to create processes and communicate among them. Erlang is conceptually similar to the occam programming language, though it recasts the ideas of communicating sequential processes (CSP) in a functional framework and uses asynchronous message passing. Processes are the primary means to structure an Erlang application. They are neither operating system processes nor operating system threads, but lightweight processes. Like operating system processes (but unlike operating system threads), they share no state with each other. The estimated minimal overhead for each is 300 words. Thus, many processes can be created without degrading performance. A benchmark with 20 million processes has been successfully performed. Erlang has supported symmetric multiprocessing since release R11B of May 2006.
Inter-process communication works via a shared-nothing asynchronous message passing system: every process has a "mailbox", a queue of messages that have been sent by other processes and not yet consumed. A process uses the codice_35 primitive to retrieve messages that match desired patterns. A message-handling routine tests messages in turn against each pattern, until one of them matches. When the message is consumed and removed from the mailbox the process resumes execution. A message may comprise any Erlang structure, including primitives (integers, floats, characters, atoms), tuples, lists, and functions.
The code example below shows the built-in support for distributed processes:
As the example shows, processes may be created on remote nodes, and communication with them is transparent in the sense that communication with remote processes works exactly as communication with local processes.
Concurrency supports the primary method of error-handling in Erlang. When a process crashes, it neatly exits and sends a message to the controlling process which can take action.
Implementation.
The Ericsson Erlang implementation loads virtual machine bytecode which is converted to threaded code at load time. It also includes a native code compiler on most platforms, developed by the High Performance Erlang Project (HiPE) at Uppsala University. Since October 2001 the HiPE system is fully integrated in Ericsson's Open Source Erlang/OTP system. It also supports interpreting, directly from source code via abstract syntax tree, via script as of R11B-5 release of Erlang.
Hot code loading and modules.
Erlang supports language-level Dynamic Software Updating. To implement this, code is loaded and managed as "module" units; the module is a compilation unit. The system can keep two versions of a module in memory at the same time, and processes can concurrently run code from each. The versions are referred to as the "new" and the "old" version. A process will not move into the new version until it makes an external call to its module.
An example of the mechanism of hot code loading:
For the second version, we add the possibility to reset the count to zero.
Only when receiving a message consisting of the atom 'code_switch' will the loop execute an external call to codeswitch/1 (codice_36 is a preprocessor macro for the current module). If there is a new version of the "counter" module in memory, then its codeswitch/1 function will be called. The practice of having a specific entry-point into a new version allows the programmer to transform state to what is required in the newer version. In our example we keep the state as an integer.
In practice, systems are built up using design principles from the Open Telecom Platform which leads to more code upgradable designs. Successful hot code loading is a tricky subject; Code needs to be written to make use of Erlang's facilities.
Distribution.
In 1998, Ericsson released Erlang as open source to ensure its independence from a single vendor and to increase awareness of the language. Erlang, together with libraries and the real-time distributed database Mnesia, forms the Open Telecom Platform (OTP) collection of libraries. Ericsson and a few other companies offer commercial support for Erlang.
Since the open source release, Erlang has been used by several firms worldwide, including Nortel and T-Mobile. Although Erlang was designed to fill a niche and has remained an obscure language for most of its existence, its popularity is growing due to demand for concurrent services.
Erlang has found some use in fielding MMORPG servers.
Erlang in Industry.
Projects using Erlang.
Projects using Erlang include:
Companies using Erlang.
Companies using Erlang in their production systems include:

</doc>
<doc id="9647" url="https://en.wikipedia.org/wiki?curid=9647" title="Euphoria (programming language)">
Euphoria (programming language)

Euphoria is a programming language originally created by Robert Craig of Rapid Deployment Software in Toronto, Canada. Initially developed (though not publicly released) on the Atari ST, the first commercial release was for the 16-bit DOS platform and was proprietary. In 2006, with the release of version 3, Euphoria became open-source software. The openEuphoria Group continues to administer and develop the project. In December 2010, the openEuphoria Group released version 4 of openEuphoria along with a new identity and mascot for the project. OpenEuphoria is currently available for Windows, Linux, OS X and three flavors of *BSD.
Euphoria is a general-purpose high-level imperative-procedural interpreted language. A translator generates C source code and the GNU compiler collection (GCC) and Open Watcom compilers are supported. Alternatively, Euphoria programs may be bound with the interpreter to create stand-alone executables. A number of graphical user interface (GUI) libraries are supported including Win32lib and wrappers for wxWidgets, GTK+ and IUP. Euphoria has a simple built-in database and wrappers for a variety of other databases.
Overview.
The Euphoria language is a general purpose procedural language that focuses on simplicity, legibility, rapid development and performance via several means.
History.
Developed as a personal project to invent a programming language from scratch, Euphoria was created by Robert Craig on an Atari Mega-ST. Many design ideas for the language came from Craig's Master's thesis in computer science at the University of Toronto. Craig's thesis was heavily influenced by the work of John Backus on functional programming (FP) languages.
Craig ported his original Atari implementation to the 16-bit DOS platform and Euphoria was first released, version 1.0, in July 1993 under a proprietary licence. The original Atari implementation is described by Craig as "primitive" and has not been publicly released. Euphoria continued to be developed and released by Craig via his company Rapid Deployment Software (RDS) and website rapideuphoria.com. In October 2006 RDS released version 3 of Euphoria and announced that henceforth Euphoria would be freely distributed under an open-source software licence.
RDS continued to develop Euphoria, culminating with the release of version 3.1.1 in August, 2007. Subsequently, RDS ceased unilateral development of Euphoria and the openEuphoria Group took over ongoing development. The openEuphoria Group released version 4 in December, 2010 along with a new logo and mascot for the openEuphoria project.
Version 3.1.1 remains an important milestone release, being the last version of Euphoria which supports the DOS platform.
Euphoria is an acronym for "End-User Programming with Hierarchical Objects for Robust Interpreted Applications" although there is some suspicion that this is a backronym.
The Euphoria interpreter was originally written in C. With the release of version 2.5 in November 2004 the Euphoria interpreter was split into two parts: a front-end parser, and a back-end interpreter. The front-end is now written in Euphoria (and used with the Euphoria-to-C translator and the Binder). The main back-end and run time library are written in C.
Features.
Euphoria was conceived and developed with the following design goals and features:
Use.
Euphoria is designed to readily facilitate handling of dynamic sets of data of varying types and is particularly useful for string and image processing. Euphoria has been used in artificial intelligence experiments, the study of mathematics, for teaching programming, and to implement fonts involving thousands of characters. A large part of the Euphoria interpreter is written in Euphoria.
Data types.
Euphoria has two basic data types:
Euphoria has two additional data types predefined:
There is no character string data type. Strings are represented by a "sequence" of "integer" values. However, because literal strings are so commonly used in programming, Euphoria interprets double-quote enclosed characters as a sequence of integers. Thus
is seen as if the coder had written:
which is the same as:
Hello World.
 puts(1, "Hello World!\n")
Examples.
Program comments start with a double hyphen codice_1 and go through the end of line.
The following code looks for an old item in a group of items. If found, it removes it by concatenating all the elements before it with all the elements after it. Note that the first element in a sequence has the index one and that $ refers to the length (i.e., total number of elements) of the sequence.
The following modification to the above example replaces an old item with a new item. As the variables "old" and "new" have been defined as objects, they could be "atoms" or "sequences". Type checking is not needed as the function will work with any sequence of data of any type and needs no external libraries.
Furthermore, no pointers are involved and subscripts are automatically checked. Thus the function cannot access memory out-of-bounds. There is no need to allocate or deallocate memory explicitly and no chance of a memory leak.
The line
shows some of the "sequence" handling facilities. A "sequence" may contain a set of any types, and this can be sliced (to take a subset of the data in a "sequence") and concatenated in expressions with no need for special functions.
Parameter passing.
Arguments to routines are always passed by value; there is no pass-by-reference facility. However, parameters are allowed to be modified "locally" (i.e., within the callee) which is implemented very efficiently as sequences have automatic copy-on-write semantics. In other words, when you pass a sequence to a routine, initially only a reference to it is passed, but at the point the routine modifies this sequence parameter the sequence is copied and the routine updates only a copy of the original.
External links.
Free downloads of Euphoria for the various platforms, packages, Windows IDE, Windows API libraries, a GTK+ wrapper for Linux, graphics libraries (DOS, OpenGL, etc.).

</doc>
<doc id="9649" url="https://en.wikipedia.org/wiki?curid=9649" title="Energy">
Energy

In physics, energy is a property of objects which can be transferred to other objects or converted into different forms. The "ability of a system to perform work" is a common description, but it is difficult to give one single comprehensive definition of energy because of its many forms. For instance, in SI units, energy is measured in joules, and one joule is defined "mechanically", being the energy transferred to an object by the mechanical work of moving it a distance of 1 metre against a force of 1 newton. However, there are many other definitions of energy, depending on the context, such as thermal energy, radiant energy, electromagnetic, nuclear, etc., where definitions are derived that are the most convenient.
Common energy forms include the kinetic energy of a moving object, the potential energy stored by an object's position in a force field (gravitational, electric or magnetic), the elastic energy stored by stretching solid objects, the chemical energy released when a fuel burns, the radiant energy carried by light, and the thermal energy due to an object's temperature. All of the many forms of energy are convertible to other kinds of energy. In Newtonian physics, there is a universal law of conservation of energy which says that energy can be neither created nor be destroyed; however, it can change from one form to another.
For "closed systems" with no external source or sink of energy, the first law of thermodynamics states that a system's energy is constant unless energy is transferred in or out by mechanical work or heat, and that no energy is lost in transfer. This means that it is impossible to create or destroy energy. While heat can always be fully converted into work in a reversible isothermal expansion of an ideal gas, for cyclic processes of practical interest in heat engines the second law of thermodynamics states that the system doing work always loses some energy as waste heat. This creates a limit to the amount of heat energy that can do work in a cyclic process, a limit called the available energy. Mechanical and other forms of energy can be transformed in the other direction into thermal energy without such limitations. The total energy of a system can be calculated by adding up all forms of energy in the system.
Examples of energy transformation include generating electric energy from heat energy via a steam turbine, or lifting an object against gravity using electrical energy driving a crane motor. Lifting against gravity performs mechanical work on the object and stores gravitational potential energy in the object. If the object falls to the ground, gravity does mechanical work on the object which transforms the potential energy in the gravitational field to the kinetic energy released as heat on impact with the ground. Our Sun transforms nuclear potential energy to other forms of energy; its total mass does not decrease due to that in itself (since it still contains the same total energy even if in different forms), but its mass does decrease when the energy escapes out to its surroundings, largely as radiant energy.
Mass and energy are closely related. According to the theory of mass–energy equivalence, any object that has mass when stationary in a frame of reference (called rest mass) also has an equivalent amount of energy whose form is called rest energy in that frame, and any additional energy acquired by the object above that rest energy will increase an object's mass. For example, if you had a sensitive enough scale, you could measure an increase in mass after heating an object.
Living organisms require available energy to stay alive, such as the energy humans get from food. Civilisation gets the energy it needs from energy resources such as fossil fuels. The processes of Earth's climate and ecosystem are driven by the radiant energy Earth receives from the sun and the geothermal energy contained within the earth.
Forms.
The total energy of a system can be subdivided and classified in various ways. For example, classical mechanics distinguishes between kinetic energy, which is determined by an object's movement through space, and potential energy, which is a function of the position of an object within a field. It may also be convenient to distinguish gravitational energy, thermal energy, several types of nuclear energy (which utilize potentials from the nuclear force and the weak force), electric energy (from the electric field), and magnetic energy (from the magnetic field), among others. Many of these classifications overlap; for instance, thermal energy usually consists partly of kinetic and partly of potential energy.
Some types of energy are a varying mix of both potential and kinetic energy. An example is mechanical energy which is the sum of (usually macroscopic) kinetic and potential energy in a system. Elastic energy in materials is also dependent upon electrical potential energy (among atoms and molecules), as is chemical energy, which is stored and released from a reservoir of electrical potential energy between electrons, and the molecules or atomic nuclei that attract them. .The list is also not necessarily complete. Whenever physical scientists discover that a certain phenomenon appears to violate the law of energy conservation, new forms are typically added that account for the discrepancy.
Heat and work are special cases in that they are not properties of systems, but are instead properties of "processes" that transfer energy. In general we cannot measure how much heat or work are present in an object, but rather only how much energy is transferred among objects in certain ways during the occurrence of a given process. Heat and work are measured as positive or negative depending on which side of the transfer we view them from.
Potential energies are often measured as positive or negative depending on whether they are greater or less than the energy of a specified base state or configuration such as two interacting bodies being infinitely far apart. Wave energies (such as radiant or sound energy), kinetic energy, and rest energy are each greater than or equal to zero because they are measured in comparison to a base state of zero energy: "no wave", "no motion", and "no inertia", respectively.
The distinctions between different kinds of energy is not always clear-cut. As Richard Feynman points out:
Some examples of different kinds of energy:
History.
The word "energy" derives from the , which possibly appears for the first time in the work of Aristotle in the 4th century BC. In contrast to the modern definition, energeia was a qualitative philosophical concept, broad enough to include ideas such as happiness and pleasure.
In the late 17th century, Gottfried Leibniz proposed the idea of the , or living force, which defined as the product of the mass of an object and its velocity squared; he believed that total "vis viva" was conserved. To account for slowing due to friction, Leibniz theorized that thermal energy consisted of the random motion of the constituent parts of matter, a view shared by Isaac Newton, although it would be more than a century until this was generally accepted. The modern analog of this property, kinetic energy, differs from "vis viva" only by a factor of two.
In 1807, Thomas Young was possibly the first to use the term "energy" instead of "vis viva", in its modern sense. Gustave-Gaspard Coriolis described "kinetic energy" in 1829 in its modern sense, and in 1853, William Rankine coined the term "potential energy". The law of conservation of energy was also first postulated in the early 19th century, and applies to any isolated system. It was argued for some years whether heat was a physical substance, dubbed the caloric, or merely a physical quantity, such as momentum. In 1845 James Prescott Joule discovered the link between mechanical work and the generation of heat.
These developments led to the theory of conservation of energy, formalized largely by William Thomson (Lord Kelvin) as the field of thermodynamics. Thermodynamics aided the rapid development of explanations of chemical processes by Rudolf Clausius, Josiah Willard Gibbs, and Walther Nernst. It also led to a mathematical formulation of the concept of entropy by Clausius and to the introduction of laws of radiant energy by Jožef Stefan. According to Noether's theorem, the conservation of energy is a consequence of the fact that the laws of physics do not change over time. Thus, since 1918, theorists have understood that the law of conservation of energy is the direct mathematical consequence of the translational symmetry of the quantity conjugate to energy, namely time.
Units of measure.
In 1843 James Prescott Joule independently discovered the mechanical equivalent in a series of experiments. The most famous of them used the "Joule apparatus": a descending weight, attached to a string, caused rotation of a paddle immersed in water, practically insulated from heat transfer. It showed that the gravitational potential energy lost by the weight in descending was equal to the internal energy gained by the water through friction with the paddle.
In the International System of Units (SI), the unit of energy is the joule, named after James Prescott Joule. It is a derived unit. It is equal to the energy expended (or work done) in applying a force of one newton through a distance of one metre. However energy is also expressed in many other units not part of the SI, such as ergs, calories, British Thermal Units, kilowatt-hours and kilocalories, which require a conversion factor when expressed in SI units.
The SI unit of energy rate (energy per unit time) is the watt, which is a joule per second. Thus, one joule is one watt-second, and 3600 joules equal one watt-hour. The CGS energy unit is the erg and the imperial and US customary unit is the foot pound. Other energy units such as the electronvolt, food calorie or thermodynamic kcal (based on the temperature change of water in a heating process), and BTU are used in specific areas of science and commerce.
Scientific use.
Classical mechanics.
In classical mechanics, energy is a conceptually and mathematically useful property, as it is a conserved quantity. Several formulations of mechanics have been developed using energy as a core concept.
Work, a form of energy, is force times distance.
This says that the work (formula_2) is equal to the line integral of the force F along a path "C"; for details see the mechanical work article. Work and thus energy is frame dependent. For example, consider a ball being hit by a bat. In the center-of-mass reference frame, the bat does no work on the ball. But, in the reference frame of the person swinging the bat, considerable work is done on the ball.
The total energy of a system is sometimes called the Hamiltonian, after William Rowan Hamilton. The classical equations of motion can be written in terms of the Hamiltonian, even for highly complex or abstract systems. These classical equations have remarkably direct analogs in nonrelativistic quantum mechanics.
Another energy-related concept is called the Lagrangian, after Joseph-Louis Lagrange. This formalism is as fundamental as the Hamiltonian, and both can be used to derive the equations of motion or be derived from them. It was invented in the context of classical mechanics, but is generally useful in modern physics. The Lagrangian is defined as the kinetic energy "minus" the potential energy. Usually, the Lagrange formalism is mathematically more convenient than the Hamiltonian for non-conservative systems (such as systems with friction).
Noether's theorem (1918) states that any differentiable symmetry of the action of a physical system has a corresponding conservation law. Noether's theorem has become a fundamental tool of modern theoretical physics and the calculus of variations. A generalisation of the seminal formulations on constants of motion in Lagrangian and Hamiltonian mechanics (1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian; for example, dissipative systems with continuous symmetries need not have a corresponding conservation law.
Chemistry.
In the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structure, it is invariably accompanied by an increase or decrease of energy of the substances involved. Some energy is transferred between the surroundings and the reactants of the reaction in the form of heat or light; thus the products of a reaction may have more or less energy than the reactants. A reaction is said to be exergonic if the final state is lower on the energy scale than the initial state; in the case of endergonic reactions the situation is the reverse. Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy. The "speed" of a chemical reaction (at given temperature "T") is related to the activation energy "E", by the Boltzmann's population factor ethat is the probability of molecule to have energy greater than or equal to "E" at the given temperature "T". This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation.The activation energy necessary for a chemical reaction can be in the form of thermal energy.
Biology.
In biology, energy is an attribute of all biological systems from the biosphere to the smallest living organism. Within an organism it is responsible for growth and development of a biological cell or an organelle of a biological organism. Energy is thus often said to be stored by cells in the structures of molecules of substances such as carbohydrates (including sugars), lipids, and proteins, which release energy when reacted with oxygen in respiration. In human terms, the human equivalent (H-e) (Human energy conversion) indicates, for a given amount of energy expenditure, the relative quantity of energy needed for human metabolism, assuming an average human energy expenditure of 12,500 kJ per day and a basal metabolic rate of 80 watts. For example, if our bodies run (on average) at 80 watts, then a light bulb running at 100 watts is running at 1.25 human equivalents (100 ÷ 80) i.e. 1.25 H-e. For a difficult task of only a few seconds' duration, a person can put out thousands of watts, many times the 746 watts in one official horsepower. For tasks lasting a few minutes, a fit human can generate perhaps 1,000 watts. For an activity that must be sustained for an hour, output drops to around 300; for an activity kept up all day, 150 watts is about the maximum. The human equivalent assists understanding of energy flows in physical and biological systems by expressing energy units in human terms: it provides a "feel" for the use of a given amount of energy.
Sunlight is also captured by plants as "chemical potential energy" in photosynthesis, when carbon dioxide and water (two low-energy compounds) are converted into the high-energy compounds carbohydrates, lipids, and proteins. Plants also release oxygen during photosynthesis, which is utilized by living organisms as an electron acceptor, to release the energy of carbohydrates, lipids, and proteins. Release of the energy stored during photosynthesis as heat or light may be triggered suddenly by a spark, in a forest fire, or it may be made available more slowly for animal or human metabolism, when these molecules are ingested, and catabolism is triggered by enzyme action.
Any living organism relies on an external source of energy—radiation from the Sun in the case of green plants, chemical energy in some form in the case of animals—to be able to grow and reproduce. The daily 1500–2000 Calories (6–8 MJ) recommended for a human adult are taken as a combination of oxygen and food molecules, the latter mostly carbohydrates and fats, of which glucose (CHO) and stearin (CHO) are convenient examples. The food molecules are oxidised to carbon dioxide and water in the mitochondria
and some of the energy is used to convert ADP into ATP.
The rest of the chemical energy in O and the carbohydrate or fat is converted into heat: the ATP is used as a sort of "energy currency", and some of the chemical energy it contains is used for other metabolism when ATP reacts with OH groups and eventually splits into ADP and phosphate (at each stage of a metabolic pathway, some chemical energy is converted into heat). Only a tiny fraction of the original chemical energy is used for work:
It would appear that living organisms are remarkably inefficient (in the physical sense) in their use of the energy they receive (chemical energy or radiation), and it is true that most real machines manage higher efficiencies. In growing organisms the energy that is converted to heat serves a vital purpose, as it allows the organism tissue to be highly ordered with regard to the molecules it is built from. The second law of thermodynamics states that energy (and matter) tends to become more evenly spread out across the universe: to concentrate energy (or matter) in one specific place, it is necessary to spread out a greater amount of energy (as heat) across the remainder of the universe ("the surroundings"). Simpler organisms can achieve higher energy efficiencies than more complex ones, but the complex organisms can occupy ecological niches that are not available to their simpler brethren. The conversion of a portion of the chemical energy to heat at each step in a metabolic pathway is the physical reason behind the pyramid of biomass observed in ecology: to take just the first step in the food chain, of the estimated 124.7 Pg/a of carbon that is fixed by photosynthesis, 64.3 Pg/a (52%) are used for the metabolism of green plants, i.e. reconverted into carbon dioxide and heat.
Earth sciences.
In geology, continental drift, mountain ranges, volcanoes, and earthquakes are phenomena that can be explained in terms of energy transformations in the Earth's interior, while meteorological phenomena like wind, rain, hail, snow, lightning, tornadoes and hurricanes are all a result of energy transformations brought about by solar energy on the atmosphere of the planet Earth.
Sunlight may be stored as gravitational potential energy after it strikes the Earth, as (for example) water evaporates from oceans and is deposited upon mountains (where, after being released at a hydroelectric dam, it can be used to drive turbines or generators to produce electricity). Sunlight also drives many weather phenomena, save those generated by volcanic events. An example of a solar-mediated weather event is a hurricane, which occurs when large unstable areas of warm ocean, heated over months, give up some of their thermal energy suddenly to power a few days of violent air movement.
In a slower process, radioactive decay of atoms in the core of the Earth releases heat. This thermal energy drives plate tectonics and may lift mountains, via orogenesis. This slow lifting represents a kind of gravitational potential energy storage of the thermal energy, which may be later released to active kinetic energy in landslides, after a triggering event. Earthquakes also release stored elastic potential energy in rocks, a store that has been produced ultimately from the same radioactive heat sources. Thus, according to present understanding, familiar events such as landslides and earthquakes release energy that has been stored as potential energy in the Earth's gravitational field or elastic strain (mechanical potential energy) in rocks. Prior to this, they represent release of energy that has been stored in heavy atoms since the collapse of long-destroyed supernova stars created these atoms.
Cosmology.
In cosmology and astronomy the phenomena of stars, nova, supernova, quasars and gamma-ray bursts are the universe's highest-output energy transformations of matter. All stellar phenomena (including solar activity) are driven by various kinds of energy transformations. Energy in such transformations is either from gravitational collapse of matter (usually molecular hydrogen) into various classes of astronomical objects (stars, black holes, etc.), or from nuclear fusion (of lighter elements, primarily hydrogen). The nuclear fusion of hydrogen in the Sun also releases another store of potential energy which was created at the time of the Big Bang. At that time, according to theory, space expanded and the universe cooled too rapidly for hydrogen to completely fuse into heavier elements. This meant that hydrogen represents a store of potential energy that can be released by fusion. Such a fusion process is triggered by heat and pressure generated from gravitational collapse of hydrogen clouds when they produce stars, and some of the fusion energy is then transformed into sunlight.
Quantum mechanics.
In quantum mechanics, energy is defined in terms of the energy operator
as a time derivative of the wave function. The Schrödinger equation equates the energy operator to the full energy of a particle or a system. Its results can be considered as a definition of measurement of energy in quantum mechanics. The Schrödinger equation describes the space- and time-dependence of a slowly changing (non-relativistic) wave function of quantum systems. The solution of this equation for a bound system is discrete (a set of permitted states, each characterized by an energy level) which results in the concept of quanta. In the solution of the Schrödinger equation for any oscillator (vibrator) and for electromagnetic waves in a vacuum, the resulting energy states are related to the frequency by Planck's relation: formula_3 (where formula_4 is Planck's constant and formula_5 the frequency). In the case of an electromagnetic wave these energy states are called quanta of light or photons.
Relativity.
When calculating kinetic energy (work to accelerate a mass from zero speed to some finite speed) relativistically – using Lorentz transformations instead of Newtonian mechanics – Einstein discovered an unexpected by-product of these calculations to be an energy term which does not vanish at zero speed. He called it rest mass energy: energy which every mass must possess even when being at rest. The amount of energy is directly proportional to the mass of body:
where
For example, consider electron–positron annihilation, in which the rest mass of individual particles is destroyed, but the inertia equivalent of the system of the two particles (its invariant mass) remains (since all energy is associated with mass), and this inertia and invariant mass is carried off by photons which individually are massless, but as a system retain their mass. This is a reversible process – the inverse process is called pair creation – in which the rest mass of particles is created from energy of two (or more) annihilating photons. In this system the matter (electrons and positrons) is destroyed and changed to non-matter energy (the photons). However, the total system mass and energy do not change during this interaction.
In general relativity, the stress–energy tensor serves as the source term for the gravitational field, in rough analogy to the way mass serves as the source term in the non-relativistic Newtonian approximation.
It is not uncommon to hear that energy is "equivalent" to mass. It would be more accurate to state that every energy has an inertia and gravity equivalent, and because mass is a form of energy, then mass too has inertia and gravity associated with it.
In classical physics, energy is a scalar quantity, the canonical conjugate to time. In special relativity energy is also a scalar (although not a Lorentz scalar but a time component of the energy–momentum 4-vector). In other words, energy is invariant with respect to rotations of space, but not invariant with respect to rotations of space-time (= boosts).
Transformation.
Energy may be transformed between different forms at various efficiencies. Items that transform between these forms are called transducers. Examples of transducers include a battery, from chemical energy to electric energy; a dam: gravitational potential energy to kinetic energy of moving water (and the blades of a turbine) and ultimately to electric energy through an electric generator; or a heat engine, from heat to work.
There are strict limits to how efficiently heat can be converted into work in a cyclic process, e.g. in a heat engine, as described by Carnot's theorem and the second law of thermodynamics. However, some energy transformations can be quite efficient. The direction of transformations in energy (what kind of energy is transformed to what other kind) is often determined by entropy (equal energy spread among all available degrees of freedom) considerations. In practice all energy transformations are permitted on a small scale, but certain larger transformations are not permitted because it is statistically unlikely that energy or matter will randomly move into more concentrated forms or smaller spaces.
Energy transformations in the universe over time are characterized by various kinds of potential energy that has been available since the Big Bang later being "released" (transformed to more active types of energy such as kinetic or radiant energy) when a triggering mechanism is available. Familiar examples of such processes include nuclear decay, in which energy is released that was originally "stored" in heavy isotopes (such as uranium and thorium), by nucleosynthesis, a process ultimately using the gravitational potential energy released from the gravitational collapse of supernovae, to store energy in the creation of these heavy elements before they were incorporated into the solar system and the Earth. This energy is triggered and released in nuclear fission bombs or in civil nuclear power generation. Similarly, in the case of a chemical explosion, chemical potential energy is transformed to kinetic energy and thermal energy in a very short time. Yet another example is that of a pendulum. At its highest points the kinetic energy is zero and the gravitational potential energy is at maximum. At its lowest point the kinetic energy is at maximum and is equal to the decrease of potential energy. If one (unrealistically) assumes that there is no friction or other losses, the conversion of energy between these processes would be perfect, and the pendulum would continue swinging forever.
Energy is also transferred from potential energy (formula_7) to kinetic energy (formula_8) and then back to potential energy constantly. This is referred to as conservation of energy. In this closed system, energy cannot be created or destroyed; therefore, the initial energy and the final energy will be equal to each other. This can be demonstrated by the following:
The equation can then be simplified further since formula_9 (mass times acceleration due to gravity times the height) and formula_10 (half mass times velocity squared). Then the total amount of energy can be found by adding formula_11.
Conservation of energy and mass in transformation.
Energy gives rise to weight when it is trapped in a system with zero momentum, where it can be weighed. It is also equivalent to mass, and this mass is always associated with it. Mass is also equivalent to a certain amount of energy, and likewise always appears associated with it, as described in mass-energy equivalence. The formula "E" = "mc"², derived by Albert Einstein (1905) quantifies the relationship between rest-mass and rest-energy within the concept of special relativity. In different theoretical frameworks, similar formulas were derived by J. J. Thomson (1881), Henri Poincaré (1900), Friedrich Hasenöhrl (1904) and others (see Mass-energy equivalence#History for further information).
Matter may be converted to energy (and vice versa), but mass cannot ever be destroyed; rather, mass/energy equivalence remains a constant for both the matter and the energy, during any process when they are converted into each other. However, since formula_12 is extremely large relative to ordinary human scales, the conversion of ordinary amount of matter (for example, 1 kg) to other forms of energy (such as heat, light, and other radiation) can liberate tremendous amounts of energy (~formula_13 joules = 21 megatons of TNT), as can be seen in nuclear reactors and nuclear weapons. Conversely, the mass equivalent of a unit of energy is minuscule, which is why a loss of energy (loss of mass) from most systems is difficult to measure by weight, unless the energy loss is very large. Examples of energy transformation into matter (i.e., kinetic energy into particles with rest mass) are found in high-energy nuclear physics.
Reversible and non-reversible transformations.
Thermodynamics divides energy transformation into two kinds: reversible processes and irreversible processes. An irreversible process is one in which energy is dissipated (spread) into empty energy states available in a volume, from which it cannot be recovered into more concentrated forms (fewer quantum states), without degradation of even more energy. A reversible process is one in which this sort of dissipation does not happen. For example, conversion of energy from one type of potential field to another, is reversible, as in the pendulum system described above. In processes where heat is generated, quantum states of lower energy, present as possible excitations in fields between atoms, act as a reservoir for part of the energy, from which it cannot be recovered, in order to be converted with 100% efficiency into other forms of energy. In this case, the energy must partly stay as heat, and cannot be completely recovered as usable energy, except at the price of an increase in some other kind of heat-like increase in disorder in quantum states, in the universe (such as an expansion of matter, or a randomisation in a crystal).
As the universe evolves in time, more and more of its energy becomes trapped in irreversible states (i.e., as heat or other kinds of increases in disorder). This has been referred to as the inevitable thermodynamic heat death of the universe. In this heat death the energy of the universe does not change, but the fraction of energy which is available to do work through a heat engine, or be transformed to other usable forms of energy (through the use of generators attached to heat engines), grows less and less.
Conservation of energy.
According to conservation of energy, energy can neither be created (produced) nor destroyed by itself. It can only be transformed. The total inflow of energy into a system must equal the total outflow of energy from the system, plus the change in the energy contained within the system. Energy is subject to a strict global conservation law; that is, whenever one measures (or calculates) the total energy of a system of particles whose interactions do not depend explicitly on time, it is found that the total energy of the system always remains constant.
Richard Feynman said during a 1961 lecture:
Most kinds of energy (with gravitational energy being a notable exception) are subject to strict local conservation laws as well. In this case, energy can only be exchanged between adjacent regions of space, and all observers agree as to the volumetric density of energy in any given space. There is also a global law of conservation of energy, stating that the total energy of the universe cannot change; this is a corollary of the local law, but not vice versa.
This law is a fundamental principle of physics. As shown rigorously by Noether's theorem, the conservation of energy is a mathematical consequence of translational symmetry of time, a property of most phenomena below the cosmic scale that makes them independent of their locations on the time coordinate. Put differently, yesterday, today, and tomorrow are physically indistinguishable. This is because energy is the quantity which is canonical conjugate to time. This mathematical entanglement of energy and time also results in the uncertainty principle - it is impossible to define the exact amount of energy during any definite time interval. The uncertainty principle should not be confused with energy conservation - rather it provides mathematical limits to which energy can in principle be defined and measured.
Each of the basic forces of nature is associated with a different type of potential energy, and all types of potential energy (like all other types of energy) appears as system mass, whenever present. For example, a compressed spring will be slightly more massive than before it was compressed. Likewise, whenever energy is transferred between systems by any mechanism, an associated mass is transferred with it.
In quantum mechanics energy is expressed using the Hamiltonian operator. On any time scales, the uncertainty in the energy is by
which is similar in form to the Heisenberg Uncertainty Principle (but not really mathematically equivalent thereto, since "H" and "t" are not dynamically conjugate variables, neither in classical nor in quantum mechanics).
In particle physics, this inequality permits a qualitative understanding of virtual particles which carry momentum, exchange by which and with real particles, is responsible for the creation of all known fundamental forces (more accurately known as fundamental interactions). Virtual photons (which are simply lowest quantum mechanical energy state of photons) are also responsible for electrostatic interaction between electric charges (which results in Coulomb law), for spontaneous radiative decay of exited atomic and nuclear states, for the Casimir force, for van der Waals bond forces and some other observable phenomena.
Energy transfer.
Closed systems.
Energy transfer can be considered for the special case of systems which are closed to transfers of matter. The portion of the energy which is transferred by conservative forces over a distance is measured as the work the source system does on the receiving system. The portion of the energy which does not do work during the transfer is called heat. Energy can be transferred between systems in a variety of ways. Examples include the transmission of electromagnetic energy via photons, physical collisions which transfer kinetic energy, and the conductive transfer of thermal energy.
Energy is strictly conserved and is also locally conserved wherever it can be defined. In thermodynamics, for closed systems, the process of energy transfer is described by the first law:
where formula_15 is the amount of energy transferred, formula_2  represents the work done on the system, and formula_17 represents the heat flow into the system. As a simplification, the heat term, formula_17, is sometimes ignored, especially when the thermal efficiency of the transfer is high.
This simplified equation is the one used to define the joule, for example.
Open systems.
Beyond the constraints of closed systems, open systems can gain or lose energy in association with matter transfer (both of these process are illustrated by fueling an auto, a system which gains in energy thereby, without addition of either work or heat). Denoting this energy by formula_15, one may write
Thermodynamics.
Internal energy.
Internal energy is the sum of all microscopic forms of energy of a system. It is the energy needed to create the system. It is related to the potential energy, e.g., molecular structure, crystal structure, and other geometric aspects, as well as the motion of the particles, in form of kinetic energy. Thermodynamics is chiefly concerned with changes in internal energy and not its absolute value, which is impossible to determine with thermodynamics alone.
First law of thermodynamics.
The first law of thermodynamics asserts that energy (but not necessarily thermodynamic free energy) is always conserved and that heat flow is a form of energy transfer. For homogeneous systems, with a well-defined temperature and pressure, a commonly used corollary of the first law is that, for a system subject only to pressure forces and heat transfer (e.g., a cylinder-full of gas) without chemical changes, the differential change in the internal energy of the system (with a "gain" in energy signified by a positive quantity) is given as
where the first term on the right is the heat transferred into the system, expressed in terms of temperature "T" and entropy "S" (in which entropy increases and the change d"S" is positive when the system is heated), and the last term on the right hand side is identified as work done on the system, where pressure is "P" and volume "V" (the negative sign results since compression of the system requires work to be done on it and so the volume change, d"V", is negative when work is done on the system).
This equation is highly specific, ignoring all chemical, electrical, nuclear, and gravitational forces, effects such as advection of any form of energy other than heat and pV-work. The general formulation of the first law (i.e., conservation of energy) is valid even in situations in which the system is not homogeneous. For these cases the change in internal energy of a "closed" system is expressed in a general form by
where formula_22 is the heat supplied to the system and formula_23 is the work applied to the system.
Equipartition of energy.
The energy of a mechanical harmonic oscillator (a mass on a spring) is alternatively kinetic and potential. At two points in the oscillation cycle it is entirely kinetic, and alternatively at two other points it is entirely potential. Over the whole cycle, or over many cycles, net energy is thus equally split between kinetic and potential. This is called equipartition principle; total energy of a system with many degrees of freedom is equally split among all available degrees of freedom.
This principle is vitally important to understanding the behaviour of a quantity closely related to energy, called entropy. Entropy is a measure of evenness of a distribution of energy between parts of a system. When an isolated system is given more degrees of freedom (i.e., given new available energy states that are the same as existing states), then total energy spreads over all available degrees equally without distinction between "new" and "old" degrees. This mathematical result is called the second law of thermodynamics.

</doc>
<doc id="9653" url="https://en.wikipedia.org/wiki?curid=9653" title="Expected value">
Expected value

In probability theory, the expected value of a random variable is intuitively the long-run average value of repetitions of the experiment it represents. For example, the expected value of a six-sided dice roll is 3.5 because, roughly speaking, the average of an extremely large number of die rolls is practically always nearly equal to 3.5. Less roughly, the law of large numbers guarantees that the arithmetic mean of the values almost surely converges to the expected value as the number of repetitions goes to infinity. The expected value is also known as the expectation, mathematical expectation, EV, average, mean value, mean, or first moment.
More practically, the expected value of a discrete random variable is the probability-weighted average of all possible values. In other words, each possible value the random variable can assume is multiplied by its probability of occurring, and the resulting products are summed to produce the expected value. The same works for continuous random variables, except the sum is replaced by an integral and the probabilities by probability densities. The formal definition subsumes both of these and also works for distributions which are neither discrete nor continuous: the expected value of a random variable is the integral of the random variable with respect to its probability measure.
The expected value does not exist for random variables having some distributions with large "tails", such as the Cauchy distribution. For random variables such as these, the long-tails of the distribution prevent the sum/integral from converging.
The expected value is a key aspect of how one characterizes a probability distribution; it is one type of location parameter. By contrast, the variance is a measure of dispersion of the possible values of the random variable around the expected value. The variance itself is defined in terms of two expectations: it is the expected value of the squared deviation of the variable's value from the variable's expected value.
The expected value plays important roles in a variety of contexts. In regression analysis, one desires a formula in terms of observed data that will give a "good" estimate of the parameter giving the effect of some explanatory variable upon a dependent variable. The formula will give different estimates using different samples of data, so the estimate it gives is itself a random variable. A formula is typically considered good in this context if it is an unbiased estimator—that is, if the expected value of the estimate (the average value it would give over an arbitrarily large number of separate samples) can be shown to equal the true value of the desired parameter.
In decision theory, and in particular in choice under uncertainty, an agent is described as making an optimal choice in the context of incomplete information. For risk neutral agents, the choice involves using the expected values of uncertain quantities, while for risk averse agents it involves maximizing the expected value of some objective function such as a von Neumann-Morgenstern utility function. One example of using expected value in reaching optimal decisions is the Gordon-Loeb Model of information security investment. According to the model, one can conclude that the amount a firm spends to protect information should generally be only a small fraction of the expected loss (i.e., the expected value of the loss resulting from a cyber/information security breach).
Definition.
Univariate discrete random variable, finite case.
Suppose random variable "X" can take value "x" with probability "p", value "x" with probability "p", and so on, up to value "x" with probability "p". Then the expectation of this random variable "X" is defined as
Since all probabilities "p" add up to one ("p" + "p" + ... + "p" = 1), the expected value can be viewed as the weighted average, with "p"’s being the weights:
If all outcomes "x" are equally likely (that is, "p" = "p" = ... = "p"), then the weighted average turns into the simple average. This is intuitive: the expected value of a random variable is the average of all values it can take; thus the expected value is what one expects to happen "on average". If the outcomes "x" are not equally probable, then the simple average must be replaced with the weighted average, which takes into account the fact that some outcomes are more likely than the others. The intuition however remains the same: the expected value of "X" is what one expects to happen "on average".
Example 1. Let "X" represent the outcome of a roll of a fair six-sided . More specifically, "X" will be the number of pips showing on the top face of the after the toss. The possible values for "X" are 1, 2, 3, 4, 5, and 6, all equally likely (each having the probability of ). The expectation of "X" is
If one rolls the "n" times and computes the average (arithmetic mean) of the results, then as "n" grows, the average will almost surely converge to the expected value, a fact known as the strong law of large numbers. One example sequence of ten rolls of the is 2, 3, 1, 2, 5, 6, 2, 2, 2, 6, which has the average of 3.1, with the distance of 0.4 from the expected value of 3.5. The convergence is relatively slow: the probability that the average falls within the range is 21.6% for ten rolls, 46.1% for a hundred rolls and 93.7% for a thousand rolls. See the figure for an illustration of the averages of longer sequences of rolls of the and how they converge to the expected value of 3.5. More generally, the rate of convergence can be roughly quantified by e.g. Chebyshev's inequality and the Berry-Esseen theorem.
Example 2. The roulette game consists of a small ball and a wheel with 38 numbered pockets around the edge. As the wheel is spun, the ball bounces around randomly until it settles down in one of the pockets. Suppose random variable "X" represents the (monetary) outcome of a $1 bet on a single number ("straight up" bet). If the bet wins (which happens with probability in American roulette), the payoff is $35; otherwise the player loses the bet. The expected profit from such a bet will be
i.e. the bet of $1 stands to lose $0.0526, so its expected value is $0.9474.
Univariate discrete random variable, countable case.
Let "X" be a discrete random variable taking values "x", "x", ... with probabilities "p", "p", ... respectively. Then the expected value of this random variable is the infinite sum
provided that this series converges absolutely (that is, the sum must remain finite if we were to replace all "x"'s with their absolute values). If this series does not converge absolutely, we say that the expected value of "X" does not exist.
For example, suppose random variable "X" takes values 1, −2, 3, −4, ..., with respective probabilities , , , , ..., where is a normalizing constant that ensures the probabilities sum up to one. Then the infinite sum
converges and its sum is equal to . However it would be incorrect to claim that the expected value of "X" is equal to this number—in fact E'X' does not exist, as this series does not converge absolutely (see harmonic series).
Univariate continuous random variable.
If the probability distribution of formula_7 admits a probability density function formula_8, then the expected value can be computed as
General definition.
In general, if "X" is a random variable defined on a probability space , then the expected value of "X", denoted by E'X', 〈"X"〉, "X" or E'X', is defined as the Lebesgue integral 
When this integral exists, it is defined as the expectation of "X". Not all random variables have a finite expected value, since the integral may not converge absolutely; furthermore, for some it is not defined at all (e.g., Cauchy distribution). Two variables with the same probability distribution will have the same expected value, if it is defined.
It follows directly from the discrete case definition that if "X" is a constant random variable, i.e. "X" = "b" for some fixed real number "b", then the expected value of "X" is also "b".
The expected value of a measurable function of "X", "g"("X"), given that "X" has a probability density function "f"("x"), is given by the inner product of "f" and "g":
This is sometimes called the law of the unconscious statistician. Using representations as Riemann–Stieltjes integral and integration by parts the formula can be restated as
As a special case let "α" denote a positive real number. Then
In particular, if α = 1 and , then this reduces to
where "F" is the cumulative distribution function of "X". This last identity is an instance of what, in a non-probabilistic setting, has been called the layer cake representation.
The law of the unconscious statistician applies also to a measurable function "g" of several random variables "X", ... "X" having a joint density "f":
Properties.
Constants.
The expected value of a constant is equal to the constant itself; i.e., if formula_17 is a constant, then formula_18.
Monotonicity.
If formula_7 and formula_20 are random variables such that formula_21 almost surely, then formula_22.
Linearity.
The expected value operator (or expectation operator) formula_23 is linear in the sense that
The second result is valid even if formula_7 is not statistically independent of formula_20. Combining the results from previous three equations, we can see that
for any two random variables formula_7 and formula_20 (which need to be defined on the same probability space) and any real numbers "a", "b" and "c".
Iterated expectation.
Iterated expectation for discrete random variables.
For any two discrete random variables formula_7, formula_20 one may define the conditional expectation:
which means that E"Y" = "y' is a function of "y". Let "g"("y") be that function of "y"; then the notation E"Y' is then a random variable in its own right, equal to "g"("Y").
Lemma. Then the expectation of "X" satisfies:
Proof.
The left-hand side of this equation is referred to as the "iterated expectation". The equation is sometimes called the "tower rule" or the "tower property"; it is treated under law of total expectation.
Iterated expectation for continuous random variables.
In the continuous case, the results are completely analogous. The definition of conditional expectation would use inequalities, density functions, and integrals to replace equalities, mass functions, and summations, respectively. However, the main result still holds:
Inequality.
If a random variable "X" is always less than or equal to another random variable "Y", the expectation of "X" is less than or equal to that of "Y":
In particular, if we set Y to |"X"| we know "X" ≤ "Y" and −"X" ≤ "Y". Therefore, we know E'X' ≤ E'Y' and E"X' ≤ E'Y'. From the linearity of expectation we know −E'X' ≤ E'Y'. Therefore, the absolute value of expectation of a random variable is less than or equal to the expectation of its absolute value:
This is a special case of Jensen's inequality.
Non-multiplicativity.
If one considers the joint probability density function of "X" and "Y", say "j(x,y)", then the expectation of "XY" is
In general, the expected value operator is not multiplicative, i.e. E'XY' is not necessarily equal to E'X'·E'Y'. In fact, the amount by which multiplicativity fails is called the covariance:
Thus multiplicativity holds precisely when , in which case "X" and "Y" are said to be uncorrelated (independent variables are a notable case of uncorrelated variables).
Now if "X" and "Y" are independent, then by definition where "f" and "g" are the marginal PDFs for "X" and "Y". Then
and .
Observe that independence of "X" and "Y" is required only to write "j"("x", "y") = "f"("x")"g"("y"), and this is required to establish the second equality above. The third equality follows from a basic application of the Fubini-Tonelli theorem.
Functional non-invariance.
In general, with the exception of linear functions, the expectation operator and functions of random variables do not commute; that is
A notable inequality concerning this topic is Jensen's inequality, involving expected values of convex (or concave) functions.
Relation to characteristic function.
The probability density function of a scalar random variable "X" is related to its characteristic function "φ" by the inversion formula:
We can use this inversion formula in expected value of a function "g(X)" to obtain
Changing the order of integration, we get
where
is the Fourier transform of formula_45 This can also be seen as a direct consequence of Plancherel theorem.
Uses and applications.
It is possible to construct an expected value equal to the probability of an event by taking the expectation of an indicator function that is one if the event has occurred and zero otherwise. This relationship can be used to translate properties of expected values into properties of probabilities, e.g. using the law of large numbers to justify estimating probabilities by frequencies.
The expected values of the powers of "X" are called the moments of "X"; the moments about the mean of "X" are expected values of powers of "X" − E'X'. The moments of some random variables can be used to specify their distributions, via their moment generating functions.
To empirically estimate the expected value of a random variable, one repeatedly measures observations of the variable and computes the arithmetic mean of the results. If the expected value exists, this procedure estimates the true expected value in an unbiased manner and has the property of minimizing the sum of the squares of the residuals (the sum of the squared differences between the observations and the estimate). The law of large numbers demonstrates (under fairly mild conditions) that, as the size of the sample gets larger, the variance of this estimate gets smaller.
This property is often exploited in a wide variety of applications, including general problems of statistical estimation and machine learning, to estimate (probabilistic) quantities of interest via Monte Carlo methods, since most quantities of interest can be written in terms of expectation, e.g. formula_46 where formula_47 is the indicator function for set formula_48, i.e. formula_49.
Expected values can also be used to compute the variance, by means of the computational formula for the variance
A very important application of the expectation value is in the field of quantum mechanics. The expectation value of a quantum mechanical operator formula_51 operating on a quantum state vector formula_52 is written as formula_53. The uncertainty in formula_51 can be calculated using the formula formula_55.
Expectation of matrices.
If "X" is an "m" × "n" matrix, then the expected value of the matrix is defined as the matrix of expected values:
This is utilized in covariance matrices.
Formulas for special cases.
Discrete distribution taking only non-negative integer values.
When a random variable takes only values in {0, 1, 2, 3, ...} we can use the following formula for computing its expectation (even when the expectation is infinite):
Proof.
Interchanging the order of summation, we have
This result can be a useful computational shortcut. For example, suppose we toss a coin where the probability of heads is "p". How many tosses can we expect until the first heads (not including the heads itself)? Let "X" be this number. We are counting only the tails and not the heads which ends the experiment; in particular, we can have "X" = 0. The expectation of "X" may be computed by formula_60. This is because, when the first "i" tosses yield tails, the number of tosses is at least "i". The last equality used the formula for a geometric progression, formula_61 where formula_62.
Continuous distribution taking non-negative values.
Analogously with the discrete case above, when a continuous random variable "X" takes only non-negative values, we can use the following formula for computing its expectation (even when the expectation is infinite):
Proof: It is first assumed that "X" has a density "f"("x"). We present two techniques:
and the bracket vanishes because (see Cumulative distribution function#Derived functions)
formula_65 as formula_66
In case no density exists, it is seen that
History.
The idea of the expected value originated in the middle of the 17th century from the study of the so-called problem of points, which seeks to divide the stakes "in a fair way" between two players who have to end their game before it's properly finished. This problem had been debated for centuries, and many conflicting proposals and solutions had been suggested over the years, when it was posed in 1654 to Blaise Pascal by French writer and amateur mathematician Chevalier de Méré. de Méré claimed that this problem couldn't be solved and that it showed just how flawed mathematics was when it came to its application to the real world. Pascal, being a mathematician, was provoked and determined to solve the problem once and for all. He began to discuss the problem in a now famous series of letters to Pierre de Fermat. Soon enough they both independently came up with a solution. They solved the problem in different computational ways but their results were identical because their computations were based on the same fundamental principle. The principle is that the value of a future gain should be directly proportional to the chance of getting it. This principle seemed to have come naturally to both of them. They were very pleased by the fact that they had found essentially the same solution and this in turn made them absolutely convinced they had solved the problem conclusively. However, they did not publish their findings. They only informed a small circle of mutual scientific friends in Paris about it.
Three years later, in 1657, a Dutch mathematician Christiaan Huygens, who had just visited Paris, published a treatise (see ) "De ratiociniis in ludo aleæ" on probability theory. In this book he considered the problem of points and presented a solution based on the same principle as the solutions of Pascal and Fermat. Huygens also extended the concept of expectation by adding rules for how to calculate expectations in more complicated situations than the original problem (e.g., for three or more players). In this sense this book can be seen as the first successful attempt of laying down the foundations of the theory of probability.
In the foreword to his book, Huygens wrote: "It should be said, also, that for some time some of the best mathematicians of France have occupied themselves with this kind of calculus so that no one should attribute to me the honour of the first invention. This does not belong to me. But these savants, although they put each other to the test by proposing to each other many questions difficult to solve, have hidden their methods. I have had therefore to examine and go deeply for myself into this matter by beginning with the elements, and it is impossible for me for this reason to affirm that I have even started from the same principle. But finally I have found that my answers in many cases do not differ from theirs." (cited by ). Thus, Huygens learned about de Méré's Problem in 1655 during his visit to France; later on in 1656 from his correspondence with Carcavi he learned that his method was essentially the same as Pascal's; so that before his book went to press in 1657 he knew about Pascal's priority in this subject.
Neither Pascal nor Huygens used the term "expectation" in its modern sense. In particular, Huygens writes: "That my Chance or Expectation to win any thing is worth just such a Sum, as wou'd procure me in the same Chance and Expectation at a fair Lay. ... If I expect a or b, and have an equal Chance of gaining them, my Expectation is worth ." More than a hundred years later, in 1814, Pierre-Simon Laplace published his tract "Théorie analytique des probabilités", where the concept of expected value was defined explicitly:
The use of the letter "E" to denote expected value goes back to W.A. Whitworth in 1901, who used a script E. The symbol has become popular since for English writers it meant "Expectation", for Germans "Erwartungswert", for Spanish "Esperanza matemática" and for French "Espérance mathématique".

</doc>
<doc id="9656" url="https://en.wikipedia.org/wiki?curid=9656" title="Electric light">
Electric light

An electric light is a device that produces visible light by the flow of electric current. It is the most common form of artificial lighting and is essential to modern society, providing interior lighting for buildings and exterior light for evening and nighttime activities. In technical usage, a replaceable component that produces light from electricity is called a lamp. Compact lamps are commonly called light bulbs; for example, the incandescent light bulb. Lamps usually have a base made of ceramic, metal, glass or plastic, which secures the lamp in the socket of a light fixture. The electrical connection to the socket may be made with a screw-thread base, two metal pins, two metal caps or a bayonet cap.
The three main categories of electric lights are incandescent lamps, which produce light by a filament heated white-hot by electric current, gas-discharge lamps, which produce light by means of an electric arc through a gas, and LED lamps, which produce light by a flow of electrons across a band gap in a semiconductor.
Before electric lighting became common in the early 20th century, people used candles, gas lights, oil lamps, and fires. Humphry Davy developed the first incandescent light in 1802, followed by the first practical electric arc light in 1806. By the 1870s, Davy's arc lamp had been successfully commercialized, and was used to light many public spaces. The development of a steadily glowing filament suitable for interior lighting took longer, but by the early twentieth century Thomas Edison and others had successfully developed options, replacing the arc light with incandescents.
The energy efficiency of electric lighting has increased radically since the first demonstration of arc lamps and the incandescent light bulb of the 19th century. Modern electric light sources come in a profusion of types and sizes adapted to a myriad of applications. Most modern electric lighting is powered by centrally generated electric power, but lighting may also be powered by mobile or standby electric generators or battery systems. Battery-powered lights, usually called "flashlights" or "torches", are used for portability and as backups when the main lights fail.
Types.
Types of electric lighting include:
Different types of lights have vastly differing efficiencies and color of light. 
The most efficient source of electric light is the low-pressure sodium lamp. It produces, for all practical purposes, a monochromatic orange/yellow light, which gives a similarly monochromatic perceprtion of any illuminated scene. For this reason, it is generally reserved for outdoor public lighting usages. Low-pressure sodium lights are favoured for public lighting by astronomers, since the light pollution that they generate can be easily filtered, contrary to broadband or continuous spectra.
Incandescent light bulb.
The modern incandescent lightbulb, with a coiled filament of tungsten, was commercialized in the 1920s developed from the carbon filament lamp introduced in about 1880. As well as bulbs for normal illumination, there is a very wide range, including low voltage, low-power types often used as components in equipment, but now largely displaced by LEDs
There is currently interest in banning some types of filament lamp in some countries, such as Australia planning to ban standard incandescent light bulbs by 2010, because they are inefficient at converting electricity to light. Sri Lanka has already banned importing filament bulbs because of high use of electricity and less light. Less than 3% of the input energy is converted into usable light. Nearly all of the input energy ends up as heat that, in warm climates, must then be removed from the building by ventilation or air conditioning, often resulting in more energy consumption. In colder climates where heating and lighting is required during the cold and dark winter months, the heat byproduct has at least some value.
Halogen lamp.
Halogen lamps are usually much smaller than standard incandescents, because for successful operation a bulb temperature over 200 °C is generally necessary. For this reason, most have a bulb of fused silica (quartz), but sometimes aluminosilicate glass. This is often sealed inside an additional layer of glass. The outer glass is a safety precaution, reducing UV emission and because halogen bulbs can occasionally explode during operation. One reason is if the quartz bulb has oily residue from fingerprints. The risk of burns or fire is also greater with bare bulbs, leading to their prohibition in some places unless enclosed by the luminaire.
Those designed for 12 V or 24 V operation have compact filaments, useful for good optical control, also they have higher efficiencies (lumens per watt) and better lives than non-halogen types. The light output remains almost constant throughout life.
Fluorescent lamp.
Fluorescent lamps consist of a glass tube that contains mercury vapour or argon under low pressure. Electricity flowing through the tube causes the gases to give off ultraviolet energy. The inside of the tubes are coated with phosphors that give off visible light when struck by ultraviolet energy. have much higher efficiency than Incandescent lamps. For the same amount of light generated, they typically use around one-quarter to one-third the power of an incandescent.
LED lamp.
Solid state LEDs have been popular as indicator lights since the 1970s. In recent years, efficacy and output have risen to the point where LEDs are now being used in lighting applications as well as in decorative applications such as holiday lighting.
Indicator LEDs are known for their extremely long life, up to 100,000 hours, but lighting LEDs are operated much less conservatively, and consequently have shorter lives.
LED technology is useful for lighting designers because of its low power consumption, low heat generation, instantaneous on/off control, and in the case of single color LEDs, continuity of color throughout the life of the diode and relatively low cost of manufacture.
Carbon arc lamp.
Carbon arc lamps consist of two carbon rod electrodes in open air, supplied by a current-limiting ballast. The electric arc is struck by touching the rods then separating them. The ensuing arc heats the carbon tips to white heat. These lamps have higher efficiency than filament lamps, but the carbon rods are short lived and require constant adjustment in use. The lamps produce significant ultra-violet output, they require ventilation when used indoors, and due to their intensity they need protecting from direct sight.
Invented by Humphry Davy around 1805, the carbon arc was the first practical electric light. They were used commercially beginning in the 1870s for large building and street lighting until they were superseded in the early 20th century by the incandescent light. Carbon arc lamps operate at high powers and produce high intensity white light. They also are a point source of light. They remained in use in limited applications that required these properties, such as movie projectors, stage lighting, and searchlights, until after World War 2.
Discharge lamp.
A discharge lamp has a glass or silica envelope containing two metal electrodes separated by a gas. Gases used include, neon, argon, xenon, sodium, metal halide, and mercury.
The core operating principle is much the same as the carbon arc lamp, but the term 'arc lamp' is normally used to refer to carbon arc lamps, with more modern types of gas discharge lamp normally called discharge lamps.
With some discharge lamps, very high voltage is used to strike the arc. This requires an electrical circuit called an igniter, which is part of the ballast circuitry. After the arc is struck, the internal resistance of the lamp drops to a low level, and the ballast limits the current to the operating current. Without a ballast, excess current would flow, causing rapid destruction of the lamp.
Some lamp types contain a little neon, which permits striking at normal running voltage, with no external ignition circuitry. Low pressure sodium lamps operate this way.
The simplest ballasts are just an inductor, and are chosen where cost is the deciding factor, such as street lighting. More advanced electronic ballasts may be designed to maintain constant light output over the life of the lamp, may drive the lamp with a square wave to maintain completely flicker-free output, and shut down in the event of certain faults.
Lamp life expectancy.
Life expectancy is defined as the number of hours of operation for a lamp until 50% of them fail. This means that it is possible for some lamps to fail after a short amount of time and for some to last significantly longer than the rated lamp life. This is an average (median) life expectancy. Production tolerances as low as 1% can create a variance of 25% in lamp life. For LEDs, lamp life is when 50% of lamps have lumen output drop to 70% or less.
Lamps are also sensitive to switching cycles. The rapid heating of a lamp filament or electrodes when a lamp is turned on is the most stressful event on the lamp. Most test cycles have the lamps on for 3 hours and then off for 20 minutes. (Some standard had to be used since it is unknown how the lamp will be used by consumers.) This switching cycle repeats until the lamps fail and the data is recorded. If switching is increased to only 1 hour on, the lamp life is usually reduced because the number of times the lamp has been turned on has increased. Rooms with frequent switching (bathroom, bedrooms, etc.) can expect much shorter lamp life than what is printed on the box.
Public lighting.
The total amount of artificial light (especially from street light) is sufficient for cities to be easily visible at night from the air, and from space. This light is the source of light pollution that burdens astronomers and others.
Uses other than illumination.
Electric lamps can be used as heat sources, for example in incubators and toys such as the Easy-Bake Oven.
Tungsten filament lamps have long been used as fast-acting thermistors in electronic circuits. Popular uses have included:
Lamp circuit symbols.
In circuit diagrams lamps usually are shown as symbols. There are two main types of symbols, these are:

</doc>
<doc id="9657" url="https://en.wikipedia.org/wiki?curid=9657" title="Edgar Rice Burroughs">
Edgar Rice Burroughs

Edgar Rice Burroughs (September 1, 1875 – March 19, 1950) was an American writer best known for his creations of the jungle hero Tarzan and the heroic Mars adventurer John Carter, although he produced works in many genres.
Biography.
Early life.
Burroughs was born on September 1, 1875, in Chicago, Illinois (he later lived for many years in the suburb of Oak Park), the fourth son of businessman and Civil War veteran Major George Tyler Burroughs (1833–1913) and his wife Mary Evaline (Zieger) Burroughs (1840–1920). His middle name is from his paternal grandmother, Mary Rice Burroughs (1802–c.1870). Burroughs was of almost entirely English ancestry, all of which had been in North America since the early colonial era. Through his grandmother Mary Rice, he was descended from Edmund Rice, one of the English Puritans who moved to Massachusetts in the early colonial period. He once remarked "I can trace my ancestry back to Deacon Edmund Rice,". The Burroughs side of the family was also of English origins and also emigrated to Massachusetts at around the same time. Many of his ancestors fought in the American Revolution. He had other ancestors who settled in Virginia during the colonial period and he often stressed that side of the family, seeing it as more romantic and war-like.
Burroughs was educated at a number of local schools, and during the Chicago influenza epidemic in 1891, he spent a half year at his brother's ranch on the Raft River in Idaho. He then attended the Phillips Academy in Andover, Massachusetts, and then the Michigan Military Academy. Graduating in 1895, and failing the entrance exam for the United States Military Academy (West Point), he ended up as an enlisted soldier with the 7th U.S. Cavalry in Fort Grant, Arizona Territory. After being diagnosed with a heart problem and thus ineligible to serve, he was discharged in 1897.
Adulthood.
After his discharge, Burroughs worked a number of different jobs. He drifted and worked on a ranch in Idaho. Then, Burroughs found work at his father's firm in 1899. He married his childhood sweetheart Emma Hulbert (1876-1944) in January 1900. In 1904, he left his job and worked less regularly, first in Idaho, then in Chicago.
By 1911, after seven years of low wages, he was working as a pencil-sharpener wholesaler and began to write fiction. By this time, Burroughs and Emma had two children, Joan (1908–72), who would later marry Tarzan film actor James Pierce, and Hulbert (1909–91). During this period, he had copious spare time and he began reading many pulp fiction magazines. In 1929 he recalled thinking that
In 1913, Burroughs and Emma had their third and last child, John Coleman Burroughs (1913–79).
In the 1920s Burroughs became a pilot, purchasing a Security Airster S-1, and encouraged his family to learn to fly.
Burroughs divorced Emma in 1934, and in 1935 he married former actress Florence Gilbert Dearholt, the former wife of his friend, Ashton Dearholt. Burroughs adopted the Dearholts' two children. He and Florence divorced in 1942.
Burroughs was in his late 60s and was in Honolulu at the time of the Japanese attack on Pearl Harbor. Despite his age, he applied for and received permission to become a war correspondent, becoming one of the oldest U.S. war correspondents during World War II. This period of his life is mentioned in William Brinkley's bestselling novel "Don't Go Near the Water".
Death.
After the war ended, Burroughs moved back to Encino, California, where after many health problems, he died of a heart attack on March 19, 1950, having written almost 80 novels. He is buried at Tarzana Los Angeles County in California, US.
American actor Reid Markel is Burroughs' great-great-grandson.
The Science Fiction Hall of Fame inducted Burroughs in 2003.
Literary career.
Aiming his work at the pulps, Burroughs had his first story, "Under the Moons of Mars", serialized by Frank Munsey in the February to July 1912 issues of "The All-Story" – under the name "Norman Bean" to protect his reputation. "Under the Moons of Mars" inaugurated the Barsoom series and earned Burroughs US$400 ($ today). It was first published as a book by A. C. McClurg of Chicago in 1917, entitled "A Princess of Mars", after three Barsoom sequels had appeared as serials, and McClurg had published the first four serial Tarzan novels as books.
Burroughs soon took up writing full-time and by the time the run of "Under the Moons of Mars" had finished he had completed two novels, including "Tarzan of the Apes", published from October 1912 and one of his most successful series.
Burroughs also wrote popular science fiction and fantasy stories involving Earthly adventurers transported to various planets (notably Barsoom, Burroughs's fictional name for Mars, and Amtor, his fictional name for Venus), lost islands, and into the interior of the hollow earth in his "Pellucidar" stories, as well as westerns and historical romances. Along with "All-Story", many of his stories were published in "The Argosy" magazine.
Tarzan was a cultural sensation when introduced. Burroughs was determined to capitalize on Tarzan's popularity in every way possible. He planned to exploit Tarzan through several different media including a syndicated Tarzan comic strip, movies and merchandise. Experts in the field advised against this course of action, stating that the different media would just end up competing against each other. Burroughs went ahead, however, and proved the experts wrong – the public wanted Tarzan in whatever fashion he was offered. Tarzan remains one of the most successful fictional characters to this day and is a cultural icon.
In either 1915 or 1919, Burroughs purchased a large ranch north of Los Angeles, California, which he named "Tarzana." The citizens of the community that sprang up around the ranch voted to adopt that name when their community, Tarzana, California was formed in 1927. Also, the unincorporated community of Tarzan, Texas, was formally named in 1927 when the US Postal Service accepted the name, reputedly coming from the popularity of the first (silent) "Tarzan of the Apes" film, starring Elmo Lincoln, and an early "Tarzan" comic strip.
In 1923 Burroughs set up his own company, Edgar Rice Burroughs, Inc., and began printing his own books through the 1930s.
Reception and criticism.
In a "Paris Review" interview, Ray Bradbury said of Burroughs that "Edgar Rice Burroughs never would have looked upon himself as a social mover and shaker with social obligations. But as it turns out – and I love to say it because it upsets everyone terribly – Burroughs is probably the most influential writer in the entire history of the world." Bradbury continued that "By giving romance and adventure to a whole generation of boys, Burroughs caused them to go out and decide to become special."
Few critical books have arisen concerning Burroughs. From an academic standpoint, the most helpful are Erling Holtsmark's two books: "Tarzan and Tradition" and "Edgar Rice Burroughs"; Stan Galloway's "The Teenage Tarzan: A Literary Analysis of Edgar Rice Burroughs' "Jungle Tales of Tarzan; and Richard Lupoff's two books: "Master of Adventure: Edgar Rice Burroughs" and "Barsoom: Edgar Rice Burroughs and the Martian Vision". Galloway was identified by James Gunn (author) as "one of the half-dozen finest Burroughs scholars in the world"; Galloway called Holtsmark his "most important predecessor."
These three texts have been published by various houses in one or two volumes. Adding to the confusion, some editions have the original (significantly longer) introduction to Part I from the first publication as a magazine serial, and others have the shorter version from the first book publication, which included all three parts under the title "The Moon Maid".

</doc>
<doc id="9658" url="https://en.wikipedia.org/wiki?curid=9658" title="Eugène Viollet-le-Duc">
Eugène Viollet-le-Duc

Eugène Emmanuel Viollet-le-Duc (27 January 1814 – 17 September 1879) was a French architect and theorist, famous for his interpretive "restorations" of medieval buildings. Born in Paris, he was a major Gothic Revival architect.
His works were largely restorative and few of his independent building designs were ever realised. Strongly contrary to the prevailing Beaux-Arts architectural trend of his time, much of his design work was largely derided by his contemporaries. He was the architect hired to design the internal structure of the Statue of Liberty, but died before the project was completed.
Early years.
Viollet-le-Duc's father was a civil servant living in Paris who collected books; his mother's Friday salons were attended by Stendhal and Sainte-Beuve. His mother's brother, Étienne-Jean Delécluze, "a painter in the mornings, a scholar in the evenings", was largely in charge of the young man's education. Viollet-le-Duc was trendy philosophically: republican, anti-clerical, rebellious, he built a barricade in the July Revolution of 1830 and refused to enter the École des Beaux-Arts. Instead he opted in favor of direct practical experience in the architectural offices of Jacques-Marie Huvé and Achille Leclère.
Architectural restorer.
Restoration work.
During the early 1830s, a popular sentiment for the restoration of medieval buildings developed in France. Viollet-le-Duc, returning during 1835 from study in Italy, was commissioned by Prosper Mérimée to restore the Romanesque abbey of Vézelay. This work was the first of a long series of restorations; Viollet-le-Duc's restorations at Notre Dame de Paris with Jean-Baptiste Lassus brought him national attention. His other main works include Mont Saint-Michel, Carcassonne, Roquetaillade castle and Pierrefonds.
Viollet-le-Duc's "restorations" frequently combined historical fact with creative modification. For example, under his supervision, Notre Dame was not only cleaned and restored but also "updated", gaining its distinctive third tower (a type of flèche) in addition to other smaller changes. Another of his most famous restorations, the medieval fortified town of Carcassonne, was similarly enhanced, gaining atop each of its many wall towers a set of pointed roofs that are actually more typical of northern France.
At the same time, in the cultural atmosphere of the Second Empire theory necessarily became diluted in practice: Viollet-le-Duc provided a Gothic reliquary for the relic of the Crown of Thorns at Notre-Dame in 1862, and yet Napoleon III also commissioned designs for a luxuriously appointed railway carriage from Viollet-le-Duc, in 14th-century Gothic style.
Among his restorations were:
Restoration of the Château de Pierrefonds, reinterpreted by Viollet-le-Duc for Napoleon III, was interrupted by the departure of the Emperor in 1870.
Influence on historic preservation.
Basic intervention theories of historic preservation are framed in the dualism of the retention of the status quo versus a "restoration" that creates something that never actually existed in the past. John Ruskin was a strong proponent of the former sense, while his contemporary, Viollet-le-Duc, advocated for the latter instance. Viollet-le-Duc wrote that restoration is a "means to reestablish buildin to a finished state, which may in fact never have actually existed at any given time." The type of restoration employed by Viollet-le-Duc, in its English form as Victorian restoration, was decried by Ruskin as "a destruction out of which no remnants can be gathered: a destruction accompanied with false description of the thing destroyed."
This argument is still a current one when restoration is being considered for a building or landscape. In removing layers of history from a building, information and age value are also removed which can never be recreated. However, adding features to a building, as Viollet-le-Duc also did, can be more appealing to modern viewers.
Publications.
Throughout his career Viollet-le-Duc made notes and drawings, not only for the buildings he was working on, but also on Romanesque, Gothic and Renaissance buildings that were to be soon demolished. His notes were helpful in his published works. His study of medieval and Renaissance periods was not limited to architecture, but extended to furniture, clothing, musical instruments, armament, geology and so forth.
All this work was published, first in serial, and then as full-scale books, as:
Architectural theory and new building projects.
Viollet-le-Duc is considered by many to be the first theorist of modern architecture. Sir John Summerson wrote that "there have been two supremely eminent theorists in the history of European architecture - Leon Battista Alberti and Eugène Viollet-le-Duc."
His architectural theory was largely based on finding the ideal forms for specific materials, and using these forms to create buildings. His writings centered on the idea that materials should be used 'honestly'. He believed that the outward appearance of a building should reflect the rational construction of the building. In "Entretiens sur l'architecture", Viollet-le-Duc praised the Greek temple for its rational representation of its construction. For him, "Greek architecture served as a model for the correspondence of structure and appearance." There is speculation that this philosophy was heavily influenced by the writings of John Ruskin, who championed honesty of materials as one of the seven main emphases of architecture.
In several unbuilt projects for new buildings, Viollet-le-Duc applied the lessons he had derived from Gothic architecture, applying its rational structural systems to modern building materials such as cast iron. He also examined organic structures, such as leaves and animal skeletons, for inspiration. He was especially interested in the wings of bats, an influence represented by his Assembly Hall project.
Viollet-le-Duc's drawings of iron trusswork were innovative for the time. Many of his designs emphasizing iron would later influence the Art Nouveau movement, most noticeably in the work of Hector Guimard, Victor Horta, Antoni Gaudí or Hendrik Petrus Berlage. His writings inspired some American architects, including Frank Furness, John Wellborn Root, Louis Sullivan, and Frank Lloyd Wright.
Military career and influence.
Viollet-le-Duc had a second career in the military, primarily in the defence of Paris during the Franco-Prussian War (1870–71). He was so influenced by the conflict that during his later years he described the idealized defense of France by the analogy of the military history of Le Roche-Pont, an imaginary castle, in his work "Histoire d'une Forteresse" ("Annals of a Fortress", twice translated into English). Accessible and well researched, it is partly fictional.
"Annals of a Fortress" strongly influenced French military defensive thinking. Viollet-le-Duc's critique of the effect of artillery (applying his practical knowledge from the 1870–1871 war) is so complete that it accurately describes the principles applied to the defence of France until World War II. The physical results of his theories are present in the fortification of Verdun prior to World War I and the Maginot Line prior to World War II. His theories are also represented by the French military theory of "Deliberate Advance", such that artillery and a strong system of fortresses in the rear of an army are essential.
Legacy.
Some of his restorations, such as that of the Château de Pierrefonds, have become very controversial because they were not intended so much to recreate a historical situation accurately as to create a "perfect building" of medieval style: "to restore an edifice", he observed in the "Dictionnaire raisonné", "is not to maintain it, repair or rebuild it, but to re-establish it in a complete state that may never have existed at a particular moment". The idea and the very word "restoration" applied to architecture Viollet-le-Duc considered part of a modern innovation. Modern conservation practice considers Viollet-le-Duc's restorations too free, too personal, too interpretive, but some of the monuments he restored might have been lost otherwise.
The English architect Benjamin Bucknall (1833–95) was a devotee of Viollet-le-Duc and during 1874 to 1881 translated several of his publications into English to popularise his principles in Great Britain. The later works of the English designer and architect William Burges were greatly influenced by Viollet-le-Duc, most strongly in Burges's designs for his own home, The Tower House in London's Holland Park district and Burges's designs for Castell Coch near Cardiff, Wales.
An exhibition, "Eugène Viollet-le-Duc 1814–1879" was presented in Paris, 1965, and a larger, centennial exhibition, 1980.
Viollet-le-Duc was the subject of a Google Doodle on January 27, 2014.
Later life.
In 1874 Viollet-le-Duc resigned as diocesan architect of Paris, and was succeeded by his contemporary, Paul Abadie. In his old age, Viollet-le-Duc relocated to Lausanne, Switzerland, where he constructed a villa (since destroyed). He died there in 1879.

</doc>
<doc id="9659" url="https://en.wikipedia.org/wiki?curid=9659" title="Endocarditis">
Endocarditis

Endocarditis is an inflammation of the inner layer of the heart, the endocardium. It usually involves the heart valves. Other structures that may be involved include the interventricular septum, the chordae tendineae, the mural endocardium, or the surfaces of intracardiac devices. Endocarditis is characterized by lesions, known as "vegetations", which is a mass of platelets, fibrin, microcolonies of microorganisms, and scant inﬂammatory cells. In the subacute form of infective endocarditis, the vegetation may also include a center of granulomatous tissue, which may fibrose or calcify.
There are several ways to classify endocarditis. The simplest classification is based on cause: either "infective" or "non-infective", depending on whether a microorganism is the source of the inflammation or not. Regardless, the diagnosis of endocarditis is based on clinical features, investigations such as an echocardiogram, and blood cultures demonstrating the presence of endocarditis-causing microorganisms. Signs and symptoms include: fever, chills, sweating, malaise, weakness, anorexia, weight loss, splenomegaly, flu-like feeling, cardiac murmur, heart failure, petechia of anterior trunk, Janeway's lesions, etc.
Cause.
Infective.
Since the valves of the heart do not receive any dedicated blood supply, defensive immune mechanisms (such as white blood cells) cannot directly reach the valves via the bloodstream. If an organism (such as bacteria) attaches to a valve surface and forms a vegetation, the host immune response is blunted. The lack of blood supply to the valves also has implications on treatment, since drugs also have difficulty reaching the infected valve.
Normally, blood flows smoothly past these valves. If they have been damaged (from rheumatic fever, for example) the risk of bacteria attachment is increased.
Rheumatic fever is common worldwide and responsible for many cases of damaged heart valves. Chronic rheumatic heart disease is characterized by repeated inflammation with fibrinous resolution. The cardinal anatomic changes of the valve include leaflet thickening, commissural fusion, and shortening and thickening of the tendinous cords. The recurrence of rheumatic fever is relatively common in the absence of maintenance of low dose antibiotics, especially during the first three to five years after the first episode. Heart complications may be long-term and severe, particularly if valves are involved. While rheumatic fever since the advent of routine penicillin administration for Strep throat has become less common in developed countries, in the older generation and in much of the less-developed world, valvular disease (including mitral valve prolapse, reinfection in the form of valvular endocarditis, and valve rupture) from undertreated rheumatic fever continues to be a problem.
In an Indian hospital between 2004 and 2005, 4 of 24 endocarditis patients failed to demonstrate classic vegetation. All had rheumatic heart disease and presented with prolonged fever. All had severe eccentric mitral regurgitation. (One had severe aortic regurgitation also.) One had flail posterior mitral leaflet.
Non-infective.
Nonbacterial thrombotic endocarditis (NBTE), also called marantic endocarditis is most commonly found on previously undamaged valves. As opposed to infective endocarditis, the vegetations in NBTE are small, sterile, and tend to aggregate along the edges of the valve or the cusps. Also unlike infective endocarditis, NBTE does not cause an inflammation response from the body. NBTE usually occurs during a hypercoagulable state such as system-wide bacterial infection, or pregnancy, though it is also sometimes seen in patients with venous catheters. NBTE may also occur in patients with cancers, particularly mucinous adenocarcinoma where Trousseau syndrome can be encountered. Typically NBTE does not cause many problems on its own, but parts of the vegetations may break off and embolize to the heart or brain, or they may serve as a focus where bacteria can lodge, thus causing infective endocarditis.
Another form of sterile endocarditis is termed Libman-Sacks endocarditis; this form occurs more often in patients with lupus erythematosus and is thought to be due to the deposition of immune complexes. Like NBTE, Libman-Sacks endocarditis involves small vegetations, while infective endocarditis is composed of large vegetations. These immune complexes precipitate an inflammation reaction, which helps to differentiate it from NBTE. Also unlike NBTE, Libman-Sacks endocarditis does not seem to have a preferred location of deposition and may form on the undersurfaces of the valves or even on the endocardium.
Diagnostics.
Examination of suspected infective endocarditis includes a detailed examination of the patient, complete history taking, and especially careful cardiac auscultation, various blood tests, ECG, cardiac ultrasound (echocardiography). Analysis of blood helps reveal the typical signs of inflammation (increased erythrocyte sedimentation rate, leukocytosis). Two or more separate blood cultures are normally drawn. Negative blood cultures, however, do not exclude the diagnosis of infective endocarditis. 
Echocardiography (through the anterior chest wall or transesophageal), plays a decisive role in the diagnosis by reliably establishing the presence of microbial vegetation and the degree of valvular dysfunction affecting the pumping function of the heart.

</doc>
<doc id="9660" url="https://en.wikipedia.org/wiki?curid=9660" title="Euler's sum of powers conjecture">
Euler's sum of powers conjecture

Euler's conjecture is a disproved conjecture in mathematics related to Fermat's last theorem. It was proposed by Leonhard Euler in 1769. It states that for all integers and greater than 1, if the sum of th powers of non-zero integers is itself a th power, then is greater than or equal to .
In symbols, the conjecture falsely states that if
where and are non-zero integers, then .
The conjecture represents an attempt to generalize Fermat's last theorem, which is the special case : if , then .
Although the conjecture holds for the case (which follows from Fermat's last theorem for the third powers), it was disproved for and . It is unknown whether the conjecture fails or holds for any value .
Background.
Euler had an equality for four fourth powers ; this however is not a counterexample because no term is isolated on one side of the equation. He also provided a complete solution to the four cubes problem as in Plato's number or the taxicab number 1729. The general solution for:
is
where and are any integers.
Counterexamples.
Euler's conjecture was disproven by L. J. Lander and T. R. Parkin in 1966 when, through a direct computer search on a CDC 6600, they found a counterexample for . A total of three primitive (that is, in which the summands do not all have a common factor) counterexamples are known:
In 1986, Noam Elkies found a method to construct an infinite series of counterexamples for the case. His smallest counterexample was
A particular case of Elkies' solutions can be reduced to the identity
where
This is an elliptic curve with a rational point at . From this initial rational point, one can compute an infinite collection of others. Substituting into the identity and removing common factors gives the numerical example cited above.
In 1988, Roger Frye found the smallest possible counterexample 
for by a direct computer search using techniques suggested by Elkies. This solution is the only one with values of the variables below 1,000,000.
Generalizations.
In 1967, L. J. Lander, T. R. Parkin, and John Selfridge conjectured that if and
where are positive integers for all and , then . In the special case , the conjecture states that if
(under the conditions given above) then .
The special case may be described as the problem of giving a partition of a perfect power into few like powers. For and or , there are many known solutions. Some of these are listed below. There are no solutions for where .

</doc>
<doc id="9662" url="https://en.wikipedia.org/wiki?curid=9662" title="Book of Exodus">
Book of Exodus

The Book of Exodus or, simply, Exodus (from , "éxodos", meaning "going out"; , "Shəmōṯ", "Names", the second word of the beginning of the text: "These are the names of the sons of Israel" ), is the second book of the Torah and the Hebrew Bible (the Old Testament).
The book tells how the Israelites leave slavery in Egypt through the strength of Yahweh, the God who has chosen Israel as his people. Led by their prophet Moses they journey through the wilderness to Mount Sinai, where Yahweh promises them the land of Canaan (the "Promised Land") in return for their faithfulness. Israel enters into a covenant with Yahweh who gives them their laws and instructions to build the Tabernacle, the means by which he will come here from heaven and dwell with them and lead them in a holy war to possess the land, and then give them peace.
Traditionally ascribed to Moses himself, modern scholarship sees the book as initially a product of the Babylonian exile (6th century BCE), based on earlier written and oral traditions, with final revisions in the Persian post-exilic period (5th century BCE). Carol Meyers in her commentary on Exodus suggests that it is arguably the most important book in the Bible, as it presents the defining features of Israel's identity: memories of a past marked by hardship and escape, a binding covenant with God, who chooses Israel, and the establishment of the life of the community and the guidelines for sustaining it.
Structure.
There is no agreement among scholars on the structure of Exodus. One strong possibility is that it is a diptych (i.e., divided into two parts), with the division between parts 1 and 2 at the crossing of the Red Sea or at the beginning of the theophany (appearance of God) in chapter 19. On this plan, the first part tells of God's rescue of his people from Egypt and their journey under his care to Sinai (chapters 1–19) and the second tells of the covenant between them (chapters 20–40).
Summary.
Egypt's Pharaoh, fearful of the Israelites' numbers, orders that all newborn boys be thrown into the Nile. A Levite woman saves her baby by setting him adrift on the river Nile in an ark of bulrushes. Pharaoh's daughter finds the child, names him Moses, and brings him up as her own. But Moses is aware of his origins, and one day, when grown, he kills an Egyptian overseer who is beating a Hebrew slave and has to flee into Midian. There he marries the daughter of Jethro a priest of Midian, and encounters God in a burning bush. Moses asks God for his name: God replies: "I AM that I AM." God tells Moses to return to Egypt and lead the Hebrews into Canaan, the land promised to Abraham.
Moses returns to Egypt and fails to convince the Pharaoh to release the Israelites. God smites the Egyptians with 10 terrible plagues (Plagues of Egypt) including a river of blood, many frogs, and the death of first-born sons. Moses leads the Israelites out of bondage after a final chase scene ensues when the Pharaoh reneges on his coerced consent (Crossing the Red Sea). The desert proves arduous, and the Israelites complain and long for Egypt, but God provides manna and miraculous water for them. The Israelites arrive at the mountain of God, where Moses' father-in-law Jethro visits Moses; at his suggestion Moses appoints judges over Israel. God asks whether they will agree to be his people. They accept. The people gather at the foot of the mountain, and with thunder and lightning, fire and clouds of smoke, and the sound of trumpets, and the trembling of the mountain, God appears on the peak, and the people see the cloud and hear the voice r possibly "sound&quot of God. Moses and Aaron are told to ascend the mountain. God pronounces the Ten Commandments (the Ethical Decalogue) in the hearing of all Israel. Moses goes up the mountain into the presence of God, who pronounces the Covenant Code (a detailed code of ritual and civil law), and promises Canaan to them if they obey. Moses comes down the mountain and writes down God's words and the people agree to keep them. God calls Moses up the mountain with Aaron and the elders of Israel, and they feast in the presence of God. God calls Moses up the mountain to receive a set of stone tablets containing the law, and he and Joshua go up, leaving Aaron below.
God gives Moses instructions for the construction of the tabernacle so that God could dwell permanently among his chosen people, as well as instructions for the priestly vestments, the altar and its appurtenances, the procedure to be used to ordain the priests, and the daily sacrifices to be offered. Aaron is appointed as the first hereditary high priest. God gives Moses the two tablets of stone containing the words of the ten commandments, written with the "finger of God".
While Moses is with God, Aaron makes a golden calf, which the people worship. God informs Moses of their apostasy and threatens to kill them all, but relents when Moses pleads for them. Moses comes down from the mountain, smashes the stone tablets in anger, and commands the Levites to massacre the unfaithful Israelites. God commands Moses to make two new tablets on which He will personally write the words that were on the first tablets. Moses ascends the mountain, God dictates the Ten Commandments (the Ritual Decalogue), and Moses writes them on the tablets.
Moses descends from the mountain, and his face is transformed, so that from that time onwards he has to hide his face with a veil. Moses assembles the Hebrews and repeats to them the commandments he has received from God, which are to keep the Sabbath and to construct the Tabernacle. "And all the construction of the Tabernacle of the Tent of Meeting was finished, and the children of Israel did according to everything that God had commanded Moses", and from that time God dwelt in the Tabernacle and ordered the travels of the Hebrews.
Composition.
Authorship.
Jewish and Christian tradition viewed Moses as the author of Exodus and the entire Pentateuch, but by the end of the 19th century the increasing awareness of the discrepancies, inconsistencies, repetitions and other features of the Pentateuch had led scholars to abandon this idea. In approximate round dates, the process which produced Exodus and the Pentateuch probably began around 600 BCE when existing oral and written traditions were brought together to form books recognisable as those we know, reaching their final form as unchangeable sacred texts around 400 BCE. It is clear that the main outlines of the narrative were certainly known long before the seventh century BCE, in the allusions to the Exodus and the wandering in the wilderness contained in the oracles of the prophets Amos and Hosea a full century before.
Genre and sources.
The story of the exodus is the founding myth of Israel, telling how the Israelites were delivered from slavery by Yahweh and therefore belong to him through the Mosaic covenant. The Book of Exodus is not historical narrative in any modern sense: modern history writing requires the critical evaluation of sources, and does not accept God as a cause of events, but in Exodus, everything is presented as the work of God, who appears frequently in person, and the historical setting is only very hazily sketched. The purpose of the book is not to record what really happened, but to reflect the historical experience of the exile community in Babylon and later Jerusalem, facing foreign captivity and the need to come to terms with their understanding of God.
Although mythical elements are not so prominent in Exodus as in Genesis, some writers claim ancient legends have an influence on the book's content: for example, the story of the infant Moses's salvation from the Nile may have some basis in an earlier legend of king Sargon, while the story of the parting of the Red Sea trades on Mesopotamian creation mythology. Similarly, the Covenant Code (the law code in Exodus 20:22–23:33) has some similarities in both content and structure with the Laws of Hammurabi. These influences serve to reinforce the conclusion that the Book of Exodus originated in the exiled Jewish community of 6th-century Babylon, but not all the sources are Mesopotamian: the story of Moses's flight to Midian following the murder of the Egyptian overseer may draw on the Egyptian "Story of Sinuhe".
Themes.
Salvation.
Biblical scholars describe the Bible's theologically-motivated history writing as "salvation history", meaning a history of God's saving actions that give identity to Israel – the promise of offspring and land to the ancestors, the exodus from Egypt (in which God saves Israel from slavery), the wilderness wandering, the revelation at Sinai, and the hope for the future life in the promised land.
Theophany.
A theophany is a manifestation (appearance) of a god – in the Bible, an appearance of the God of Israel, accompanied by storms – the earth trembles, the mountains quake, the heavens pour rain, thunder peals and lightning flashes. The theophany in Exodus begins "the third day" from their arrival at Sinai in chapter 19: Yahweh and the people meet at the mountain, God appears in the storm and converses with Moses, giving him the Ten Commandments while the people listen. The theophany is therefore a public experience of divine law.
The second half of Exodus marks the point at which, and describes the process through which, God's theophany becomes a permanent presence for Israel via the Tabernacle. That so much of the book (chapters 25–31, 35–40) is spent describing the plans of the Tabernacle demonstrates the importance it played in the perception of Second Temple Judaism at the time of the text's redaction by the Priestly writers: the Tabernacle is the place where God is physically present, where, through the priesthood, Israel could be in direct, literal communion with him.
Covenant.
The heart of Exodus is the Sinaitic covenant. A covenant is a legal document binding two parties to take on certain obligations towards each other. There are several covenants in the Bible, and in each case they exhibit at least some of the elements found in real-life treaties of the ancient Middle East: a preamble, historical prologue, stipulations, deposition and reading, list of witnesses, blessings and curses, and ratification by animal sacrifice. Biblical covenants, in contrast to Eastern covenants in general, are between a god, Yahweh, and a people, Israel, instead of between a strong ruler and a weaker vassal.
Election of Israel.
Israel is elected for salvation because the "sons of Israel" are "the firstborn son" of the God of Israel, descended through Shem and Abraham to the chosen line of Jacob whose name is changed to Israel. The goal of the divine plan as revealed in Exodus is a return to humanity's state in Eden, so that God can dwell with the Israelites as he had with Adam and Eve through the Ark and Tabernacle, which together form a model of the universe; in later Abrahamic religions this came to be interpreted as Israel being the guardian of God's plan for humanity, to bring "God's creation blessing to mankind" begun in Adam.

</doc>
<doc id="9663" url="https://en.wikipedia.org/wiki?curid=9663" title="Electronics">
Electronics

Electronics is the science of how to control electric energy, energy in which the electrons have a fundamental role. Electronics deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes and integrated circuits, and associated passive electrical components and interconnection technologies. Commonly, electronic devices contain circuitry consisting primarily or exclusively of active semiconductors supplemented with passive elements; such a circuit is described as an electronic circuit.
The nonlinear behaviour of active components and their ability to control electron flows makes amplification of weak signals possible, and electronics is widely used in information processing, telecommunication, and signal processing. The ability of electronic devices to act as switches makes digital information processing possible. Interconnection technologies such as circuit boards, electronics packaging technology, and other varied forms of communication infrastructure complete circuit functionality and transform the mixed components into a regular working system.
Electronics is distinct from electrical and electro-mechanical science and technology, which deal with the generation, distribution, switching, storage, and conversion of electrical energy to and from other energy forms using wires, motors, generators, batteries, switches, relays, transformers, resistors, and other passive components. This distinction started around 1906 with the invention by Lee De Forest of the triode, which made electrical amplification of weak radio signals and audio signals possible with a non-mechanical device. Until 1950 this field was called "radio technology" because its principal application was the design and theory of radio transmitters, receivers, and vacuum tubes.
Today, most electronic devices use semiconductor components to perform electron control. The study of semiconductor devices and related technology is considered a branch of solid-state physics, whereas the design and construction of electronic circuits to solve practical problems come under electronics engineering. This article focuses on engineering aspects of electronics.
Branches of electronics.
Electronics has branches as follows:
Electronic devices and components.
An electronic component is any physical entity in an electronic system used to affect the electrons or their associated fields in a manner consistent with the intended function of the electronic system. Components are generally intended to be connected together, usually by being soldered to a printed circuit board (PCB), to create an electronic circuit with a particular function (for example an amplifier, radio receiver, or oscillator). Components may be packaged singly, or in more complex groups as integrated circuits. Some common electronic components are capacitors, inductors, resistors, diodes, transistors, etc. Components are often categorized as active (e.g. transistors and thyristors) or passive (e.g. resistors, diodes, inductors and capacitors).
History of electronic components.
Vacuum tubes (Thermionic valves) were one of the earliest electronic components. They were almost solely responsible for the electronics revolution of the first half of the Twentieth Century. They took electronics from parlor tricks and gave us radio, television, phonographs, radar, long distance telephony and much more. They played a leading role in the field of microwave and high power transmission as well as television receivers until the middle of the 1980s. Since that time, solid state devices have all but completely taken over. Vacuum tubes are still used in some specialist applications such as high power RF amplifiers, cathode ray tubes, specialist audio equipment, guitar amplifiers and some microwave devices.
In April 1955 the IBM 608 was the first IBM product to use transistor circuits without any vacuum tubes and is believed to be the world's first all-transistorized calculator to be manufactured for the commercial market. The 608 contained more than 3,000 germanium transistors. Thomas J. Watson Jr. ordered all future IBM products to use transistors in their design. From that time on transistors were almost exclusively used for computer logic and peripherals.
Types of circuits.
Circuits and components can be divided into two groups: analog and digital. A particular device may consist of circuitry that has one or the other or a mix of the two types.
Analog circuits.
Most analog electronic appliances, such as radio receivers, are constructed from combinations of a few types of basic circuits. Analog circuits use a continuous range of voltage or current as opposed to discrete levels as in digital circuits.
The number of different analog circuits so far devised is huge, especially because a 'circuit' can be defined as anything from a single component, to systems containing thousands of components.
Analog circuits are sometimes called linear circuits although many non-linear effects are used in analog circuits such as mixers, modulators, etc. Good examples of analog circuits include vacuum tube and transistor amplifiers, operational amplifiers and oscillators.
One rarely finds modern circuits that are entirely analog. These days analog circuitry may use digital or even microprocessor techniques to improve performance. This type of circuit is usually called "mixed signal" rather than analog or digital.
Sometimes it may be difficult to differentiate between analog and digital circuits as they have elements of both linear and non-linear operation. An example is the comparator which takes in a continuous range of voltage but only outputs one of two levels as in a digital circuit. Similarly, an overdriven transistor amplifier can take on the characteristics of a controlled switch having essentially two levels of output. In fact, many digital circuits are actually implemented as variations of analog circuits similar to this example—after all, all aspects of the real physical world are essentially analog, so digital effects are only realized by constraining analog behavior.
Digital circuits.
Digital circuits are electric circuits based on a number of discrete voltage levels. Digital circuits are the most common physical representation of Boolean algebra, and are the basis of all digital computers. To most engineers, the terms "digital circuit", "digital system" and "logic" are interchangeable in the context of digital circuits.
Most digital circuits use a binary system with two voltage levels labeled "0" and "1". Often logic "0" will be a lower voltage and referred to as "Low" while logic "1" is referred to as "High". However, some systems use the reverse definition ("0" is "High") or are current based. Quite often the logic designer may reverse these definitions from one circuit to the next as he sees fit to facilitate his design. The definition of the levels as "0" or "1" is arbitrary.
Ternary (with three states) logic has been studied, and some prototype computers made.
Computers, electronic clocks, and programmable logic controllers (used to control industrial processes) are constructed of digital circuits. Digital signal processors are another example.
Building blocks:
Highly integrated devices:
Heat dissipation and thermal management.
Heat generated by electronic circuitry must be dissipated to prevent immediate failure and improve long term reliability. Heat dissipation is mostly achieved by passive conduction/convection. Means to achieve greater dissipation include heat sinks and fans for air cooling, and other forms of computer cooling such as water cooling. These techniques use convection, conduction, and radiation of heat energy.
Noise.
Electronic noise is defined as unwanted disturbances superposed on a useful signal that tend to obscure its information content. Noise is not the same as signal distortion caused by a circuit. Noise is associated with all electronic circuits. Noise may be electromagnetically or thermally generated, which can be decreased by lowering the operating temperature of the circuit. Other types of noise, such as shot noise cannot be removed as they are due to limitations in physical properties.
Electronics theory.
Mathematical methods are integral to the study of electronics. To become proficient in electronics it is also necessary to become proficient in the mathematics of circuit analysis.
Circuit analysis is the study of methods of solving generally linear systems for unknown variables such as the voltage at a certain node or the current through a certain branch of a network. A common analytical tool for this is the SPICE circuit simulator.
Also important to electronics is the study and understanding of electromagnetic field theory.
Electronics lab.
Due to the complex nature of electronics theory, laboratory experimentation is an important part of the development of electronic devices. These experiments are used to test or verify the engineer’s design and detect errors. Historically, electronics labs have consisted of electronics devices and equipment located in a physical space, although in more recent years the trend has been towards electronics lab simulation software, such as CircuitLogix, Multisim, and PSpice.
Computer aided design (CAD).
Today's electronics engineers have the ability to design circuits using premanufactured building blocks such as power supplies, semiconductors (i.e. semiconductor devices, such as transistors), and integrated circuits. Electronic design automation software programs include schematic capture programs and printed circuit board design programs. Popular names in the EDA software world are NI Multisim, Cadence (ORCAD), EAGLE PCB and Schematic, Mentor (PADS PCB and LOGIC Schematic), Altium (Protel), LabCentre Electronics (Proteus), gEDA, KiCad and many others.
Construction methods.
Many different methods of connecting components have been used over the years. For instance, early electronics often used point to point wiring with components attached to wooden breadboards to construct circuits. Cordwood construction and wire wrap were other methods used. Most modern day electronics now use printed circuit boards made of materials such as FR4, or the cheaper (and less hard-wearing) Synthetic Resin Bonded Paper (SRBP, also known as Paxoline/Paxolin (trade marks) and FR2) - characterised by its brown colour. Health and environmental concerns associated with electronics assembly have gained increased attention in recent years, especially for products destined to the European Union, with its Restriction of Hazardous Substances Directive (RoHS) and Waste Electrical and Electronic Equipment Directive (WEEE), which went into force in July 2006.
Degradation.
Rasberry crazy ants have been known to consume the insides of electrical wiring, and nest inside of electronics; they prefer DC to AC currents. This behavior is not well understood by scientists.

</doc>
<doc id="9664" url="https://en.wikipedia.org/wiki?curid=9664" title="Erewhon">
Erewhon

Erewhon: or, Over the Range () is a novel by Samuel Butler which was first published anonymously in 1872. The title is also the name of a country, supposedly discovered by the protagonist. In the novel, it is not revealed where Erewhon is, but it is clear that it is a fictional country. Butler meant the title to be read as "nowhere" backwards even though the letters "h" and "w" are transposed, as it would have been pronounced in his day (and still is in some dialects of English). The book is a satire on Victorian society.
The first few chapters of the novel dealing with the discovery of Erewhon are in fact based on Butler's own experiences in New Zealand where, as a young man, he worked as a sheep farmer on Mesopotamia Station for about four years (1860–64), and explored parts of the interior of the South Island and which he wrote about in his "A First Year in Canterbury Settlement" (1863).
Content.
The greater part of the book consists of a description of Erewhon. The nature of this nation is intended to be ambiguous. At first glance, Erewhon appears to be a Utopia, yet it soon becomes clear that this is far from the case. Yet for all the failings of Erewhon, it is also clearly not a dystopia, such as that depicted in George Orwell's "Nineteen Eighty-Four". As a satirical utopia, "Erewhon" has sometimes been compared to "Gulliver's Travels" (1726), a classic novel by Jonathan Swift; the image of Utopia in this latter case also bears strong parallels with the self-view of the British Empire at the time. It can also be compared to the William Morris novel, "News from Nowhere".
"Erewhon" satirises various aspects of Victorian society, including criminal punishment, religion and anthropocentrism. For example, according to Erewhonian law, offenders are treated as if they were ill, whereas ill people are looked upon as criminals. Another feature of Erewhon is the absence of machines; this is due to the widely shared perception by the Erewhonians that they are potentially dangerous. This last aspect of "Erewhon" reveals the influence of Charles Darwin's evolution theory; Butler had read "On the Origin of Species" soon after it was published in 1859.
The Book of the Machines.
Butler developed the three chapters of "Erewhon" that make up "The Book of the Machines" from a number of articles that he had contributed to "The Press", which had just begun publication in Christchurch, New Zealand, beginning with "Darwin among the Machines" (1863). Butler was the first to write about the possibility that machines might develop consciousness by Darwinian Selection. Many dismissed this as a joke; but, in his preface to the second edition, Butler wrote, "I regret that reviewers have in some cases been inclined to treat the chapters on Machines as an attempt to reduce Mr. Darwin's theory to an absurdity. Nothing could be further from my intention, and few things would be more distasteful to me than any attempt to laugh at Mr. Darwin."
Reception.
After its first release, this book sold far better than any of Butler's other works, perhaps because the British public assumed that the anonymous author was some better-known figure (the favourite being Lord Lytton, who had published "The Coming Race" two years previously). In a 1945 broadcast, George Orwell praised the book and said that when Butler wrote "Erewhon" it needed "imagination of a very high order to see that machinery could be dangerous as well as useful." He recommended the novel, though not its sequel, "Erewhon Revisited".
Influence and legacy.
Today scientists and philosophers seriously debate whether computers and robots could develop a kind of consciousness (artificial intelligence, AI), and organic interaction (artificial life) similar to or exceeding that of human beings. This is also a popular theme in science-fiction novels and movies; some raise the same question ("Dune's" "Butlerian Jihad", for example, which was named such as a reference to Erewhon), while others explore what the relationship between human beings and machines with artificial intelligence would be, and even whether AI is desirable. However, it should be noted that Butler wrote of machines developing consciousness by "natural selection", not artificially, although machine algorithms are approaching a level of autonomy which could be considered natural. 
The French philosopher Gilles Deleuze used ideas from Butler's book at various points in the development of his philosophy of difference. In "Difference and Repetition" (1968), Deleuze refers to what he calls "Ideas" as "erewhons." "Ideas are not concepts," he explains, but rather "a form of eternally positive differential multiplicity, distinguished from the identity of concepts." "Erewhon" refers to the "nomadic distributions" that pertain to simulacra, which "are not universals like the categories, nor are they the "hic et nunc" or "now here", the diversity to which categories apply in representation." "Erewhon," in this reading, is "not only a disguised "no-where" but a rearranged "now-here"."
In his collaboration with Félix Guattari, "Anti-Oedipus" (1972), Deleuze draws on Butler's "The Book of the Machines" to "go beyond" the "usual polemic between vitalism and mechanism" as it relates to their concept of "desiring-machines":
A reference to "Erewhon" and specifically "The Book of Machines" opens Miguel de Unamuno's short story, "Mecanópolis,". This story was written in Spanish and tells of a man who visits a city (called Mecanópolis) which is inhabited solely by machines.
George B. Dyson uses the heading of Butler's original article in "Darwin Among the Machines: The Evolution of Global Intelligence" (1998) ISBN 0-7382-0030-1.
Fritz Leiber's extensive tales of "Fafhrd and the Gray Mouser", written mostly in the 1960s and 1970s, take place on a world called Nehwon ("No When" backwards), a homage to Butler as well as a reference to the occasional contemporary and futuristic elements added to the medieval milieu of the stories.
In Anne McCaffrey's 1988 novel "Nimisha's Ship", the heroine Nimisha is pulled through a wormhole to the far side of the galaxy, and names the planet she settles on "Erehwon" in reference to an "old earth story" that several characters try, but fail, to remember. McCaffrey does not transpose the "h" and "w" as did Butler.
In David Weber's Honorverse series, the planet Erewhon was initially settled by interstellar criminals as a front for organised crime, with many of its place names referencing 21st century laundry appliances.
In 1994, a group of ex-Yugoslavian writers in Amsterdam, who had established the PEN centre of Yugoslav Writers in Exile, published a single issue of a literary journal "Erewhon".
One of New Zealand's largest sheep stations located near where Butler lived is named "Erewhon" in his honour.

</doc>
<doc id="9665" url="https://en.wikipedia.org/wiki?curid=9665" title="Ectopia (medicine)">
Ectopia (medicine)

In medicine, an ectopia is a displacement or malposition of an organ or other body part. Most ectopias are congenital, but some may happen later in life.

</doc>
<doc id="9667" url="https://en.wikipedia.org/wiki?curid=9667" title="Entorhinal cortex">
Entorhinal cortex

The entorhinal cortex (EC) (ento = interior, rhino = nose, entorhinal = interior to the rhinal sulcus) is an area of the brain located in the medial temporal lobe and functioning as a hub in a widespread network for memory and navigation. The EC is the main interface between the hippocampus and neocortex. The EC-hippocampus system plays an important role in declarative (autobiographical/episodic/semantic) memories and in particular spatial memories including memory formation, memory consolidation, and memory optimization in sleep. The EC is also responsible for the pre-processing (familiarity) of the input signals in the reflex nictitating membrane response of classical trace conditioning, the association of impulses from the eye and the ear occurs in the entorhinal cortex.
Anatomy.
In rodents, the EC is located at the caudal end of the temporal lobe. In primates it is located at the rostral end of the temporal lobe and stretches dorsolaterally. It is usually divided into medial and lateral regions with three bands with distinct properties and connectivity running perpendicular across the whole area. A distinguishing characteristic of the EC is the lack of cell bodies where layer IV should be; this layer is called the "lamina dissecans".
Inputs and outputs.
The superficial layers - layers II and III - of EC project to the dentate gyrus and hippocampus: Layer II projects primarily to dentate gyrus and hippocampal region CA3; layer III projects primarily to hippocampal region CA1 and the subiculum. These layers receive input from other cortical areas, especially associational, perirhinal, and parahippocampal cortices, as well as prefrontal cortex. EC as a whole, therefore, receives highly processed input from every sensory modality, as well as input relating to ongoing cognitive processes, though it should be stressed that, within EC, this information remains at least partially segregated.
The deep layers, especially layer V, receive one of the three main outputs of the hippocampus and, in turn, reciprocate connections from other cortical areas that project to superficial EC.
The rodent entorhinal cortex shows a modular organization, with different properties and connections in different areas.
Neuron information processing.
In 2005, it was discovered that entorhinal cortex contains a neural map of the spatial environment in rats. In 2014, John O'Keefe, May-Britt Moser and Edvard Moser received the Nobel Prize for Physiology or Medicine, partly because of this discovery.
Neurons in the lateral entorhinal cortex exhibit little spatial selectivity, whereas neurons of the medial entorhinal cortex (MEA), exhibit multiple "place fields" that are arranged in a hexagonal pattern, and are, therefore, called "grid cells". These fields and spacing between fields increase from the dorso-lateral MEA to the ventro-medial MEA.
The same group of researchers have found speed cells in the medial entorhinal cortex of rats. The speed of movement is translated from proprioceptive information and is represented as firing rates in these cells. The cells are known to fire in correlation to future speed of the rodent. 
Single-unit recording of neurons in humans playing video games find path cells in the EC, the activity of which indicates whether a person is taking a clockwise or counterclockwise path. Such EC "direction" path cells show this directional activity irrespective of the location of where a person experiences themselves, which contrasts them to place cells in the hippocampus, which are activated by specific locations.
EC neurons process general information such as directional activity in the environment, which contrasts to that of the hippocampal neurons, which usually encode information about specific places. This suggests that EC encodes general properties about current contexts that are then used by hippocampus to create unique representations from combinations of these properties.
Alzheimer's disease.
The entorhinal cortex is the first area of the brain to be affected in Alzheimer's disease; a recent functional magnetic resonance imaging study has localised the area to the lateral entorhinal cortex. Lopez "et al." have showed, in a multimodal study, that there are differences in the volume of the LEFT entorhinal cortex between progressing (to Alzheimer's disease) and stable mild cognitive impairment patients. These authors also found that the volume of the left entorhinal cortex inversely correlates with the level of alpha band phase synchronization between the right anterior cingulate and temporo-occipital regions.
In 2012, neuroscientists at UCLA conducted an experiment using a virtual taxi video game connected to seven epilepsy patients with electrodes already implanted in their brains, allowing the researchers to monitor neuronal activity whenever memories were being formed. As the researchers stimulated the nerve fibers of each of the patients' entorhinal cortex as they were learning, they were then able to better navigate themselves through various routes and recognize landmarks more quickly. This signified an improvement in the patients' spatial memory.
For delineating the Entorhinal cortex, see Desikan RS, Ségonne F, Fischl B, Quinn BT, Dickerson BC, Blacker D, Buckner RL, Dale AM, Maguire RP, Hyman BT, Albert MS, Killiany RJ. An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest. Neuroimage. 2006 Jul 1;31(3):968-80.

</doc>
<doc id="9668" url="https://en.wikipedia.org/wiki?curid=9668" title="Ernst Haeckel">
Ernst Haeckel

Ernst Heinrich Philipp August Haeckel (; 16 February 1834 – 9 August 1919) was a German biologist, naturalist, philosopher, physician, professor, and artist who discovered, described and named thousands of new species, mapped a genealogical tree relating all life forms, and coined many terms in biology, including "anthropogeny", "ecology", "phylum", "phylogeny", "stem cell", and "Protista." Haeckel promoted and popularised Charles Darwin's work in Germany and developed the influential but no longer widely held recapitulation theory ("ontogeny recapitulates phylogeny") claiming that an individual organism's biological development, or ontogeny, parallels and summarises its species' evolutionary development, or phylogeny.
The published artwork of Haeckel includes over 100 detailed, multi-colour illustrations of animals and sea creatures (see: "Kunstformen der Natur", "Art Forms of Nature"). As a philosopher, Ernst Haeckel wrote "Die Welträtsel" (1895–1899; in English: "The Riddle of the Universe", 1901), the genesis for the term "world riddle" ("Welträtsel"); and "Freedom in Science and Teaching" to support teaching evolution.
Life.
Ernst Haeckel was born on 16 February 1834, in Potsdam (then part of Prussia).
In 1852, Haeckel completed studies at the "Domgymnasium", the cathedral high school of Merseburg.
He then studied medicine in Berlin and Würzburg, particularly with Albert von Kölliker, Franz Leydig, Rudolf Virchow (with whom he later worked briefly as assistant), and with the anatomist-physiologist Johannes Peter Müller (1801–1858). Together with Hermann Steudner he attended botany lectures in Würzburg. In 1857, Haeckel attained a doctorate in medicine, (M.D.), and afterwards he received a license to practice medicine. The occupation of physician appeared less worthwhile to Haeckel, after contact with suffering patients.
Haeckel studied under Karl Gegenbaur at the University of Jena for three years, earning a doctorate in zoology, before becoming a professor of comparative anatomy at the University of Jena, where he remained for 47 years, from 1862 to 1909. Between 1859 and 1866, Haeckel worked on many phyla such radiolarians, poriferans (sponges) and annelids (segmented worms). During a trip to the Mediterranean, Haeckel named nearly 150 new species of radiolarians.
From 1866 to 1867, Haeckel made an extended journey to the Canary Islands with Hermann Fol and during this period, met with Charles Darwin, in 1866 at Down House in Kent, Thomas Huxley and Charles Lyell. In 1867, he married Agnes Huschke. Their son Walter was born in 1868, their daughters Elizabeth in 1871 and Emma in 1873. In 1869, he traveled as a researcher to Norway, in 1871 to Croatia (lived on the island of Hvar in a monastery), and in 1873 to Egypt, Turkey, and to Greece. Haeckel retired from teaching in 1909, and in 1910 he withdrew from the Evangelical church.
On the occasion of his 80th birthday celebration, he was presented with a two volume work, entitled "Was wir Ernst Haeckel verdanken (What We Owe to Ernst Haeckel)", edited at the request of the German Monistenbund by Heinrich Schmidt of Jena.
Haeckel's wife, Agnes, died in 1915, and Haeckel became substantially frailer, with a broken leg (thigh) and broken arm. He sold his "Villa Medusa" in Jena in 1918 to the Carl Zeiss foundation, and it presently contains a historic library. Haeckel died on 9 August 1919.
Politics.
Haeckel's political beliefs were influenced by his affinity for the German Romantic movement coupled with his acceptance of a form of Lamarckism. Rather than being a strict Darwinian, Haeckel believed that the characteristics of an organism were acquired through interactions with the environment and that ontogeny reflected phylogeny. He believed the social sciences to be instances of "applied biology", and that phrase was picked up and used for Nazi propaganda.
In 1905, Haeckel founded a group called the Monist League () to promote his religious and political beliefs. This group lasted until 1933 and included such notable members as Wilhelm Ostwald, Georg von Arco, Helene Stöcker and Walter Arthur Berendsohn.
"First World War".
Haeckel was the first person known to use the term "First World War". Shortly after the start of the war Haeckel wrote:
The "European War" became known as "The Great War", and it was not until 1920, in the book "The First World War 1914–1918" by Charles à Court Repington, that the term "First World War" was used as the official name for the conflict.
Research.
Haeckel was a zoologist, an accomplished artist and illustrator, and later a professor of comparative anatomy. Although Haeckel's ideas are important to the history of evolutionary theory, and although he was a competent invertebrate anatomist most famous for his work on radiolaria, many speculative concepts that he championed are now considered incorrect. For example, Haeckel described and named hypothetical ancestral microorganisms that have never been found.
He was one of the first to consider psychology as a branch of physiology. He also proposed the kingdom "Protista" in 1866. His chief interests lay in evolution and life development processes in general, including development of nonrandom form, which culminated in the beautifully illustrated "Kunstformen der Natur" ("Art forms of nature"). Haeckel did not support natural selection, rather believing in Lamarckism.
Haeckel advanced a version of the earlier recapitulation theory previously set out by Étienne Serres in the 1820s and supported by followers of Étienne Geoffroy Saint-Hilaire including Robert Edmond Grant. It proposed a link between ontogeny (development of form) and phylogeny (evolutionary descent), summed up by Haeckel in the phrase "ontogeny recapitulates phylogeny". His concept of recapitulation has been refuted in the form he gave it (now called "strong recapitulation"), in favour of the ideas first advanced by Karl Ernst von Baer. The strong recapitulation hypothesis views ontogeny as repeating forms of the ancestors, while weak recapitulation means that what is repeated (and built upon) is the ancestral embryonic development process. Haeckel supported the theory with embryo drawings that have since been shown to be oversimplified and in part inaccurate, and the theory is now considered an oversimplification of quite complicated relationships. Haeckel introduced the concept of heterochrony, the change in timing of embryonic development over the course of evolution.
Haeckel was a flamboyant figure, who sometimes took great, non-scientific leaps from available evidence. For example, at the time when Darwin published "On the Origin of Species by Means of Natural Selection" (1859), Haeckel postulated that evidence of human evolution would be found in the Dutch East Indies (now Indonesia). At that time, no remains of human ancestors had yet been found. He described these theoretical remains in great detail and even named the as-yet unfound species, "Pithecanthropus alalus", and instructed his students such as Richard and Oskar Hertwig to go and find it.
One student did find some remains: a Dutchman named Eugène Dubois searched the East Indies from 1887 to 1895, discovering the remains of Java Man in 1891, consisting of a skullcap, thighbone, and a few teeth. These remains are among the oldest hominid remains ever found. Dubois classified Java Man with Haeckel's "Pithecanthropus" label, though they were later reclassified as "Homo erectus". Some scientists of the day suggested Dubois' Java Man as a potential intermediate form between modern humans and the common ancestor we share with the other great apes. The current consensus of anthropologists is that the direct ancestors of modern humans were African populations of "Homo erectus" (possibly "Homo ergaster"), rather than the Asian populations exemplified by Java Man and Peking Man.
Polygenism and racial theory.
The creationist polygenism of Samuel George Morton and Louis Agassiz, which presented human races as separately created species, was rejected by Charles Darwin, who argued for the monogenesis of the human species and the African origin of modern humans. In contrast to most of Darwin's supporters, Haeckel put forward a doctrine of evolutionary polygenism based on the ideas of the linguist August Schleicher, in which several different language groups had arisen separately from speechless prehuman "Urmenschen", which themselves had evolved from simian ancestors. These separate languages had completed the transition from animals to man, and, under the influence of each main branch of languages, humans had evolved – in a kind of Lamarckian use-inheritance – as separate species, which could be subdivided into races. From this, Haeckel drew the implication that languages with the most potential yield the human races with the most potential, led by the Semitic and Indo-Germanic groups, with Berber, Jewish, Greco-Roman and Germanic varieties to the fore. As Haeckel stated:
Haeckel's view can be seen as a forerunner of the views of Carleton Coon, who also believed that human races evolved independently and in parallel with each other. These ideas eventually fell from favour.
Haeckel also applied the hypothesis of polygenism to the modern diversity of human groups. He became a key figure in social darwinism and leading proponent of scientific racism, stating for instance:
Haeckel divided human beings into ten races, of which the Caucasian was the highest and the primitives were doomed to extinction. Haeckel claimed that Negros have stronger and more freely movable toes than any other race which is evidence that Negros are related to apes because when apes stop climbing in trees they hold on to the trees with their toes, Haeckel compared Negros to "four-handed" apes. Haeckel also believed Negros were savages and that Whites were the most civilised.
However, Robert J. Richards notes: "Haeckel, on his travels to Ceylon and Indonesia, often formed closer and more intimate relations with natives, even members of the untouchable classes, than with the European colonials."
In his "Ontology and Phylogeny" Harvard paleontologist Stephen Jay Gould wrote: "aekel' evolutionary racism; his call to the German people for racial purity and unflinching devotion to a "just" state; his belief that harsh, inexorable laws of evolution ruled human civilization and nature alike, conferring upon favored races the right to dominate others . . . all contributed to the rise of Nazism."
In Alfred Rosenberg's The Myth of the Twentieth Century, Rosenberg explicitly mentions having read Haeckel.
In the same line of thought, historian Daniel Gasman states that Haeckel's ideology stimulated the birth of Fascist ideology in Italy and France.
Asia hypothesis.
Haeckel claimed the origin of humanity was to be found in Asia: he believed that Hindustan (Indian Subcontinent) was the actual location where the first humans had evolved. Haeckel argued that humans were closely related to the primates of Southeast Asia and rejected Darwin's hypothesis of Africa.
Haeckel later claimed that the missing link was to be found on the lost continent of Lemuria located in the Indian Ocean, he believed that Lemuria was the home of the first humans and that Asia was the home of many of the earliest primates, he thus supported that Asia was the cradle of hominid evolution. Haeckel also claimed that Lemuria connected Asia and Africa which allowed the migration of humans to the rest of the world.
In Haeckel’s book "The History of Creation" (1884) he included migration routes which he thought the first humans had used outside of Lemuria.
Embryology and recapitulation theory.
When Haeckel was a student in the 1850s he showed great interest in embryology, attending the rather unpopular lectures twice and in his notes sketched the visual aids: textbooks had few illustrations, and large format plates were used to show students how to see the tiny forms under a reflecting microscope, with the translucent tissues seen against a black background. Developmental series were used to show stages within a species, but inconsistent views and stages made it even more difficult to compare different species. It was agreed by all European evolutionists that all vertebrates looked very similar at an early stage, in what was thought of as a common ideal type, but there was a continuing debate from the 1820s between the Romantic recapitulation theory that human embryos developed through stages of the forms of all the major groups of adult animals, literally manifesting a sequence of organisms on a linear chain of being, and Karl Ernst von Baer's opposing view that the early general forms diverged into four major groups of specialised forms without ever resembling the adult of another species, showing affinity to an archetype but no relation to other types or any transmutation of species. By the time Haeckel was teaching he was able to use a textbook with woodcut illustrations written by his own teacher Albert von Kölliker, which purported to explain human development while also using other mammalian embryos to claim a coherent sequence. Despite the significance to ideas of transformism, this was not really polite enough for the new popular science writing, and was a matter for medical institutions and for experts who could make their own comparisons.
Darwin, Naturphilosophie and Lamarck.
Darwin's "On the Origin of Species", which made a powerful impression on Haeckel when he read it in 1864, was very cautious about the possibility of ever reconstructing the history of life, but did include a section reinterpreting von Baer's embryology and revolutionising the field of study, concluding that "Embryology rises greatly in interest, when we thus look at the embryo as a picture, more or less obscured, of the common parent-form of each great class of animals." It mentioned von Baer's 1828 anecdote (misattributing it to Louis Agassiz) that at an early stage embryos were so similar that it could be impossible to tell whether an unlabelled specimen was of a mammal, a bird, or of a reptile, and Darwin's own research using embryonic stages of barnacles to show that they are crustaceans, while cautioning against the idea that one organism or embryonic stage is "higher" or "lower", or more or less evolved. Haeckel disregarded such caution, and in a year wrote his massive and ambitious "Generelle Morphologie", published in 1866, presenting a revolutionary new synthesis of Darwin's ideas with the German tradition of "Naturphilosophie" going back to Goethe and with the progressive evolutionism of Lamarck in what he called "Darwinismus". He used morphology to reconstruct the evolutionary history of life, in the absence of fossil evidence using embryology as evidence of ancestral relationships. He invented new terms, including ontogeny and phylogeny, to present his evolutionised recapitulation theory that "ontogeny recapitulated phylogeny". The two massive volumes sold poorly, and were heavy going: with his limited understanding of German, Darwin found them impossible to read. Haeckel's publisher turned down a proposal for a ""strictly scholarly" and "objective"" second edition.
Embryological drawings.
Haeckel's aim was a reformed morphology with evolution as the organising principle of a cosmic synthesis unifying science, religion, and art. He was giving successful "popular lectures" on his ideas to students and townspeople in Jena, in an approach pioneered by his teacher Rudolf Virchow. To meet his publisher's need for a popular work he used a student's transcript of his lectures as the basis of his "Natürliche Schöpfungsgeschichte" of 1868, presenting a comprehensive presentation of evolution. In the Spring of that year he drew figures for the book, synthesising his views of specimens in Jena and published pictures to represent types. After publication he told a colleague that the images "are completely exact, partly copied from nature, partly assembled from all illustrations of these early stages that have hitherto become known." There were various styles of embryological drawings at that time, ranging from more schematic representations to "naturalistic" illustrations of specific specimens. Haeckel believed privately that his figures were both exact and synthetic, and in public asserted that they were schematic like most figures used in teaching. The images were reworked to match in size and orientation, and though displaying Haeckel's own views of essential features, they support von Baer's concept that vertebrate embryos begin similarly and then diverge. Relating different images on a grid conveyed a powerful evolutionary message. As a book for the general public, it followed the common practice of not citing sources.
The book sold very well, and while some anatomical experts hostile to Haeckel's evolutionary views expressed some private concerns that certain figures had been drawn rather freely, the figures showed what they already knew about similarities in embryos. The first published concerns came from Ludwig Rütimeyer, a professor of zoology and comparative anatomy at the University of Basel who had placed fossil mammals in an evolutionary lineage early in the 1860s and had been sent a complimentary copy. At the end of 1868 his review in the "Archiv für Anthropologie" wondered about the claim that the work was "popular and scholarly", doubting whether the second was true, and expressed horror about such public discussion of man's place in nature with illustrations such as the evolutionary trees being shown to non-experts. Though he made no suggestion that embryo illustrations should be directly based on specimens, to him the subject demanded the utmost "scrupulosity and conscientiousness" and an artist must "not arbitrarily model or generalise his originals for speculative purposes" which he considered proved by comparison with works by other authors. In particular, "one and the same, moreover incorrectly interpreted woodcut, is presented to the reader three times in a row and with three different captions as h embryo of the dog, the chick, n the turtle." He accused Haeckel of "playing fast and loose with the public and with science", and failing to live up to the obligation to the truth of every serious researcher. Haeckel responded with angry accusations of bowing to religious prejudice, but in the second (1870) edition changed the duplicated embryo images to a single image captioned "embryo of a mammal or bird". Duplication using galvanoplastic stereotypes (clichés) was a common technique in textbooks, but not on the same page to represent different eggs or embryos. In 1891 Haeckel made the excuse that this "extremely rash foolishness" had occurred in undue haste but was "bona ﬁde", and since repetition of incidental details was obvious on close inspection, it is unlikely to have been intentional deception.
The revised 1870 second edition of 1,500 copies attracted more attention, being quickly followed by further revised editions with larger print runs as the book became a prominent part of the optimistic, nationalist, anticlerical "culture of progress" in Otto von Bismarck's new German Empire. The similarity of early vertebrate embryos became common knowledge, and the illustrations were praised by experts such as Michael Foster of the University of Cambridge. In the introduction to his 1871 "The Descent of Man, and Selection in Relation to Sex", Darwin gave particular praise to Haeckel, writing that if "Natürliche Schöpfungsgeschichte" "had appeared before my essay had been written, I should probably never have completed it." The first chapter included an illustration: "As some of my readers may never have seen a drawing of an embryo, I have given one of man and another of a dog, at about the same early stage of development, carefully copied from two works of undoubted accuracy" with a footnote citing the sources and noting that "Häckel has also given analogous drawings in his "Schöpfungsgeschichte."" The fifth edition of Haeckel's book appeared in 1874, with its frontispiece a heroic portrait of Haeckel himself, replacing the previous controversial image of the heads of apes and humans.
Controversy.
Later in 1874, Haeckel's simpliﬁed embryology textbook "Anthropogenie" made the subject into a battleground over Darwinism aligned with Bismarck's "Kulturkampf" ("culture struggle") against the Catholic Church. Haeckel took particular care over the illustrations, changing to the leading zoological publisher Wilhelm Engelmann of Leipzig and obtaining from them use of illustrations from their other textbooks as well as preparing his own drawings including a dramatic double page illustration showing "early", "somewhat later" and "still later" stages of 8 different vertebrates. Though Haeckel's views had attracted continuing controversy, there had been little dispute about the embryos and he had many expert supporters, but Wilhelm His revived the earlier criticisms and introduced new attacks on the 1874 illustrations. Others joined in, both expert anatomists and Catholic priests and supporters were politically opposed to Haeckel's views.
While it has been widely claimed that Haeckel was charged with fraud by five professors and convicted by a university court at Jena, there does not appear to be an independently verifiable source for this claim. Recent analyses (Richardson 1998, Richardson and Keuck 2002) have found that some of the criticisms of Haeckel's embryo drawings were legitimate, but others were unfounded.
There were multiple versions of the embryo drawings, and Haeckel rejected the claims of fraud. It was later said that "there is evidence of sleight of hand" on both sides of the feud between Haeckel and Wilhelm His. Robert J. Richards, in a paper published in 2008, defends the case for Haeckel, shedding doubt against the fraud accusations based on the material used for comparison with what Haeckel could access at the time. The controversy involves several different issues (see more details at: recapitulation theory).
Awards and honours.
He was awarded the title of Excellency by Kaiser Wilhelm II in 1907 and the Linnean Society of London's prestigious Darwin-Wallace Medal in 1908. In the United States, "Mount Haeckel", a summit in the Eastern Sierra Nevada, overlooking the Evolution Basin, is named in his honour, as is another "Mount Haeckel", a summit in New Zealand; and the asteroid 12323 Haeckel.
Publications.
Darwin's 1859 book "On the Origin of Species" had immense popular influence, but although its sales exceeded its publisher's hopes it was a technical book rather than a work of popular science: long, difficult and with few illustrations. One of Haeckel's books did a great deal to explain his version of "Darwinism" to the world. It was a bestselling, provocatively illustrated book in German, titled "Natürliche Schöpfungsgeschichte", published in Berlin in 1868, and translated into English as "The History of Creation" in 1876. It was frequently reprinted until 1926.
Haeckel argued that human evolution consisted of precisely 22 phases, the 21st – the "missing link" — being a halfway step between apes and humans. He even formally named this missing link "Pithecanthropus alalus", translated as "ape man without speech."
Haeckel's entire literary output was extensive, working as a professor at the University of Jena for 47 years, and even at the time of the celebration of his 60th birthday at Jena in 1894, Haeckel had produced 42 works with nearly 13,000 pages, besides numerous scientific memoirs and illustrations.
Haeckel's monographs include:
As well as several "Challenger" reports:
Among his many books, Ernst Haeckel wrote:
Books of travel:
For a extensive list of works of and about Haeckel, see his entry in the .

</doc>
<doc id="9670" url="https://en.wikipedia.org/wiki?curid=9670" title="Evolutionism">
Evolutionism

Evolutionism was a common 19th century belief that organisms inherently improve themselves through progressive inherited change over time, and increase in complexity through evolution. The belief went on to include cultural evolution and social evolution. In the 1970s the term Neo-Evolutionism was used to describe the idea "that human beings sought to preserve a familiar style of life unless change was forced on them by factors that were beyond their control".
The term is sometimes also colloquially used to refer to acceptance of the modern evolutionary synthesis, a scientific theory that describes how biological evolution occurs. In addition, the term is used in a broader sense to cover a world-view on a wide variety of topics, including chemical evolution as an alternative term for abiogenesis or for nucleosynthesis of chemical elements, galaxy formation and evolution, stellar evolution, spiritual evolution, technological evolution and universal evolution, which seeks to explain every aspect of the world in which we live.
Since the overwhelming majority of scientists accept the modern evolutionary synthesis as the best explanation of current data, the term is seldom used in the scientific community; to say someone is a scientist implies acceptance of evolutionary views, unless specifically noted otherwise. In the creation-evolution controversy, creationists often call those who accept the validity of the modern evolutionary synthesis "evolutionists" and the theory itself as "evolutionism." Some creationists and creationist organizations, such as the Institute of Creation Research, use these terms in an effort to make it appear that evolutionary biology is a form of secular religion.
19th-century use.
Evolution originally was used to refer to an orderly sequence of events with the outcome somehow contained at the start. Darwin did not use the term in "Origin of Species" until its sixth edition in 1872, (though earlier editions did use the word "evolved") by which time Herbert Spencer had given it scientific currency with a broad definition of progression in complexity in 1862. Edward B. Tylor and Lewis H Morgan brought the term "evolution" to anthropology though they tended toward the older pre-Spencerian definition helping to form the concept of unilineal evolution used during the later part of what Trigger calls the Antiquarianism-Imperial Synthesis period (c1770-c1900).
Modern use.
In modern times, the term "evolution" is widely used, but the terms "evolutionism" and "evolutionist" are seldom used in the scientific community to refer to the biological discipline as the term is considered both redundant and anachronistic, though it has been used by creationists in discussing the creation-evolution controversy.
The Institute for Creation Research, in order to imply placement of evolution in the category of religions, including atheism, fascism, humanism and occultism, commonly uses the words "evolutionism" and "evolutionist" to describe the consensus of mainstream science and the scientists subscribing to it, thus implying through language that the issue is a matter of religious belief.
The BioLogos Foundation, an organization that promotes the idea of theistic evolution, uses the term "evolutionism" to describe "the atheistic worldview that so often accompanies the acceptance of biological evolution in public discourse." It views this as a subset of scientism.

</doc>
<doc id="9672" url="https://en.wikipedia.org/wiki?curid=9672" title="Entscheidungsproblem">
Entscheidungsproblem

In mathematics and computer science, the (, German for 'decision problem') is a challenge posed by David Hilbert in 1928. The asks for an algorithm that takes as input a statement of a first-order logic (possibly with a finite number of axioms beyond the usual axioms of first-order logic) and answers "Yes" or "No" according to whether the statement is "universally valid", i.e., valid in every structure satisfying the axioms. By the completeness theorem of first-order logic, a statement is universally valid if and only if it can be deduced from the axioms, so the can also be viewed as asking for an algorithm to decide whether a given statement is provable from the axioms using the rules of logic.
In 1936, Alonzo Church and Alan Turing published independent papers showing that a general solution to the Entscheidungsproblem is impossible, assuming that the intuitive notion of "effectively calculable" is captured by the functions computable by a Turing machine (or equivalently, by those expressible in the lambda calculus). This assumption is now known as the Church–Turing thesis.
History of the problem.
The origin of the goes back to Gottfried Leibniz, who in the seventeenth century, after having constructed a successful mechanical calculating machine, dreamt of building a machine that could manipulate symbols in order to determine the truth values of mathematical statements. He realized that the first step would have to be a clean formal language, and much of his subsequent work was directed towards that goal. In 1928, David Hilbert and Wilhelm Ackermann posed the question in the form outlined above.
In continuation of his "program," Hilbert posed three questions at an international conference in 1928, the third of which became known as "Hilbert's ." As late as 1930, he believed that there would be no such thing as an unsolvable problem.
Negative answer.
Before the question could be answered, the notion of "algorithm" had to be formally defined. This was done by Alonzo Church in 1936 with the concept of "effective calculability" based on his λ calculus and by Alan Turing in the same year with his concept of Turing machines. Turing immediately recognized that these are equivalent models of computation. 
The negative answer to the "" was then given by Alonzo Church in 1935–36 and independently shortly thereafter by Alan Turing in 1936. Church proved that there is no computable function which decides for two given λ-calculus expressions whether they are equivalent or not. He relied heavily on earlier work by Stephen Kleene. Turing reduced the halting problem for Turing machines to the . The work of both authors was heavily influenced by Kurt Gödel's earlier work on his incompleteness theorem, especially by the method of assigning numbers (a Gödel numbering) to logical formulas in order to reduce logic to arithmetic.
The is related to Hilbert's tenth problem, which asks for an algorithm to decide whether Diophantine equations have a solution. The non-existence of such an algorithm, established by Yuri Matiyasevich in 1970, also implies a negative answer to the Entscheidungsproblem.
Some first-order theories are algorithmically decidable; examples of this include Presburger arithmetic, real closed fields and static type systems of many programming languages. The general first-order theory of the natural numbers expressed in Peano's axioms cannot be decided with such an algorithm, however.
Practical decision procedures.
Having practical decision procedures for classes of logical formulas is of considerable interest for program verification and circuit verification. Pure Boolean logical formulas are usually decided using SAT-solving techniques based on the DPLL algorithm. Conjunctive formulas over linear real or rational arithmetic can be decided using the simplex algorithm, formulas in linear integer arithmetic (Presburger arithmetic) can be decided using Cooper's algorithm or William Pugh's Omega test. Formulas with negations, conjunctions and disjunctions combine the difficulties of satisfiability testing with that of decision of conjunctions; they are generally decided nowadays using SMT-solving technique, which combine SAT-solving with decision procedures for conjunctions and propagation techniques. Real polynomial arithmetic, also known as the theory of real closed fields, is decidable; this is Tarski–Seidenberg theorem, which has been implemented in computers by using the cylindrical algebraic decomposition.

</doc>
<doc id="9674" url="https://en.wikipedia.org/wiki?curid=9674" title="Einhard">
Einhard

Einhard (also Eginhard or Einhart; c. 775 – March 14, 840) was a Frankish scholar and courtier. Einhard was a dedicated servant of Charlemagne and his son Louis the Pious; his main work is a biography of Charlemagne, the "Vita Karoli Magni", "one of the most precious literary bequests of the early Middle Ages."
Public life.
Einhard was from the eastern German-speaking part of the Frankish Kingdom. Born into a family of relatively low status, his parents sent him to be educated by the monks of Fulda - one of the most impressive centres of learning in the Frank lands - perhaps due to his small stature (Einhard referred to himself as a "tiny manlet") which restricted his riding and sword-fighting ability, Einhard concentrated his energies towards scholarship and especially to the mastering of Latin. Despite such humble origins, he was accepted into the hugely wealthy court of Charlemagne around 791 or 792. Charlemagne actively sought to amass scholarly men around him and established a royal school led by the Northumbrian scholar Alcuin. Einhard evidently was a talented builder and construction manager, because Charlemagne put him in charge of the completion of several palace complexes including Aachen and Ingelheim. Despite the fact that Einhard was on intimate terms with Charlemagne, he never achieved office in his reign. In 814, on Charlemagne's death his son Louis the Pious made Einhard his private secretary. Einhard retired from court during the time of the disputes between Louis and his sons in the spring of 830.
He died at Seligenstadt in 840.
Private life.
Einhard was married to Emma, of whom (as of most laywomen of the period) little is known. There is a possibility that their marriage bore a son, Vussin. Their marriage also appears to have been exceptionally liberal for the period, with Emma being as active as Einhard, if not more so, in the handling of their property. It is said that in the later years of their marriage Emma and Einhard abstained from sexual relations, choosing instead to focus their attentions on their many religious commitments. Though he was undoubtedly devoted to her, Einhard wrote nothing of his wife until after her death on 13 December 835, when he wrote to a friend that he was reminded of her loss in ‘every day, in every action, in every undertaking, in all the administration of the house and household, in everything needing to be decided upon and sorted out in my religious and earthly responsibilities’.
Religious beliefs.
Einhard made numerous references to himself as a "sinner", a description of himself that shows his Augustinian influenced world view. To assuage such feelings of guilt he erected churches at both of his estates in Michelstadt and Mulinheim. In Michelstadt he also saw fit to build a basilica completed in 827 and then sent a servant, Ratleic, to Rome with an end to find relics for the new building. Once in Rome, Ratleic robbed a catacomb of the bones of the Martyrs Marcellinus and Peter and had them translated to Michelstadt. Once there, the relics made it known they were unhappy with their new tomb and thus had to be moved again to Mulinheim. Once established there, they proved to be miracle workers. Although unsure as to why these saints should choose such a "sinner" as their patron, Einhard nonetheless set about ensuring they continued to receive a resting place fitting of their honour. Between 831 and 834 he founded a Benedictine Monastery and, after the death of his wife, served as its Abbot until his own death in 840.
Local lore.
Local lore from Seligenstadt portrays Einhard as the lover of Emma, one of Charlemagne's daughters, and has the couple elope from court. Charlemagne found them at Seligenstadt (then called Obermühlheim) and forgave them. This account is used to explain the name "Seligenstadt" by folk etymology. Einhard and his wife were originally buried in one sarcophagus in the choir of the church in Seligenstadt, but in 1810 the sarcophagus was presented by the Grand Duke of Hesse to the count of Erbach, who claims descent from Einhard as the husband of Imma, the reputed daughter of Charlemagne. The count put it in the famous chapel of his castle at Erbach in the Odenwald.
Works.
The most famous of Einhard's works is his biography of Charlemagne, the "Vita Karoli Magni", "The Life of Charlemagne" (c. 817–836), which provides much direct information about Charlemagne's life and character, written sometime between 817 and 830. In composing this he relied heavily upon the Royal Frankish Annals. Einhard's literary model was the classical work of the Roman historian Suetonius, the "Lives of the Caesars", though it is important to stress that the work is very much Einhard's own, that is to say he adapts the models and sources for his own purposes. His work was written as a praise of Charlemagne, whom he regarded as a foster-father ("nutritor") and to whom he was a debtor "in life and death". The work thus contains an understandable degree of bias, Einhard taking care to exculpate Charlemagne in some matters, not mention others, and to gloss over certain issues which would be of embarrassment to Charlemagne, such as the morality of his daughters; by contrast, other issues are curiously not glossed over, like his concubines.
Einhard is also responsible for three other extant works: a collection of letters, "On the Translations and the Miracles of SS. Marcellinus and Petrus", and "On the Adoration of the Cross". The latter dates from ca. 830 and was not rediscovered until 1885, when Ernst Dümmler identified a text in a manuscript in Vienna as the missing "Libellus de adoranda cruce", which Einhard had dedicated to his pupil Lupus Servatus.

</doc>
<doc id="9675" url="https://en.wikipedia.org/wiki?curid=9675" title="Ester">
Ester

In chemistry, esters are chemical compounds derived from an acid (organic or inorganic) in which at least one -OH (hydroxyl) group is replaced by an -O-alkyl (alkoxy) group. Usually, esters are derived from a carboxylic acid and an alcohol. Glycerides, which are fatty acid esters of glycerol, are important esters in biology, being one of the main classes of lipids, and making up the bulk of animal fats and vegetable oils. Esters with low molecular weight are commonly used as fragrances and found in essential oils and pheromones. Phosphoesters form the backbone of DNA molecules. Nitrate esters, such as nitroglycerin, are known for their explosive properties, while polyesters are important plastics, with monomers linked by ester moieties.
Nomenclature.
Etymology.
The word 'ester' was coined in 1848 by German chemist Leopold Gmelin, probably as a contraction of the German Essigäther, "acetic ether".
IUPAC nomenclature.
Ester names are derived from the parent alcohol and the parent acid, where the latter may be organic or inorganic. Esters derived from the simplest carboxylic acids are commonly named according to the more traditional, so-called "trivial names" e.g. as formate, acetate, propionate, and butyrate, as opposed to the IUPAC nomenclature methanoate, ethanoate, propanoate and butanoate. Esters derived from more complex carboxylic acids are, on the other hand, more frequently named using the systematic IUPAC name, based on the name for the acid followed by the suffix "-oate". For example, the ester hexyl octanoate, also known under the trivial name hexyl caprylate, has the formula CH(CH)CO(CH)CH.
The chemical formulas of organic esters usually take the form RCOR', where R and R' are the hydrocarbon parts of the carboxylic acid and the alcohol, respectively. For example, butyl acetate (systematically butyl ethanoate), derived from butanol and acetic acid (systematically ethanoic acid) would be written CHCOCH. Alternative presentations are common including BuOAc and CHCOOCH.
Cyclic esters are called lactones, regardless of whether they are derived from an organic or an inorganic acid. One example of a (organic) lactone is "gamma"-valerolactone.
Orthoesters.
An uncommon class of organic esters are the orthoesters, which have the formula RC(OR'). Triethylorthoformate (HC(OCH)) is derived, in terms of its name (but not its synthesis) from orthoformic acid (HC(OH)) and ethanol.
Inorganic esters.
Esters can also be derived from an inorganic acid and an alcohol. Thus, the nomenclature extends to inorganic oxo acids, e.g. phosphoric acid, sulfuric acid, nitric acid and boric acid. For example, triphenyl phosphate is the ester derived from phosphoric acid and phenol. Organic carbonates are derived from carbonic acid; for example, ethylene carbonate is derived from carbonic acid and ethylene glycol.
Structure and bonding.
Esters contain a carbonyl center, which gives rise to 120 °C-C-O and O-C-O angles. Unlike amides, esters are structurally flexible functional groups because rotation about the C-O-C bonds has a low barrier. Their flexibility and low polarity is manifested in their physical properties; they tend to be less rigid (lower melting point) and more volatile (lower boiling point) than the corresponding amides. The pKa of the alpha-hydrogens on esters is around 25.
Physical properties and characterization.
Esters are more polar than ethers but less polar than alcohols. They participate in hydrogen bonds as hydrogen-bond acceptors, but cannot act as hydrogen-bond donors, unlike their parent alcohols. This ability to participate in hydrogen bonding confers some water-solubility. Because of their lack of hydrogen-bond-donating ability, esters do not self-associate. Consequently, esters are more volatile than carboxylic acids of similar molecular weight.
Characterization and analysis.
Esters are generally identified by gas chromatography, taking advantage of their volatility. IR spectra for esters feature an intense sharp band in the range 1730–1750 cm assigned to ν. This peak changes depending on the functional groups attached to the carbonyl. For example, a benzene ring or double bond in conjugation with the carbonyl will bring the wavenumber down about 30 cm.
Applications and occurrence.
Esters are widespread in nature and are widely used in industry. In nature, fats are, in general, triesters derived from glycerol and fatty acids. Esters are responsible for the aroma of many fruits, including apples, durians, pears, bananas, pineapples, and strawberries. Several billion kilograms of polyesters are produced industrially annually, important products being polyethylene terephthalate, acrylate esters, and cellulose acetate.
Preparation.
Esterification is the general name for a chemical reaction in which two reactants (typically an alcohol and an acid) form an ester as the reaction product. Esters are common in organic chemistry and biological materials, and often have a characteristic pleasant, fruity odor. This leads to their extensive use in the fragrance and flavor industry. Ester bonds are also found in many polymers.
Esterification of carboxylic acids.
The classic synthesis is the Fischer esterification, which involves treating a carboxylic acid with an alcohol in the presence of a dehydrating agent:
The equilibrium constant for such reactions is about 5 for typical esters, e.g., ethyl acetate. The reaction is slow in the absence of a catalyst. Sulfuric acid is a typical catalyst for this reaction. Many other acids are also used such as polymeric sulfonic acids. Since esterification is highly reversible, the yield of the ester can be improved using Le Chatelier's principle:
Reagents are known that drive the dehydration of mixtures of alcohols and carboxylic acids. One example is the Steglich esterification, which is a method of forming esters under mild conditions. The method is popular in peptide synthesis, where the substrates are sensitive to harsh conditions like high heat. DCC (dicyclohexylcarbodiimide) is used to activate the carboxylic acid to further reaction. DMAP (4-dimethylaminopyridine) is used as an acyl-transfer catalyst.
Another method for the dehydration of mixtures of alcohols and carboxylic acids is the Mitsunobu reaction:
Carboxylic acids can be esterified using diazomethane:
Using this diazomethane, mixtures of carboxylic acids can be converted to their methyl esters in near quantitative yields, e.g., for analysis by gas chromatography. The method is useful in specialized organic synthetic operations but is considered too expensive for large scale applications.
Alcoholysis of acyl chlorides and acid anhydrides.
Alcohols react with acyl chlorides and acid anhydrides to give esters:
The reactions are irreversible simplifying work-up. Since acyl chlorides and acid anhydrides also react with water, anhydrous conditions are preferred. The analogous acylations of amines to give amides are less sensitive because amines are stronger nucleophiles and react more rapidly than does water. This method is employed only for laboratory-scale procedures, as it is expensive.
Alkylation of carboxylate salts.
Although not widely employed for esterifications, salts of carboxylate anions can be alkylating agent with alkyl halides to give esters. In the case that an alkyl chloride is used, an iodide salt can catalyze the reaction (Finkelstein reaction). The carboxylate salt is often generated "in situ". In difficult cases, the silver carboxylate may be used, since the silver ion coordinates to the halide aiding its departure and improving the reaction rate. This reaction can suffer from anion availability problems and, therefore, can benefit from the addition of phase transfer catalysts or highly polar aprotic solvents such as DMF.
Transesterification.
Transesterification, which involves changing one ester into another one, is widely practiced:
Like the hydrolysation, transesterification is catalysed by acids and bases. The reaction is widely used for degrading triglycerides, e.g. in the production of fatty acid esters and alcohols. Poly(ethylene terephthalate) is produced by the transesterification of dimethyl terephthalate and ethylene glycol: 
Carbonylation.
Alkenes undergo "hydroesterification" in the presence of metal carbonyl catalysts. Esters of propionic acid are produced commercially by this method:
The carbonylation of methanol yields methyl formate, which is the main commercial source of formic acid. The reaction is catalyzed by sodium methoxide:
Addition of carboxylic acids to alkenes.
In the presence of palladium-based catalysts, ethylene, acetic acid, and oxygen react to give vinyl acetate:
Direct routes to this same ester are not possible because vinyl alcohol is unstable.
Reactions.
Esters react with nucleophiles at the carbonyl carbon. The carbonyl is weakly electrophilic but is attacked by strong nucleophilies (amines, alkoxides, hydride sources, organolithium compounds, etc.). The C-H bonds adjacent to the carbonyl are weakly acidic but undergo deprotonation with strong bases. This process is the one that usually initiates condensation reactions. The carbonyl oxygen is weakly basic (less so than in amides) but forms adducts.
Addition of nucleophiles at carbonyl.
Esterification is a reversible reaction. Esters undergo hydrolysis under acid and basic conditions. Under acidic conditions, the reaction is the reverse reaction of the Fischer esterification. Under basic conditions, hydroxide acts as a nucleophile, while an alkoxide is the leaving group. This reaction, saponification, is the basis of soap making.
The alkoxide group may also be displaced by stronger nucleophiles such as ammonia or primary or secondary amines to give amides: (ammonolysis reaction)
This reaction is not usually reversible. Hydrazines and hydroxylamine can be used in place of amines. Esters can be converted to isocyanates through intermediate hydroxamic acids in the Lossen rearrangement.
Sources of carbon nucleophiles, e.g., Grignard reagents and organolithium compounds, add readily to the carbonyl.
Reduction.
Compared to ketones and aldehydes, esters are relatively resistant to reduction. The introduction of catalytic hydrogenation in the early part of the 20th century was a breakthrough; esters of fatty acids are hydrogenated to fatty alcohols.
A typical catalyst is copper chromite. Prior to the development of catalytic hydrogenation, esters were reduced on a large scale using the Bouveault-Blanc reduction. This method, which is largely obsolete, uses sodium in the presence of proton sources.
Especially for fine chemical syntheses, lithium aluminium hydride is used to reduce esters to two primary alcohols. The related reagent sodium borohydride is slow in this reaction. DIBAH reduces esters to aldehydes.
Direct reduction to give the corresponding ether is difficult as the intermediate hemiacetal tends to decompose to give an alcohol and an aldehyde (which is rapidly reduced to give a second alcohol). The reaction can be achieved using triethylsilane with a variety of Lewis acids.
Claisen condensation and related reactions.
As for aldehydes, the hydrogen atoms on the carbon adjacent ("α to") the carboxyl group in esters are sufficiently acidic to undergo deprotonation, which in turn leads to a variety of useful reactions. Deprotonation requires relatively strong bases, such as alkoxides. Deprotonation gives a nucleophilic enolate, which can further react, e.g., the Claisen condensation and its intramolecular equivalent, the Dieckmann condensation. This conversion is exploited in the malonic ester synthesis, wherein the diester of malonic acid reacts with an electrophile (e.g., alkyl halide), and is subsequently decarboxylated. Another variation is the Fráter–Seebach alkylation.
Protecting groups.
As a class, esters serve as protecting groups for carboxylic acids. Protecting a carboxylic acid is useful in peptide synthesis, to prevent self-reactions of the bifunctional amino acids. Methyl and ethyl esters are commonly available for many amino acids; the "t"-butyl ester tends to be more expensive. However, "t"-butyl esters are particularly useful because, under strongly acidic conditions, the "t"-butyl esters undergo elimination to give the carboxylic acid and isobutylene, simplifying work-up.
List of ester odorants.
Many esters have distinctive fruit-like odors, and many occur naturally in the essential oils of plants. This has also led to their commonplace use in artificial flavorings and fragrances when those odors aim to be mimicked.

</doc>
<doc id="9677" url="https://en.wikipedia.org/wiki?curid=9677" title="Endosymbiont">
Endosymbiont

An endosymbiont is any organism that lives within the body or cells of another organism, i.e. forming an endosymbiosis (Greek: ἔνδον "endon" "within", σύν "syn" "together" and βίωσις "biosis" "living"). Examples are nitrogen-fixing bacteria (called rhizobia), which live in root nodules on legume roots, single-cell algae inside reef-building corals, and bacterial endosymbionts that provide essential nutrients to about 10–15% of insects.
Many instances of endosymbiosis are obligate; that is, either the endosymbiont or the host cannot survive without the other, such as the gutless marine worms of the genus "Riftia", which get nutrition from their endosymbiotic bacteria. The most common examples of obligate endosymbioses are mitochondria and chloroplasts. Some human parasites, e.g. "Wuchereria bancrofti" and "Mansonella perstans", thrive in their intermediate insect hosts because of an obligate endosymbiosis with "Wolbachia spp." They can both be eliminated from said hosts by treatments that target this bacterium. However, not all endosymbioses are obligate. Also, some endosymbioses can be harmful to either of the organisms involved.
It is generally agreed that certain organelles of the eukaryotic cell, especially mitochondria and plastids such as chloroplasts, originated as bacterial endosymbionts. This theory is called the endosymbiotic theory, and was first articulated by the Russian botanist Konstantin Mereschkowski in 1910, even though the first paper that referenced this theory was published in 1905.
Endosymbiosis theory and mitochondria and chloroplasts.
The endosymbiosis theory explains the origins of organelles such as mitochondria and chloroplasts in eukaryotic cells. The theory proposes that chloroplasts and mitochondria evolved from certain types of bacteria that eukaryotic cells engulfed through endophagocytosis. These cells and the bacteria trapped inside them entered a symbiotic relationship, a close association between different types of organisms over an extended time. However, to be specific, the relationship was endosymbiotic, meaning that one of the organisms (the bacteria) lived within the other (the eukaryotic cells).
According to endosymbiosis theory, an anaerobic cell probably ingested an aerobic bacterium but failed to digest it. The aerobic bacterium flourished within the cell because the cell's cytoplasm was abundant in half-digested food molecules. The bacterium digested these molecules with oxygen and gained great amounts of energy. Because the bacterium had so much energy, it probably leaked some of it as adenosine triphosphate into the cell's cytoplasm. This benefited the anaerobic cell because it was now able to breathe aerobically, which means more potential for energy gain. Eventually, the aerobic bacterium could no longer live independently from the cell, and it, therefore, became a mitochondrion. The origin of the chloroplast is very similar to that of the mitochondrion. A cell must have captured a photosynthetic cyanobacterium and failed to digest it. The cyanobacterium thrived in the cell and eventually evolved into the first chloroplast. Other eukaryotic organelles may have also evolved through endosymbiosis; it has been proposed that cilia, flagella, centrioles, and microtubules may have originated from a symbiosis between a Spirochaete bacterium and an early eukaryotic cell, but this is not widely accepted among biologists.
There are several examples of evidence that support endosymbiosis theory. Mitochondria and chloroplasts contain their own small supply of DNA, which may be remnants of the genome the organelles had when they were independent aerobic bacteria. The single most convincing evidence of the descent of organelles from bacteria is the position of mitochondria and plastid DNA sequences in phylogenetic trees of bacteria. Mitochondria have sequences that clearly indicate origin from a group of bacteria called the alphaproteobacteria. Plastids have DNA sequences that indicate origin from the cyanobacteria (blue-green algae). In addition, there are organisms alive today, called living intermediates, that are in a similar endosymbiotic condition to the prokaryotic cells and the aerobic bacteria. Living intermediates show that the evolution proposed by the endosymbiont theory is possible. For example, the giant amoeba "Pelomyxa" lacks mitochondria but has aerobic bacteria that carry out a similar role. A variety of corals, clams, snails, and one species of "Paramecium" permanently host algae in their cells. Many of the insect endosymbionts have been shown to have ancient associations with their hosts, involving strictly vertical inheritance. In addition, these insect symbionts have similar patterns of genome evolution to those found in true organelles: genome reduction, rapid rates of gene evolution, and bias in nucleotide base composition favoring adenine and thymine, at the expense of guanine and cytosine.
Further evidence of endosymbiosis are the prokaryotic ribosomes found within chloroplasts and mitochondria as well as the double-membrane enclosing them. It used to be widely assumed that the inner membrane is the original membrane of the once independent prokaryote, while the outer one is the food vacuole (phagosomal membrane) it was enclosed in initially. However, this view neglects the fact that i) both modern cyanobacteria and alpha-proteobacteria are Gram-negative bacteria, which are surrounded by double membranes; ii) the outer membranes of the endosymbiotic organelles (chloroplasts and mitochondria) are very similar to those of these bacteria in their lipid and protein compositions. Accumulating biochemical data strongly suggests that the double-membrane-enclosing chloroplasts and mitochondria derived from those of the ancestral bacteria, and the phagosomal membrane disappeared during organelle evolution. Triple or quadruple membranes are found among certain algae, probably resulting from repeated endosymbiosis (although little else was retained of the engulfed cell).
These modern organisms with endosymbiotic relationships with aerobic bacteria have verified the endosymbiotic theory, which explains the origin of mitochondria and chloroplasts from bacteria. Researchers in molecular and evolutionary biology no longer question this theory, although some of the details, such as the mechanisms for loss of genes from organelles to host nuclear genomes, are still being worked out.
Bacterial endosymbionts in marine invertebrates.
Extracellular endosymbionts are also represented in all four extant classes of Echinodermata (Crinoidea, Ophiuroidea, Echinoidea, and Holothuroidea). Little is known of the nature of the association (mode of infection, transmission, metabolic requirements, etc.) but phylogenetic analysis indicates that these symbionts belong to the alpha group of the class Proteobacteria, relating them to "Rhizobium" and "Thiobacillus". Other studies indicate that these subcuticular bacteria may be both abundant within their hosts and widely distributed among the Echinoderms in general.
Some marine oligochaeta (e.g., Olavius or Inanidrillus) have obligate extracellular endosymbionts that fill the entire body of their host. These marine worms are nutritionally dependent on their symbiotic chemoautotrophic bacteria lacking any digestive or excretory system (no gut, mouth, or nephridia).
"Symbiodinium" dinoflagellate endosymbionts in marine metazoa and protists.
Dinoflagellate endosymbionts of the genus "Symbiodinium", commonly known as zooxanthellae, are found in corals, mollusks (esp. giant clams, the "Tridacna"), sponges, and foraminifera. These endosymbionts drive the formation of coral reefs by capturing sunlight and providing their hosts with energy for carbonate deposition.
Previously thought to be a single species, molecular phylogenetic evidence over the past couple decades has shown there to be great diversity in "Symbiodinium". In some cases, there is specificity between host and "Symbiodinium" clade. More often, however, there is an ecological distribution of "Symbiodinium", the symbionts switching between hosts with apparent ease. When reefs become environmentally stressed, this distribution of symbionts is related to the observed pattern of coral bleaching and recovery. Thus, the distribution of "Symbiodinium" on coral reefs and its role in coral bleaching presents one of the most complex and interesting current problems in reef ecology.
Endosymbionts in protists.
"Mixotricha paradoxa" is a protozoan that lacks mitochondria. However, spherical bacteria live inside the cell and serve the function of the mitochondria. "Mixotricha" also has three other species of symbionts that live on the surface of the cell.
"Paramecium bursaria", a species of ciliate, has a mutualistic symbiotic relationship with green alga called Zoochlorella. The algae live inside the cell, in the cytoplasm.
"Paulinella chromatophora" is a freshwater amoeboid which has recently (evolutionarily speaking) taken on a cyanobacterium as an endosymbiont.
Bacterial endosymbionts in insects.
Scientists classify insect endosymbionts in two broad categories, 'Primary' and 'Secondary'. Primary endosymbionts (sometimes referred to as P-endosymbionts) have been associated with their insect hosts for many millions of years (from 10 to several hundred million years in some cases). They form obligate associations (see below), and display cospeciation with their insect hosts. Secondary endosymbionts exhibit a more recently developed association, are sometimes horizontally transferred between hosts, live in the hemolymph of the insects (not specialized bacteriocytes, see below), and are not obligate.
Among primary endosymbionts of insects, the best-studied are the pea aphid ("Acyrthosiphon pisum") and its endosymbiont "Buchnera sp." APS, the tsetse fly "Glossina morsitans morsitans" and its endosymbiont "Wigglesworthia glossinidia brevipalpis" and the endosymbiotic protists in lower termites. As with endosymbiosis in other insects, the symbiosis is obligate in that neither the bacteria nor the insect is viable without the other. Scientists have been unable to cultivate the bacteria in lab conditions outside of the insect. With special nutritionally-enhanced diets, the insects can survive, but are unhealthy, and at best survive only a few generations.
In some insect groups, these endosymbionts live in specialized insect cells called bacteriocytes (also called "mycetocytes"), and are maternally-transmitted, i.e. the mother transmits her endosymbionts to her offspring. In some cases, the bacteria are transmitted in the egg, as in "Buchnera"; in others like "Wigglesworthia", they are transmitted via milk to the developing insect embryo. In termites, the endosymbionts reside within the hindguts and are transmitted through trophallaxis among colony members.
The primary endosymbionts are thought to help the host either by providing nutrients that the host cannot obtain itself or by metabolizing insect waste products into safer forms. For example, the putative primary role of "Buchnera" is to synthesize essential amino acids that the aphid cannot acquire from its natural diet of plant sap. Likewise, the primary role of "Wigglesworthia", it is presumed, is to synthesize vitamins that the tsetse fly does not get from the blood that it eats. In lower termites, the endosymbiotic protists play a major role in the digestion of lignocellulosic materials that constitute a bulk of the termites' diet.
Bacteria benefit from the reduced exposure to predators and competition from other bacterial species, the ample supply of nutrients and relative environmental stability inside the host.
Genome sequencing reveals that obligate bacterial endosymbionts of insects have among the smallest of known bacterial genomes and have lost many genes that are commonly found in closely related bacteria. Several theories have been put forth to explain the loss of genes. It is presumed that some of these genes are not needed in the environment of the host insect cell. A complementary theory suggests that the relatively small numbers of bacteria inside each insect decrease the efficiency of natural selection in 'purging' deleterious mutations and small mutations from the population, resulting in a loss of genes over many millions of years. Research in which a parallel phylogeny of bacteria and insects was inferred supports the belief that the primary endosymbionts are transferred only vertically (i.e., from the mother), and not horizontally (i.e., by escaping the host and entering a new host).
Attacking obligate bacterial endosymbionts may present a way to control their insect hosts, many of which pests or carriers of human disease. For example, aphids are crop pests and the tsetse fly carries the organism "Trypanosoma brucei" that causes African sleeping sickness. Other motivations for their study is to understand symbiosis, and to understand how bacteria with severely depleted genomes are able to survive, thus improving our knowledge of genetics and molecular biology.
Less is known about secondary endosymbionts. The pea aphid ("Acyrthosiphon pisum") is known to contain at least three secondary endosymbionts, "Hamiltonella defensa", "Regiella insecticola", and "Serratia symbiotica". "H. defensa" aids in defending the insect from parasitoids. "Sodalis glossinidius" is a secondary endosymbiont of tsetse flies that lives inter- and intracellularly in various host tissues, including the midgut and hemolymph. Phylogenetic studies have not indicated a correlation between evolution of "Sodalis" and tsetse. Unlike tsetse's P-symbiont "Wigglesworthia", though, "Sodalis" has been cultured "in vitro".
Viral endosymbionts and endogenous retrovirus.
During pregnancy in viviparous mammals, endogenous retroviruses (ERVs) are activated and produced in high quantities during the implantation of the embryo. On one hand, they are hypothesized to act as immunosuppressors, perhaps protecting the embryo from the immune system of the mother; on the other hand viral fusion proteins cause the formation of the placental syncytium in order to limit the exchange of migratory cells between the developing embryo and the body of the mother, where an epithelium will not be adequate because certain blood cells are specialized to be able to insert themselves between adjacent epithelial cells. The ERV is an endogenized form of what was once an infectious retrovirus. The immunodepressive action was important for the infection of the original virus. The fusion proteins may have been a way to spread the infection to other cells by simply merging them with the infected one. It is believed that the ancestors of modern viviparous mammals evolved after an infection of an ancestor with this virus, perhaps improving the ability of the fetus to survive the immune system of the mother.
The human genome project found several thousand ERVs, which are organized into 24 families.

</doc>
<doc id="9678" url="https://en.wikipedia.org/wiki?curid=9678" title="Exponential function">
Exponential function

In mathematics, an exponential function is a function of the form
The input variable occurs as an exponent – hence the name. A function of the form is also considered an exponential function, and a function of the form can be re-written as by the use of logarithms and so is an exponential function.
In contexts where the base "b" is not specified, especially in more theoretical contexts, the term exponential function is almost always understood to mean the natural exponential function
also written as
where is Euler's number, a transcendental number approximately . The reason this number is considered the "natural" base of exponential functions is that this function is its own derivative. Every exponential function is directly proportional to its own derivative, but only when the base is does the constant of proportionality equal 1.
The exponential function is used to model a relationship in which a constant change in the independent variable gives the same proportional change (i.e. percentage increase or decrease) in the dependent variable. The function is often written as , especially when it is impractical to write the independent variable as a superscript. The exponential function is widely used in physics, chemistry, engineering, mathematical biology, economics and mathematics.
The graph of is upward-sloping, and increases faster as increases. The graph always lies above the -axis but can get arbitrarily close to it for negative ; thus, the -axis is a horizontal asymptote. The slope of the tangent to the graph at each point is equal to its -coordinate at that point. The inverse function is the natural logarithm ; because of this, some old texts refer to the exponential function as the antilogarithm.
In general, the variable can be any real or complex number or even an entirely different kind of mathematical object; see the formal definition below.
Formal definition.
The exponential function can be characterized in a variety of equivalent ways. In particular it may be defined by the following power series:
Using an alternate definition for the exponential function leads to the same result when expanded as a Taylor series.
Less commonly, is defined as the solution to the equation
It is also the following limit:
Overview.
The exponential function arises whenever a quantity grows or decays at a rate proportional to its current value. One such situation is continuously compounded interest, and in fact it was this that led Jacob Bernoulli in 1683 to the number
now known as . Later, in 1697, Johann Bernoulli studied the calculus of the exponential function.
If a principal amount of 1 earns interest at an annual rate of compounded monthly, then the interest earned each month is times the current value, so each month the total value is multiplied by , and the value at the end of the year is . If instead interest is compounded daily, this becomes . Letting the number of time intervals per year grow without bound leads to the limit definition of the exponential function,
first given by Euler.
This is one of a number of characterizations of the exponential function; others involve series or differential equations.
From any of these definitions it can be shown that the exponential function obeys the basic exponentiation identity,
which is why it can be written as .
The derivative (rate of change) of the exponential function is the exponential function itself. More generally, a function with a rate of change "proportional" to the function itself (rather than equal to it) is expressible in terms of the exponential function. This function property leads to exponential growth and exponential decay.
The exponential function extends to an entire function on the complex plane. Euler's formula relates its values at purely imaginary arguments to trigonometric functions. The exponential function also has analogues for which the argument is a matrix, or even an element of a Banach algebra or a Lie algebra.
Derivatives and differential equations.
The importance of the exponential function in mathematics and the sciences stems mainly from properties of its derivative. In particular,
Proof:
That is, is its own derivative and hence is a simple example of a Pfaffian function. Functions of the form for constant are the only functions with that property (by the Picard–Lindelöf theorem). Other ways of saying the same thing include:
If a variable's growth or decay rate is proportional to its size—as is the case in unlimited population growth (see Malthusian catastrophe), continuously compounded interest, or radioactive decay—then the variable can be written as a constant times an exponential function of time. Explicitly for any real constant , a function satisfies if and only if for some constant .
Furthermore, for any differentiable function , we find, by the chain rule:
Continued fractions for.
A continued fraction for can be obtained via an identity of Euler:
The following generalized continued fraction for converges more quickly:
or, by applying the substitution :
with a special case for :
This formula also converges, though more slowly, for . For example:
Complex plane.
As in the real case, the exponential function can be defined on the complex plane in several equivalent forms. One such definition parallels the power series definition for real numbers, where the real variable is replaced by a complex one:
The exponential function is periodic with imaginary period and can be written as
where and are real values and on the right the real functions must be used if used as a definition (see also Euler's formula). This formula connects the exponential function with the trigonometric functions and to the hyperbolic functions.
When considered as a function defined on the complex plane, the exponential function retains the properties
for all and .
The exponential function is an entire function as it is holomorphic over the whole complex plane. It takes on every complex number excepting 0 as value; that is, 0 is a lacunary value of the exponential function. This is an example of Picard's little theorem that any non-constant entire function takes on every complex number as value with at most one value excepted.
Extending the natural logarithm to complex arguments yields the complex logarithm , which is a multivalued function.
We can then define a more general exponentiation:
for all complex numbers and . This is also a multivalued function, even when is real. This distinction is problematic, as the multivalued functions and are easily confused with their single-valued equivalents when substituting a real number for . The rule about multiplying exponents for the case of positive real numbers must be modified in a multivalued context:
See failure of power and logarithm identities for more about problems with combining powers.
The exponential function maps any line in the complex plane to a logarithmic spiral in the complex plane with the center at the origin. Two special cases might be noted: when the original line is parallel to the real axis, the resulting spiral never closes in on itself; when the original line is parallel to the imaginary axis, the resulting spiral is a circle of some radius.
Computation of where both and are complex.
Complex exponentiation can be defined by converting to polar coordinates and using the identity :
However, when is not an integer, this function is multivalued, because is not unique (see failure of power and logarithm identities).
Matrices and Banach algebras.
The power series definition of the exponential function makes sense for square matrices (for which the function is called the matrix exponential) and more generally in any Banach algebra . In this setting, , and is invertible with inverse for any in . If , then , but this identity can fail for noncommuting and .
Some alternative definitions lead to the same function. For instance, can be defined as
Or can be defined as , where is the solution to the differential equation with initial condition .
Lie algebras.
Given a Lie group and its associated Lie algebra formula_28, the exponential map is a map formula_28 satisfying similar properties. In fact, since is the Lie algebra of the Lie group of all positive real numbers under multiplication, the ordinary exponential function for real arguments is a special case of the Lie algebra situation. Similarly, since the Lie group of invertible matrices has as Lie algebra , the space of all matrices, the exponential function for square matrices is a special case of the Lie algebra exponential map.
The identity can fail for Lie algebra elements and that do not commute; the Baker–Campbell–Hausdorff formula supplies the necessary correction terms.
Double exponential function.
The term double exponential function can have two meanings:
Factorials grow faster than exponential functions, but slower than double-exponential functions. Fermat numbers, generated by and double Mersenne numbers generated by are examples of double exponential functions.
Similar properties of and the function.
The function is not in (i.e., is not the quotient of two polynomials with complex coefficients).
For distinct complex numbers }, the set } is linearly independent over .
The function is transcendental over .
exp and expm1.
Some calculators provide a dedicated function designed to provide a higher precision than achievable by using directly.
Based on a proposal by William Kahan and first implemented in the Hewlett-Packard HP-41C calculator in 1979, some scientific calculators, computer algebra systems and programming languages (for example C99) support a special exponential minus 1 function alternatively named codice_1, codice_2, codice_3, or codice_4 to provide more accurate results for values of near zero compared to using codice_5 directly. This function is implemented using a different internal algorithm to avoid an intermediate result near 1, thereby allowing both the argument and the result to be near zero. Similar inverse functions named codice_6, codice_7 or codice_8 exist as well.

</doc>
<doc id="9679" url="https://en.wikipedia.org/wiki?curid=9679" title="Prince Eugene of Savoy">
Prince Eugene of Savoy

Prince Eugene of Savoy (French: "François-Eugène de Savoie", Italian: "Principe Eugenio di Savoia-Carignano", German: "Prinz Eugen von Savoyen"; 18 October 1663 – 21 April 1736) was a general of the Imperial Army and statesman of the Holy Roman Empire and the Archduchy of Austria and one of the most successful military commanders in modern European history, rising to the highest offices of state at the Imperial court in Vienna.
Born in Paris, Eugene grew up around the French court of King Louis XIV. Based on his poor physique and bearing, the Prince was initially prepared for a career in the church, but by the age of 19 he had determined on a military career. Rejected by Louis XIV for service in the French army, Eugene moved to Austria and transferred his loyalty to the Habsburg Monarchy.
Spanning six decades, Eugene served three Holy Roman Emperors: Leopold I, Joseph I, and Charles VI. He first saw action against the Ottoman Turks at the Siege of Vienna in 1683 and the subsequent War of the Holy League, before serving in the Nine Years' War, fighting alongside his cousin, the Duke of Savoy. However, the Prince's fame was secured with his decisive victory against the Ottomans at the Battle of Zenta in 1697, earning him Europe-wide fame. Eugene enhanced his standing during the War of the Spanish Succession, where his partnership with the Duke of Marlborough secured victories against the French on the fields of Blenheim (1704), Oudenarde (1708), and Malplaquet (1709); he gained further success in the war as Imperial commander in northern Italy, most notably at the Battle of Turin (1706). Renewed hostilities against the Ottomans in the Austro-Turkish War consolidated his reputation, with victories at the battles of Petrovaradin (1716), and the decisive encounter at Belgrade (1717).
Throughout the late 1720s, Eugene's influence and skilful diplomacy managed to secure the Emperor powerful allies in his dynastic struggles with the Bourbon powers, but physically and mentally fragile in his later years, Eugene enjoyed less success as commander-in-chief of the army during his final conflict, the War of the Polish Succession. Nevertheless, in Austria, Eugene's reputation remains unrivalled. Although opinions differ as to his character, there is no dispute over his great achievements: he helped to save the Habsburg Empire from French conquest; he broke the westward thrust of the Ottomans, liberating central Europe after a century and a half of Turkish occupation; and he was one of the great patrons of the arts whose building legacy can still be seen in Vienna today. Eugene died in his sleep at his home on 21 April 1736, aged 72.
Early life (1663–99).
Hôtel de Soissons.
Prince Eugene was born in the Hôtel de Soissons in Paris on 18 October 1663. His mother, Olympia Mancini, was one of Cardinal Mazarin's nieces whom he had brought to Paris from Rome in 1647 to further his, and, to a lesser extent, their ambitions. The Mancinis were raised at the Palais-Royal along with the young Louis XIV, with whom Olympia formed an intimate relationship. Yet to her great disappointment, her chance to become queen passed by, and in 1657, Olympia married Eugene Maurice, Count of Soissons, Count of Dreux and Prince of Savoy. Together they had had five sons (Eugene being the youngest) and three daughters, but neither parent spent much time with the children: his father, a brave, unglamorous French soldier, spent much of his time away campaigning, while Olympia's passion for court intrigue meant the children received little attention from her.
The King remained strongly attached to Olympia, so much so that many believed them to be lovers; but her scheming eventually led to her downfall. After falling out of favour at court, Olympia turned to Catherine Deshayes (known as "La Voisin"), and the arts of black magic and astrology. It was a fatal relationship. Embroiled in the "affaire des poisons", suspicions now abounded of her involvement in her husband's premature death in 1673, and even implicated her in a plot to kill the King himself. Whatever the truth, Olympia, rather than face trial, subsequently fled France for Brussels in January 1680, leaving Eugene in the care of his father's mother, Marie de Bourbon, and her daughter, Hereditary Princess of Baden, mother of Prince Louis of Baden.
From the age of ten, Eugene had been brought up for a career in the church; a personal choice of the King, basing the decision on the young Prince's poor physique and bearing. Certainly Eugene's appearance was not impressive — "He was never good-looking …" wrote the Duchess of Orléans, "It is true that his eyes are not ugly, but his nose ruins his face; he has two large teeth which are visible at all times."
In February 1683, to the surprise of his family, Eugene declared his intention of joining the army. Now 19 years old, Eugene applied directly to Louis XIV for command of a company in French service, but the King – who had shown no compassion for Olympia's children since her disgrace – refused him out of hand. "The request was modest, not so the petitioner," he remarked. "No one else ever presumed to stare me out so insolently."
Denied a military career in France, Eugene decided to seek service abroad. One of Eugene's brothers, Louis Julius, had entered Imperial service the previous year, but he had been immediately killed fighting the Ottoman Turks in 1683. When news of his death reached Paris, Eugene decided to travel to Austria in the hope of taking over his brother's command. It was not an unnatural decision: his cousin, Louis of Baden, was already a leading general in the Imperial army, as was a more distant cousin, Maximilian II Emanuel, Elector of Bavaria. On the night of July 26, 1683, Eugene left Paris and headed east.
Great Turkish War.
By May 1683, the Ottoman threat to Emperor Leopold I's capital, Vienna, was very real. The Grand Vizier, Kara Mustafa Pasha – encouraged by Imre Thököly's Magyar rebellion – had invaded Hungary with between 100,000–200,000 men; within two months approximately 90,000 were beneath Vienna's walls. With the 'Turks at the gates', the Emperor fled for the safe refuge of Passau up the Danube, a more distant and secure part of his dominion. It was at Leopold I's camp that Eugene arrived in mid-August.
Although Eugene was not of Austrian extraction, he did have Habsburg antecedents. His grandfather, Thomas Francis, founder of the Carignano line of the House of Savoy, was the son of Catherine Michelle – a daughter of Philip II of Spain – and the great-grandson of the Emperor Charles V. But of more immediate consequence to Leopold I was the fact that Eugene was the second cousin of Victor Amadeus, the Duke of Savoy, a connection that the Emperor hoped might prove useful in any future confrontation with France. These ties, together with his ascetic manner and appearance (a positive advantage to him at the sombre court of Leopold I), ensured the refugee from the hated French king a warm welcome at Passau, and a position in Imperial service. Though French was his favored language, he communicated with Leopold in Italian, as the Emperor (though he knew it perfectly) disliked French. But Eugene also had a reasonable command of German, which he understood very easily, something that helped him much in the military.
Eugene was in no doubt where his new allegiance lay – "I will devote all my strength, all my courage, and if need be, my last drop of blood, to the service of your Imperial Majesty." This loyalty was immediately put to the test. By September, the Imperial forces under the Duke of Lorraine, together with a powerful Polish army under King John III Sobieski, were poised to strike the Sultan's army. On the morning of 12 September, the Christian forces drew up in line of battle on the south-eastern slopes of the Vienna Woods, looking down on the massed enemy camp. The day-long Battle of Vienna resulted in the lifting of the 60-day siege, and the Sultan's forces were routed and in retreat. Serving under Baden, Eugene distinguished himself in the battle, earning commendation from Lorraine and the Emperor; he later received the nomination for the colonelcy of the Dragoon Regiment Kufstein.
Holy League.
In March 1684, Leopold I formed the Holy League with Poland and Venice to counter the Ottoman threat. For the next two years, Eugene continued to perform with distinction on campaign and establish himself as a dedicated, professional soldier; by the end of 1685, still only 22 years old, he was made a Major-General. However, little is known of Eugene's life during these early campaigns. Contemporary observers make only passing comments of his actions, and his own surviving correspondence, largely to his cousin Victor Amadeus, are typically reticent about his own feelings and experiences. Nevertheless, it is clear that Baden was impressed with Eugene's qualities – "This young man will, with time, occupy the place of those whom the world regards as great leaders of armies."
In June 1686, the Duke of Lorraine besieged Buda (Budapest), the centre of the Ottoman occupation in Hungary. After resisting for 78 days, the city fell on 2 September, and Turkish resistance collapsed throughout the region as far away as Transylvania and Serbia. Further success followed in 1687, where, commanding a cavalry brigade, Eugene made an important contribution to the victory at the Battle of Mohács on 12 August. Such was the scale of their defeat that the Ottoman army mutinied – a revolt which spread to Constantinople. The Grand Vizier, Suluieman Pasha, was executed and Sultan Mehmed IV, deposed. Once again, Eugene's courage earned him recognition from his superiors, who granted him the honour of personally conveying the news of victory to the Emperor in Vienna. For his services, Eugene was promoted to Lieutenant-General in November 1687. He was also gaining wider recognition. King Charles II of Spain bestowed upon him the Order of the Golden Fleece, while his cousin, Victor Amadeus, provided him with money and two profitable abbeys in Piedmont. However, Eugene's military career suffered a temporary setback in 1688 when, on 6 September, the Prince suffered a severe wound to his knee by a musket ball during the Siege of Belgrade. It was not until January 1689 that he could return to active service.
Interlude in the west: Nine Years' War.
Just as Belgrade was falling to Imperial forces under Max Emmanuel in the east, French troops in the west were crossing the Rhine into the Holy Roman Empire. Louis XIV had hoped that a show of force would lead to a quick resolution to his dynastic and territorial disputes with the princes of the Empire along his eastern border, but his intimidatory moves only strengthened German resolve, and in May 1689, Leopold I and the Dutch signed an offensive compact aimed at repelling French aggression.
The Nine Years' War was professionally and personally frustrating for the Prince. Initially fighting on the Rhine with Max Emmanuel – receiving a slight head wound at the Siege of Mainz in 1689 – Eugene subsequently transferred himself to Piedmont after Victor Amadeus joined the Alliance against France in 1690. Promoted to general of cavalry, he arrived in Turin with his friend the Prince of Commercy; but it proved an inauspicious start. Against Eugene's advice, Amadeus insisted on engaging the French at Staffarda and suffered a serious defeat – only Eugene's handling of the Savoyard cavalry in retreat saved his cousin from disaster. Eugene remained unimpressed with the men and their commanders throughout the war in Italy. "The enemy would long ago have been beaten," he wrote to Vienna, "if everyone had done their duty." So contemptuous was he of the Imperial commander, Count Caraffa, he threatened to leave Imperial service.
In Vienna, Eugene's attitude was dismissed as the arrogance of a young upstart, but so impressed was the Emperor by his passion for the Imperial cause, he promoted him to Field-Marshal in 1693. When Caraffa's replacement, Count Caprara, was himself transferred in 1694, it seemed that Eugene's chance for command and decisive action had finally arrived. But Amadeus, doubtful of victory and now more fearful of Habsburg influence in Italy than he was of French, had begun secret dealings with Louis XIV aimed at extricating himself from the war. By 1696, the deal was done, and Amadeus transferred his troops and his loyalty to the enemy. Eugene was never to fully trust his cousin again; although he continued to pay due reverence to the Duke as head of his family, their relationship would forever after remain strained.
Military honours in Italy undoubtedly belonged to the French commander Marshal Catinat, but Eugene, the one Allied general determined on action and decisive results, did well to emerge from the Nine Years' War with an enhanced reputation. With the signing of the Treaty of Ryswick in September/October 1697, the desultory war in the west was finally brought to an inconclusive end, and Leopold I could once again devote all his martial energies into defeating the Ottoman Turks in the east.
Zenta.
The distractions of the war against Louis XIV had enabled the Turks to recapture Belgrade in 1690. In August 1691, the Austrians, under Louis of Baden, regained the advantage by heavily defeating the Turks at the Battle of Slankamen on the Danube, securing Habsburg possession of Hungary and Transylvania. However, when Baden was transferred west to fight the French in 1692, his successors, first Caprara, then from 1696, Frederick Augustus, the Elector of Saxony, proved incapable of delivering the final blow. On the advice of the President of the Imperial War Council, Rüdiger Starhemberg, Eugene was offered supreme command of Imperial forces in April 1697. This was Eugene's first truly independent command – no longer need he suffer under the excessively cautious generalship of Caprara and Caraffa, or be thwarted by the deviations of Victor Amadeus. But on joining his army, he found it in a state of 'indescribable misery'. Confident and self-assured, the Prince of Savoy (ably assisted by Commercy and Guido Starhemberg) set about restoring order and discipline.
Leopold I had warned Eugene to act cautiously, but when the Imperial commander learnt of Sultan Mustafa II's march on Transylvania, Eugene abandoned all ideas of a defensive campaign and moved to intercept the Turks as they crossed the River Tisza at Zenta on 11 September 1697. It was late in the day before the Imperial army struck. The Turkish cavalry had already crossed the river so Eugene decided to attack immediately, arranging his men in a half-moon formation. The vigour of the assault wrought terror and confusion amongst the Turks, and by nightfall, the battle was won. For the loss of some 2,000 dead and wounded, Eugene had inflicted approximately 25,000 casualties on his enemy – including the Grand Vizier, Elmas Mehmed Pasha – annihilating the Turkish army. Although the Ottomans lacked western organisation and training, the Savoyard prince had revealed his tactical skill, his capacity for bold decision, and his ability to inspire his men to excel in battle against a dangerous foe.
After a brief terror-raid into Ottoman-held Bosnia, culminating in the sack of Sarajevo, Eugene returned to Vienna in November to a triumphal reception. His victory at Zenta had turned him into a European hero, and with victory came reward. Land in Hungary, given him by the Emperor, yielded a good income, enabling the Prince to cultivate his newly acquired tastes in art and architecture (see below); but for all his new-found wealth and property, he was, nevertheless, without personal ties or family commitments. Of his four brothers, only one was still alive at this time. His fourth brother, Emmanuel, had died aged 14 in 1676; his third, Louis Julius (already mentioned) had died on active service in 1683, and his second brother, Philippe, died of smallpox in 1693. Eugene's remaining brother, Louis Thomas – ostracised for incurring the displeasure of Louis XIV – travelled Europe in search of a career, before arriving in Vienna in 1699. With Eugene's help, Louis found employment in the Imperial army, only to be killed in action against the French in 1702. Of Eugene's sisters, the youngest had died in childhood. The other two, Marie Jeanne-Baptiste and Louise Philiberte, led dissolute lives. Expelled from France, Marie joined her mother in Brussels, before eloping with a renegade priest to Geneva, living with him unhappily until her premature death in 1705. Of Louise, little is known after her early salacious life in Paris, but in due course, she lived for a time in a convent in Savoy before her death in 1726.
The Battle of Zenta proved to be the decisive victory in the long war against the Turks. With Leopold I's interests now focused on Spain and the imminent death of Charles II, the Emperor terminated the conflict with the Sultan, and signed the Treaty of Karlowitz on 26 January 1699.
Mid life (1700–20).
War of the Spanish Succession.
With the death of the infirm and childless Charles II of Spain on 1 November 1700, the succession of the Spanish throne and subsequent control over her empire once again embroiled Europe in war – the War of the Spanish Succession. On his deathbed Charles II had bequeathed the entire Spanish inheritance to Louis XIV's grandson, Philip, Duke of Anjou. This threatened to unite the Spanish and French kingdoms under the House of Bourbon – something unacceptable to England, the Dutch Republic, and Leopold I, who had himself a claim to the Spanish throne. From the beginning, the Emperor had refused to accept the will of Charles II, and he did not wait for England and the Dutch Republic to begin hostilities. Before a new Grand Alliance could be concluded Leopold I prepared to send an expedition to seize the Spanish lands in Italy.
Eugene crossed the Alps with some 30,000 men in May/June 1701. After a series of brilliant manoeuvres the Imperial commander defeated Catinat at the Battle of Carpi on 9 July. "I have warned you that you are dealing with an enterprising young prince," wrote Louis XIV to his commander, "he does not tie himself down to the rules of war." On 1 September Eugene defeated Catinat's successor, Marshal Villeroi, at the Battle of Chiari, in a clash as destructive as any in the Italian theatre. But as so often throughout his career the Prince faced war on two fronts – the enemy in the field and the government in Vienna. Starved of supplies, money and men, Eugene was forced into unconventional means against the vastly superior enemy. During a daring raid on Cremona on the night of 31 January/1 February 1702 Eugene captured the French commander-in-chief. Yet the coup was less successful than hoped: Cremona remained in French hands, and the Duke of Vendôme, whose talents far exceeded Villeroi's, became the theatre's new commander. Villeroi's capture caused a sensation in Europe, and had a galvanising effect on English public opinion. "The surprise at Cremona," wrote the diarist John Evelyn, "… was the greate discourse of this weeke"; but appeals for succour from Vienna remained unheeded, forcing Eugene to seek battle and gain a 'lucky hitt'. The resulting Battle of Luzzara on 15 August proved inconclusive. Although Eugene's forces inflicted double the number of casualties on the French the battle settled little except to deter Vendôme trying an all-out assault on Imperial forces that year, enabling Eugene to hold on south of the Alps. With his army rotting away, and personally grieving for his long standing friend Prince Commercy who had died at Luzzara, Eugene returned to Vienna in January 1703.
President of the Imperial War Council.
Eugene's European reputation was growing (Cremona and Luzzara had been celebrated as victories throughout the Allied capitals), yet because of the condition and morale of his troops the 1702 campaign had not been a success. Austria itself was now facing the direct threat of invasion from across the border in Bavaria where the state's Elector, Maximilian Emanuel, had declared for the Bourbons in August the previous year. Meanwhile, in Hungary a small-scale revolt had broken out in May and was fast gaining momentum. With the monarchy at the point of complete financial breakdown Leopold I was at last persuaded to change the government. At the end of June 1703 Gundaker Starhemberg replaced Gotthard Salaburg as President of the Treasury, and Prince Eugene succeeded Henry Mansfeld as the new President of the Imperial War Council ("Hofkriegsratspräsident").
As head of the war council Eugene was now part of the Emperor's inner circle, and the first president since Montecuccoli to remain an active commander. Immediate steps were taken to improve efficiency within the army: encouragement and, where possible, money, was sent to the commanders in the field; promotion and honours were distributed according to service rather than influence; and discipline improved. But the Austrian monarchy faced severe peril on several fronts in 1703: by June the Duke of Villars had reinforced the Elector of Bavaria on the Danube thus posing a direct threat to Vienna, while Vendôme remained at the head of a large army in northern Italy opposing Guido Starhemberg's weak Imperial force. Of equal alarm was Francis II Rákóczi's revolt which, by the end of the year, had reached as far as Moravia and Lower Austria.
Blenheim.
Dissension between Villars and the Elector of Bavaria had prevented an assault on Vienna in 1703, but in the Courts of Versailles and Madrid, ministers confidently anticipated the city's fall. The Imperial ambassador in London, Count Wratislaw, had pressed for Anglo-Dutch assistance on the Danube as early as February 1703, but the crisis in southern Europe seemed remote from the Court of St. James's where colonial and commercial considerations were more to the fore of men's minds. Only a handful of statesmen in England or the Dutch Republic realised the true implications of Austria's peril; foremost amongst these was the English Captain-General, the Duke of Marlborough.
By early 1704 Marlborough had resolved to march south and rescue the situation in southern Germany and on the Danube, personally requesting the presence of Eugene on campaign so as to have "a supporter of his zeal and experience". The Allied commanders met for the first time at the small village of Mundelsheim on 10 June, and immediately formed a close rapport – the two men becoming, in the words of Thomas Lediard, 'Twin constellations in glory'. This professional and personal bond ensured mutual support on the battlefield, enabling many successes during the Spanish Succession war. The first of these victories, and the most celebrated, came on 13 August 1704 at the Battle of Blenheim. Eugene commanded the right wing of the Allied army, holding the Elector of Bavaria's and Marshal Marsin's superior forces, while Marlborough broke through the Marshal Tallard's center, inflicting over 30,000 casualties. The battle proved decisive: Vienna was saved and Bavaria was knocked out of the war. Both Allied commanders were full of praise for each other's performance. Eugene's holding operation, and his pressure for action leading up to the battle, proved crucial for the Allied success.
In Europe Blenheim is regarded as much a victory for Eugene as it is for Marlborough, a sentiment echoed by Sir Winston Churchill (Marlborough's descendant and biographer), who pays tribute to "the glory of Prince Eugene, whose fire and spirit had exhorted the wonderful exertions of his troops." France now faced the real danger of invasion, but Leopold I in Vienna was still under severe strain: Rákóczi's revolt was a major threat; and Guido Starhemberg and Victor Amadeus (who had once again switched loyalties and rejoined the Grand Alliance in 1703) had been unable to halt the French under Vendôme in northern Italy. Only Amadeus' capital, Turin, held on.
Turin and Toulon.
Eugene returned to Italy in April 1705, but his attempts to move west towards Turin were thwarted by Vendôme's skilful manoeuvres. Lacking boats and bridging materials, and with desertion and sickness rife within his army, the outnumbered Imperial commander was helpless. Leopold I's assurances of money and men had proved illusory, but desperate appeals from Amadeus and criticism from Vienna goaded the Prince into action, resulting in the Imperialists' bloody defeat at the Battle of Cassano on 16 August. However, following Leopold I's death and the accession of Joseph I to the Imperial throne in May 1705, Eugene at last began to receive the personal backing he desired. Joseph I proved to be a strong supporter of Eugene's supremacy in military affairs; he was the most effective emperor the Prince served and the one he was happiest under. Promising support, Joseph I persuaded Eugene to return to Italy and restore Habsburg honour.
The Imperial commander arrived in theatre in mid-April 1706, just in time to organise an orderly retreat of what was left of Count Reventlow's inferior army following his defeat by Vendôme at the Battle of Calcinato on 19 April. Vendôme now prepared to defend the lines along the river Adige, determined to keep Eugene cooped to the east while the Marquis of La Feuillade threatened Turin. However, feigning attacks along the Adige, Eugene descended south across the river Po in mid-July, outmanoeuvring the French commander and gaining a favourable position from which he could at last move west towards Piedmont and relieve Savoy's capital.
Events elsewhere were now to have major consequences for the war in Italy. With Villeroi's crushing defeat by Marlborough at the Battle of Ramillies on 23 May, Louis XIV recalled Vendôme north to take command of French forces in Flanders. It was a transfer that Saint-Simon considered something of a deliverance for the French commander who was "now beginning to feel the unlikelihood of success n Ital … for Prince Eugene, with the reinforcements that had joined him after the Battle of Calcinato, had entirely changed the outlook in that theatre of the war." The Duke of Orléans, under the direction of Marsin, replaced Vendôme, but indecision and disorder in the French camp led to their undoing. After uniting his forces with Victor Amadeus at Villastellone in early September, Eugene attacked, overwhelmed, and decisively defeated the French forces besieging Turin on 7 September. Eugene's success broke the French hold on northern Italy, and the whole Po valley fell under Allied control. Eugene had gained a victory as signal as his colleague had at Ramillies – "It is impossible for me to express the joy it has given me;" wrote Marlborough, "for I not only esteem but I really love the prince. This glorious action must bring France so low, that if our friends could but be persuaded to carry on the war with vigour one year longer, we cannot fail, with the blessing of God, to have such a peace as will give us quiet for all our days."
The Imperial victory in Italy marked the beginning of Austrian rule in Lombardy, and earned Eugene the Governorship of Milan. But the following year was to prove a disappointment for the Prince and the Grand Alliance as a whole. The Emperor and Eugene (whose main goal after Turin was to take Naples and Sicily from Philip duc d'Anjou's supporters), reluctantly agreed to Marlborough's plan for an attack on Toulon – the seat of French naval power in the Mediterranean. However, disunion between the Allied commanders – Victor Amadeus, Eugene, and the English Admiral Shovell – doomed the Toulon enterprise to failure. Although Eugene favoured some sort of attack on France's south-eastern border it was clear he felt the expedition impractical, and had shown none of the "alacrity which he had displayed on other occasions." Substantial French reinforcements finally brought an end to the venture, and on 22 August 1707 the Imperial army began its retirement. The subsequent capture of Susa could not compensate for the total collapse of the Toulon expedition and with it any hope of an Allied war-winning blow that year.
Oudenarde and Malplaquet.
At the beginning of 1708 Eugene successfully evaded calls for him to take charge in Spain (in the end Guido Starhemberg was sent), thus enabling him to take command of the Imperial army on the Moselle and once again unite with Marlborough in the Spanish Netherlands. Eugene (without his army) arrived at the Allied camp at Assche, west of Brussels, in early July, providing a welcome boost to morale after the early defection of Bruges and Ghent to the French. " … our affairs improved through God's support and Eugene's aid," wrote the Prussian General Natzmer, "whose timely arrival raised the spirits of the army again and consoled us." Heartened by the Prince's confidence the Allied commanders devised a bold plan to engage the French army under Vendôme and the Duke of Burgundy. On 10 July the Anglo-Dutch army made a forced march to surprise the French, reaching the river Scheldt just as the enemy were crossing to the north. The ensuing battle on 11 July – more a contact action rather than a set-piece engagement – ended in a resounding success for the Allies, aided by the dissension of the two French commanders. While Marlborough remained in overall command, Eugene had led the crucial right flank and centre. Once again the Allied commanders had co-operated remarkably well. "Prince Eugene and I," wrote the Duke, "shall never differ about our share of the laurels."
Marlborough now favoured a bold advance along the coast to bypass the major French fortresses, followed by a march on Paris. But fearful of unprotected supply-lines, the Dutch and Eugene favoured a more cautious approach. Marlborough acquiesced and resolved upon the siege of Vauban's great fortress, Lille. While the Duke commanded the covering force, Eugene oversaw the siege of the town which surrendered on 22 October; however, it was not until 10 December that the resolute Marshal Boufflers yielded the citadel. Yet for all the difficulties of the siege (Eugene was badly wounded above his left eye by a musket ball, and even survived an attempt to poison him), the campaign of 1708 had been a remarkable success. The French were driven out of almost all the Spanish Netherlands. "He who has not seen this," wrote Eugene, "has seen nothing."
The recent defeats, together with the severe winter of 1708–09, had caused extreme famine and privation in France. Louis XIV was close to accepting Allied terms, but the conditions demanded by the leading Allied negotiators, Anthonie Heinsius, Charles Townshend, Marlborough, and Eugene – principally that Louis XIV should use his own troops to force Philip V off the Spanish throne – proved unacceptable to the French. Neither Eugene nor Marlborough had objected to the Allied demands at the time, but neither wanted the war with France to continue, and would have preferred further talks to deal with the Spanish issue. But the French King offered no further proposals. Lamenting the collapse of the negotiations, and aware of the vagaries of war, Eugene wrote to the Emperor in mid-June 1709. "There can be no doubt that the next battle will be the biggest and bloodiest that has yet been fought."
After the fall of Tournai on 3 September (itself a major undertaking), the Allied generals turned their attention towards Mons. Marshal Villars, recently joined by Boufflers, moved his army south-west of the town and began to fortify his position. Marlborough and Eugene favoured an engagement before Villars could render his position impregnable; but they also agreed to wait for reinforcements from Tournai which did not arrive until the following night, thus giving the French further opportunity to prepare their defences. Notwithstanding the difficulties of the attack, however, the Allied generals did not shrink from their original determination. The subsequent Battle of Malplaquet, fought on 11 September 1709, was the bloodiest engagement of the war. On the left flank, the Prince of Orange led his Dutch infantry in desperate charges only to have it cut to pieces; on the other flank, Eugene attacked and suffered almost as severely. But sustained pressure on his extremities forced Villars to weaken his centre, thus enabling Marlborough to breakthrough and claim victory. Villars was unable to save Mons, which subsequently capitulated on 21 October, but his resolute defence at Malplaquet – inflicting up to 25% casualties on the Allies – may have saved France from destruction.
Final campaigning: Eugene alone.
In August 1709 Eugene's chief political opponent and critic in Vienna, Prince Salm, retired as court chamberlain. Eugene and Wratislaw were now the undisputed leaders of the Austrian government: all major departments of state were in their hands or those of their political allies. However, another attempt at a negotiated settlement at Geertruidenberg in April 1710 failed, largely because the English Whigs still felt strong enough to refuse concessions, while Louis XIV saw little reason to accept what he had refused the previous year. Eugene and Marlborough could not be accused of wrecking the negotiations, but neither showed regret at the breakdown of the talks. There was no alternative but to continue the war, and in June the Allied commanders captured Douai. This success was followed by a series of minor sieges, and by the close of 1710 the Allies had cleared much of France's protective ring of fortresses. Yet there had been no final, decisive breakthrough, and this was to be the last year that Eugene and Marlborough would work together.
Following the death of Joseph I on 17 April 1711 his brother, Charles, the pretender to the Spanish throne, became emperor. In England the new Tory government (the 'peace party' who had deposed the Whigs in October 1710) declared their unwillingness to see Charles VI become Emperor as well as King of Spain, and had already begun secret negotiations with the French. In January 1712 Eugene arrived in England hoping to divert the government away from its peace policy, but despite the social success the visit was a political failure: Queen Anne and her ministers remained determined to end the war regardless of the Allies. Eugene had also arrived too late to save Marlborough who, seen by the Tories as the main obstacle to peace, had already been dismissed on charges of embezzlement. Elsewhere, however, the Austrians had made some progress – the Hungarian revolt had finally came to end. Although Eugene would have preferred to crush the rebels the Emperor had offered lenient conditions, leading to the signing of the Treaty of Szatmár on 30 April 1711.
Hoping to influence public opinion in England and force the French into making substantial concessions, Eugene prepared for a major campaign. However, on 21 May 1712 – when the Tories felt they had secured favourable terms with their unilateral talks with the French – the Duke of Ormonde (Marlborough's successor) received the so-called 'restraining orders', forbidding him to take part in any military action. Eugene took the fortress of Le Quesnoy in early July, before besieging Landrecies, but Villars, taking advantage of Allied disunity, outmanoeuvred Eugene and defeated the Earl of Albermarle's Dutch garrison at Denain on 24 July. The French followed the victory by seizing the Allies' main supply magazine at Marchiennes, before reversing their earlier losses at Douai, Le Quesnoy and Bouchain. In one summer the whole forward Allied position laboriously built up over the years to act as the springboard into France had been precipitously abandoned.
With the death in December of his friend and close political ally, Count Wratislaw, Eugene became undisputed 'first minister' in Vienna. His position was built on his military successes, but his actual power was expressed through his role as president of the war council, and as "de facto" president of the conference which dealt with foreign policy. In this position of influence Eugene took the lead in pressing Charles VI towards peace. The government had come to accept that further war in the Netherlands or Spain was impossible without the aid of the Maritime Powers; yet the Emperor, still hoping that somehow he could place himself on the throne in Spain, refused to make peace at the Utrecht conference along with the other Allies. Reluctantly, Eugene prepared for another campaign, but lacking troops, finance, and supplies his prospects in 1713 were poor. Villars, with superior numbers, was able to keep Eugene guessing as to his true intent. Through successful feints and stratagems Landau fell to the French commander in August, followed in November by Freiburg. Eugene was reluctant to carry on the war, and wrote to the Emperor in June that a bad peace would be better than being 'ruined equally by friend and foe'. With Austrian finances exhausted and the German states reluctant to continue the war, Charles VI was compelled to enter into negotiations. Eugene and Villars (who had been old friends since the Turkish campaigns of the 1680s) initiated talks on 26 November. Eugene proved an astute and determined negotiator, and gained favourable terms by the Treaty of Rastatt signed on 7 March 1714 and the Treaty of Baden signed on 7 September 1714. Despite the failed campaign in 1713 the Prince was able to declare that, "in spite of the military superiority of our enemies and the defection of our Allies, the conditions of peace will be more advantageous and more glorious than those we would have obtained at Utrecht."
Austro-Turkish War.
Eugene's main reason for desiring peace in the west was the growing danger posed by the Turks in the east. Turkish military ambitions had revived after 1711 when they had mauled Peter the Great's army on the river Pruth: in December 1714 Sultan Ahmed III's forces attacked the Venetians in the Morea. To Vienna it was clear that the Turks intended to attack Hungary and undo the whole Karlowitz settlement of 1699. After the Porte rejected an offer of mediation in April 1716, Charles VI despatched Eugene to Hungary to lead his relatively small but professional army. Of all Eugene's wars this was the one in which he exercised most direct control; it was also a war which, for the most part, Austria fought and won on her own.
Eugene left Vienna in early June 1716 with a field army of between 80,000–90,000 men. By early August 1716 the Ottoman Turks, some 200,000 men under the sultan's son-in-law, the Grand Vizier Damat Ali Pasha, were marching from Belgrade towards Eugene's position west of the fortress of Petrovaradin on the north bank of the Danube. The Grand Vizier had intended to seize the fortress; but Eugene gave him no chance to do so. After resisting calls for caution and forgoing a council of war, the Prince decided to attack immediately on the morning of 5 August with approximately 70,000 men. The Turkish janissaries had some initial success, but after an Imperial cavalry attack on their flank, Ali Pasha's forces fell into confusion. Although the Imperials lost almost 5,000 dead or wounded, the Turks, who retreated in disorder to Belgrade, seem to have lost double that amount, including the Grand Vizier himself who had entered the mêlée and subsequently died of his wounds.
Eugene proceeded to take the Banat fortress of Timișoara (Temeswar in German) in mid-October 1716 (thus ending 164 years of Turkish rule), before turning his attention to the next campaign and to what he considered the main goal of the war, Belgrade. Situated at the confluence of the Rivers Danube and Sava, Belgrade held a garrison of 30,000 men under Mustapha Pasha. Imperial troops besieged the place in mid-June 1717, and by the end of July large parts of the city had been destroyed by artillery fire. By the first days of August, however, a huge Turkish field army (150,000–200,000 strong), under the new Grand Vizier, Halil Pasha, had arrived on the plateau east of the city to relieve the garrison. News spread through Europe of Eugene's imminent destruction; but he had no intention of lifting the siege. With his men suffering from dysentery, and continuous bombardment from the plateau, Eugene, aware that a decisive victory alone could extricate his army, decided to attack the relief force. On the morning of 16 August 40,000 Imperial troops marched through the fog, caught the Turks unawares, and routed Halil Pasha's army; a week later Belgrade surrendered, effectively bringing an end to the war. The victory was the crowning point of Eugene's military career and had confirmed him as the leading European general. His ability to snatch victory at the moment of defeat had shown the Prince at his best.
The principal objectives of the war had been achieved: the task Eugene had begun at Zenta was complete, and the Karlowitz settlement secured. By the terms of the Treaty of Passarowitz, signed on 21 July 1718, the Turks surrendered the Banat of Temeswar, along with Belgrade and most of Serbia, although they regained the Morea from the Venetians. The war had dispelled the immediate Turkish threat to Hungary, and was a triumph for the Empire and for Eugene personally.
Quadruple Alliance.
While Eugene fought the Turks in the east, unresolved issues following the Utrecht/Rastatt settlements led to hostilities between the Emperor and Philip V of Spain in the west. Charles VI had refused to recognise Philip V as King of Spain, a title which he himself claimed; in return, Philip V had refused to renounce his claims to Naples, Milan, and the Netherlands, all of which had transferred to the House of Austria following the Spanish Succession war. Philip V was roused by his influential wife, Elisabeth Farnese, daughter of the Hereditary Prince of Parma, who personally held dynastic claims in the name of her son, Don Charles, to the duchies of Tuscany, Parma and Piacenza. Representatives from a newly formed Anglo-French alliance – who were desirous of European peace for their own dynastic securities and trade opportunities – called on both parties to recognise each other's sovereignty. Yet Philip V remained intractable, and on 22 August 1717 his chief minister, Alberoni, effected the invasion of Austrian Sardinia in what seemed like the beginning of the reconquest of Spain's former Italian empire.
Eugene returned to Vienna from his recent victory at Belgrade (before the conclusion of the Turkish war) determined to prevent an escalation of the conflict, complaining that, "two wars cannot be waged with one army"; only reluctantly did the Prince release some troops from the Balkans for the Italian campaign. Rejecting all diplomatic overtures Philip V unleashed another assault in June 1718, this time against Savoyard Sicily as a preliminary to attacking the Italian mainland. Realising that only the British fleet could prevent further Spanish landings, and that pro-Spanish groups in France might push the regent, Duke of Orléans, into war against Austria, Charles VI had no option but to sign the Quadruple Alliance on 2 August 1718, and formally renounce his claim to Spain. Despite the Spanish fleet's destruction off Cape Passaro, Philip V and Elisabeth remained resolute, and rejected the treaty.
Although Eugene could have gone south after the conclusion of the Turkish war, he chose instead to conduct operations from Vienna; but Austria's military effort in Sicily proved derisory, and Eugene's chosen commanders, Zum Jungen, and later Count Mercy, performed poorly. It was only from pressure exerted by the French army advancing into the Basque provinces of northern Spain in April 1719, and the British Navy's attacks on the Spanish fleet and shipping, that compelled Philip V and Elisabeth to dismiss Alberoni and join the Quadruple Alliance on 25 January 1720. Nevertheless, the Spanish attacks had strained Charles VI's government, causing tension between the Emperor and his Spanish Council on the one hand, and the conference, headed by Eugene, on the other. Despite Charles VI's own personal ambitions in the Mediterranean it was clear to the Emperor that Eugene had put the safeguarding of his conquests in Hungary before everything else, and that military failure in Sicily also had to rest on Eugene. Consequently, the Prince's influence over the Emperor declined considerably.
Later life (1721–36).
Governor-General of the Southern Netherlands.
Eugene had become governor of the Southern Netherlands – then the Austrian Netherlands – in June 1716, but he was an absent ruler, directing policy from Vienna through his chosen representative the Marquis of Prié. De Prié proved unpopular with the local population and the guilds who, following the Barrier Treaty of 1715, were obliged to meet the financial demands of the administration and the Dutch barrier garrisons; with Eugene's backing and encouragement civil disturbances in Antwerp and Brussels were forcibly suppressed. After displeasing the Emperor over his initial opposition to the formation of the Ostend Company, de Prié also lost the support of the native nobility from within his own council of state in Brussels, particularly from the Marquis de Mérode-Westerloo. One of Eugene's former favourites, General Bonneval, also joined the nobles in opposition to de Prié, further undermining the Prince. When de Prié's position became untenable Eugene felt compelled to resign his post as governor on 16 November 1724. As compensation Charles VI conferred on him the honorary position as vicar-general of Italy, worth 140,000 gulden a year, and an estate at Siebenbrunn in Lower Austria said to be worth double that amount. But his resignation distressed him, and to compound his concerns Eugene caught a severe bout of influenza that Christmas, marking the beginning of permanent bronchitis and acute infections every winter for the remaining twelve years of his life.
'Cold war'.
The 1720s saw rapidly changing alliances between the European powers and almost constant diplomatic confrontation, largely over unsolved issues regarding the Quadruple Alliance. The Emperor and the Spanish King continued to use each other's titles, and Charles VI still refused to remove the remaining legal obstacles to Don Charles' eventual succession to the duchies of Parma and Tuscany. Yet in a surprise move Spain and Austria moved closer with the signing of the Treaty of Vienna in April/May 1725. In response Britain, France, and Prussia joined together in the Alliance of Hanover to counter the danger to Europe of an Austro-Spanish hegemony. For the next three years there was the continual threat of war between the Hanover Treaty powers and the Austro-Spanish bloc.
From 1726 Eugene gradually began to regain his political influence. With his many contacts throughout Europe Eugene, backed by Gundaker Starhemberg and Count Schönborn, the Imperial vice-chancellor, managed to secure powerful allies and strengthen the Emperor's position – his skill in managing the vast secret diplomatic network over the coming years was the main reason why Charles VI once again came to depend upon him. In August 1726 Russia acceded to the Austro-Spanish alliance, and in October Frederick William of Prussia followed suit by defecting from the Allies with the signing of a mutual defensive treaty with the Emperor. Despite the conclusion of the brief Anglo-Spanish conflict, war between the European powers persisted throughout 1727–28. However, in 1729 Elisabeth Farnese abandoned the Austro-Spanish alliance. Realizing that Charles VI could not be drawn into the marriage pact she wanted, Elisabeth concluded that the best way to secure her son's succession to Parma and Tuscany now lay with Britain and France. To Eugene it was 'an event that which is seldom to be found in history'. Following the Prince's determined lead to resist all pressure, Charles VI sent troops into Italy to prevent the entry of Spanish garrisons into the contested duchies. By the beginning of 1730 Eugene, who had remained bellicose throughout the whole period, was again in control of Austrian policy.
In Britain there now emerged a new political re-alignment as the Anglo-French "entente" became increasingly defunct. Believing that a resurgent France now posed the greatest danger to their security British ministers, headed by Robert Walpole, moved to reform the Anglo-Austrian alliance, leading to the signing of the Second Treaty of Vienna on 16 March 1731. Eugene had been the Austrian minister most responsible for the alliance, believing once again it would provide security against France and Spain. The treaty compelled Charles VI to sacrifice the Ostend Company (a rival to the English and Dutch trading companies) and accept, unequivocally, the accession of Don Charles to Parma and Tuscany. In return King George II as King of Great Britain and Elector of Hanover guaranteed the Pragmatic Sanction, the device to secure the rights of the Emperor's daughter, Maria Theresa, to the entire Habsburg inheritance. It was largely through Eugene's diplomacy that in January 1732 the Imperial diet also guaranteed the Pragmatic Sanction which, together with the Treaties with Britain, Russia, and Prussia, marked the culmination of the Prince's diplomacy. But the Treaty of Vienna had infuriated the court of King Louis XV: the French had been ignored and the Pragmatic Sanction guaranteed, thus increasing Habsburg influence and confirming Austria's vast territorial size. The Emperor also intended Maria Theresa to marry Francis Stephen of Lorraine which would present an unacceptable threat on France's border. By the beginning of 1733 the French army was ready for war: all that was needed was the excuse.
War of the Polish Succession.
In 1733 the Polish King and Elector of Saxony, Augustus the Strong, died. There were two candidates for his successor: first, Stanisław Leszczyński, the father-in-law of Louis XV; second, the Elector of Saxony's son, Augustus, supported by Russia, Austria, and Prussia. The Polish succession had afforded Louis XV's chief minister, Fleury, the opportunity to attack Austria and take Lorraine from Francis Stephen. In order to gain Spanish support France backed the succession of Elisabeth Farnese's sons to further Italian lands.
Eugene entered the War of the Polish Succession as President of the Imperial War Council and commander-in-chief of the army, but he was severely handicapped by the quality of his troops and the shortage of funds; now in his seventies, the Prince was also burdened by rapidly declining physical and mental powers. France declared war on Austria on 10 October 1733, but without the funds from the Maritime Powers – who, despite the Vienna treaty, remained neutral throughout the war – Austria could not hire the necessary troops to wage an offensive campaign. "The danger to the monarchy," wrote Eugene to the Emperor in October, "cannot be exaggerated". By the end of the year Franco-Spanish forces had seized Lorraine and Milan; by early 1734 Spanish troops had taken Sicily.
Eugene took command on the Rhine in April 1734, but vastly outnumbered he was forced onto the defensive. In June Eugene set out to relieve Philippsburg, yet his former drive and energy was now gone. Accompanying Eugene was a young Frederick the Great, sent by his father to learn the art of war. Frederick gained considerable knowledge from Eugene, recalling in later life his great debt to his Austrian mentor, but the Prussian prince was aghast at Eugene's condition, writing later, "his body was still there but his soul had gone." Eugene conducted another cautious campaign in 1735, once again pursuing a sensible defensive strategy on limited resources; but his short-term memory was by now practically non-existent, and his political influence disappeared completely – Gundaker Starhemberg and Johann Christoph von Bartenstein now dominated the conference in his place. However, fortunately for Charles VI Fleury was determined to limit the scope of the war, and in October 1735 he granted generous peace preliminaries to the Emperor.
Private life and death.
Despite being one of the richest and most celebrated men of his age, Eugene never married and the suggestion is that he was predominantly homosexual. History knows little of his life before 1683. In his early boyhood in Paris "he belonged to a small, effeminate set that included such unabashed perverts as the young abbé de Choisy who was invariably dressed as a girl" wrote the English historian Nicholas Henderson. The Duchess of Orléans, who had known Eugene from those days, would later write to her aunt, Princess Sophia of Hanover, describing Eugene's antics with lackeys and pages. He was "a vulgar whore" along with the Prince of Turenne, and "often played the woman with young people" with the nickname of 'Madame Simone' or 'Madam l'Ancienne'. He preferred a "couple of fine page boys" to any woman, and was refused an ecclesiastical benefice due to his "depravity" Eugene's behaviour may have been a result of his mother's lax household and her own failure to show any affection towards him.
Of related interest is a popular soldier's song which parodied an imaginary voyage by Eugene and the marquis de la Moussaye on the Rhine. A storm breaks and the general fears the worst, but the Marquis consoles him: "Our lives are safe/ For we are sodomites/ Destined to perish only by fire/ We shall land." A comment made by Johann Matthias von der Schulenburg in 1709, who had served under Eugene, could be read that the prince enjoyed "la petite débauche et la pn au-delà de tout," or that he derived his sexual gratification from the virile member of others.
During the last 20 years of his life Eugene was particularly close to Countess Eleonora Batthyány, daughter of Count Theodor von Strattman.
Much about their acquaintance remains speculative (Eugene never mentions her in any of his surviving letters), and there is certainly no suggestion of a sexual relationship, but although they lived apart most foreign diplomats regarded Eleonora as his "official lover". Eugene and Eleonora were constant companions, meeting for dinner, receptions and card games almost every day till his death. But their surviving correspondence does not indicate any real intimacy in the relationship. Eugene's other friends such as the papal nuncio, Passionei, made up for the family he still lacked.
For his only surviving nephew, Emmanuel, the son of his brother Louis Thomas, Eugene arranged a marriage with one of the daughters of Prince Liechenstein, but Emmanuel died of smallpox in 1729. With the death of Emmanuel's son in 1734, no close male relatives remained to succeed the Prince. His closest relative, therefore, was Louis Thomas's unmarried daughter, Princess Maria Anna Victoria of Savoy, whom Eugene had never met and, as he had heard nothing but bad of her, made no effort to do so.
Eugene returned to Vienna from the War of the Polish Succession in October 1735, weak and feeble; when Maria Theresa and Francis Stephen married in February 1736 Eugene was too ill to attend. After playing cards at Countess Batthyány's on the evening of 20 April he returned to his bed at the Stadtpalais. When his servants arrived to wake him the next morning, 21 April 1736, they found Prince Eugene dead after choking from phlegm in his throat, presumably after suffering from pneumonia. Eugene's heart was buried with those of others of his family in Turin. His remains were carried in a long procession to St. Stephen's Cathedral, where the body was interred in the "Kreuzkapelle".
Countess Batthyány expressed in a letter dated 23 December 1720, that at the "Kreuzkapelle" a solemn requiem would be held annually. She dedicated for this purpose two thousand guilders.
Patron of the arts.
Eugene's rewards for his victories, his share of booty, his revenues from his abbeys in Savoy, and a steady income from his Imperial offices and governorships, enabled him to contribute to the landscape of Baroque architecture. Eugene spent most of his life in Vienna at his Winter Palace, the Stadtpalais, built by Fischer von Erlach. The palace acted as his official residence and home, but for reasons that remain speculative the Prince's association with Fischer ended before the building was complete, favouring instead Johann Lukas von Hildebrandt as his chief architect. Eugene first employed Hildebrandt to finish the Stadtpalais before commissioning him to prepare plans for a palace (Savoy Castle) on his Danubian island at Ráckeve. Began in 1701 the single-story building took twenty years to complete; yet, probably because of the Rákóczi revolt, the Prince seems to have visited it only once – after the siege of Belgrade in 1717.
Of more importance was the grandiose complex of the two Belvedere palaces in Vienna. The single-storey Lower Belvedere, with its exotic gardens and zoo, was completed in 1716. The Upper Belvedere, completed between 1720 and 1722, is a more substantial building; with sparkling white stucco walls and copper roof it became a wonder of Europe. Eugene and Hildebrandt also converted an existing structure on his Marchfeld estate into a country seat, the Schlosshof, situated between the Rivers Danube and Morava. The building, completed in 1729, was far less elaborate than his other projects but it was strong enough to serve as a fortress in case of need. Eugene spent much of his spare time there in his last years accommodating large hunting parties.
In the years following the Peace of Rastatt Eugene became acquainted with a large number of scholarly men. Given his position and responsiveness they were keen to meet him: few could exist without patronage and this was probably the main reason for Gottfried Leibniz's association with him in 1714. Eugene also befriended the French writer Jean-Baptiste Rousseau who, by 1716, was receiving financial support from Eugene. Rousseau stayed on attached to the Prince's household, probably helping in the library, until he left for the Netherlands in 1722. Another acquaintance, Montesquieu, already famous for his "Persian Letters" when he arrived in Vienna in 1728, favourably recalled his time spent at the Prince's table. Nevertheless, Eugene had no literary pretensions of his own, and was not tempted like Maurice de Saxe or Marshal Villars to write his memoirs or books on the art of war. He did, however, become a collector on the grandest scale: his picture galleries were filled with 16th- and 17th-century Italian, Dutch and Flemish art; his library at the Stadtpalais crammed with over 15,000 books, 237 manuscripts as well as a huge collection of prints (of particular interest were books on natural history and geography). "It is hardly believable," wrote Rousseau, "that a man who carries on his shoulders the burden of almost all the affairs of Europe … should find as much time to read as though he had nothing else to do." At Eugene's death his possessions and estates, except those in Hungary which the crown reclaimed, went to his niece, Princess Maria Anna Victoria, who at once decided to sell everything. The artwork was bought by Charles Emmanuel III of Sardinia. Eugene's library, prints and drawings were purchased by the Emperor in 1737 and have since passed into Austrian national collections.
Assessment.
Napoleon considered Eugene one of the seven greatest commanders of history. Although later military critics have disagreed with that assessment, Eugene was undoubtedly the greatest Austrian general. He was no military innovator, but he had the ability to make an inadequate system work. He was equally adept as an organizer, strategist, and tactician, believing in the primacy of battle and his ability to seize the opportune moment to launch a successful attack. "The important thing," wrote Maurice de Saxe in his "Reveries", "is to see the opportunity and to know how to use it. Prince Eugene possessed this quality which is the greatest in the art of war and which is the test of the most elevated genius." This fluidity was key to his battlefield successes in Italy and in his wars against the Turks. Nevertheless, in the Low Countries, particularly after the battle of Oudenarde in 1708, Eugene, like his cousin Louis of Baden, tended to play safe and become bogged down in a conservative strategy of sieges and defending supply lines. After the attempt on Toulon in 1707, he also became very wary of combined land/sea operations. To historian Derek McKay, however, the main criticism of him as a general is his legacy – he left no school of officers nor an army able to function without him.
Eugene was a disciplinarian – when ordinary soldiers disobeyed orders he was prepared to shoot them himself – but he rejected blind brutality, writing "you should only be harsh when, as often happens, kindness proves useless". On the battlefield Eugene demanded courage in his subordinates, and expected his men to fight where and when he wanted; his criteria for promotion were based primarily on obedience to orders and courage on the battlefield rather than social position. On the whole his men responded because he was willing to push himself as hard as them. However, his position as President of the Imperial War Council proved less successful. Following the long period of peace after the Austro-Turkish War, the idea of creating a separate field army or providing garrison troops with effective training for them to be turned into such an army quickly was never considered by Eugene. By the time of the War of the Polish Succession, therefore, the Austrians were outclassed by a better prepared French force. For this Eugene was largely to blame – in his view (unlike the drilling and manoeuvres carried out by the Prussians which to Eugene seemed irrelevant to real warfare) the time to create actual fighting men was when war came. But although Frederick the Great had been struck by the muddle of the Austrian army and its poor organisation during the Polish Succession war, he later amended his initial harsh judgements. "If I understand anything of my trade," commented Frederick in 1758, "especially in the more difficult aspects, I owe that advantage to Prince Eugene. From him I learnt to hold grand objectives constantly in view, and direct all my resources to those ends." To historian Christopher Duffy it was this awareness of the 'grand strategy' that was Eugene's legacy to Frederick.
To his responsibilities Eugene attached his own personal values – physical courage, loyalty to his sovereign, honesty, self-control in all things – and he expected these qualities from his commanders. Eugene's approach was dictatorial, but he was willing to co-operate with someone he regarded as his equal, such as Baden or Marlborough. Yet the contrast to his co-commander of the Spanish Succession war were stark. "Marlborough," wrote Churchill, "was the model husband and father, concerned with building up a home, founding a family, and gathering a fortune to sustain it"; whereas Eugene, the bachelor, was "disdainful of money, content with his bright sword and his lifelong animosities against Louis XIV. The result was an austere figure, inspiring respect and admiration rather than affection. The huge equestrian statue in the centre of Vienna commemorates Eugene's achievements. It is inscribed on one side, 'To the wise counsellor of three Emperors', and on the other, 'To the glorious conqueror of Austria's enemies'.
Usage.
Several ships have been named in Eugene's honour: "Prinz Eugen", an Austrian World War I battleship; the HMS Prince Eugene, British Royal Navy World War I monitor; Italian light cruiser and the German cruiser "Prinz Eugen" (later USS "Prinz Eugen"), a World War II heavy cruiser.
 

</doc>
<doc id="9683" url="https://en.wikipedia.org/wiki?curid=9683" title="Emanuel Leutze">
Emanuel Leutze

Emanuel Gottlieb Leutze (May 24, 1816July 18, 1868) was a German American history painter best known for his painting "Washington Crossing the Delaware". He is associated with the Düsseldorf school of painting.
Biography.
Philadelphia.
Leutze was born in Schwäbisch Gmünd, Württemberg, Germany, and was brought to the United States as a child. His parents settled first in Fredericksburg, Virginia, and then at Philadelphia. His early education was good, though not especially in the direction of art. The first development of his artistic talent occurred while he was attending the sickbed of his father, when he attempted drawing to occupy the long hours of waiting. His father died in 1831. At 14, he was painting portraits for $5 apiece. Through such work, he supported himself after the death of his father. In 1834, he received his first instruction in art in classes of John Rubens Smith, a portrait painter in Philadelphia. He soon became skilled, and promoted a plan for publishing, in Washington, portraits of eminent American statesmen; however, he met with but slight encouragement.
Europe.
In 1840, one of his paintings attracted attention and procured him several orders, which enabled him to go to the Kunstakademie Düsseldorf, where he studied with Lessing. In 1842 he went to Munich, studying the works of Cornelius and Kaulbach, and, while there, finished his "Columbus before the Queen". The following year he visited Venice and Rome, making studies from Titian and Michelangelo. His first work, "Columbus before the Council of Salamanca" was purchased by the Düsseldorf Art Union. A companion picture, "Columbus in Chains", procured him the gold medal of the Brussels Art Exhibition, and was subsequently purchased by the Art Union in New York; it was the basis of the 1893 $2 Columbian stamp. In 1845, after a tour in Italy, he returned to Düsseldorf, marrying Juliane Lottner and making his home there for 14 years.
During his years in Düsseldorf, he was a resource for visiting Americans: he found them places to live and work, provided introductions, and emotional and even financial support. For many years, he was the president of the Düsseldorf Artists' Association; in 1848, he was an early promoter of the “Malkasten” art association; and in 1857, he led the call for a gathering of artists which led to the founding of the Allgemeine deutsche Kunstgenossenschaft.
A strong supporter of Europe's Revolutions of 1848, Leutze decided to paint an image that would encourage Europe's liberal reformers with the example of the American Revolution. Using American tourists and art students as models and assistants, Leutze finished "Washington Crossing the Delaware" in 1850. It is owned by the Metropolitan Museum of Art in New York. In 1854, Leutze finished his depiction of the Battle of Monmouth, "Washington rallying the troops at Monmouth," commissioned by an important Leutze patron, banker David Leavitt of New York City and Great Barrington, Massachusetts.
New York City and Washington, D.C..
In 1859, Leutze returned to the United States and opened a studio in New York City. He divided his time between New York City and Washington, D.C. In 1859, he painted a portrait of Chief Justice Roger Brooke Taney which hangs in the Harvard Law School. In a 1992 opinion, Justice Antonin Scalia described the portrait of Taney, made two years after Taney's infamous decision in Dred Scott v. Sandford, as showing Taney "in black, sitting in a shadowed red armchair, left hand resting upon a pad of paper in his lap, right hand hanging limply, almost lifelessly, beside the inner arm of the chair. He sits facing the viewer and staring straight out. There seems to be on his face, and in his deep-set eyes, an expression of profound sadness and disillusionment."
Leutze also executed other portraits, including one of fellow painter William Morris Hunt. That portrait was owned by Hunt's brother Leavitt Hunt, a New York attorney and sometime Vermont resident, and was shown at an exhibition devoted to William Morris Hunt's work at the Museum of Fine Arts, Boston in 1878.
In 1860 Leutze was commissioned by the U.S. Congress to decorate a stairway in the Capitol Building in Washington, DC, for which he painted a large composition, "Westward the Course of Empire Takes Its Way", which is also commonly known as "Westward Ho!".
Late in life, he became a member of the National Academy of Design. He was also a member of the Union League Club of New York, which has a number of his paintings. He died in Washington, D.C., in his 52nd year, of heatstroke. He was interred at Glenwood Cemetery. At the time of his death, a painting, "The Emancipation of the Slaves", was in preparation.
Leutze's portraits are known less for their artistic quality than for their patriotic emotionalism. "Washington Crossing the Delaware" firmly ranks among the American national iconography, and is thus often caricatured.

</doc>
<doc id="9684" url="https://en.wikipedia.org/wiki?curid=9684" title="Erasmus Alberus">
Erasmus Alberus

Erasmus Alberus (c. 1500–1553), German humanist, reformer, and poet.
Life.
He was born in the village of Bruchenbrücken (now part of Friedberg, Hesse) about the year 1500. Although his father Tilemann Alber was a schoolmaster, his early education was neglected. 
Ultimately in 1518 he found his way to the University of Wittenberg, where he studied theology. He had the good fortune to attract the attention of Martin Luther and Philipp Melanchthon, and subsequently became one of Luther's most active helpers in the Protestant Reformation.
Not only did he fight for the Protestant cause as a preacher and theologian, but he was almost the only member of Luther's party who was able to confront the Roman Catholics with the weapon of literary satire. In 1542 he published a prose satire to which Luther wrote the preface, "Der Barfusser Monche Eulenspiegel und Alkoran," a parodic adaptation of the "Liber conformitatum" of the Franciscan Bartolommeo Rinonico of Pisa, in which the Franciscan order is held up to ridicule.
Of higher literary value is the didactic and satirical "Buch von der Tugend und Weisheit" (1550), a collection of forty-nine fables in which Alberus embodies his views on the relations of Church and State. His satire is incisive, but in a scholarly and humanistic way; it does not appeal to popular passions with the fierce directness which enabled the master of Catholic satire, Thomas Murner, to inflict such telling blows.
Several of Alberus's hymns, all of which show the influence of his master Luther, have been retained in the German Protestant hymnal.
After Luther's death, Alberus was for a time a deacon in Wittenberg; he became involved, however, in the political conflicts of the time, and was in Magdeburg in 1550-1551, while that town was besieged by Maurice, Elector of Saxony. In 1552 he was appointed Generalsuperintendent at Neubrandenburg in Mecklenburg, where he died on the 5 May 1553.

</doc>
<doc id="9685" url="https://en.wikipedia.org/wiki?curid=9685" title="Earley parser">
Earley parser

In computer science, the Earley parser is an algorithm for parsing strings that belong to a given context-free language, though (depending on the variant) it may suffer problems with certain nullable grammars. The algorithm, named after its inventor, Jay Earley, is a chart parser that uses dynamic programming; it is mainly used for parsing in computational linguistics. It was first introduced in his dissertation in 1968 (and later appeared in abbreviated, more legible form in a journal).
Earley parsers are appealing because they can parse all context-free languages, unlike LR parsers and LL parsers, which are more typically used in compilers but which can only handle restricted classes of languages. The Earley parser executes in cubic time in the general case formula_1, where "n" is the length of the parsed string, quadratic time for unambiguous grammars formula_2, and linear time for almost all LR(k) grammars. It performs particularly well when the rules are written left-recursively.
Earley recogniser.
The following algorithm describes the Earley recogniser. The recogniser can be easily modified to create a parse tree as it recognises, and in that way can be turned into a parser.
The algorithm.
In the following descriptions, α, β, and γ represent any string of terminals/nonterminals (including the empty string), X and Y represent single nonterminals, and "a" represents a terminal symbol.
Earley's algorithm is a top-down dynamic programming algorithm. In the following, we use Earley's dot notation: given a production X → αβ, the notation X → α • β represents a condition in which α has already been parsed and β is expected.
Input position 0 is the position prior to input. Input position "n" is the position after accepting the "n"th token. (Informally, input positions can be thought of as locations at token boundaries.) For every input position, the parser generates a "state set". Each state is a tuple (X → α • β, "i"), consisting of
(Earley's original algorithm included a look-ahead in the state; later research showed this to have little practical effect on the parsing efficiency, and it has subsequently been dropped from most implementations.)
The state set at input position "k" is called S("k"). The parser is seeded with S(0) consisting of only the top-level rule. The parser then repeatedly executes three operations: "prediction", "scanning", and "completion".
It is important to note that duplicate states are not added to the state set, only new ones. These three operations are repeated until no new states can be added to the set. The set is generally implemented as a queue of states to process, with the operation to be performed depending on what kind of state it is.
Pseudocode.
Adapted from Speech and Language Processing by Daniel Jurafsky and James H. Martin, 
function EARLEY-PARSE(words, grammar)
procedure PREDICTOR((A → α•B, i), j, grammar)
procedure SCANNER((A → α•B, i), j)
procedure COMPLETER((B → γ•, j), k)
Example.
Consider the following simple grammar for arithmetic expressions:
<P> ::= <S> # the start rule
<S> ::= <S> "+" <M> | <M>
<M> ::= <M> "*" <T> | <T>
<T> ::= "1" | "2" | "3" | "4"
With the input:
This is the sequence of state sets:
S(0): • 2 + 3 * 4.
 (1) P → • S (0) # start rule
S(1): 2 • + 3 * 4.
 (1) T → number • (0) # scan from S(0)(6)
S(2): 2 + • 3 * 4.
 (1) S → S + • M (0) # scan from S(1)(5)
S(3): 2 + 3 • * 4.
 (1) T → number • (2) # scan from S(2)(4)
S(4): 2 + 3 * • 4.
 (1) M → M * • T (2) # scan from S(3)(3)
S(5): 2 + 3 * 4 •.
 (1) T → number • (4) # scan from S(4)(2)
The state (P → S •, 0) represents a completed parse. This state also appears in S(3) and S(1), which are complete sentences.
Constructing the parse forest.
Earley's dissertation briefly describes an algorithm for constructing parse trees by adding a set of pointers from each non-terminal in an Earley item back to the items which caused it to be recognized. But Tomita noticed that this does not take into account the relations between symbols, so if we consider the grammar S → SS | b and the string bbb, it only notes that each S can match one or two b's, and thus produces spurious derivations for bb and bbbb as well as the two correct derivations for bbb.
It is relatively straightforward to take the complete items from the chart and search through them from the top down, assembling the ones which fit together to make the parse forest.
Another method, described in, is to build the parse forest as you go, augmenting each Earley item with a pointer to a shared packed parse forest (SPPF) node labelled with a triple (s, i, j) where s is a symbol or an LR(0) item (production rule with dot), and i and j give the section of the input string derived by this node. A node's contents are either a pair of child pointers giving a single derivation, or a list of "packed" nodes each containing a pair of pointers and representing one derivation. SPPF nodes are unique (there is only one with a given label), but may contain more than one derivation for ambiguous parses. So even if an operation does not add an Earley item (because it already exists), it may still add a derivation to the item's parse forest.
Note also that SPPF nodes are never labeled with a completed LR(0) item: instead they are labelled with the symbol which is produced so that all derivations are combined under one node regardless of which alternative production they come from.

</doc>
<doc id="9686" url="https://en.wikipedia.org/wiki?curid=9686" title="Ethiopian cuisine">
Ethiopian cuisine

Ethiopian cuisine () characteristically consists of vegetable and often very spicy meat dishes. This is usually in the form of "wat" (also "w'et" or "wot"), a thick stew, served atop "injera", a large sourdough flatbread, which is about in diameter and made out of fermented teff flour. Ethiopians eat exclusively with their right hands, using pieces of "injera" to pick up bites of entrées and side dishes. Utensils are optional.
The Ethiopian Orthodox Church prescribes a number of fasting ("tsom", ) periods, including Wednesdays, Fridays, and the entire Lenten season, so Ethiopian cuisine contains many dishes that are vegan.
Overview.
A typical dish consists of injera accompanied by a spicy stew, which frequently includes beef, lamb, vegetables and various types of legumes, such as lentils. Gurage cuisine also makes use of the false banana plant ("enset", Ge'ez: እንሰት "inset"), a type of ensete. The plant is pulverized and fermented to make a bread-like food called "qocho" or "kocho" (Ge'ez: ቆጮ "ḳōč̣ō"), which is eaten with kitfo. The root of this plant may be powdered and prepared as a hot drink called "bulla" (Ge'ez: ቡላ "būlā"), which is often given to those who are tired or ill. Another typical Gurage preparation is coffee with butter ("kebbeh"). "Kita" herb bread is also baked.
Pasta is frequently available throughout Ethiopia, including rural areas. Coffee is also a large part of Ethiopian culture and cuisine. After every meal, a coffee ceremony is enacted and espresso coffee is served.
Traditional ingredients.
"Berbere", a combination of powdered chili pepper and other spices (somewhat analogous to Southwestern American chili powder), is an important ingredient used in many dishes. Also essential is "niter kibbeh", a clarified butter infused with ginger, garlic, and several spices.
"Mitmita" (, ) is a powdered seasoning mix used in Ethiopian and Eritrean cuisine. It is orange-red in color and contains ground birdseye chili peppers (piri piri), cardamom seed, cloves and salt. It occasionally has other spices including cinnamon, cumin and ginger.
In their adherence to strict fasting, Ethiopian cooks have developed a rich array of cooking oil sources—besides sesame and safflower—for use as a substitute for animal fats which is forbidden during fasting periods. Ethiopian cuisine also uses "nug" (also spelled "noog", also known as "niger seed").
Dishes.
Wat.
"Wat" begins with a large amount of chopped red onion, which is simmered or sauteed in a pot. Once the onions have softened, "niter kebbeh" (or, in the case of vegan dishes, vegetable oil) is added. Following this, "berbere" is added to make a spicy "keiy wat" or "keyyih tsebhi". Turmeric is used instead of "berbere" for a milder "alicha wat" or both spices are omitted when making vegetable stews, such as "atkilt wat". Meat such as beef (, ), chicken (, or ), fish (, ), goat or lamb (, or ) is also added. Legumes such as split peas (, or ') and lentils (, or "birsin"); or vegetables such as potatoes (, ), carrots and chard () are also used instead in vegan dishes. 
Each variation is named by appending the main ingredient to the type of wat (e.g. "kek alicha wat"). However, the word "keiy" is usually not necessary, as the spicy variety is assumed when it is omitted (e.g. "doro wat"). The term "atkilt wat" is sometimes used to refer to all vegetable dishes, but a more specific name can also be used (as in "dinich'na caroht wat", which translates to "potatoes and carrots stew"; but notice the word "atkilt" is usually omitted when using the more specific term).
Tibs.
Meat along with vegetables are sautéed to make "tibs" (also "tebs", "t'ibs", "tibbs", etc., Ge'ez: ጥብስ "ṭibs"). Tibs is served in a variety of manners, and can range from hot to mild or contain little to no vegetables. There are many variations of the delicacy, depending on type, size or shape of the cuts of meat used. 
The mid-18th century European visitor to Ethiopia, Remedius Prutky, describes "tibs" as a portion of grilled meat served "to pay a particular compliment or show especial respect to someone." This is perhaps still true as the dish is still prepared today to commemorate special events and holidays.
Kinche (Qinch'e)
Kinche (Qinch’e) is a very common Ethiopian breakfast, its equivalent of oatmeal. It’s incredibly simple, inexpensive, and nutritious. It is made from cracked wheat. It can be boiled in either milk or water. The flavor of the Kinche came from the nit'ir qibe, which is a spiced butter.
Gurage dishes.
Kitfo.
Another distinctively Ethiopian dish is "kitfo" (frequently spelled "ketfo"). It consists of raw (or rare) beef mince marinated in "mitmita" (Ge'ez: ሚጥሚጣ "mīṭmīṭā" a very spicy chili powder similar to the "berbere") and "niter kibbeh". "Gored gored" is very similar to "kitfo", but uses cubed rather than ground beef.
Ayibe.
"Ayibe" is a cottage cheese that is mild and crumbly. It is much closer in texture to crumbled feta. Although not quite pressed, the whey has been drained and squeezed out. It is often served as a side dish to soften the effect of very spicy food. It has little to no distinct taste of its own. However, when served separately, ayibe is often mixed with a variety of mild or hot spices typical of Gurage cuisine.
Gomen kitfo.
"Gomen kitfo" is another typical Gurage dish. Collard greens (ጎመን "gōmen") are boiled, dried and then finely chopped and served with butter, chili and spices. It is a dish specially prepared for the occasion of Meskel, a very popular holiday marking the discovery of the True Cross. It is served along with "ayibe" or sometimes even "kitfo" in this tradition called "dengesa".
Breakfast.
"Fit-fit" or "fir-fir" is a common breakfast dish. It is made from shredded "injera" or "kitcha" stir-fried with spices or wat. Another popular breakfast food is "fatira". The delicacy consists of a large fried pancake made with flour, often with a layer of egg. It is eaten with honey. "Chechebsa" (or "kita firfir") resembles a pancake covered with "berbere" and "niter kibbeh", or other spices, and may be eaten with a spoon. "Genfo" is a kind of porridge, which is another common breakfast dish. It is usually served in a large bowl with a dug-out made in the middle of the genfo and filled with spiced "niter kibbeh". A variation of "ful", a fava bean stew with condiments, served with baked rolls instead of "injera", is also common for breakfast.
Snacks.
Typical Ethiopian snacks are "dabo kolo" (small pieces of baked bread that are similar to pretzels) or "kolo" (roasted barley sometimes mixed with other local grains). "Kolo" is often sold by kiosks and street venders wrapped in a paper cone. Snacking on popcorn is also common.
Beverages.
Coffee.
According to some sources, drinking of coffee ("buna") is likely to have originated in Ethiopia. A key national beverage, it is an important part of local commerce.
The coffee ceremony is the traditional serving of coffee, usually after a big meal. It often involves the use of a "jebena" (ጀበና), a clay coffee pot in which the coffee is boiled. The preparer roasts the coffee beans right in front of guests, then walks around wafting the smoke throughout the room so participants may sample the scent of coffee. Then the preparer grinds the coffee beans in a traditional tool called a "mokecha". The coffee is put into the "jebena", boiled with water, and then served with small cups called "si'ni". Coffee is usually served with sugar, but is also served with salt in many parts of Ethiopia. In some parts of the country, "niter kibbeh" is added instead of sugar or salt.
Snacks, such as popcorn or toasted barley (or "kollo"), are often served with the coffee. In most homes, a dedicated coffee area is surrounded by fresh grass, with special furniture for the coffee maker. A complete ceremony has three rounds of coffee (Abol, Tona and Bereka) and is accompanied by the burning of frankincense.
Tea ("shahee") will most likely be served if coffee is declined.
Non-alcoholic brews.
"Atmet" is a barley and oat-flour based drink that is cooked with water, sugar and "kibe" (Ethiopian clarified butter) until the ingredients have married and become a consistency slightly thicker than egg-nog. Though this drink is often given to women who are nursing, the sweetness and smooth texture make it a comfort drink for anyone who enjoys its flavor.
Manufactured drinks.
Ambo Mineral Water or "Ambo wuha" is a bottled carbonated mineral water, sourced from the springs in Ambo Senkele near the town of Ambo.
Spirits.
Tej is a potent honey wine. It is similar to mead, which is frequently served in bars (in particular, in a "tej bet" or "tej house"). "Katikala" and "araqe" are inexpensive local spirits that are very strong.
Tella is a home-brewed beer served in "tella bet" (""tella" houses") which specialize in serving "tella" only. "Tella" is the most common beverage made and served in households during holidays.
Gursha.
A "gursha" (var. "gorsha", "goorsha") is an act of friendship and love. When eating injera, a person uses his or her right hand to strip off a piece, wraps it around some "wat" or "kitfo", and then puts it into his or her mouth. During a meal with friends or family, it is a common custom to feed others in the group with one's hand by putting the rolled injera or a spoon full of other dishes into another's mouth. This is called a "gursha", and the larger the gursha, the stronger the friendship or bond (only surpassed by the brewing of Tej together). This tradition was featured in "The Food Wife," an episode of "The Simpsons" that uses Ethiopian cuisine as a plot point.

</doc>
<doc id="9688" url="https://en.wikipedia.org/wiki?curid=9688" title="Epistle of James">
Epistle of James

The Epistle of James (), the Book of James, or simply James, is one of the twenty-two epistles (didactic letters) in the New Testament.
The author identifies himself as "James, a servant of God and of the Lord Jesus Christ," who is writing to "the twelve tribes scattered abroad" (James 1:1). The epistle is traditionally attributed to James the Just, and the audience is generally considered to be Jewish Christians who were dispersed outside of Palestine due to persecution.
Framed within an overall theme of patient perseverance during trials and temptations, James writes to encourage believers to live consistently with what they have learned in Christ. He desires for his readers to mature in their faith in Christ by living what they say they believe. James condemns various sins including pride, hypocrisy, favoritism, and slander. James encourages believers to humbly live Godly wisdom rather than worldly wisdom, and to pray in all situations.
Composition.
Authorship.
There are four views concerning authorship and dating of the Epistle of James:
The writer only refers to himself as "James, a servant of God and of the Lord Jesus Christ." There are seven possible authors of James. As many as six different men may be referred to in the Bible as James, and if none of these men wrote this letter, a seventh man not mentioned in the Bible by the name of James could be the author.
Jesus had two apostles named James: James, the son of Zebedee and James, the son of Alphaeus, but it is unlikely that either of these wrote the letter. James, the son of Zebedee, was martyred about 44 AD. This would be very early for him to have been the writer. The other apostle James, the son of Alphaeus, is not prominent in the Scriptural record, and very little is known about him.
Rather, evidence points to James the brother of Jesus, to whom Jesus evidently had made a special appearance after his resurrection described in the New Testament. This James was prominent among the disciples. The writer of the letter of James identifies himself as "a slave of God and of the Lord Jesus Christ", in much the same way as did Jude, who introduced the Epistle of Jude by calling himself "a slave of Jesus Christ, but a brother of James". (Jas 1:1; Jude 1) Furthermore, the salutation of James’ letter includes the term “Greetings!” in the same way as did the letter concerning circumcision that was sent to the congregations. In this latter instance it was apparently Jesus’ brother James who spoke prominently in the assembly of "the apostles and the older men" at Jerusalem.
From the middle of the 3rd century, patristic authors cited the "Epistle" as written by James the Just, a relation of Jesus and first Bishop of Jerusalem. Not numbered among the Twelve Apostles, unless he is identified as James the Less, James was nonetheless a very important figure: Paul described him as "the brother of the Lord" in Galatians 1:19 and as one of the three "pillars of the Church" in 2:9. He is traditionally considered the first of the Seventy Disciples.
John Calvin and others suggested that the author was the Apostle James, son of Alphaeus, who is referred to as James the Less. The Protestant reformer Martin Luther denied it was the work of an apostle and termed it an "epistle of straw" as compared to some other books in the New Testament, not least because of the conflict he thought it raised with Paul on the doctrine of justification (see below).
Dating.
Many scholars consider the epistle to be written in the late 1st or early 2nd centuries. Among the reasons for this are:
If written by James the Just, the time of the writing would be sometime before AD 70. Jerusalem would also be the place of origin.
The earliest extant manuscripts of James usually date to the mid-to-late third century.
Genre.
James is considered New Testament wisdom literature because, "like Proverbs and Sirach, it consists largely of moral exhortations and precepts of a traditional and eclectic nature."
Structure.
Some view the epistle as having no overarching outline, "...James may have simply grouped together small "thematic essays" without having more linear, Greco-Roman structures in mind." This view is generally supported by those who believe that the epistle may not be a true piece of correspondence between specific parties, but rather an example of wisdom literature formulated as a letter for circulation. The "Catholic Encyclopedia" says, "the subjects treated of in the Epistle are many and various; moreover, St. James not infrequently, whilst elucidating a certain point, passes abruptly to another, and presently resumes once more his former argument."
Others view the letter as having only broad topical or thematic structure. These authors generally organize James under three (Ralph Martin) to seven (Luke Johnson) general key themes or segments.
A third group of scholars believe that James was more purposeful in structuring his letter, linking each paragraph theologically and thematically.
This third view of the structuring of James is a historical approach that is supported by scholars who are not content with leaving the book as, "New Testament wisdom literature, like a small book of proverbs," or, "like a loose collection of random pearls dropped in no particular order onto a piece of string."
A fourth group uses modern discourse analysis or Greco-Roman rhetorical structures to describe the structure of James.
The United Bible Societies' "Greek New Testament" divides the letter into the following sections:
Historical context.
Some scholars believe that the Epistle of James should be understood in its historical context. Understanding the circumstances of James' writing helps these scholars to better understand James' organization of this letter. This approach views James' Epistle as having a legitimate purpose for its composition––it is a response to the suffering of its recipients.
A 2013 journal article explores a violent historical background behind the epistle and offers the suggestion that it was indeed written by James the brother of Jesus, and therefore written before AD 62, the year of James' murder. The decade of the 50's saw the growth of turmoil and violence in Palestine as Jews became more and more frustrated with corruption, injustice and poverty. It continued into the 60's and four years after the murder of James, war broke out with Rome - a war that would lead to the destruction of Jerusalem and the scattering of the people. The epistle of James is renowned for exhortations on fighting poverty and caring for the poor in practical ways (1:26–27; 2:1-4; 2:14-19; 5:1-6), standing up for the oppressed (2:1-4; 5:1-6) and not being "like the world" in the way one responds to evil in the world (1:26-27; 2:11; 3:13-18; 4:1-10). Worldly wisdom is rejected and people are exhorted to embrace heavenly wisdom, which includes peacemaking and pursuing righteousness and justice (3:13-18).
This approach sees the epistle as a real letter with a real immediate purpose: to encourage Christian Jews not to revert to violence in their response to injustice and poverty, but rather to stay focused on doing good, staying holy, and embracing the wisdom of heaven not the wisdom of the world.
Doctrine.
Justification.
The letter contains the following famous passage concerning salvation and justification:
This passage has been cited in Christian theological debates, especially regarding the doctrine of justification. Gaius Marius Victorinus (4th century) associated James's teaching on works with the heretical Symmachian sect, followers of Symmachus the Ebionite, and openly questioned whether James's teachings were heretical. This passage has also been contrasted with the teachings of Paul the Apostle on justification; indeed, some scholars believe that this passage is a response to Paul. One issue in the debate is the meaning of the Greek word δικαιόω (dikaiόο) ‘render righteous or such as he ought to be’, with some among the participants taking the view that James is responding to a misunderstanding of Paul.
Roman Catholicism and Eastern Orthodoxy argue that this passage disproves the doctrine of justification by faith alone (or "sola fide"), whereas the early and many modern Protestants continue to believe that Catholic and Orthodox interpretations do not fully understand the meaning of the term "justification" and resolve James' and Paul's apparent conflict regarding faith and works in alternate ways from the Catholics and Orthodox:
Anointing of the Sick.
James's epistle is also the chief Biblical text for the Anointing of the Sick. James wrote:
G. A. Wells suggested this passage was evidence of late authorship of the epistle, on the grounds that the healing of the sick being done through an official body of presbyters (elders) indicated a considerable development of ecclesiastical organisation, "whereas in Paul's day to heal and work miracles pertained to believers indiscriminately (I Corinthians, XII:9)."
Canonicity.
The Epistle was first explicitly referred to and quoted by Origen of Alexandria, and possibly a bit earlier by Irenaeus of Lyons as well as Clement of Alexandria in a lost work according to Eusebius, although it was not mentioned by Tertullian, who was writing at the end of the Second Century. It is also absent from the Muratorian fragment, the earliest known list of New Testament books.
The Epistle of James was included among the twenty-seven New Testament books first listed by Athanasius of Alexandria in his "Thirty-Ninth Festal Epistle" (AD 367) and was confirmed as a canonical epistle of the New Testament by a series of councils in the Fourth Century. Today, virtually all denominations of Christianity consider this book to be a canonical epistle of the New Testament.
In the first centuries of the Church the authenticity of the Epistle was doubted by some, including Theodore, Bishop of Mopsuestia in Cilicia. Because of the silence of several of the western churches regarding it, Eusebius classes it among the Antilegomena or contested writings ("Historia ecclesiae", 3.25; 2.23). St. Jerome gives a similar appraisal but adds that with time it had been universally admitted. Gaius Marius Victorinus, in his commentary on the Epistle to the Galatians, openly questioned whether the teachings of James were heretical.
Its late recognition in the Church, especially in the West, may be explained by the fact that it was written for or by Jewish Christians, and therefore not widely circulated among the Gentile Churches. There is some indication that a few groups distrusted the book because of its doctrine. In Reformation times a few theologians, most notably Martin Luther in his early career, argued that this epistle should not be part of the canonical New Testament.
Martin Luther's description of the Epistle of James changes. In some cases, Luther argues that it was not written by an apostle; but in other cases, he describes James as the work of an apostle. He even cites it as authoritative teaching from God and describes James as "a good book, because it sets up no doctrines of men but "vigorously promulgates the law of God"." Lutherans hold that the Epistle is rightly part of the New Testament, citing its authority in the Book of Concord, however it remains part of the Lutheran antilegomena.

</doc>
<doc id="9689" url="https://en.wikipedia.org/wiki?curid=9689" title="Epistle of Jude">
Epistle of Jude

The Epistle of Jude, often shortened to Jude, is the penultimate book of the New Testament and is attributed to Jude, the servant of Jesus and the brother of James the Just. 
Composition.
The letter of Jude was one of the disputed books of the Canon. Although its canonical status was contested, its authenticity was never doubted by the Early Church. The links between the Epistle and 2 Peter, its use of the Apocryphal Books, and its brevity raised concern. It is one of the shortest books in the Bible, being only 25 verses long.
Content.
Jude urges his readers to defend the deposit of Christ's doctrine that had been closed by the time he wrote his epistle, and to remember the words of the apostles spoken somewhat before. He uses language similar to the second epistle of Peter to answer concerns that the Lord seemed to tarry, "How that they told you there should be mockers in the last time, who should walk after their own ungodly lusts..."
Jude then asks the reader to recall how even after the Lord saved his own people out of the land of Egypt, he did not hesitate to destroy those who fell into unbelief, much as he punished the angels who fell from their original exalted status.
Jude quotes directly from the Book of Enoch, part of the scripture of the Ethiopian and Eritrean churches but rejected by other churches. He cites Enoch's prophecy that the Lord would come with many thousands of his saints to render judgement on the whole world. He also paraphrases an incident in a text that has been lost about Satan and Michael quarreling over the body of Moses.
Outline.
I. Salutation (1-2)
II. Occasion for the Letter (3-4)
III. Warning against the False Teachers (5-16)
IV. Exhortation to Believers (17-23)
V. Concluding Doxology (24-25)
Canonical status.
The Epistle of Jude is held as canonical in the Christian Church. Although some scholars consider the letter a pseudonymous work written between the end of the 1st century and the first quarter of the 2nd century, arguing from the references to the apostles, tradition; and the book's competent Greek style, conservative scholars date it between 66 and 90.
"More remarkable is the evidence that by the end of the second century Jude was widely accepted as canonical." Clement of Alexandria, Tertullian and the Muratorian canon considered the letter canonical. The first historical record of doubts as to authorship are found in the writings of Origen of Alexandria, who spoke of the doubts held by some—albeit not him. Eusebius classified it with the "disputed writings, the "antilegomena."" The letter was eventually accepted as part of the Canon by the Church Fathers such as Athanasius and the Synods of Laodicea (c. 363) and Carthage (397).
Authorship.
The Epistle title is written as follows: "Jude, a servant of Jesus Christ and brother of James" (NRSV). There is a dispute as to whether "brother" means someone who has the same father and mother, or a half-brother or cousin or more distant familial relationship. This dispute over the true meaning of "brother" grew as the doctrine of the Virgin Birth evolved.
The debate has continued over the author's identity as the apostle, the brother of Jesus, both, or neither. Some scholars have argued that since the author of that letter has not identified himself as an apostle and actually refers to the apostles as a third party, he cannot be identified with the Jude who is listed as one of the Twelve (and, in the Gospel of Luke, is explicitly identified as being as "of James"). Others have drawn the opposite conclusion, i.e., that as an apostle, he would not have made such a claim on his own behalf. The many Judes, named in the gospels and among the relatives of Jesus, and his relationship to James the Just called the brother of Jesus has caused much confusion. Not a lot is known of Jude, which would explain the apparent need to identify him by reference to his better-known brother. It is agreed that he is not the Jude who betrayed Jesus, Judas Iscariot.
Style.
The "Epistle of Jude" is a brief book of only a single chapter with 25 verses. It was composed as an "encyclical letter"—that is, one not directed to the members of one church in particular, but intended rather to be circulated and read in all churches. The form, as opposed to the earlier letters of Paul, suggests that the author knew Paul's "Epistle to the Ephesians" or even that the Pauline epistles had already been collected and were circulating when the text was written.
The wording and syntax of this epistle in its original Greek demonstrates that the author was capable and fluent. The epistle is addressed to Christians in general, and it warns them about the doctrine of certain errant teachers to whom they were exposed. Examples of heterodox opinions that were circulating in the early 2nd century include Docetism, Marcionism, and Gnosticism.
The epistle's style is combative, impassioned, and rushed. Many examples of evildoers and warnings about their fates are given in rapid succession. The epithets contained in this writing are considered to be some of the strongest found in the New Testament.
The epistle concludes with a doxology, which is considered to be one of the highest in quality contained in the Bible. 
Jude and 2 Peter.
Part of Jude is very similar to 2 Peter (mainly 2 Peter chapter 2), so much so that most scholars agree that there is a dependence between the two; that either one letter used the other directly, or they both drew on a common source.
Because this epistle is much shorter than 2 Peter, and due to various stylistic details, some writers consider that Jude was the source for the similar passages of 2 Peter. However other writers, noting that Jude 18 quotes 2 Peter 3:3 as past tense, consider that Jude came after 2 Peter.
Some scholars who consider Jude to predate 2 Peter note that the latter appears to quote the former but excises the reference to the non-canonical Enoch.
References to other books.
The Epistle of Jude references at least two other books, with one being non-canonical in all churches and the other non-canonical in most churches.
The Book of Enoch is not considered canonical by most churches, although it is by the Ethiopian Orthodox church. According to Western scholars the older sections of the Book of Enoch (mainly in the "Book of the Watchers") date from about 300 BC and the latest part ("Book of Parables") probably was composed at the end of 1st century BC. It is generally accepted by scholars that the author of the Epistle of Jude was familiar with the Book of Enoch and was influenced by it in thought and diction. Jude 1:14–15 quotes 1Enoch 1:9 which is part of the pseudepigrapha and is among the Dead Sea Scrolls Q Enoch (4Q204[4QENAR]) COL I 16–1.
External links.
Online translations of the Epistle of Jude:
Audiobook Version:
Additional information:

</doc>
<doc id="9692" url="https://en.wikipedia.org/wiki?curid=9692" title="Eusebius Amort">
Eusebius Amort

Eusebius Amort (November 15, 1692 – February 5, 1775) was a German Roman Catholic theologian.
Life.
Amort was born at Bibermuhle, near Tolz, in Upper Bavaria. He studied at Munich, and at an early age joined the Canons Regular at Polling, where, shortly after his ordination in 1717, he taught theology and philosophy.
The Parnassus Boicus learned society was based on a plan started in 1720 by three Augustinian fathers: Eusebius Amort, Gelasius Hieber (1671-1731), a famous preacher in the German language and Agnellus Kandler (1692-1745), a genealogist and librarian. The initial plans fell through, but in 1722 they issued the first volume of the "Parnassus Boicus" journal, communicating interesting information from the arts and sciences.
In 1733 Amort went to Rome as theologian to Cardinal Niccolo Maria Lercari (d. 1757).
He returned to Polling in 1735 and devoted the rest of his life to the revival of learning in Bavaria. He died at Polling in 1775.
Works.
Amort, who had the reputation of being the most learned man of his age, was a voluminous writer on every conceivable subject, from poetry to astronomy, from dogmatic theology to mysticism. His best known works are:
The list of his other works, including his three erudite contributions to the question of authorship of the "Imitatio Christi", will be found in C. Toussaint's scholarly article in Alfred Vacant's "Dictionnaire de theologie" (1900, cols 1115-1117).
References.
Citations
Sources

</doc>
<doc id="9693" url="https://en.wikipedia.org/wiki?curid=9693" title="Episcopi vagantes">
Episcopi vagantes

The "Oxford Dictionary of the Christian Church" mentions as the main lines of succession deriving from "" in the 20th century those founded by Arnold Mathew, Joseph René Vilatte, and Leon Chechemian. Others that could be added are those derived from Aftimios Ofiesh, Carlos Duarte Costa, Emmanuel Milingo, and Pierre Martin Ngô Đình Thục.
Theological issues.
In Western Christianity it has traditionally been taught, since as far back as the time of the Donatist controversy of the fourth and fifth centuries, that any bishop can consecrate any other baptised man as a bishop provided that the bishop observes the minimum requirements for the sacramental validity of the ceremony. This means that the consecration is considered valid even if it flouts certain ecclesiastical laws, and even if the participants are schismatics or heretics.
According to a theological view affirmed, for instance, by the International Bishops' Conference of the Old Catholic Church with regard to ordinations by Arnold Mathew, an episcopal ordination is for service within a specific Christian church, and an ordination ceremony that concerns only the individual himself does not make him truly a bishop. The Holy See has not commented on the validity of this theory, but has declared with regard to ordinations of this kind carried out, for example, by Emmanuel Milingo, that the Church "does not recognize and does not intend to recognize in the future those ordinations or any of the ordinations derived from them and therefore the canonical state of the alleged bishops remains that in which they were before the ordination conferred by Mr Milingo". Other theologians also, notably those of the Eastern Orthodox Church, dispute the notion that such ordinations have effect, a notion that opens up the possibility of valid but irregular consecrations proliferating outside the structures of the "official" denominations.
A Catholic ordained to the episcopacy without a mandate from the Pope is automatically excommunicated and is thereby forbidden to celebrate the sacraments, according to canon law.
Eastern Orthodox.
Vlassios Pheidas, on an official Church of Greece site, uses the canonical language of the Orthodox tradition, to describe the conditions in ecclesial praxis when sacraments, including Holy Orders, are real, valid, and efficacious. He notes language is itself part of the ecclesiological problem.
This applies to the validity and efficacy of the ordination of bishops and the other sacraments, not only of the Independent Catholic Churches, but also of all other Christian churches, including the Roman Catholic Church, Oriental Orthodoxy and the Assyrian Church of the East.
Anglican.
Anglican bishop Colin Buchanan, in "Historical Dictionary of Anglicanism", says that the Anglican Communion has held an Augustinian view of orders, by which "the validity of Episcopal ordinations (to whichever order) is based solely upon the historic succession in which the ordaining bishop stands, irrespective of their contemporary ecclesial context."
He describes the circumstances of Archbishop Matthew Parker's consecration as one of the reasons why this theory is "generally held". Parker was chosen by Queen Elizabeth I of England to be the first Church of England Archbishop of Canterbury after the death of the previous office holder, Cardinal Reginald Pole, the last Roman Catholic Archbishop of Canterbury. Buchanan notes the Roman Catholic Church also focuses on issues of intention and not just breaks in historical succession. He does not explain whether intention has an ecclesiological role, for Anglicans, in conferring or receiving sacraments.
History.
According to Buchanan, "the real rise of the problem" happened on the 19th century, in the "wake of the Anglo-Catholic movement", "through mischievous activities of a tiny number of independently acting bishops". They exist worldwide, he writes, "mostly without congregations", and "many in different stages of delusion and fantasy, not least in the Episcopal titles they confer on themselves"; "the distinguishing mark" to "specifically identif" an "episcopus vagans" is "the lack of a true see or the lack of a real church life to oversee". Paul Halsall, on the Internet History Sourcebooks Project, did not list a single church edifice of independent bishops, in a 1996–1998 New York City building architecture survey of religious communities, which maintain bishops claiming apostolic succession and claim cathedral status but noted there "are now literally hundreds of these ", of lesser or greater spiritual probity. They seem to have a tendency to call living room sanctuaries 'cathedrals';" those buildings were not perceived as cultural symbols and did not meet the survey criteria. David V. Barrett wrote, in "A brief guide to secret religions", that "one hallmark of such bishops is that they often collect as many lineages as they can to strengthen their Episcopal legitimacy—at least in their own eyes" and their groups have more clergy than members.
Many "episcopi vagantes" claim succession from the Old Catholic See of Utrecht, or from Eastern Orthodox, Oriental Orthodox, or Eastern Catholic Churches. A few others derive their orders from Roman Catholic bishops who have consecrated their own bishops after disputes with the Holy See.
Barrett wrote that leaders "of some esoteric movements, are also priests or bishops in small non-mainstream Christian Churches"; he explains, this type of "independent or autocephalous" group has "little in common with the Church it developed from, the Old Catholic Church, and even less in common with the Roman Catholic Church" but still claims its authority from Apostolic succession.
Many, if not most, "episcopi vagantes" are associated with Independent Catholic Churches. They may be very liberal or very conservative. "Episcopi vagantes" may also include some conservative "Continuing Anglicans" who have broken with the Anglican Communion over various issues such as Prayer Book revision, the ordination of women and the ordination of unmarried, non-celibate individuals (including homosexuals).
Buchanan writes that based the criteria of having "a true see" or having "a real church life to oversee", the bishops of most forms of Continuing Anglican movement are not necessarily classified as vagantes, but "are always in danger of becoming such".
Particular consecrations.
Arnold Mathew, according to Buchanan, "lapsed into the vagaries of an """ Stephen Edmonds, in "Oxford Dictionary of National Biography", wrote that in 1910 Mathew's wife separated from him; that same year, he declared himself and his church seceded from the Union of Utrecht. Within a few months, on , he was excommunicated by the Roman Catholic Church; sued "The Times" for libel based on the words "pseudo-bishop" used to describe him in the newspaper's translation from the Latin text ""; and, lost his case in 1913. Henry R.T. Brandreth wrote, in "Episcopi Vagantes and the Anglican Church", "ne of the most regrettable features of Mathew's episcopate was the founding of the Order of Corporate Reunion in 1908. This claimed to be a revival of Frederick George Lee's movement, but was in fact unconnected with it." Brandreth thought it "seems still to exist in a shadowy underground way" in 1947, but disconnected. Colin Holden, in "Ritualist on a Tricycle", places Mathew and his into perspective, he wrote Mathew was an "", lived in a cottage provided for him, and performed his conditional acts, sometimes called according to Holden "bedroom ordinations", in his cottage. Mathew questioned the validity of Anglican ordinations and became involved with the , in 1911 according to Edmonds, and he openly advertised his offer to reordain Anglican clergy who requested it. This angered the Church of England. In 1912, D. J. Scannell O'Neill wrote in "The Fortnightly Review" that London "seems to have more than her due share of bishops" and enumerates what he refers to as "these hireling shepherds". He also announces that one of them, Mathew, revived the and published "The Torch", a monthly review, advocating the reconstruction of Western Christianity and reunion with Eastern Christianity. "The Torch" stated "that the ordinations of the Church of England are not recognized by any church claiming to be Catholic" so the promoters involved Mathew to conditionally ordain group members who are "clergy of the Established Church" and "sign a profession of the Catholic Faith". It stipulated Mathew's services were not a system of simony and given without simoniac expectations. The group sought to enroll "earnest-minded Catholics who sincerely desire to help forward the work of orporate eunion with the Holy See". Nigel Yates, in "Anglican Ritualism in Victorian Britain, 1830-1910", described it as "an even more bizarre scheme to promote a Catholic Uniate Church in Britain" than Lee and Ambrose Lisle March Phillipps de Lisle's "Association for the Promotion of the Unity of Christendom". It was editorialized by O'Neill as the "most charitable construction to be placed on this latest move of Mathew is that he is not mentally sound. Being an Irishman, it is strange that he has not sufficient humor to see the absurdity of falling away from the Catholic Church in order to assist others to unite with the Holy See." Edmonds reports that "anything between 4 and 265 was suggested" as to how many took up his offer of reordination.
When it declared devoid of canonical effect the consecration ceremony conducted by Archbishop Pierre Martin Ngô Đình Thục for the Carmelite Order of the Holy Face group at midnight of 31 December 1975, the Holy See refrained from pronouncing on its validity. It made the same statement with regard also to later ordinations by those bishops, saying that, "as for those who have already thus unlawfully received ordination or any who may yet accept ordination from these, whatever may be the validity of the orders ("quidquid sit de ordinum validitate"), the Church does not and will not recognise their ordination ("ipsorum ordinationem"), and will consider them, for all legal effects, as still in the state in which they were before, except that the ... penalties remain until they repent".
A similar declaration was issued with regard to Archbishop Emmanuel Milingo's conferring of episcopal ordination on four men - all of whom, by virtue of previous Independent Catholic consecrations, claimed already to be bishops - on 24 September 2006: the Holy See, as well as stating that, in accordance with Canon 1382 of the Code of Canon Law, all five men involved incurred automatic ("latae sententiae") excommunication through their actions, declared that "the Church does not recognise and does not intend in the future to recognise these ordinations or any ordinations derived from them, and she holds that the canonical state of the four alleged bishops is the same as it was prior to the ordination."
In contrast, the Holy See has not questioned the validity of the consecrations that the late Archbishop Marcel Lefebvre performed in 1988 for the service of the relatively numerous followers of the Traditionalist Catholic Society of St. Pius X that he had founded, and of the bishops who, under pressure from the Chinese Catholic Patriotic Association, "have been ordained without the Pontifical mandate and who have not asked for, or have not yet obtained, the necessary legitimation", and who consequently, Pope Benedict XVI declared, "are to be considered illegitimate, but validly ordained".
Use as cultural reference.
Victor LaValle, in the novel "Big Machine" (2009), included three "" as part of his character's childhood involvement with an independent church:
Calvin Baker, in his novel "Dominion" (2006), includes an "" as one of his characters:
Intertextuality of ' language. Jim Higgins saw, in "More Years for the Locust", "similarities between Marxist obscurantism and an addiction to Christian arcana" and used ' pejoratively as his example of "the ever-growing proliferation of sects, sectlets and insects claiming direct descent from the master" with "fissiparous tendencies". He saw humor in the ludicrous characters and farce in their titles.

</doc>
<doc id="9695" url="https://en.wikipedia.org/wiki?curid=9695" title="Elizabeth Garrett Anderson">
Elizabeth Garrett Anderson

Elizabeth Garrett Anderson, LSA, MD (9 June 1836 – 17 December 1917), was an English physician and feminist, the first Englishwoman to qualify as a physician and surgeon in Britain, the co-founder of the first hospital staffed by women, the first dean of a British medical school, the first female doctor of medicine in France, the first woman in Britain to be elected to a school board and, as Mayor of Aldeburgh, the first female mayor and magistrate in Britain.
Early life.
Elizabeth Garrett was born on 9 June 1836 in Whitechapel, London, the second of twelve children of Newson Garrett (1812–1893), from Leiston, Suffolk, and his wife, Louisa Dunnell (1813–1903), from London.
The Garrett ancestors had been ironworkers in East Suffolk since the early seventeenth century. Newson was the youngest of three sons and not academically inclined, although he possessed the family’s entrepreneurial spirit. When he finished school, the town of Leiston offered little to Newson, so he left for London to make his fortune. There, he fell in love with his brother's sister-in-law, Louisa Dunnell, the daughter of an innkeeper of Suffolk origin. After their wedding, the couple went to live in a pawnbroker's shop at 1 Commercial Road, Whitechapel. The Garretts had their first three children in quick succession: Louie, Elizabeth and their brother (Newson Dunnell) who died at the age of six months. While Louisa grieved the loss of her third child, it was not easy to raise their two daughters in the London of that time. When Garrett was 3 years old, the family moved to 142 Long Acre, where they were to live for 2 years, whilst two more children were born and her father moved up in the world, becoming not only the manager of a larger pawnbroker's shop, but also a silversmith. Garrett's grandfather, owner of the family engineering works, Richard Garrett & Sons, had died in 1837, leaving the business to his eldest son, Garrett's uncle. Despite his lack of capital, Newson was determined to be successful and in 1841, at the age of 29, he moved his family to Suffolk, where he bought a barley and coal merchants business in Snape, constructing Snape Maltings, a fine range of buildings for malting barley.
The Garretts lived in a square Georgian house opposite the church in Aldeburgh until 1852. Newson's malting business expanded and five more children were born, Alice (1842), Millicent (1847), who was to become a leader in the constitutional campaign for women's suffrage, Sam (1850), Josephine (1853) and George (1854). By 1850, Newson was a prosperous businessman and was able to build Alde House, a mansion on a hill behind Aldeburgh. A “by-product of the industrial revolution”, Garrett grew up in an atmosphere of “triumphant economic pioneering” and the Garrett children were to grow up to become achievers in the professional classes of late-Victorian England. Garrett was encouraged to take an interest in local politics and, contrary to practices at the time, was allowed the freedom to explore the town with its nearby salt-marshes, beach and the small port of Slaughden with its boatbuilders' yards and sailmakers' lofts.
Early education.
There was no school in Aldeburgh so Garrett learned the three Rs from her mother. When she was 10 years old, a governess, Miss Edgeworth, a poor gentlewoman, was employed to educate Garrett and her sister. Mornings were spent in the schoolroom; there were regimental afternoon walks; educating the young ladies continued at mealtimes when Edgeworth ate with the family; at night, the governess slept in a curtained off area in the girls' bedroom. Garrett despised her governess and sought to outwit the teacher in the classroom. When Garrett was 13 and her sister 15, they were sent to a private school, the Boarding School for Ladies in Blackheath, London, which was run by the step aunts of the poet Robert Browning. There, English literature, French, Italian and German as well as deportment, were taught. 
Later in life, Garrett recalled the stupidity of her teachers there, though her schooling there did help establish a love of reading. Her main complaint about the school was the lack of science and mathematics instruction. Her reading matter included Tennyson, Wordsworth, Milton, Coleridge, Trollope, Thackeray and George Eliot. Elizabeth and Louie were known as “the bathing Garretts”, as their father had insisted they be allowed a hot bath once a week. However, they made what were to be lifelong friends there. When they finished in 1851, they were sent on a short tour abroad, ending with a memorable visit to the Great Exhibition in Hyde Park, London.
After this formal education, Garrett spent the next nine years tending to domestic duties, but she continued to study Latin and arithmetic in the mornings and also read widely. Her sister Millicent recalled Garrett's weekly lectures, “Talks on Things in General”, when her younger siblings would gather her while she discussed politics and current affairs from Garibaldi to Macaulay's "History of England". In 1854, when she was eighteen, Garrett and her sister went on a long visit to their school friends, Jane and Anne Crow, in Gateshead where she met Emily Davies, the early feminist and future co-founder of Girton College, Cambridge. Davies was to be a lifelong friend and confidante, always ready to give sound advice during the important decisions of Garrett’s career. It may have been in the "English Woman's Journal", first issued in 1858, that Garrett first read of Elizabeth Blackwell, who had become the first female doctor in the United States in 1849. When Blackwell visited London in 1859, Garrett travelled to the capital. By then, her sister Louie was married and living in London. Garrett joined the Society for Promoting the Employment of Women, which organised Blackwell's lectures on "Medicine as a Profession for Ladies" and set up a private meeting between Garrett and the doctor. It is said that during a visit to Alde House around 1860, one evening while sitting by the fireside, Garrett and Davies selected careers for advancing the frontiers of women's rights; Garrett was to open the medical profession to women, Davies the doors to a university education for women, while 13-year-old Millicent was allocated politics and votes for women. At first Newson was opposed to the radical idea of his daughter becoming a physician but came round and agreed to do all in his power, both financially and otherwise, to support Garrett.
Medical education.
After an initial unsuccessful visit to leading doctors in Harley Street, Garrett decided to first spend six months as a surgery nurse at Middlesex Hospital, London in August 1860. On proving to be a good nurse, she was allowed to attend an outpatients' clinic, then her first operation. She unsuccessfully attempted to enroll in the hospital's Medical School but was allowed to attend private tuition in Latin, Greek and "materia medica" with the hospital's apothecary, while continuing her work as a nurse. She also employed a tutor to study anatomy and physiology three evenings a week. Eventually she was allowed into the dissecting room and the chemistry lectures. Gradually, Garrett became an unwelcome presence among the male students, who in 1861 presented a memorial to the school against her admittance as a fellow student, despite the support she enjoyed from the administration. She was obliged to leave the Middlesex Hospital but she did so with an honours certificate in chemistry and "materia medica". Garrett then applied to several medical schools, including Oxford, Cambridge, Glasgow, Edinburgh, St Andrews and the Royal College of Surgeons, all of which refused her admittance.
A companion to her in this struggle was the lesser known Dr Sophia Jex-Blake. Whilst both are considered "outstanding" medical figures of the late 19th century, Anderson was able to obtain her credentials by way of a "side door" through a loophole in admissions at the Society of Apothecaries.
Having privately obtained a certificate in anatomy and physiology and in 1862, she was finally admitted by the Society of Apothecaries who, as a condition of their charter, could not legally exclude her on account of her sex. She continued her battle to qualify by studying privately with various professors, including some at the University of St Andrews, the Edinburgh Royal Maternity and the London Hospital Medical School. 
In 1865, she finally took her exam and obtained a licence (LSA) from the Society of Apothecaries to practise medicine, the first woman qualified in Britain to do so (previously there was Dr James Barry who was assigned a female gender at birth but lived his adult life as a man). On the day, three out of seven candidates passed the exam, Garrett with the highest marks. The Society of Apothecaries immediately amended its regulations to prevent other women obtaining a licence meaning that Jex-Blake however could not follow this same path; the new rule disallowed privately educated women to be eligible for examination. It was not until 1876 that the new Medical Act (39 and 40 Vict, Ch. 41) passed which allowed British medical authorities to license all qualified applicants whatever their gender.
Career.
Though she was now a licentiate of the Society of Apothecaries, as a woman, Garrett could not take up a medical post in any hospital. So in late 1865, Garrett opened her own practice at 20 Upper Berkeley Street, London. At first, patients were scarce but the practice gradually grew. After six months in practice, she wished to open an outpatients dispensary, to enable poor women to obtain medical help from a qualified practitioner of their own gender. In 1865, there was outbreak of cholera in Britain, affecting both rich and poor, and in their panic, some people forgot any prejudices they had in relation to a female physician. The first death due to cholera occurred in 1866, but by then Garrett had already opened St Mary's Dispensary for Women and Children, at 69 Seymour Place. In the first year, she tended to 3,000 new patients, who made 9,300 outpatient visits to the dispensary. On hearing that the Dean of the faculty of medicine at the University of Sorbonne, Paris was in favour of admitting women as medical students, Garrett studied French so that she could apply for a medical degree, which she obtained in 1870 after some difficulty.
The same year she was elected to the first London School Board, an office newly opened to women; Garrett's was the highest vote among all the candidates. Also in that year, she was made one of the visiting physicians of the East London Hospital for Children, becoming the first woman in Britain to be appointed to a medical post, but she found the duties of these two positions to be incompatible with her principal work in her private practice and the dispensary, as well as her role as a new mother, so she resigned from these posts by 1873. In 1872, the dispensary became the New Hospital for Women and Children, treating women from all over London for gynaecological conditions; the hospital moved to new premises in Marylebone Street in 1874. Around this time, Garrett also entered into discussion with male medical views regarding women. In 1874, Henry Maudsley’s article on Sex and Mind in Education appeared, which argued that education for women caused over-exertion and thus reduced their reproductive capacity, sometimes causing “nervous and even mental disorders”. Garrett’s counter-argument was that the real danger for women was not education but boredom and that fresh air and exercise were preferable to sitting by the fire with a novel. In the same year, she co-founded London School of Medicine for Women with Sophia Jex-Blake and became a lecturer in what was the only teaching hospital in Britain to offer courses for women. She continued to work there for the rest of her career and was dean of the school from 1883 to 1902. This school was later called the Royal Free Hospital of Medicine, which later became part of what is now the medical school of University College London.
BMA membership.
In 1873 she gained membership of the British Medical Association and remained the only female member for 19 years, due to the Association's vote against the admission of further women – "one of several instances where Garrett, uniquely, was able to enter a hitherto all male medical institution which subsequently moved formally to exclude any women who might seek to follow her." In 1897, Garrett Anderson was elected president of the East Anglian branch of the British Medical Association.
Garrett Anderson worked steadily at the development of the New Hospital for Women, and (from 1874) at the creation of the London School of Medicine for Women, where she served as its dean. Both institutions have since been handsomely and suitably housed and equipped, the New Hospital for Women (in the Euston Road) for many years being worked entirely by medical women, and the schools (in Hunter Street, WC1) having over 200 students, most of them preparing for the medical degree of London University (the present-day University College London), which was opened to women in 1877.
On 9 November 1908, she was elected mayor of Aldeburgh, the first female mayor in England. Her father was mayor in 1889.
She died in 1917 and is buried in the churchyard of St Peter and St Paul's Church, Aldeburgh.
Women’s Suffrage Movement.
Garrett Anderson was also active in the women's suffrage movement. In 1866, Garrett Anderson and Davies presented petitions signed by more than 1,500 asking that female heads of household be given the vote. That year, Garrett Anderson joined the first British Women's Suffrage Committee. She was not as active as her sister, Millicent Garrett Fawcett, though Garrett Anderson became a member of the Central Committee of the National Society for Women's Suffrage in 1889. After her husband's death in 1907, she became more active. As mayor of Aldeburgh, she gave speeches for suffrage, before the increasing militant activity in the movement led to her withdrawal in 1911. Her daughter Louisa, also a physician, was more active and more militant, spending time in prison in 1912 for her suffrage activities.
Personal life.
Elizabeth Garrett Anderson once remarked that “a doctor leads two lives, the professional and the private, and the boundaries between the two are never traversed”. In 1871, she married James George Skelton Anderson (d. 1907) of the Orient Steamship Company co-owned by his uncle Arthur Anderson, but she did not give up her medical practice. She had three children, Louisa (1873–1943), Margaret (1874–1875), who died of meningitis, and Alan (1877–1952). Louisa also became a pioneering doctor of medicine and feminist activist.
They retired to Aldeburgh in 1902, moving to Alde House in 1903, after the death of Elizabeth’s mother. Skelton died of stroke in 1907. She enjoyed a happy marriage and in later life, devoted time to Alde House, gardening, and travelling with younger members of the extended family. 
Legacy.
The New Hospital for Women was renamed the Elizabeth Garrett Anderson Hospital in 1918 and amalgamated with the Obstetric Hospital in 2001 to form the Elizabeth Garrett Anderson and Obstetric Hospital before relocating to become the University College Hospital Elizabeth Garrett Anderson Wing at UCH.
The former Elizabeth Garrett Anderson Hospital buildings are incorporated into the new National Headquarters for the public service trade union UNISON. The Elizabeth Garrett Anderson Gallery, a permanent installation set within the restored hospital building, uses a variety of media to set the story of Garrett Anderson, her hospital, and women's struggle to achieve equality in the field of medicine within the wider framework of 19th and 20th century social history.
There is a secondary school for girls in Islington, London which is named after her; Elizabeth Garrett Anderson School.
The archives of Elizabeth Garrett Anderson are held at The Women's Library at the Library of the London School of Economics, ref 7EGA
The archives of the Elizabeth Garrett Anderson Hospital (formerly the New Hospital for Women) are held at the London Metropolitan Archives.

</doc>
<doc id="9696" url="https://en.wikipedia.org/wiki?curid=9696" title="Erosion">
Erosion

In earth science, erosion is the action of surface processes (such as water flow or wind) that remove soil, rock, or dissolved material from one location on the Earth's crust, then transport it away to another location. The particulate breakdown of rock or soil into clastic sediment is referred to as "physical" or "mechanical" erosion; this contrasts with "chemical" erosion, where soil or rock material is removed from an area by its dissolving into a solvent (typically water), followed by the flow away of that solution. Eroded sediment or solutes may be transported just a few millimetres, or for thousands of kilometres.
Natural rates of erosion are controlled by the action of geomorphic drivers, such as rainfall; bedrock wear in rivers; coastal erosion by the sea and waves; glacial plucking, abrasion, and scour; areal flooding; wind abrasion; groundwater processes; and mass movement processes in steep landscapes like landslides and debris flows. The rates at which such processes act control how fast a surface is eroded. Typically, physical erosion proceeds fastest on steeply sloping surfaces, and rates may also be sensitive to some climatically-controlled properties including amounts of water supplied (e.g., by rain), storminess, wind speed, wave fetch, or atmospheric temperature (especially for some ice-related processes). Feedbacks are also possible between rates of erosion and the amount of eroded material that is already carried by, for example, a river or glacier. Processes of erosion that produce sediment or solutes from a place contrast with those of deposition, which control the arrival and emplacement of material at a new location.
While erosion is a natural process, human activities have increased by 10-40 times the rate at which erosion is occurring globally. Excessive (or accelerated) erosion causes both "on-site" and "off-site" problems. On-site impacts include decreases in agricultural productivity and (on natural landscapes) ecological collapse, both because of loss of the nutrient-rich upper soil layers. In some cases, the eventual end result is desertification. Off-site effects include sedimentation of waterways and eutrophication of water bodies, as well as sediment-related damage to roads and houses. Water and wind erosion are the two primary causes of land degradation; combined, they are responsible for about 84% of the global extent of degraded land, making excessive erosion one of the most significant environmental problems world-wide.
Intensive agriculture, deforestation, roads, anthropogenic climate change and urban sprawl are amongst the most significant human activities in regard to their effect on stimulating erosion. However, there are many prevention and remediation practices that can curtail or limit erosion of vulnerable soils.
Physical processes.
Rainfall and surface runoff.
Rainfall, and the surface runoff which may result from rainfall, produces four main types of soil erosion: "splash erosion", "sheet erosion", "rill erosion", and "gully erosion". Splash erosion is generally seen as the first and least severe stage in the soil erosion process, which is followed by sheet erosion, then rill erosion and finally gully erosion (the most severe of the four).
In "splash erosion", the impact of a falling raindrop creates a small crater in the soil, ejecting soil particles. The distance these soil particles travel can be as much as 0.6 m (two feet) vertically and 1.5 m (five feet) horizontally on level ground.
If the soil is saturated, or if the rainfall rate is greater than the rate at which water can infiltrate into the soil, surface runoff occurs. If the runoff has sufficient flow energy, it will transport loosened soil particles (sediment) down the slope. "Sheet erosion" is the transport of loosened soil particles by overland flow.
"Rill erosion" refers to the development of small, ephemeral concentrated flow paths which function as both sediment source and sediment delivery systems for erosion on hillslopes. Generally, where water erosion rates on disturbed upland areas are greatest, rills are active. Flow depths in rills are typically of the order of a few centimetres (about an inch) or less and along-channel slopes may be quite steep. This means that rills exhibit hydraulic physics very different from water flowing through the deeper, wider channels of streams and rivers.
"Gully erosion" occurs when runoff water accumulates and rapidly flows in narrow channels during or immediately after heavy rains or melting snow, removing soil to a considerable depth.
Rivers and streams.
"Valley" or "stream erosion" occurs with continued water flow along a linear feature. The erosion is both downward, deepening the valley, and headward, extending the valley into the hillside, creating head cuts and steep banks. In the earliest stage of stream erosion, the erosive activity is dominantly vertical, the valleys have a typical V cross-section and the stream gradient is relatively steep. When some base level is reached, the erosive activity switches to lateral erosion, which widens the valley floor and creates a narrow floodplain. The stream gradient becomes nearly flat, and lateral deposition of sediments becomes important as the stream meanders across the valley floor. In all stages of stream erosion, by far the most erosion occurs during times of flood, when more and faster-moving water is available to carry a larger sediment load. In such processes, it is not the water alone that erodes: suspended abrasive particles, pebbles and boulders can also act erosively as they traverse a surface, in a process known as "traction".
"Bank erosion" is the wearing away of the banks of a stream or river. This is distinguished from changes on the bed of the watercourse, which is referred to as "scour". Erosion and changes in the form of river banks may be measured by inserting metal rods into the bank and marking the position of the bank surface along the rods at different times.
"Thermal erosion" is the result of melting and weakening permafrost due to moving water. It can occur both along rivers and at the coast. Rapid river channel migration observed in the Lena River of Siberia is due to thermal erosion, as these portions of the banks are composed of permafrost-cemented non-cohesive materials. Much of this erosion occurs as the weakened banks fail in large slumps. Thermal erosion also affects the Arctic coast, where wave action and near-shore temperatures combine to undercut permafrost bluffs along the shoreline and cause them to fail. Annual erosion rates along a segment of the Beaufort Sea shoreline averaged per year from 1955 to 2002.
Coastal erosion.
Shoreline erosion, which occurs on both exposed and sheltered coasts, primarily occurs through the action of currents and waves but sea level (tidal) change can also play a role.
"Hydraulic action" takes place when air in a joint is suddenly compressed by a wave closing the entrance of the joint. This then cracks it. "Wave pounding" is when the sheer energy of the wave hitting the cliff or rock breaks pieces off. "Abrasion" or "corrasion" is caused by waves launching seaload at the cliff. It is the most effective and rapid form of shoreline erosion (not to be confused with "corrosion"). "Corrosion" is the dissolving of rock by carbonic acid in sea water. Limestone cliffs are particularly vulnerable to this kind of erosion. "Attrition" is where particles/seaload carried by the waves are worn down as they hit each other and the cliffs. This then makes the material easier to wash away. The material ends up as shingle and sand. Another significant source of erosion, particularly on carbonate coastlines, is the boring, scraping and grinding of organisms, a process termed "bioerosion".
Sediment is transported along the coast in the direction of the prevailing current (longshore drift). When the upcurrent amount of sediment is less than the amount being carried away, erosion occurs. When the upcurrent amount of sediment is greater, sand or gravel banks will tend to form as a result of deposition. These banks may slowly migrate along the coast in the direction of the longshore drift, alternately protecting and exposing parts of the coastline. Where there is a bend in the coastline, quite often a buildup of eroded material occurs forming a long narrow bank (a spit). Armoured beaches and submerged offshore sandbanks may also protect parts of a coastline from erosion. Over the years, as the shoals gradually shift, the erosion may be redirected to attack different parts of the shore.
Chemical erosion.
Chemical erosion is the loss of matter in a landscape in the form of solutes. Chemical erosion is usually calculated from the solutes found in streams. Anders Rapp pioneered the study of chemical erosion in his work about Kärkevagge published in 1960.
Glaciers.
Glaciers erode predominantly by three different processes: abrasion/scouring, plucking, and ice thrusting. In an abrasion process, debris in the basal ice scrapes along the bed, polishing and gouging the underlying rocks, similar to sandpaper on wood. Glaciers can also cause pieces of bedrock to crack off in the process of plucking. In ice thrusting, the glacier freezes to its bed, then as it surges forward, it moves large sheets of frozen sediment at the base along with the glacier. This method produced some of the many thousands of lake basins that dot the edge of the Canadian Shield. The erosion caused by glaciers worldwide erodes mountains so effectively that the term "glacial buzz-saw" has become widely used, which describes the limiting effect of glaciers on the height of mountain ranges. As mountains grow higher, they generally allow for more glacial activity (especially in the accumulation zone above the glacial equilibrium line altitude), which causes increased rates of erosion of the mountain, decreasing mass faster than isostatic rebound can add to the mountain. This provides a good example of a negative feedback loop. Ongoing research is showing that while glaciers tend to decrease mountain size, in some areas, glaciers can actually reduce the rate of erosion, acting as a "glacial armour".
These processes, combined with erosion and transport by the water network beneath the glacier, leave moraines, drumlins, ground moraine (till), kames, kame deltas, moulins, and glacial erratics in their wake, typically at the terminus or during glacier retreat.
Floods.
At extremely high flows, kolks, or vortices are formed by large volumes of rapidly rushing water. Kolks cause extreme local erosion, plucking bedrock and creating pothole-type geographical features called Rock-cut basins. Examples can be seen in the flood regions result from glacial Lake Missoula, which created the channeled scablands in the Columbia Basin region of eastern Washington.
Wind erosion.
Wind erosion is a major geomorphological force, especially in arid and semi-arid regions. It is also a major source of land degradation, evaporation, desertification, harmful airborne dust, and crop damage—especially after being increased far above natural rates by human activities such as deforestation, urbanization, and agriculture.
Wind erosion is of two primary varieties: "deflation", where the wind picks up and carries away loose particles; and "abrasion", where surfaces are worn down as they are struck by airborne particles carried by wind. Deflation is divided into three categories: (1) "surface creep", where larger, heavier particles slide or roll along the ground; (2) "saltation", where particles are lifted a short height into the air, and bounce and saltate across the surface of the soil; and (3) "suspension", where very small and light particles are lifted into the air by the wind, and are often carried for long distances. Saltation is responsible for the majority (50-70%) of wind erosion, followed by suspension (30-40%), and then surface creep (5-25%).
Wind erosion is much more severe in arid areas and during times of drought. For example, in the Great Plains, it is estimated that soil loss due to wind erosion can be as much as 6100 times greater in drought years than in wet years.
Mass movement.
"Mass movement" is the downward and outward movement of rock and sediments on a sloped surface, mainly due to the force of gravity.
Mass movement is an important part of the erosional process, and is often the first stage in the breakdown and transport of weathered materials in mountainous areas. It moves material from higher elevations to lower elevations where other eroding agents such as streams and glaciers can then pick up the material and move it to even lower elevations. Mass-movement processes are always occurring continuously on all slopes; some mass-movement processes act very slowly; others occur very suddenly, often with disastrous results. Any perceptible down-slope movement of rock or sediment is often referred to in general terms as a landslide. However, landslides can be classified in a much more detailed way that reflects the mechanisms responsible for the movement and the velocity at which the movement occurs. One of the visible topographical manifestations of a very slow form of such activity is a scree slope.
"Slumping" happens on steep hillsides, occurring along distinct fracture zones, often within materials like clay that, once released, may move quite rapidly downhill. They will often show a spoon-shaped isostatic depression, in which the material has begun to slide downhill. In some cases, the slump is caused by water beneath the slope weakening it. In many cases it is simply the result of poor engineering along highways where it is a regular occurrence.
"Surface creep" is the slow movement of soil and rock debris by gravity which is usually not perceptible except through extended observation. However, the term can also describe the rolling of dislodged soil particles in diameter by wind along the soil surface.
Factors affecting erosion rates.
Climate.
The amount and intensity of precipitation is the main climatic factor governing soil erosion by water. The relationship is particularly strong if heavy rainfall occurs at times when, or in locations where, the soil's surface is not well protected by vegetation. This might be during periods when agricultural activities leave the soil bare, or in semi-arid regions where vegetation is naturally sparse. Wind erosion requires strong winds, particularly during times of drought when vegetation is sparse and soil is dry (and so is more erodible). Other climatic factors such as average temperature and temperature range may also affect erosion, via their effects on vegetation and soil properties. In general, given similar vegetation and ecosystems, areas with more precipitation (especially high-intensity rainfall), more wind, or more storms are expected to have more erosion.
In some areas of the world (e.g. the mid-western USA), rainfall intensity is the primary determinant of erosivity, with higher intensity rainfall generally resulting in more soil erosion by water. The size and velocity of rain drops is also an important factor. Larger and higher-velocity rain drops have greater kinetic energy, and thus their impact will displace soil particles by larger distances than smaller, slower-moving rain drops.
In other regions of the world (e.g. western Europe), runoff and erosion result from relatively low intensities of stratiform rainfall falling onto previously saturated soil. In such situations, rainfall amount rather than intensity is the main factor determining the severity of soil erosion by water.
Vegetative cover.
Vegetation acts as an interface between the atmosphere and the soil. It increases the permeability of the soil to rainwater, thus decreasing runoff. It shelters the soil from winds, which results in decreased wind erosion, as well as advantageous changes in microclimate. The roots of the plants bind the soil together, and interweave with other roots, forming a more solid mass that is less susceptible to both water and wind erosion. The removal of vegetation increases the rate of surface erosion.
Topography.
The topography of the land determines the velocity at which surface runoff will flow, which in turn determines the erosivity of the runoff. Longer, steeper slopes (especially those without adequate vegetative cover) are more susceptible to very high rates of erosion during heavy rains than shorter, less steep slopes. Steeper terrain is also more prone to mudslides, landslides, and other forms of gravitational erosion processes.
Tectonics.
Tectonic processes control rates and distributions of erosion at the Earth's surface. If tectonic action causes part of the Earth's surface (e.g., a mountain range) to be raised or lowered relative to surrounding areas, this must necessarily change the gradient of the land surface. Because erosion rates are almost always sensitive to local slope (see above), this will change the rates of erosion in the uplifted area. Active tectonics also brings fresh, unweathered rock towards the surface, where it is exposed to the action of erosion.
However, erosion can also affect tectonic processes. The removal by erosion of large amounts of rock from a particular region, and its deposition elsewhere, can result in a lightening of the load on the lower crust and mantle. Because tectonic processes are driven by gradients in the stress field developed in the crust, this unloading can in turn cause tectonic or isostatic uplift in the region. In some cases, it has been hypothesised that these twin feedbacks can act to localise and enhance zones of very rapid exhumation of deep crustal rocks beneath places on the Earth's surface with extremely high erosion rates, for example, beneath the extremely steep terrain of Nanga Parbat in the western Himalayas. Such a place has been called a "tectonic aneurysm".
Erosion of Earth systems.
Mountain ranges.
Mountain ranges are known to take many million of years to erode to the degree they effectively cease to exist. Scholars Pitman and Golovchenko estimate that it takes probably more than 450 million years to erode a mountain mass similar to the Himalaya into an almost-flat peneplain if there are no major sea level changes. Erosion of mountains massifs can create a pattern of equally high summits called summit accordance.
Examples of heavily eroded mountain ranges include the Timanides of Northern Russia. Erosion of this orogen has produced sediments that are now found in the East European Platform, including the Cambrian Sablya Formation near Lake Ladoga. Studies of these sediments points that its likely that the erosion of the orogen was beginning in the Cambrian and then became stronger in Ordovician.
Soils.
If the rate of erosion is higher than the rate of soil formation the soils are being destroyed by erosion. Where soil is not destroyed by erosion, erosion can in some cases prevent the formation of soil features that form slowly. Inceptisols are common soils that form in areas of fast erosion.
While erosion of soils is a natural process, human activities have increased by 10-40 times the rate at which erosion is occurring globally. Excessive (or accelerated) erosion causes both "on-site" and "off-site" problems. On-site impacts include decreases in agricultural productivity and (on natural landscapes) ecological collapse, both because of loss of the nutrient-rich upper soil layers. In some cases, the eventual end result is desertification. Off-site effects include sedimentation of waterways and eutrophication of water bodies, as well as sediment-related damage to roads and houses. Water and wind erosion are the two primary causes of land degradation; combined, they are responsible for about 84% of the global extent of degraded land, making excessive erosion one of the most significant environmental problems world-wide.

</doc>
<doc id="9697" url="https://en.wikipedia.org/wiki?curid=9697" title="Euclidean space">
Euclidean space

In geometry, Euclidean space encompasses the two-dimensional Euclidean plane, the three-dimensional space of Euclidean geometry, and certain other spaces. It is named after the Ancient Greek mathematician Euclid of Alexandria. The term "Euclidean" distinguishes these spaces from other types of spaces considered in modern geometry. Euclidean spaces also generalize to higher dimensions.
Classical Greek geometry defined the Euclidean plane and Euclidean three-dimensional space using certain postulates, while the other properties of these spaces were deduced as theorems. Geometric constructions are also used to define rational numbers. When algebra and mathematical analysis became developed enough, this relation reversed and now it is more common to define Euclidean space using Cartesian coordinates and the ideas of analytic geometry. It means that points of the space are specified with collections of real numbers, and geometric shapes are defined as equations and inequalities. This approach brings the tools of algebra and calculus to bear on questions of geometry and has the advantage that it generalizes easily to Euclidean spaces of more than three dimensions.
From the modern viewpoint, there is essentially only one Euclidean space of each dimension. With Cartesian coordinates it is modelled by the real coordinate space () of the same dimension. In one dimension, this is the real line; in two dimensions, it is the Cartesian plane; and in higher dimensions it is a coordinate space with three or more real number coordinates. Mathematicians denote the -dimensional Euclidean space by if they wish to emphasize its Euclidean nature, but is used as well since the latter is assumed to have the standard Euclidean structure, and these two structures are not always distinguished. Euclidean spaces have finite dimension.
Intuitive overview.
One way to think of the Euclidean plane is as a set of points satisfying certain relationships, expressible in terms of distance and angle. For example, there are two fundamental operations (referred to as motions) on the plane. One is translation, which means a shifting of the plane so that every point is shifted in the same direction and by the same distance. The other is rotation about a fixed point in the plane, in which every point in the plane turns about that fixed point through the same angle. One of the basic tenets of Euclidean geometry is that two figures (usually considered as subsets) of the plane should be considered equivalent (congruent) if one can be transformed into the other by some sequence of translations, rotations and reflections (see below).
In order to make all of this mathematically , the theory must clearly define the notions of distance, angle, translation, and rotation for a mathematically described space. Even when used in physical theories, Euclidean space is an abstraction detached from actual physical locations, specific reference frames, measurement instruments, and so on. A purely mathematical definition of Euclidean space also ignores questions of units of length and other physical dimensions: the distance in a "mathematical" space is a number, not something expressed in inches or metres. The standard way to define such space, as carried out in the remainder of this article, is to define the Euclidean plane as a two-dimensional real vector space equipped with an inner product. The reason for working with arbitrary vector spaces instead of is that it is often preferable to work in a "coordinate-free" manner (that is, without choosing a preferred basis). For then:
Once the Euclidean plane has been described in this language, it is actually a simple matter to extend its concept to arbitrary dimensions. For the most part, the vocabulary, formulae, and calculations are not made any more difficult by the presence of more dimensions. (However, rotations are more subtle in high dimensions, and visualizing high-dimensional spaces remains difficult, even for experienced mathematicians.)
A Euclidean space is not technically a vector space but rather an affine space, on which a vector space acts by translations, or, conversely, a Euclidean vector is the difference (displacement) in an ordered pair of points, not a single point. Intuitively, the distinction says merely that there is no canonical choice of where the origin should go in the space, because it can be translated anywhere. When a certain point is chosen, it can be declared the origin and subsequent calculations may ignore the difference between a point and its coordinate vector, as said above. See point–vector distinction for details.
Euclidean structure.
These are distances between points and the angles between lines or vectors, which satisfy certain conditions (see below), which makes a set of points a Euclidean space. The natural way to obtain these quantities is by introducing and using the standard inner product (also known as the dot product) on . The inner product of any two real -vectors and is defined by
where and are th coordinates of vectors and respectively.
The result is always a real number.
Distance.
The inner product of with itself is always non-negative. This product allows us to define the "length" of a vector through square root:
This length function satisfies the required properties of a norm and is called the Euclidean norm on .
Finally, one can use the norm to define a metric (or distance function) on by
This distance function is called the Euclidean metric. This formula expresses a special case of the Pythagorean theorem.
This distance function (which makes a metric space) is sufficient to define all Euclidean geometry, including the dot product. Thus, a real coordinate space together with this Euclidean structure is called Euclidean space. Its vectors form an inner product space (in fact a Hilbert space), and a normed vector space.
The metric space structure is the main reason behind the use of real numbers , not some other ordered field, as the mathematical foundation of Euclidean (and many other) spaces. Euclidean space is a complete metric space, a property which is impossible to achieve operating over rational numbers, for example.
Angle.
The (non-reflex) angle () between vectors and is then given by
where is the arccosine function. It is useful only for , and the case is somewhat special. Namely, on an oriented Euclidean plane one can define an angle between two vectors as a number defined modulo 1 turn (usually denoted as either or 360°), such that . This oriented angle is equal either to the angle from the formula above or to . If one non-zero vector is fixed (such as the first basis vector), then each non-zero vector is uniquely defined by its magnitude and angle.
The angle does not change if vectors and are multiplied by positive numbers.
Unlike the aforementioned situation with distance, the scale of angles is the same in pure mathematics, physics, and computing. It does not depend on the scale of distances; all distances may be multiplied by some fixed factor, and all angles will be preserved. Usually, the angle is considered a dimensionless quantity, but there are different units of measurement, such as radian (preferred in pure mathematics and theoretical physics) and degree (°) (preferred in most applications).
Rotations and reflections.
Symmetries of a Euclidean space are transformations which preserve the Euclidean metric (called "isometries"). Although aforementioned translations are most obvious of them, they have the same structure for any affine space and do not show a distinctive character of Euclidean geometry. Another family of symmetries leave one point fixed, which may be seen as the origin without loss of generality. All transformations, which preserves the origin and the Euclidean metric, are linear maps. Such transformations must, for any and , satisfy:
Such transforms constitute a group called the "orthogonal group" . Its elements are exactly solutions of a matrix equation
where is the transpose of and is the identity matrix.
But a Euclidean space is orientable. Each of these transformations either preserves or reverses orientation depending on whether its determinant is +1 or −1 respectively. Only transformations which preserve orientation, which form the "special orthogonal" group , are considered (proper) rotations. This group has, as a Lie group, the same dimension and is the identity component of .
Groups are well-studied for . There are no non-trivial rotations in 0- and 1-spaces. Rotations of a Euclidean plane () are parametrized by the angle (modulo 1 turn). Rotations of a 3-space are parametrized with axis and angle, whereas a rotation of a 4-space is a superposition of two 2-dimensional rotations around perpendicular planes.
Among linear transforms in which reverse the orientation are hyperplane reflections. This is the only possible case for , but starting from three dimensions, such isometry in the general position is a rotoreflection.
Euclidean group.
The Euclidean group , also referred to as the group of all isometries , treats translations, rotations, and reflections in a uniform way, considering them as group actions in the context of group theory, and especially in Lie group theory. These group actions preserve the Euclidean structure.
As the group of all isometries, , the Euclidean group is important because it makes Euclidean geometry a case of Klein geometry, a theoretical framework including many alternative geometries.
The structure of Euclidean spaces – distances, lines, vectors, angles (up to sign), and so on – is invariant under the transformations of their associated Euclidean group. For instance, translations form a commutative subgroup that acts freely and transitively on , while the stabilizer of any point there is the aforementioned.
Along with translations, rotations, reflections, as well as the identity transformation, Euclidean motions comprise also glide reflections (for ), screw operations and rotoreflections (for ), and even more complex combinations of primitive transformations for .
The group structure determines which conditions a metric space needs to satisfy to be a Euclidean space:
Non-Cartesian coordinates.
Cartesian coordinates are arguably the standard, but not the only possible option for a Euclidean space.
Skew coordinates are compatible with the affine structure of , but make formulae for angles and distances more complicated.
Another approach, which goes in line with ideas of differential geometry and conformal geometry, is orthogonal coordinates, where coordinate hypersurfaces of different coordinates are orthogonal, although curved. Examples include the polar coordinate system on Euclidean plane, the second important plane coordinate system.
See below about expression of the Euclidean structure in curvilinear coordinates.
Geometric shapes.
Lines, planes, and other subspaces.
The simplest (after points) objects in Euclidean space are flats, or Euclidean "subspaces" of lesser dimension. Points are 0-dimensional flats, 1-dimensional flats are called "(straight) lines", and 2-dimensional flats are "planes". -dimensional flats are called "hyperplanes".
Any two distinct points lie on exactly one line. Any line and a point outside it lie on exactly one plane. More generally, the properties of flats and their incidence of Euclidean space are shared with affine geometry, whereas the affine geometry is devoid of distances and angles.
Line segments and triangles.
This is not only a line which a pair of distinct points defines. Points on the line which lie between and , together with and themselves, constitute a line segment . Any line segment has the length, which equals to distance between and . If , then the segment is degenerate and its length equals to 0, otherwise the length is positive.
A (non-degenerate) triangle is defined by three points not lying on the same line. Any triangle lies on one plane. The concept of triangle is not specific to Euclidean spaces, but Euclidean triangles have numerous special properties and define many derived objects.
A triangle can be thought of as a 3-gon on a plane, a special (and the first meaningful in Euclidean geometry) case of a polygon.
Polytopes and root systems.
Polytope is a concept that generalizes polygons on a plane and polyhedra in 3-dimensional space (which are among the earliest studied geometrical objects). A simplex is a generalization of a line segment (1-simplex) and a triangle (2-simplex). A tetrahedron is a 3-simplex.
The concept of a polytope belongs to affine geometry, which is more general than Euclidean. But Euclidean geometry distinguish "regular polytopes". For example, affine geometry does not see the difference between an equilateral triangle and a right triangle, but in Euclidean space the former is regular and the latter is not.
Root systems are special sets of Euclidean vectors. A root system is often identical to the set of vertices of a regular polytope.
Topology.
Since Euclidean space is a metric space, it is also a topological space with the natural topology induced by the metric. The metric topology on is called the Euclidean topology, and it is identical to the standard topology on . A set is open if and only if it contains an open ball around each of its points; in other words, open balls form a base of the topology. The topological dimension of the Euclidean -space equals , which implies that spaces of different dimension are not homeomorphic. A finer result is the invariance of domain, which proves that any subset of -space, that is (with its subspace topology) homeomorphic to an open subset of -space, is itself open.
Applications.
Aside from countless uses in fundamental mathematics, a Euclidean model of the physical space can be used to solve many practical problems with sufficient precision. Two usual approaches are a fixed, or "stationary" reference frame (i.e. the description of a motion of objects as their positions that change continuously with time), and the use of Galilean space-time symmetry (such as in Newtonian mechanics). To both of them the modern Euclidean geometry provides a convenient formalism; for example, the space of Galilean velocities is itself a Euclidean space (see relative velocity for details).
Topographical maps and technical drawings are planar Euclidean. An idea behind them is the scale invariance of Euclidean geometry, that permits to represent large objects in a small sheet of paper, or a screen.
Alternatives and generalizations.
Although Euclidean spaces are no longer considered to be the only possible setting for a geometry, they act as prototypes for other geometric objects. Ideas and terminology from Euclidean geometry (both traditional and analytic) are pervasive in modern mathematics, where other geometric objects share many similarities with Euclidean spaces, share part of their structure, or embed Euclidean spaces.
Curved spaces.
A smooth manifold is a Hausdorff topological space that is locally diffeomorphic to Euclidean space. Diffeomorphism does not respect distance and angle, but if one additionally prescribes a smoothly varying inner product on the manifold's tangent spaces, then the result is what is called a Riemannian manifold. Put differently, a manifold is a space constructed by deforming and patching together Euclidean spaces. Such a space enjoys notions of distance and angle, but they behave in a curved, non-Euclidean manner. The simplest Riemannian manifold, consisting of with a constant inner product, is essentially identical to Euclidean -space itself. Less trivial examples are -sphere and hyperbolic spaces. Discovery of the latter in the 19th century was branded as the non-Euclidean geometry.
Also, the concept of a Riemannian manifold permits an expression of the Euclidean structure in any smooth coordinate system, via metric tensor. From this tensor one can compute the Riemann curvature tensor. Where the latter equals to zero, the metric structure is locally Euclidean (it means that at least some open set in the coordinate space is isometric to a piece of Euclidean space), no matter whether coordinates are affine or curvilinear.
Indefinite quadratic form.
If one replaces the inner product of a Euclidean space with an indefinite quadratic form, the result is a pseudo-Euclidean space. Smooth manifolds built from such spaces are called pseudo-Riemannian manifolds. Perhaps their most famous application is the theory of relativity, where flat spacetime is a pseudo-Euclidean space called Minkowski space, where rotations correspond to motions of hyperbolic spaces mentioned above. Further generalization to curved spacetimes form pseudo-Riemannian manifolds, such as in general relativity.
Other number fields.
Another line of generalization is to consider other number fields than one of real numbers. Over complex numbers, a Hilbert space can be seen as a generalization of Euclidean dot product structure, although the definition of the inner product becomes a sesquilinear form for compatibility with metric structure.

</doc>
<doc id="9700" url="https://en.wikipedia.org/wiki?curid=9700" title="Edwin Austin Abbey">
Edwin Austin Abbey

Edwin Austin Abbey (April 1, 1852 – August 1, 1911) was an American muralist, illustrator, and painter. He flourished at the beginning of what is now referred to as the "golden age" of illustration, and is best known for his drawings and paintings of Shakespearean and Victorian subjects, as well as for his painting of Edward VII's coronation. His most famous set of murals, "The Quest of the Holy Grail", adorns the Boston Public Library.
Biography.
Abbey was born in Philadelphia in 1852. He studied art at the Pennsylvania Academy of the Fine Arts under Christian Schuessele. Abbey began as an illustrator, producing numerous illustrations and sketches for such magazines as Harper's Weekly (1871–1874) and Scribner's Magazine. His illustrations began appearing in Harper's Weekly at an early age: before Abbey was twenty years old. He moved to New York City in 1871. His illustrations were strongly influenced by French and German black and white art. He also illustrated several best-selling books, including "Christmas Stories" by Charles Dickens (1875), "Selections from the Poetry of Robert Herrick" (1882), and "She Stoops to Conquer" by Oliver Goldsmith (1887). Abbey also illustrated a four-volume set of "The Comedies of Shakespeare" for Harper & Brothers in 1896.
He moved to England in 1878, at the request of his employers, to gather material concerning Robert Herrick, and he settled permanently there in 1883. In 1883, he was elected to the Royal Institute of Painters in Water-Colours. About this time, he was appraised critically by the American writer, S.G.W. Benjamin:
He was made a full member of the Royal Academy in 1898. In 1902 he was chosen to paint the coronation of King Edward VII. It was the official painting of the occasion and, hence, resides at Buckingham Palace. He did receive a knighthood, although some say he refused it in 1907. Friendly with other expatriate American artists, he summered at Broadway, Worcestershire, England, where he painted and vacationed alongside John Singer Sargent at the home of Francis Davis Millet.
He completed murals for the Boston Public Library in the 1890s. The frieze for the Library was titled "The Quest for the Holy Grail." It took Abbey eleven years to complete this series of murals in his England studio.
Pennsylvania State Capitol.
In 1908–09, Abbey began an ambitious program of murals and other artworks for the newly completed Pennsylvania State Capitol in Harrisburg, Pennsylvania. These included allegorical medallion murals representing "Science", "Art", "Justice", and "Religion" for the dome of the Rotunda, four large lunette murals beneath the dome, and multiple works for the House and Senate Chambers. He was working on the "Reading of the Declaration of Independence" mural in early 1911, when his health began to fail. He was diagnosed with cancer. Studio assistant Ernest Board continued work on the mural with little supervision from Abbey, and with contributions by John Singer Sergeant.
Abbey died in August 1911, leaving two rooms of the commission unfinished. The remainder of the work was given to Violet Oakley, who completed the commission from start to finish using her own designs.
Legacy.
Abbey was elected to the National Academy of Design, in 1902, and The American Academy of Arts and Letters. He was a prolific illustrator, and attention to detail, including historical accuracy, influenced successive generations of illustrators.
In 1890, Edwin married Gertrude Mead, the daughter of a wealthy New York merchant. Mrs Abbey encouraged her husband to secure more ambitious commissions, although with their marriage commencing when both were in their forties, the couple remained childless. After her husband’s death, Gertrude was active in preserving her husband’s legacy, writing about his work and giving her substantial collection and archive to Yale. Edwin had been a keen supporter of the newly founded British School at Rome (BSR), so, in his memory, she donated £6000 to assist in building the artists’ studio block and, in 1926, founded the Incorporated Edwin Austin Abbey Memorial Scholarships. The scholarships were established to enable British and American painters to pursue their practice. Recipients of Abbey funding – Scholars and, more recently, Fellows – devote their scholarship to working in the studios at the BSR, where there has, ever since, been at least one Abbey-funded artist in residence. Previous award holders include Stephen Farthing, Chantal Joffe and Spartacus Chetwynd. The Abbey Fellowships (formerly ‘Awards’) were established in their present form in 1990, and the Abbey studios also host the BSR’s other fine art residencies, such as the Derek Hill Foundation Scholarship and the Sainsbury Scholarship in Painting and Drawing. A bust of Edwin Abbey, by Sir Thomas Brock, stands in the courtyard of the BSR.
Abbey is buried in the churchyard of Old St Andrew's Church in Kingsbury, London. His grave is Grade II listed.

</doc>
<doc id="9703" url="https://en.wikipedia.org/wiki?curid=9703" title="Evolutionary psychology">
Evolutionary psychology

Evolutionary psychology (EP) is a theoretical approach in the social and natural sciences that examines psychological structure from a modern evolutionary perspective. It seeks to identify which human psychological traits are evolved adaptations – that is, the functional products of natural selection or sexual selection in human evolution. Adaptationist thinking about physiological mechanisms, such as the heart, lungs, and immune system, is common in evolutionary biology. Some evolutionary psychologists apply the same thinking to psychology, arguing that the modularity of mind is similar to that of the body and with different modular adaptations serving different functions. Evolutionary psychologists argue that much of human behavior is the output of psychological adaptations that evolved to solve recurrent problems in human ancestral environments.
Evolutionary psychologists suggest that EP is not simply a subdiscipline of psychology but that evolutionary theory can provide a foundational, metatheoretical framework that integrates the entire field of psychology, in the same way it has for biology.
Evolutionary psychologists hold that behaviors or traits that occur universally in all cultures are good candidates for evolutionary adaptations including the abilities to infer others' emotions, discern kin from non-kin, identify and prefer healthier mates, and cooperate with others. They report successful tests of theoretical predictions related to such topics as infanticide, intelligence, marriage patterns, promiscuity, perception of beauty, bride price, and parental investment.
The theories and findings of EP have applications in many fields, including economics, environment, health, law, management, psychiatry, politics, and literature.
Criticism of evolutionary psychology involve questions of testability, cognitive and evolutionary assumptions (such as modular functioning of the brain, and large uncertainty about the ancestral environment), importance of non-genetic and non-adaptive explanations, as well as political and ethical issues due to interpretations of research results.
Scope.
Principles.
Evolutionary psychology is an approach that views human nature as the product of a universal set of evolved psychological adaptations to recurring problems in the ancestral environment. Proponents of EP suggest that it seeks to integrate psychology into the other natural sciences, rooting it in the organizing theory of biology (evolutionary theory), and thus understanding psychology as a branch of biology. Anthropologist John Tooby and psychologist Leda Cosmides note:
Evolutionary psychology is the long-forestalled scientific attempt to assemble out of the disjointed, fragmentary, and mutually contradictory human disciplines a single, logically integrated research framework for the psychological, social, and behavioral sciences—a framework
that not only incorporates the evolutionary sciences on a full and equal basis, but that systematically works out all of the revisions in existing belief and research practice that such a synthesis requires.
Just as human physiology and evolutionary physiology have worked to identify physical adaptations of the body that represent "human physiological nature," the purpose of evolutionary psychology is to identify evolved emotional and cognitive adaptations that represent "human psychological nature." According to Steven Pinker, EP is "not a single theory but a large set of hypotheses" and a term that "has also come to refer to a particular way of applying evolutionary theory to the mind, with an emphasis on adaptation, gene-level selection, and modularity." Evolutionary psychology adopts an understanding of the mind that is based on the computational theory of mind. It describes mental processes as computational operations, so that, for example, a fear response is described as arising from a neurological computation that inputs the perceptional data, e.g. a visual image of a spider, and outputs the appropriate reaction, e.g. fear of possibly dangerous animals.
While philosophers have generally considered the human mind to include broad faculties, such as reason and lust, evolutionary psychologists describe evolved psychological mechanisms as narrowly focused to deal with specific issues, such as catching cheaters or choosing mates. EP views the human brain as comprising many functional mechanisms, called "psychological adaptations" or evolved cognitive mechanisms or "cognitive modules", designed by the process of natural selection. Examples include language-acquisition modules, incest-avoidance mechanisms, cheater-detection mechanisms, intelligence and sex-specific mating preferences, foraging mechanisms, alliance-tracking mechanisms, agent-detection mechanisms, and others. Some mechanisms, termed "domain-specific", deal with recurrent adaptive problems over the course of human evolutionary history. "Domain-general" mechanisms, on the other hand, are proposed to deal with evolutionary novelty.
EP has roots in cognitive psychology and evolutionary biology but also draws on behavioral ecology, artificial intelligence, genetics, ethology, anthropology, archaeology, biology, and zoology. EP is closely linked to sociobiology, but there are key differences between them including the emphasis on "domain-specific" rather than "domain-general" mechanisms, the relevance of measures of current fitness, the importance of mismatch theory, and psychology rather than behavior. Most of what is now labeled as sociobiological research is now confined to the field of behavioral ecology.
Nikolaas Tinbergen's four categories of questions can help to clarify the distinctions between several different, but complementary, types of explanations. Evolutionary psychology focuses primarily on the "why?" questions, while traditional psychology focuses on the "how?" questions.
Premises.
Evolutionary psychology is founded on several core premises.
History.
Evolutionary psychology has its historical roots in Charles Darwin's theory of natural selection. In "The Origin of Species", Darwin predicted that psychology would develop an evolutionary basis:
Two of his later books were devoted to the study of animal emotions and psychology; "The Descent of Man, and Selection in Relation to Sex" in 1871 and "The Expression of the Emotions in Man and Animals" in 1872. Darwin's work inspired William James's functionalist approach to psychology. Darwin's theories of evolution, adaptation, and natural selection have provided insight into why brains function the way they do.
The content of EP has derived from, on one hand, the biological sciences (especially evolutionary theory as it relates to ancient human environments, the study of paleoanthropology and animal behavior) and, on the other, the human sciences, especially psychology.
Evolutionary biology as an academic discipline emerged with the modern evolutionary synthesis in the 1930s and 1940s. In the 1930s the study of animal behavior (ethology) emerged with the work of Dutch biologist Nikolaas Tinbergen and Austrian biologists Konrad Lorenz and Karl von Frisch.
W.D. Hamilton's (1964) papers on inclusive fitness and Robert Trivers's (1972) theories on reciprocity and parental investment helped to establish evolutionary thinking in psychology and the other social sciences. In 1975, Edward O. Wilson combined evolutionary theory with studies of animal and social behavior, building on the works of Lorenz and Tinbergen, in his book "".
In the 1970s, two major branches developed from ethology. Firstly, the study of animal "social" behavior (including humans) generated sociobiology, defined by its pre-eminent proponent Edward O. Wilson in 1975 as "the systematic study of the biological basis of all social behavior" and in 1978 as "the extension of population biology and evolutionary theory to social organization." Secondly, there was behavioral ecology which placed less emphasis on "social" behavior by focusing on the ecological and evolutionary basis of both animal and human behavior.
In the 1970s and 1980s university departments began to include the term "evolutionary biology" in their titles. The modern era of evolutionary psychology was ushered in, in particular, by Donald Symons' 1979 book "The Evolution of Human Sexuality" and Leda Cosmides and John Tooby's 1992 book "The Adapted Mind".
From psychology there are the primary streams of developmental, social and cognitive psychology. Establishing some measure of the relative influence of genetics and environment on behavior has been at the core of behavioral genetics and its variants, notably studies at the molecular level that examine the relationship between genes, neurotransmitters and behavior. Dual inheritance theory (DIT), developed in the late 1970s and early 1980s, has a slightly different perspective by trying to explain how human behavior is a product of two different and interacting evolutionary processes: genetic evolution and cultural evolution. DIT is seen by some as a "middle-ground" between views that emphasize human universals versus those that emphasize cultural variation.
Theoretical foundations.
The theories on which evolutionary psychology is based originated with Charles Darwin's work, including his speculations about the evolutionary origins of social instincts in humans. Modern evolutionary psychology, however, is possible only because of advances in evolutionary theory in the 20th century.
Evolutionary psychologists say that natural selection has provided humans with many psychological adaptations, in much the same way that it generated humans' anatomical and physiological adaptations. As with adaptations in general, psychological adaptations are said to be specialized for the environment in which an organism evolved, the environment of evolutionary adaptedness, or EEA. Sexual selection provides organisms with adaptations related to mating. For male mammals, which have a relatively high maximal potential reproduction rate, sexual selection leads to adaptations that help them compete for females. For female mammals, with a relatively low maximal potential reproduction rate, sexual selection leads to choosiness, which helps females select higher quality mates. Charles Darwin described both natural selection and sexual selection, and he relied on group selection to explain the evolution of altruistic (self-sacrificing) behavior. But group selection was considered a weak explanation, because in any group the less altruistic individuals will be more likely to survive, and the group will become less self-sacrificing as a whole.
In 1964, William D. Hamilton proposed inclusive fitness theory, emphasizing a gene-centered view of evolution view of evolution. Hamilton noted that genes can increase the replication of copies of themselves into the next generation by influencing the organism's social traits in such a way that (statistically) results in helping the survival and reproduction of other copies of the same genes (most simply, identical copies in the organism's close relatives). According to "Hamilton's rule", self-sacrificing behaviors (and the genes influencing them) can evolve if they typically help the organism's close relatives so much that it more than compensates for the individual animal's sacrifice. Inclusive fitness theory resolved the issue of how "altruism" can evolve. Other theories also help explain the evolution of altruistic behavior, including evolutionary game theory, tit-for-tat reciprocity, and generalized reciprocity. These theories not only help explain the development of altruistic behavior, but also account for hostility toward cheaters (individuals that take advantage of others' altruism).
Several mid-level evolutionary theories inform evolutionary psychology. The r/K selection theory proposes that some species prosper by having many offspring, while others follow the strategy of having fewer offspring but investing much more in each one. Humans follow the second strategy. Parental investment theory explains how parents invest more or less in individual offspring based on how successful those offspring are likely to be, and thus how much they might improve the parents' inclusive fitness. According to the Trivers-Willard hypothesis, parents in good conditions tend to invest more in sons (who are best able to take advantage of good conditions), while parents in poor conditions tend to invest more in daughters (who are best able to have successful offspring even in poor conditions). According to life history theory, animals evolve life histories to match their environments, determining details such as age at first reproduction and number of offspring. Dual inheritance theory posits that genes and human culture have interacted, with genes affecting the development of culture, and culture, in turn, affecting human evolution on a genetic level (see also the Baldwin effect).
Evolved psychological mechanisms.
Evolutionary psychology is based on the hypothesis that, just like hearts, lungs, livers, kidneys, and immune systems, cognition has functional structure that has a genetic basis, and therefore has evolved by natural selection. Like other organs and tissues, this functional structure should be universally shared amongst a species, and should solve important problems of survival and reproduction.
Evolutionary psychologists seek to understand psychological mechanisms by understanding the survival and reproductive functions they might have served over the course of evolutionary history. These might include abilities to infer others' emotions, discern kin from non-kin, identify and prefer healthier mates, cooperate with others and follow leaders. Consistent with the theory of natural selection, evolutionary psychology sees humans as often in conflict with others, including mates and relatives. For instance, a mother may wish to wean her offspring from breastfeeding earlier than does her infant, which frees up the mother to invest in additional offspring. Evolutionary psychology also recognizes the role of kin selection and reciprocity in evolving prosocial traits such as altruism. Like chimps and bonobos, humans have subtle and flexible social instincts, allowing them to form extended families, lifelong friendships, and political alliances. In studies testing theoretical predictions, evolutionary psychologists have made modest findings on topics such as infanticide, intelligence, marriage patterns, promiscuity, perception of beauty, bride price and parental investment.
Products of evolution: adaptations, exaptations, byproducts, and random variation.
Not all traits of organisms are adaptations. As noted in the table below, traits may also be exaptations, byproducts of adaptations (sometimes called "spandrels"), or random variation between individuals.
Psychological adaptations are hypothesized to be innate or relatively easy to learn, and to manifest in cultures worldwide. For example, the ability of toddlers to learn a language with virtually no training is likely to be a psychological adaptation. On the other hand, ancestral humans did not read or write, thus today, learning to read and write require extensive training, and presumably represent byproducts of cognitive processing that use psychological adaptations designed for other functions. However, variations in manifest behavior can result from universal mechanisms interacting with different local environments. For example, Caucasians who move from a northern climate to the equator will have darker skin. The mechanisms regulating their pigmentation do not change; rather the input to the those mechanisms change, resulting in different output.
One of the tasks of evolutionary psychology is to identify which psychological traits are likely to be adaptations, byproducts or random variation. George C Williams suggested that an "adaptation is a special and onerous concept that should only be used where it is really necessary." As noted by Williams and others, adaptations can be identified by their improbable complexity, species universality, and adaptive functionality.
Obligate and facultative adaptations.
A question that may be asked about an adaptation is whether it is generally obligate (relatively robust in the face of typical environmental variation) or facultative (sensitive to typical environmental variation). The sweet taste of sugar and the pain of hitting one's knee against concrete are the result of fairly obligate psychological adaptations; typical environmental variability during development does not much affect their operation. By contrast, facultative adaptations are somewhat like "if-then" statements. For example, adult attachment style seems particularly sensitive to early childhood experiences. As adults, the propensity to develop close, trusting bonds with others is dependent on whether early childhood caregivers could be trusted to provide reliable assistance and attention. The adaptation for skin to tan is conditional to exposure to sunlight; this is an example of another facultative adaptation. When a psychological adaptation is facultative, evolutionary psychologists concern themselves with how developmental and environmental inputs influence the expression of the adaptation.
Cultural universals.
Evolutionary psychologists hold that behaviors or traits that occur universally in all cultures are good candidates for evolutionary adaptations. Cultural universals include behaviors related to language, cognition, social roles, gender roles, and technology. Evolved psychological adaptations (such as the ability to learn a language) interact with cultural inputs to produce specific behaviors (e.g., the specific language learned). Basic gender differences, such as greater eagerness for sex among men and greater coyness among women, are explained as sexually dimorphic psychological adaptations that reflect the different reproductive strategies of males and females. Evolutionary psychologists contrast their approach to what they term the "standard social science model," according to which the mind is a general-purpose cognition device shaped almost entirely by culture.
Environment of evolutionary adaptedness.
EP argues that to properly understand the functions of the brain, one must understand the properties of the environment in which the brain evolved. That environment is often referred to as the "environment of evolutionary adaptedness" (EEA).
The idea of an "environment of evolutionary adaptedness" was first explored as a part of attachment theory by John Bowlby. This is the environment to which a particular evolved mechanism is adapted. More specifically, the EEA is defined as the set of historically recurring selection pressures that formed a given adaptation, as well as those aspects of the environment that were necessary for the proper development and functioning of the adaptation.
Humans, comprising the genus "Homo", appeared between 1.5 and 2.5 million years ago, a time that roughly coincides with the start of the Pleistocene 2.6 million years ago. Because the Pleistocene ended a mere 12,000 years ago, most human adaptations either newly evolved during the Pleistocene, or were maintained by stabilizing selection during the Pleistocene. Evolutionary psychology therefore proposes that the majority of human psychological mechanisms are adapted to reproductive problems frequently encountered in Pleistocene environments. In broad terms, these problems include those of growth, development, differentiation, maintenance, mating, parenting, and social relationships.
The EEA is significantly different from modern society. The ancestors of modern humans lived in smaller groups, had more cohesive cultures, and had more stable and rich contexts for identity and meaning. Researchers look to existing hunter-gatherer societies for clues as to how hunter-gatherers lived in the EEA. Unfortunately, the few surviving hunter-gatherer societies are different from each other, and they have been pushed out of the best land and into harsh environments, so it is not clear how closely they reflect ancestral culture.
Evolutionary psychologists sometimes look to chimpanzees, bonobos, and other great apes for insight into human ancestral behavior. Christopher Ryan and Cacilda Jetha argue that evolutionary psychologists have overemphasized the similarity of humans and chimps, which are more violent, while underestimating the similarity of humans and bonobos, which are more peaceful.
Mismatches.
Since an organism's adaptations were suited to its ancestral environment, a new and different environment can create a mismatch. Because humans are mostly adapted to Pleistocene environments, psychological mechanisms sometimes exhibit "mismatches" to the modern environment. One example is the fact that although about 10,000 people are killed with guns in the US annually, whereas spiders and snakes kill only a handful, people nonetheless learn to fear spiders and snakes about as easily as they do a pointed gun, and more easily than an unpointed gun, rabbits or flowers. A potential explanation is that spiders and snakes were a threat to human ancestors throughout the Pleistocene, whereas guns (and rabbits and flowers) were not. There is thus a mismatch between humans' evolved fear-learning psychology and the modern environment.
This mismatch also shows up in the phenomena of the supernormal stimulus, a stimulus that elicits a response more strongly than the stimulus for which the response evolved. The term was coined by Niko Tinbergen to refer to non-human animal behavior, but psychologist Deirdre Barrett said that supernormal stimulation governs the behavior of humans as powerfully as that of other animals. She explained junk food as an exaggerated stimulus to cravings for salt, sugar, and fats, and she says that television is an exaggeration of social cues of laughter, smiling faces and attention-grabbing action. Magazine centerfolds and double cheeseburgers pull instincts intended for an EEA where breast development was a sign of health, youth and fertility in a prospective mate, and fat was a rare and vital nutrient.
Psychologist Mark van Vugt recently argued that modern organizational leadership is a mismatch. His argument is that humans are not adapted to work in large, anonymous bureaucratic structures with formal hierarchies. The human mind still responds to personalized, charismatic leadership primarily in the context of informal, egalitarian settings. Hence the dissatisfaction and alienation that many employees experience. Salaries, bonuses and other privileges exploit instincts for relative status, which attract particularly males to senior executive positions.
Research methods.
Evolutionary theory is heuristic in that it may generate hypotheses that might not be developed from other theoretical approaches. One of the major goals of adaptationist research is to identify which organismic traits are likely to be adaptations, and which are byproducts or random variations. As noted earlier, adaptations are expected to show evidence of complexity, functionality, and species universality, while byproducts or random variation will not. In addition, adaptations are expected to manifest as proximate mechanisms that interact with the environment in either a generally obligate or facultative fashion (see above). Evolutionary psychologists are also interested in identifying these proximate mechanisms (sometimes termed "mental mechanisms" or "psychological adaptations") and what type of information they take as input, how they process that information, and their outputs. Evolutionary developmental psychology, or "evo-devo," focuses on how adaptations may be activated at certain developmental times (e.g., losing baby teeth, adolescence, etc.) or how events during the development of an individual may alter life history trajectories.
Evolutionary psychologists use several strategies to develop and test hypotheses about whether a psychological trait is likely to be an evolved adaptation. Buss (2011) notes that these methods include:
Evolutionary psychologists also use various sources of data for testing, including experiments, archaeological records, data from hunter-gatherer societies, observational studies, neuroscience data, self-reports and surveys, public records, and human products.
Recently, additional methods and tools have been introduced based on fictional scenarios, mathematical models, and multi-agent computer simulations.
Major areas of research.
Foundational areas of research in evolutionary psychology can be divided into broad categories of adaptive problems that arise from the theory of evolution itself: survival, mating, parenting, family and kinship, interactions with non-kin, and cultural evolution.
Survival and individual level psychological adaptations.
Problems of survival are thus clear targets for the evolution of physical and psychological adaptations. Major problems the ancestors of present-day humans faced included food selection and acquisition; territory selection and physical shelter; and avoiding predators and other environmental threats.
Consciousness.
Consciousness meets George Williams' criteria of species universality, complexity, and functionality, and it is a trait that apparently increases fitness.
In his paper "Evolution of consciousness," John Eccles argues that special anatomical and physical adaptations of the mammalian cerebral cortex gave rise to consciousness. In contrast, others have argued that the recursive circuitry underwriting consciousness is much more primitive, having evolved initially in pre-mammalian species because it improves the capacity for interaction with both social "and" natural environments by providing an energy-saving "neutral" gear in an otherwise energy-expensive motor output machine. Once in place, this recursive circuitry may well have provided a basis for the subsequent development of many of the functions that consciousness facilitates in higher organisms, as outlined by Bernard J. Baars. Richard Dawkins suggested that humans evolved consciousness in order to make themselves the subjects of thought. Daniel Povinelli suggests that large, tree-climbing apes evolved consciousness to take into account one's own mass when moving safely among tree branches. Consistent with this hypothesis, Gordon Gallup found that chimps and orangutans, but not little monkeys or terrestrial gorillas, demonstrated self-awareness in mirror tests.
The concept of consciousness can refer to voluntary action, awareness, or wakefulness. However, even voluntary behavior involves unconscious mechanisms. Many cognitive processes take place in the cognitive unconscious, unavailable to conscious awareness. Some behaviors are conscious when learned but then become unconscious, seemingly automatic. Learning, especially implicitly learning a skill, can take place outside of consciousness. For example, plenty of people know how to turn right when they ride a bike, but very few can accurately explain how they actually do so. Evolutionary psychology approaches self-deception as an adaptation that can improve one's results in social exchanges.
Sleep may have evolved to conserve energy when activity would be less fruitful or more dangerous, such as at night, especially in winter.
Sensation and perception.
Many experts, such as Jerry Fodor, write that the purpose of perception is knowledge, but evolutionary psychologists hold that its primary purpose is to guide action. For example, they say, depth perception seems to have evolved not to help us know the distances to other objects but rather to help us move around in space. Evolutionary psychologists say that animals from fiddler crabs to humans use eyesight for collision avoidance, suggesting that vision is basically for directing action, not providing knowledge.
Building and maintaining sense organs is metabolically expensive, so these organs evolve only when they improve an organism's fitness. More than half the brain is devoted to processing sensory information, and the brain itself consumes roughly one-fourth of one's metabolic resources, so the senses must provide exceptional benefits to fitness. Perception accurately mirrors the world; animals get useful, accurate information through their senses.
Scientists who study perception and sensation have long understood the human senses as adaptations. Depth perception consists of processing over half a dozen visual cues, each of which is based on a regularity of the physical world. Vision evolved to respond to the narrow range of electromagnetic energy that is plentiful and that does not pass through objects. Sound waves go around corners and interact with obstacles, creating a complex pattern that includes useful information about the sources of and distances to objects. Larger animals naturally make lower-pitched sounds as a consequence of their size. The range over which an animal hears, on the other hand, is determined by adaptation. Homing pigeons, for example, can hear very low-pitched sound (infrasound) that carries great distances, even though most smaller animals detect higher-pitched sounds. Taste and smell respond to chemicals in the environment that are thought to have been significant for fitness in the EEA. For example, salt and sugar were apparently both valuable to the human or pre-human inhabitants of the EEA, so present day humans have an intrinsic hunger for salty and sweet tastes. The sense of touch is actually many senses, including pressure, heat, cold, tickle, and pain. Pain, while unpleasant, is adaptive. An important adaptation for senses is range shifting, by which the organism becomes temporarily more or less sensitive to sensation. For example, one's eyes automatically adjust to dim or bright ambient light. Sensory abilities of different organisms often coevolve, as is the case with the hearing of echolocating bats and that of the moths that have evolved to respond to the sounds that the bats make.
Evolutionary psychologists contend that perception demonstrates the principle of modularity, with specialized mechanisms handling particular perception tasks. For example, people with damage to a particular part of the brain suffer from the specific defect of not being able to recognize faces (prosopagnosia). EP suggests that this indicates a so-called face-reading module.
Learning and facultative adaptations.
In evolutionary psychology, learning is said to be accomplished through evolved capacities, specifically facultative adaptations. Facultative adaptations express themselves differently depending on input from the environment. Sometimes the input comes during development and helps shape that development. For example, migrating birds learn to orient themselves by the stars during a critical period in their maturation. Evolutionary psychologists believe that humans also learn language along an evolved program, also with critical periods. The input can also come during daily tasks, helping the organism cope with changing environmental conditions. For example, animals evolved Pavlovian conditioning in order to solve problems about causal relationships. Animals accomplish learning tasks most easily when those tasks resemble problems that they faced in their evolutionary past, such as a rat learning where to find food or water. Learning capacities sometimes demonstrate differences between the sexes. In many animal species, for example, males can solve spatial problem faster and more accurately than females, due to the effects of male hormones during development. The same might be true of humans.
Emotion and motivation.
Motivations direct and energize behavior, while emotions provide the affective component to motivation, positive or negative. In the early 1970s, Paul Ekman and colleagues began a line of research which suggests that many emotions are universal. He found evidence that humans share at least five basic emotions: fear, sadness, happiness, anger, and disgust. Social emotions evidently evolved to motivate social behaviors that were adaptive in the EEA. For example, spite seems to work against the individual but it can establish an individual's reputation as someone to be feared. Shame and pride can motivate behaviors that help one maintain one's standing in a community, and self-esteem is one's estimate of one's status.
Motivation has a neurobiologial basis in the reward system of the brain. Recently, it has been suggested that reward systems may evolve in such a way that there may be an inherent or unavoidable trade-off in the motivational system for activities of short versus long duration.
Cognition.
Cognition refers to internal representations of the world and internal information processing. From an EP perspective, cognition is not "general purpose," but uses heuristics, or strategies, that generally increase the likelihood of solving problems that the ancestors of present-day humans routinely faced. For example, present day humans are far more likely to solve logic problems that involve detecting cheating (a common problem given humans' social nature) than the same logic problem put in purely abstract terms. Since the ancestors of present-day humans did not encounter truly random events, present day humans may be cognitively predisposed to incorrectly identify patterns in random sequences. "Gamblers' Fallacy" is one example of this. Gamblers may falsely believe that they have hit a "lucky streak" even when each outcome is actually random and independent of previous trials. Most people believe that if a fair coin has been flipped 9 times and Heads appears each time, that on the tenth flip, there is a greater than 50% chance of getting Tails. Humans find it far easier to make diagnoses or predictions using frequency data than when the same information is presented as probabilities or percentages, presumably because the ancestors of present-day humans lived in relatively small tribes (usually with fewer than 150 people) where frequency information was more readily available.
Personality.
Evolutionary psychology is primarily interested in finding commonalities between people, or basic human psychological nature. From an evolutionary perspective, the fact that people have fundamental differences in personality traits initially presents something of a puzzle. (Note: The field of behavioral genetics is concerned with statistically partitioning differences between people into genetic and environmental sources of variance. However, understanding the concept of heritability can be tricky—heritability refers only to the differences between people, never the degree to which the traits of an individual are due to environmental or genetic factors, since traits are always a complex interweaving of both.)
Personality traits are conceptualized by evolutionary psychologists as due to normal variation around an optimum, due to frequency-dependent selection (behavioral polymorphisms), or as facultative adaptations. Like variability in height, some personality traits may simply reflect inter-individual variability around a general optimum. Or, personality traits may represent different genetically predisposed "behavioral morphs" – alternate behavioral strategies that depend on the frequency of competing behavioral strategies in the population. For example, if most of the population is generally trusting and gullible, the behavioral morph of being a "cheater" (or, in the extreme case, a sociopath) may be advantageous. Finally, like many other psychological adaptations, personality traits may be facultative—sensitive to typical variations in the social environment, especially during early development. For example, later born children are more likely than first borns to be rebellious, less conscientious and more open to new experiences, which may be advantageous to them given their particular niche in family structure. It is important to note that shared environmental influences do play a role in personality and are not always of less importance than genetic factors. However, shared environmental influences often decrease to near zero after adolescence but do not completely disappear.
Language.
According to Steven Pinker, who builds on the work by Noam Chomsky, the universal human ability to learn to talk between the ages of 1 – 4, basically without training, suggests that language acquisition is a distinctly human psychological adaptation (see, in particular, Pinker's "The Language Instinct"). Pinker and Bloom (1990) argue that language as a mental faculty shares many likenesses with the complex organs of the body which suggests that, like these organs, language has evolved as an adaptation, since this is the only known mechanism by which such complex organs can develop.
Pinker follows Chomsky in arguing that the fact that children can learn any human language with no explicit instruction suggests that language, including most of grammar, is basically innate and that it only needs to be activated by interaction. Chomsky himself does not believe language to have evolved as an adaptation, but suggests that it likely evolved as a byproduct of some other adaptation, a so-called spandrel. But Pinker and Bloom argue that the organic nature of language strongly suggests that it has an adaptational origin.
Evolutionary psychologists hold that the FOXP2 gene may well be associated with the evolution of human language. In the 1980s, psycholinguist Myrna Gropnik identified a dominant gene that causes language impairment in the KE family of Britain. This gene turned out to be a mutation of the FOXP2 gene. Humans have a unique allele of this gene, which has otherwise been closely conserved through most of mammalian evolutionary history. This unique allele seems to have first appeared between 100 and 200 thousand years ago, and it is now all but universal in humans. However, the once-popular idea that FOXP2 is a 'grammar gene' or that it triggered the emergence of language in "Homo sapiens" is now widely discredited.
Currently several competing theories about the evolutionary origin of language coexist, none of them having achieved a general consensus. Researchers of language acquisition in primates and humans such as Michael Tomasello and Talmy Givón, argue that the innatist framework has understated the role of imitation in learning and that it is not at all necessary to posit the existence of an innate grammar module to explain human language acquisition. Tomasello argues that studies of how children and primates actually acquire communicative skills suggests that humans learn complex behavior through experience, so that instead of a module specifically dedicated to language acquisition, language is acquired by the same cognitive mechanisms that are used to acquire all other kinds of socially transmitted behavior.
On the issue of whether language is best seen as having evolved as an adaptation or as a spandrel, evolutionary biologist W. Tecumseh Fitch, following Stephen J. Gould, argues that it is unwarranted to assume that every aspect of language is an adaptation, or that language as a whole is an adaptation. He criticizes some strands of evolutionary psychology for suggesting a pan-adaptionist view of evolution, and dismisses Pinker and Bloom's question of whether "Language has evolved as an adaptation" as being misleading. He argues instead that from a biological viewpoint the evolutionary origins of language is best conceptualized as being the probable result of a convergence of many separate adaptations into a complex system. A similar argument is made by Terrence Deacon who in "The Symbolic Species" argues that the different features of language have co-evolved with the evolution of the mind and that the ability to use symbolic communication is integrated in all other cognitive processes.
If the theory that language could have evolved as a single adaptation is accepted, the question becomes which of its many functions has been the basis of adaptation, several evolutionary hypotheses have been posited: that it evolved for the purpose of social grooming, that it evolved to as a way to show mating potential or that it evolved to form social contracts. Evolutionary psychologists recognize that these theories are all speculative and that much more evidence is required to understand how language might have been selectively adapted.
Mating.
Given that sexual reproduction is the means by which genes are propagated into future generations, sexual selection plays a large role in human evolution. Human mating, then, is of interest to evolutionary psychologists who aim to investigate evolved mechanisms to attract and secure mates. Several lines of research have stemmed from this interest, such as studies of mate selection mate poaching, mate retention, mating preferences and conflict between the sexes.
In 1972 Robert Trivers published an influential paper on sex differences that is now referred to as parental investment theory. The size differences of gametes (anisogamy) is the fundamental, defining difference between males (small gametes—sperm) and females (large gametes—ova). Trivers noted that anisogamy typically results in different levels of parental investment between the sexes, with females initially investing more. Trivers proposed that this difference in parental investment leads to the sexual selection of different reproductive strategies between the sexes and to sexual conflict. For example, he suggested that the sex that invests less in offspring will generally compete for access to the higher-investing sex to increase their inclusive fitness (also see Bateman's principle<ref name=doi10.1038/hdy.1948.21></ref>). Trivers posited that differential parental investment led to the evolution sexual dimorphisms in mate choice, intra- and inter- sexual reproductive competition, and courtship displays. In mammals, including humans, females make a much larger parental investment than males (i.e. gestation followed by childbirth and lactation). Parental investment theory is a branch of life history theory.
Buss and Schmitt's (1993) "Sexual Strategies Theory" proposed that, due to differential parental investment, humans have evolved sexually dimorphic adaptations related to "sexual accessibility, fertility assessment, commitment seeking and avoidance, immediate and enduring resource procurement, paternity certainty, assessment of mate value, and parental investment." Their "Strategic Interference Theory" suggested that conflict between the sexes occurs when the preferred reproductive strategies of one sex interfere with those of the other sex, resulting in the activation of emotional responses such as anger or jealousy.
Women are generally more selective when choosing mates, especially under short-term mating conditions. However, under some circumstances, short term mating can provide benefits to women as well, such as fertility insurance, trading up to better genes, reducing risk of inbreeding, and insurance protection of her offspring.
Due to male paternity insecurity, sex differences have been found in such domains as sexual jealousy. Females generally react more adversely to emotional infidelity and males will react more to sexual infidelity. This particular pattern is predicted because the costs involved in mating for each sex are distinct. Women, on average, should prefer a mate who can offer resources (e.g., financial, commitment), thus, a woman risks losing such resources with a mate who commits emotional infidelity. Men, on the other hand, are never certain of the genetic paternity of their children because they do not bear the offspring themselves ("paternity insecurity"). This suggests that for men sexual infidelity would generally be more aversive than emotional infidelity because investing resources in another man's offspring does not lead to propagation of their own genes.
Another interesting line of research is that which examines women's mate preferences across the ovulatory cycle. The theoretical underpinning of this research is that ancestral women would have evolved mechanisms to select mates with certain traits depending on their hormonal status. For example, the theory hypothesizes that, during the ovulatory phase of a woman's cycle (approximately days 10–15 of a woman's cycle), a woman who mated with a male with high genetic quality would have been more likely, on average, to produce and rear a healthy offspring than a woman who mated with a male with low genetic quality. These putative preferences are predicted to be especially apparent for short-term mating domains because a potential male mate would only be offering genes to a potential offspring. This hypothesis allows researchers to examine whether women select mates who have characteristics that indicate high genetic quality during the high fertility phase of their ovulatory cycles. Indeed, studies have shown that women's preferences vary across the ovulatory cycle. In particular, Haselton and Miller (2006) showed that highly fertile women prefer creative but poor men as short-term mates. Creativity may be a proxy for good genes. Research by Gangestad et al. (2004) indicates that highly fertile women prefer men who display social presence and intrasexual competition; these traits may act as cues that would help women predict which men may have, or would be able to acquire, resources.
Parenting.
Reproduction is always costly for women, and can also be for men. Individuals are limited in the degree to which they can devote time and resources to producing and raising their young, and such expenditure may also be detrimental to their future condition, survival and further reproductive output.
Parental investment is any parental expenditure (time, energy etc.) that benefits one offspring at a cost to parents' ability to invest in other components of fitness (Clutton-Brock 1991: 9; Trivers 1972). Components of fitness (Beatty 1992) include the well being of existing offspring, parents' future reproduction, and inclusive fitness through aid to kin (Hamilton, 1964). Parental investment theory is a branch of life history theory.
Robert Trivers' theory of parental investment predicts that the sex making the largest investment in lactation, nurturing and protecting offspring will be more discriminating in mating and that the sex that invests less in offspring will compete for access to the higher investing sex (see Bateman's principle). Sex differences in parental effort are important in determining the strength of sexual selection.
The benefits of parental investment to the offspring are large and are associated with the effects on condition, growth, survival and ultimately, on reproductive success of the offspring. However, these benefits can come at the cost of parent's ability to reproduce in the future e.g. through the increased risk of injury when defending offspring against predators, the loss of mating opportunities whilst rearing offspring and an increase in the time to the next reproduction. Overall, parents are selected to maximize the difference between the benefits and the costs, and parental care will be likely to evolve when the benefits exceed the costs.
The Cinderella effect is an alleged high incidence of stepchildren being physically, emotionally or sexually abused, neglected, murdered, or otherwise mistreated at the hands of their stepparents at significantly higher rates than their genetic counterparts. It takes its name from the fairy tale character Cinderella, who in the story was cruelly mistreated by her stepmother and stepsisters. Daly and Wilson (1996) noted: "Evolutionary thinking led to the discovery of the most important risk factor for child homicide – the presence of a stepparent. Parental efforts and investments are valuable resources, and selection favors those parental psyches that allocate effort effectively to promote fitness. The adaptive problems that challenge parental decision making include both the accurate identification of one's offspring and the allocation of one's resources among them with sensitivity to their needs and abilities to convert parental investment into fitness increments…. Stepchildren were seldom or never so valuable to one's expected fitness as one's own offspring would be, and those parental psyches that were easily parasitized by just any appealing youngster must always have incurred a selective disadvantage"(Daly & Wilson, 1996, pp. 64–65). However, they note that not all stepparents will "want" to abuse their partner's children, or that genetic parenthood is any insurance against abuse. They see step parental care as primarily "mating effort" towards the genetic parent.
Family and kin.
Inclusive fitness is the sum of an organism's classical fitness (how many of its own offspring it produces and supports) and the number of equivalents of its own offspring it can add to the population by supporting others. The first component is called classical fitness by Hamilton (1964).
From the gene's point of view, evolutionary success ultimately depends on leaving behind the maximum number of copies of itself in the population. Until 1964, it was generally believed that genes only achieved this by causing the individual to leave the maximum number of viable offspring. However, in 1964 W. D. Hamilton proved mathematically that, because close relatives of an organism share some identical genes, a gene can also increase its evolutionary success by promoting the reproduction and survival of these related or otherwise similar individuals. Hamilton concluded that this leads natural selection to favor organisms that would behave in ways that maximize their inclusive fitness. It is also true that natural selection favors behavior that maximizes personal fitness.
Hamilton's rule describes mathematically whether or not a gene for altruistic behavior will spread in a population:
where
The concept serves to explain how natural selection can perpetuate altruism. If there is an '"altruism gene"' (or complex of genes) that influences an organism's behavior to be helpful and protective of relatives and their offspring, this behavior also increases the proportion of the altruism gene in the population, because relatives are likely to share genes with the altruist due to common descent. Altruists may also have some way to recognize altruistic behavior in unrelated individuals and be inclined to support them. As Dawkins points out in "The Selfish Gene" (Chapter 6) and "The Extended Phenotype", this must be distinguished from the green-beard effect.
Although it is generally true that humans tend to be more altruistic toward their kin than toward non-kin, the relevant proximate mechanisms that mediate this cooperation have been debated (see kin recognition), with some arguing that kin status is determined primarily via social and cultural factors (such as co-residence, maternal association of sibs, etc.), while others have argued that kin recognition can also mediated by biological factors such as facial resemblance and immunogenetic similarity of the major histocompatibility complex (MHC). For a discussion of the interaction of these social and biological kin recognition factors see Lieberman, Tooby, and Cosmides (2007) (PDF).
Whatever the proximate mechanisms of kin recognition there is substantial evidence that humans act generally more altruistically to close genetic kin compared to genetic non-kin.
Interactions with non-kin / reciprocity.
Although interactions with non-kin are generally less altruistic compared to those with kin, cooperation can be maintained with non-kin via mutually beneficial reciprocity as was proposed by Robert Trivers. If there are repeated encounters between the same two players in an evolutionary game in which each of them can choose either to "cooperate" or "defect," then a strategy of mutual cooperation may be favored even if it pays each player, in the short term, to defect when the other cooperates. Direct reciprocity can lead to the evolution of cooperation only if the probability, w, of another encounter between the same two individuals exceeds the cost-to-benefit ratio of the altruistic act:
Reciprocity can also be indirect if information about previous interactions is shared. Reputation allows evolution of cooperation by indirect reciprocity. Natural selection favors strategies that base the decision to help on the reputation of the recipient: studies show that people who are more helpful are more likely to receive help. The calculations of indirect reciprocity are complicated and only a tiny fraction of this universe has been uncovered, but again a simple rule has emerged. Indirect reciprocity can only promote cooperation if the probability, q, of knowing someone’s reputation exceeds the cost-to-benefit ratio of the altruistic act:
One important problem with this explanation is that individuals may be able to evolve the capacity to obscure their reputation, reducing the probability, q, that it will be known.
Trivers argues that friendship and various social emotions evolved in order to manage reciprocity. Liking and disliking, he says, evolved to help present day humans' ancestors form coalitions with others who reciprocated and to exclude those who did not reciprocate. Moral indignation may have evolved to prevent one's altruism from being exploited by cheaters, and gratitude may have motivated present day humans' ancestors to reciprocate appropriately after benefiting from others' altruism. Likewise, present day humans feel guilty when they fail to reciprocate. These social motivations match what evolutionary psychologists expect to see in adaptations that evolved to maximize the benefits and minimize the drawbacks of reciprocity.
Evolutionary psychologists say that humans have psychological adaptations that evolved specifically to help us identify nonreciprocators, commonly referred to as "cheaters." In 1993, Robert Frank and his associates found that participants in a prisoner's dilemma scenario were often able to predict whether their partners would "cheat," based on a half hour of unstructured social interaction. In a 1996 experiment, for example, Linda Mealey and her colleagues found that people were better at remembering the faces of people when those faces were associated with stories about those individuals cheating (such as embezzling money from a church).
Strong reciprocity (or "tribal reciprocity").
Humans may have an evolved set of psychological adaptations that predispose them to be more cooperative than otherwise would be expected with members of their tribal in-group, and, more nasty to members of tribal out groups. These adaptations may have be a consequent of tribal warfare. Humans may also have predispositions for "altruistic punishment"—to punish in-group members who violate in-group rules, even when this altruistic behavior cannot be justified in terms of helping those you are related to (kin selection), cooperating with those who you will interact with again (direct reciprocity), or cooperating to better your reputation with others (indirect reciprocity).
Evolution and culture.
Memetics is a theory of mental content based on an analogy with evolution, originating from Richard Dawkins' 1976 book "The Selfish Gene." It purports to be an approach to evolutionary models of cultural information transfer. A meme, analogous to a gene, is essentially a "unit of culture"—an idea, belief, pattern of behavior, etc. which is "hosted" in one or more individual minds, and which can reproduce itself from mind to mind. Thus what would otherwise be regarded as one individual influencing another to adopt a belief is seen memetically as a meme reproducing itself. As with genetics, particularly under Dawkins's interpretation, a meme's success may be due to its contribution to the effectiveness of its host. Memetics is notable for sidestepping the traditional concern with the "truth" of ideas and beliefs.
Susan Blackmore (2002) re-stated the definition of meme as: whatever is copied from one person to another person, whether habits, skills, songs, stories, or any other kind of information. Further she said that memes, like genes, are replicators in the sense as defined by Dawkins. That is, they are information that is copied. Memes are copied by imitation, teaching and other methods. The copies are not perfect: memes are copied with variation; moreover, memes compete for humans' limited memory capacity and for the chance to be copied again. Only some of the variants can survive. The combination of these three elements (copies; variation; competition for survival) forms precisely the condition for Darwinian evolution, and so memes (and hence human cultures) evolve. Large groups of memes that are copied and passed on together are called co-adapted meme complexes, or "memeplexes". In her definition, the way that a meme replicates is through imitation.
Dual inheritance theory (DIT), also known as gene-culture coevolution, suggests that cultural information and genes co-evolve. Marcus Feldman and Luigi Luca Cavalli-Sforza (1976) published perhaps the first dynamic models of gene-culture coevolution. These models were to form the basis for subsequent work on DIT, heralded by the publication of three seminal books in 1980 and 1981. Charles Lumsden and E.O. Wilson's "Genes, Mind and Culture" (1981). also outlined a series of mathematical models of how genetic evolution might favor the selection of cultural traits and how cultural traits might, in turn, affect the speed of genetic evolution. Another 1981 book relevant to this topic was Cavalli-Sforza and Feldman's "Cultural Transmission and Evolution: A Quantitative Approach". Borrowing heavily from population genetics and epidemiology, this book built a mathematical theory concerning the spread of cultural traits. It describes the evolutionary implications of vertical transmission, passing cultural traits from parents to offspring; oblique transmission, passing cultural traits from any member of an older generation to a younger generation; and horizontal transmission, passing traits between members of the same population.
Robert Boyd and Peter Richerson's (1985) "Culture and the Evolutionary Process" presents models of the evolution of social learning under different environmental conditions, the population effects of social learning, various forces of selection on cultural learning rules, different forms of biased transmission and their population-level effects, and conflicts between cultural and genetic evolution.
Along with game theory, Herbert Gintis suggested that Dual inheritance theory has potential for unifying the behavioral sciences, including economics, biology, anthropology, sociology, psychology and political science because it addresses both the genetic and cultural components of human inheritance. Laland and Brown hold a similar view.
In psychology sub-fields.
Developmental psychology.
According to Paul Baltes, the benefits granted by evolutionary selection decrease with age. Natural selection has not eliminated many harmful conditions and nonadaptive characteristics that appear among older adults, such as Alzheimer disease. If it were a disease that killed 20-year-olds instead of 70-year-olds this may have been a disease that natural selection could have eliminated ages ago. Thus, unaided by evolutionary pressures against nonadaptive conditions, modern humans suffer the aches, pains, and infirmities of aging and as the benefits of evolutionary selection decrease with age, the need for culture increases.
Social psychology.
As humans are a highly social species, there are many adaptive problems associated with navigating the social world (e.g., maintaining allies, managing status hierarchies, interacting with outgroup members, coordinating social activities, collective decision-making). Researchers in the emerging field of evolutionary social psychology have made many discoveries pertaining to topics traditionally studied by social psychologists, including person perception, social cognition, attitudes, altruism, emotions, group dynamics, leadership, motivation, prejudice, intergroup relations, and cross-cultural differences.
When endeavouring to solve a problem humans at an early age show determination while chimpanzees have no comparable facial expression. Researchers suspect the human determined expression evolved because when a human is determinedly working on a problem other people will frequently help.
Abnormal psychology.
Adaptationist hypotheses regarding the etiology of psychological disorders are often based on analogies between physiological and psychological dysfunctions, as noted in the table below. Prominent theorists and evolutionary psychiatrists include Michael T. McGuire and Randolph M. Nesse. They, and others, suggest that mental disorders are due to the interactive effects of both nature and nurture, and often have multiple contributing causes.
Evolutionary psychologists have suggested that schizophrenia and bipolar disorder may reflect a side-effect of genes with fitness benefits, such as increased creativity. (Some individuals with bipolar disorder are especially creative during their manic phases and the close relatives of schizophrenics have been found to be more likely to have creative professions.) A 1994 report by the American Psychiatry Association found that people suffered from schizophrenia at roughly the same rate in Western and non-Western cultures, and in industrialized and pastoral societies, suggesting that schizophrenia is not a disease of civilization nor an arbitrary social invention. Sociopathy may represent an evolutionarily stable strategy, by which a small number of people who cheat on social contracts benefit in a society consisting mostly of non-sociopaths. Mild depression may be an adaptive response to withdraw from, and re-evaluate, situations that have led to disadvantageous outcomes (the "analytical rumination hypothesis") (see Evolutionary approaches to depression).
Some of these speculations have yet to be developed into fully testable hypotheses, and a great deal of research is required to confirm their validity.
Psychology of religion.
Adaptationist perspectives on religious belief suggest that, like all behavior, religious behaviors are a product of the human brain. As with all other organ functions, cognition's functional structure has been argued to have a genetic foundation, and is therefore subject to the effects of natural selection and sexual selection. Like other organs and tissues, this functional structure should be universally shared amongst humans and should have solved important problems of survival and reproduction in ancestral environments. However, evolutionary psychologists remain divided on whether religious belief is more likely a consequence of evolved psychological adaptations, or is a byproduct of other cognitive adaptations.
Reception.
Critics of evolutionary psychology accuse it of promoting genetic determinism, panadaptionism (the idea that all behaviors and anatomical features are adaptations), unfalsifiable hypotheses, distal or ultimate explanations of behavior when proximate explanations are superior, and malevolent political or moral ideas.
Ethical implications.
Critics have argued that evolutionary psychology might be used to justify existing social hierarchies and reactionary policies. It has also been suggested by critics that evolutionary psychologists' theories and interpretations of empirical data rely heavily on ideological assumptions about race and gender.
In response to such criticism, evolutionary psychologists often caution against committing the naturalistic fallacy – the assumption that "what is natural" is necessarily a moral good. However, their caution against committing the naturalistic fallacy has been criticized as means to stifle legitimate ethical discussions.
Standard social science model.
Evolutionary psychology has been entangled in the larger philosophical and social science controversies related to the debate on nature and nurture. Evolutionary psychologists typically contrast evolutionary psychology with what they call the standard social science model (SSSM). They characterize the SSSM as the "blank slate", social constructionist, or "cultural determinist" perspective that they say dominated the social sciences throughout the 20th century and assumed that the mind was shaped almost entirely by culture.
Critics have argued that evolutionary psychologists created a false dichotomy between their own view and the caricature of the SSSM. Other critics regard the SSSM as a rhetorical device or a straw man and suggest that the scientists whom evolutionary psychologists associate with the SSSM did not believe that the mind was a blank state devoid of any natural predispositions.
Reductionism and determinism.
Some critics view evolutionary psychology as a form of genetic reductionism and genetic determinism, a common critique being that evolutionary psychology does not address the complexity of individual development and experience and fails to explain the influence of genes on behavior in individual cases. Evolutionary psychologists respond that EP works within a nature-nurture interactionist framework that acknowledges that many psychological adaptations are facultative (sensitive to environmental variations during individual development). EP is generally not focused on proximate analyses of behavior but rather its focus is on the study of distal/ultimate causality (the evolution of psychological adaptations). The field of behavioral genetics is focused on the study of the proximate influence of genes on behavior.
Testability of hypotheses.
A frequent critique of the discipline is that the hypotheses of evolutionary psychology are frequently arbitrary and difficult or impossible to adequately test, thus questioning its status as an actual scientific discipline, for example because many current traits probably evolved to serve different functions than they do now. While evolutionary psychology hypotheses are difficult to test, evolutionary psychologists assert that it is not impossible. Part of the critique of the scientific base of evolutionary psychology includes a critique of the concept of the Environments of Evolutionary Adaptation (EEA). Some critics have argued that researchers know so little about the environment in which "Homo sapiens" evolved that explaining specific traits as an adaption to that environment becomes highly speculative. Evolutionary psychologists respond that they do know many things about this environment, including the facts that present day humans' ancestors were hunter-gatherers, that they generally lived in small tribes, etc.
Modularity of mind.
Evolutionary psychologists generally presume that, like the body, the mind is made up of many evolved modular adaptations, although there is some disagreement within the discipline regarding the degree of general plasticity, or "generality," of some modules. It has been suggested that modularity evolves because, compared to non-modular networks, it would have conferred an advantage in terms of fitness and because connection costs are lower.
In contrast, some academics argue that it is unnecessary to posit the existence of highly domain specific modules, and, suggest that the neural anatomy of the brain supports a model based on more domain general faculties and processes. Moreover, empirical support for the domain-specific theory stems almost entirely from performance on variations of the Wason selection task which is extremely limited in scope as it only tests one subtype of deductive reasoning.
Evolutionary psychology defense.
Evolutionary psychologists have addressed many of their critics (see, for example, books by Segerstråle (2000), "Defenders of the Truth: The Battle for Science in the Sociobiology Debate and Beyond," Barkow (2005), "Missing the Revolution: Darwinism for Social Scientists," and Alcock (2001), "The Triumph of Sociobiology".). Among their rebuttals are that some criticisms are straw men, are based on an incorrect nature versus nurture dichotomy, are based on misunderstandings of the discipline, etc. Robert Kurzban suggested that "...critics of the field, when they err, are not slightly missing the mark. Their confusion is deep and profound. It’s not like they are marksmen who can’t quite hit the center of the target; they’re holding the gun backwards."

</doc>
<doc id="9705" url="https://en.wikipedia.org/wiki?curid=9705" title="Languages of Europe">
Languages of Europe

Most languages of Europe belong to the Indo-European language family. This family is divided into a number of branches, including Romance, Germanic, Baltic, Slavic, Albanian, Celtic, Armenian, Iranian, and Hellenic (Greek). The Uralic languages, which include Hungarian, Finnish, and Estonian, also have a significant presence in Europe. The Turkic and Mongolic families also have several European members, while the North Caucasian and Kartvelian families are important in the southeastern extremity of geographical Europe. The Basque language of the western Pyrenees is an isolate unrelated to any other group, while Maltese, which is descended from Sicilian Arabic, is the only Semitic language in Europe with national language status.
Indo-European languages.
The Indo-European language family descended from Proto-Indo-European, believed to have been spoken thousands of years ago. Indo-European languages are spoken throughout Europe.
Albanian.
Albanian has two major dialects, Tosk Albanian and Gheg Albanian. It is spoken in Albania and Kosovo, where it has official status, and is also spoken in neighboring Macedonia, Serbia and Montenegro.
Armenian.
Armenian has two major dialects, Western Armenian and Eastern Armenian. It is spoken in Armenia, where it has sole official status, and is also spoken in neighboring Georgia, Iran, and Azerbaijan (mainly in Nagorno-Karabakh Republic). It is also spoken in Turkey by a very small minority (Western Armenian and Homshetsi), and by small minorities in many other countries where members of the widely dispersed Armenian diaspora reside.
Baltic languages.
The Baltic languages are spoken in Lithuania (Lithuanian, Samogitian) and Latvia (Latvian, Latgalian). Samogitian and Latgalian are usually considered to be dialects of Lithuanian and Latvian respectively.
There are also several extinct Baltic languages, including: Galindian, Curonian, Old Prussian, Selonian, Semigallian and Sudovian.
Celtic.
There are about six living Celtic languages, spoken in areas of northwestern Europe dubbed the "Celtic nations". All six are members of the Insular Celtic family, which in turn is divided into:
Continental Celtic languages had previously been spoken across Europe from Iberia and Gaul to Asia Minor, but became extinct in the first millennium AD.
Germanic.
The Germanic languages make up the predominant language family in northwestern Europe, reaching from Iceland to Sweden and from parts of the United Kingdom and Ireland to Austria. There are two extant major sub-divisions: West Germanic and North Germanic. A third group, East Germanic, is now extinct; the only known surviving East Germanic texts are written in the Gothic language.
West Germanic.
There are three major groupings of West Germanic languages: Anglo-Frisian, Low Franconian (now primarily modern Dutch) and High German.
Anglo-Frisian.
The Anglo-Frisian language family has two major groups:
High German.
German is spoken throughout Germany, Austria, Liechtenstein, Luxembourg, the East Cantons of Belgium, much of Switzerland (including the northeast areas bordering on Germany and Austria) and northern Italy (South Tyrol).
There are several groups of German dialects:
Low German.
Low German is a separate language group from High German, but is still considered a dialect. It is spoken in various regions throughout Northern Germany, but has no official status, as the official language is Standard German.
North Germanic.
The North Germanic languages are spoken in Scandinavian countries and include Danish (Kingdom of Denmark), Norwegian (Norway), Swedish (Sweden and parts of Finland), Elfdalian or Övdalian (in a small part of central Sweden), Faroese (Faroe Islands), and Icelandic (Iceland).
Indo-Aryan languages.
The Indo-Aryan languages have one major representation, it being Romani.
Iranian languages.
The Iranian languages in Europe include Kurdish, Persian (incl. Tat Persian), and Ossetian.
Romance languages.
The Romance languages descended from the Vulgar Latin spoken across most of the lands of the Roman Empire. Some of the Romance languages are official in the European Union and the Latin Union and the more prominent ones are studied in many educational institutions worldwide.
The list below is a summary of Romance languages commonly encountered in Europe:
Slavic.
Slavic languages are spoken in large areas of Central Europe, Southern Europe and Eastern Europe including Russia.
Languages not from the Indo-European family.
Basque.
The Basque language (or "Euskara") is a language isolate and the ancestral language of the Basque people who inhabit the Basque Country, a region in the western Pyrenees mountains mostly in northeastern Spain and partly in southwestern France of about 3 million inhabitants, where it is spoken fluently by about 750,000 and understood by more than 1.5 million people.
Basque is directly related to ancient Aquitanian, and it is likely that an early form of the Basque language was present in Western Europe before the arrival of the Indo-European languages in the area. The language may have been spoken since Paleolithic times.
Basque is also spoken by immigrants in Australia, Costa Rica, Mexico, the Philippines and the United States, especially in the states of Nevada, Idaho, and California.
Kartvelian languages.
The Kartvelian language family consists of Georgian and the related languages of Svan, Mingrelian, and Laz. Proto-Kartvelian is believed to be a common ancestor language of all Kartvelian languages, with the earliest split occurring in the second millennium BC or earlier when Svan was separated. Megrelian and Laz split from Georgian roughly a thousand years later, roughly at the beginning of the first millennium BC (e.g., Klimov, T. Gamkrelidze, G. Machavariani).
The group is considered as isolated, and although for simplicity it is at times grouped with North Caucasian languages, no linguistic relationship exists between the two language families.
North Caucasian.
North Caucasian languages (sometimes called simply "Caucasic", as opposed to Kartvelian, and to avoid confusion with the concept of the "Caucasian race") is a blanket term for two language families spoken chiefly in the north Caucasus and Turkey—the Northwest Caucasian family (including Abkhaz, spoken in Abkhazia, and Circassian) and the Northeast Caucasian family, spoken mainly in the border area of the southern Russian Federation (including Dagestan, Chechnya, and Ingushetia).
Many linguists, notably Sergei Starostin and Sergei Nikolayev, believe that the two groups sprang from a common ancestor about 5,000 years ago. However this view is difficult to evaluate, and remains controversial.
Uralic.
Europe has a number of Uralic languages and language families, including Estonian, Finnish, and Hungarian.
Mongolic.
The Mongolic languages originated in Asia, and most did not proliferate west to Europe. Kalmyk is spoken in the Republic of Kalmykia, part of the Russian Federation, and is thus the only native Mongolic language spoken in Europe.
General issues.
Lingua Franca—past and present.
Europe has had a number of languages that were considered linguae francae over some ranges for some periods according to some historians. Typically in the rise of a national language the new language becomes a lingua franca to peoples in the range of the future nation until the consolidation and unification phases. If the nation becomes internationally influential, its language may become a lingua franca among nations that speak their own national languages. Europe has had no lingua franca ranging over its entire territory spoken by all or most of its populations during any historical period. Some linguae francae of past and present over some of its regions for some of its populations are:
First dictionaries and grammars.
The earliest dictionaries were glossaries, i.e., more or less structured lists of lexical pairs (in alphabetical order or according to conceptual fields). The Latin-German (Latin-Bavarian) Abrogans was among the first. A new wave of lexicography can be seen from the late 15th century onwards (after the introduction of the printing press, with the growing interest in standardizing languages).
Language and identity, standardization processes.
In the Middle Ages the two most important defining elements of Europe were "Christianitas" and "Latinitas". Thus language—at least the supranational language—played an elementary role. The concept of the nation state became increasingly important. Nations adopted particular dialects as their national language. This, together with improved communications, led to official efforts to standardise the national language, and a number of language academies were established (e.g., 1582 Accademia della Crusca in Florence, 1617 Fruchtbringende Gesellschaft in Weimar, 1635 Académie française in Paris, 1713 Real Academia Española in Madrid). Language became increasingly linked to nation as opposed to culture, and was also used to promote religious and ethnic identity (e.g., different Bible translations in the same language for Catholics and Protestants).
The first languages for which standardisation was promoted included Italian ("questione della lingua": Modern Tuscan/Florentine vs. Old Tuscan/Florentine vs. Venetian → Modern Florentine + archaic Tuscan + Upper Italian), French (the standard is based on Parisian), English (the standard is based on the London dialect) and (High) German (based on the dialects of the chancellery of Meissen in Saxony, Middle German, and the chancellery of Prague in Bohemia ("Common German")). But several other nations also began to develop a standard variety in the 16th century.
Scripts.
The main scripts used in Europe today are the Latin and Cyrillic; Greek, Armenian and Georgian also have their own scripts. All of the aforementioned are alphabets.
History.
The Greek alphabet was derived from the Phoenician and Latin was derived from the Greek via the Old Italic alphabet.
In the Early Middle Ages, Ogham was used in Ireland and runes (derived the Old Italic script) in Scandinavia. Both were replaced in general use by the Latin alphabet by the Late Middle Ages. The Cyrillic script was derived from the Greek with the first texts appearing around 940 AD.
Around 1900 there were mainly two typeface variants of the Latin alphabet used in Europe: Antiqua and Fraktur. Fraktur was used most for German, Estonian, Latvian, Norwegian and Danish whereas Antiqua was used for Italian, Spanish, French, Portuguese, English, Romanian, Swedish and Finnish. The Fraktur variant was banned by Hitler in 1941, having been described as "Schwabacher Jewish letters". Other scripts have historically been in use in Europe, including Arabic during the era of the Ottoman Empire, Phoenician, from which modern Latin letters descend, Ancient Egyptian hieroglyphs on Egyptian artefacts traded during Antiquity, and various runic systems used in Northern Europe preceding Christianisation.
Hungarian rovás was used by the Hungarian people in the early Middle Ages, but it was gradually replaced with the Latin-based Hungarian alphabet when Hungary became a kingdom, though it was revived in the 20th century and has certain marginal, but growing area of usage since then.
Linguistic diversity and conflict.
The most ancient historical social structure of Europe is that of politically independent tribes, each with its own ethnic identity, based among other cultural factors on its language: for example, the Latini speaking Latin in Latium. Linguistic conflict has been important in European history. Historical attitudes towards linguistic diversity are illustrated by two French laws: the Ordonnance de Villers-Cotterêts (1539), which said that every document in France should be written in French (neither in Latin nor in Occitan) and the Loi Toubon (1994), which aimed to eliminate Anglicisms from official documents. States and populations within a state have often resorted to war to settle their differences. There have been attempts to prevent such hostilities: one such initiative was promoted by the Council of Europe, founded in 1949, which affirms the right of minority language speakers to use their language fully and freely. The Council of Europe is committed to protecting linguistic diversity.
Currently all European countries except France, Andorra and Turkey have signed the Framework Convention for the Protection of National Minorities, while Greece, Iceland and Luxembourg have signed it, but have not ratified it. This framework entered into force in 1998.
Language and the European Union.
Official status.
The European Union designates one or more languages as "official and working" with regard to any member state if they are the official languages of that state. The decision as to whether they are and their use by the EU as such is entirely up to the laws and policies of the member states. In the case of multiple official languages the member state must designate which one is to be the working language.
As the EU is an entirely voluntary association established by treaty — a member state may withdraw at any time — each member retains its sovereignty in deciding what use to make of its own languages; it must agree to legislate any EU acceptance criteria before membership. The EU designation as official and working is only an agreement concerning the languages to be used in transacting official business between the member state and the EU, especially in the translation of documents passed between the EU and the member state. The EU does not attempt in any way to govern language use in a member state.
Currently the EU has designated by agreement with the member states 24 languages as "official and working:" Bulgarian, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, German, Greek, Hungarian, Irish, Italian, Latvian, Lithuanian, Maltese, Polish, Portuguese, Romanian, Slovak, Slovenian, Spanish and Swedish. This designation provides member states with two "entitlements:" the member state may communicate with the EU in the designated one of those languages and view "EU regulations and other legislative documents" in that language.
Proficiency.
The European Union and the Council of Europe have been collaborating in a number of tasks, among which is the education of member populations in languages for "the promotion of plurilingualism" among EU member states, The joint document, "Common European Framework of Reference for Languages: Learning, Teaching, Assessment (CEFR)", is an educational standard defining "the competencies necessary for communication" and related knowledge for the benefit of educators in setting up educational programs. That document defines three general levels of knowledge: A Basic User, B Independent User and C Proficient User. The ability to speak the language falls under competencies B and C ranging from "can keep going comprehensibly" to "can express him/herself at length with a natural, effortless, unhesitating flow."
These distinctions were simplified in a 2005 independent survey requested by the EU's Directorate-General for Education and Culture regarding the extent to which major European languages were spoken in member states. The results were published in a 2006 document, "Europeans and Their Languages", or "Eurobarometer 243", which is disavowed as official by the European Commission, but does supply some scientific data concerning language use in the EU. In this study, statistically relevant samples of the population in each country were asked to fill out a survey form concerning the languages that they spoke with sufficient competency "to be able to have a conversation". Some of the results showing the distribution of major languages are shown in the maps below. The darkest colors report the highest proportion of speakers. Only EU members were studied. Thus data on Russian speakers were gathered, but Russia is not an EU member and so Russian does not appear in Russia on the maps. It does appear as spoken to the greatest extent in the Baltic countries, which are EU members that were formerly under Soviet rule; followed by former Eastern bloc countries such as Poland, the Czech Republic, and the northeastern part of Germany (former socialist East Germany).
Number of speakers.
The following is a table displaying the number of speakers of a given European language in Europe only. There is a relatively high level of language endangerment in Europe; only 42 languages have more than 1 million speakers.

</doc>
<doc id="9706" url="https://en.wikipedia.org/wiki?curid=9706" title="Eindhoven University of Technology">
Eindhoven University of Technology

The Eindhoven University of Technology is a university of technology located in Eindhoven, Netherlands. Its motto is "Mens agitat molem" (Mind moves matter). The university was the second of its kind in the Netherlands, only Delft University of Technology existed previously. Until mid-1980 it was known as the (abbr. ). In 2011 QS World University Rankings placed Eindhoven at 146th internationally, but 61st globally for Engineering & IT. Furthermore, in 2011 Academic Ranking of World Universities (ARWU) rankings, TU/e was placed at the 52-75 bucket internationally in Engineering/Technology and Computer Science (ENG) category and at 34th place internationally in the field of Computer Science. In 2003 a European Commission report ranked TU/e at third place among all European research universities (after Cambridge and Oxford and at equal rank with TU Munich), thus making it the highest ranked Technical University in Europe.
Overview.
The Eindhoven University of Technology was founded as the "Technische Hogeschool Eindhoven" (THE) on 23 June 1956 by the Dutch government. The University was acknowledged for its research in Automobile sector. It was the second institute of its kind in the Netherlands, preceded only by the Delft University of Technology. It is located on its own campus in the center of Eindhoven, just north of the central station. It is currently home to about 240 professors, 7200 students, 250 PDEng-students, 600 Ph.D. students, 200 post-doc students and 3000 regular employees. It supports about 100 student associations and 15 alumni associations. Yearly, the Eindhoven University of Technology produces almost 3000 scientific publications, 140 PhD-awards, and 40 patents.
The Eindhoven University of Technology is main participant in the technological top institutes DPI and M2i. One of its former students is Gerard Kleisterlee, a former CEO of Philips.
The university is in an area where several companies active in technology are doing their research, like Philips, ASML and DAF. The university maintains close contacts with most of these companies.
As of 29 April 2005, Prof.dr.ir. C.J. van Duijn has the position of rector magnificus.
In 2006, the university celebrated its 50th birthday.
In a 2003 European Commission report, TU/e was ranked as 3rd among European research universities (after Cambridge and Oxford, at equality with TU Munich and thus making it the highest ranked Technical University in Europe), based on the impact of its scientific research. In 'The Times Higher Education Supplement World University Ranking 2005'. it was ranked 74th among world universities, and 67th in 2006.
The university operates several international cooperations with other universities all over the world; the Brain Bridge with Zhejiang University, People's Republic of China, is an example of such a cooperation. Also, the university maintains partnerships with several Dutch universities and announced a "preferred partnership" with the Universiteit Utrecht on 3 January 2011.
Strategic Vision 2020.
On 3 January 2011, ir. Arno Peels presented the university's strategic vision document for the period up to 2020, the "Strategic Plan 2020". Despite the economic crisis and the budget cutbacks announced by the Dutch government for the period up to 2014, the university has set itself an ambitious strategic vision for the period up to 2020. This vision includes establishing a University College to foster both in-depth and wide-interest, society-interest driven education for upcoming engineers; establishing a combined Graduate School to manage the graduate programs; an increase of the student body by 50 percent; a 50 percent increase in the number of annual Ph.D graduations; an increase of knowledge valorisation to a campus-wide score of 4.2; increasing the international position of the university to within the top-100 universities; and increasing the embedding of the university within the city and the Brainport region by transforming the campus into a high-grade science park with laboratories, housing facilities for 700 students and researchers and supporting facilities. Particularly the science park of the vision is costly, with an expected 700 million euro investment in the campus needed for realization of the plan.
Organization.
The Eindhoven University of Technology is a public university of the Netherlands. As such its general structure and management is determined by the "Wet op het Hoger Onderwijs en Wetenschappelijk Onderzoek" (English: "Law on Higher Education and Scientific Research"). Between that law and the statutes of the university itself, the management of the university is organized according to the following chart:
Executive college.
The day-to-day running of the university is in the hands of the Executive College (Dutch: "College van Bestuur"). The College provides oversight for the departments, the service organizations and the Innovation Lab, plus the local activities of the Stan Ackermans Institute. The College consists of three people, plus a secretary:
Oversight of the executive college.
There are two bodies that provide oversight over the Executive College:
Departments and service organizations.
Most of the work at the university is done in the departments and the service organizations.
Both for the departments and the service organizations, the staff (and students) are involved with the running of the body. For that reason both types of bodies have advisory councils which have advisory and co-decision authorities.
TU/e Holding B.V..
Over the past two decades, the TU/e has increasingly developed commercial interests and off-campus ties. These include commercial agreements and contracts directly between the university and external companies, but also interests in spinoff companies. In order to manage these kinds of contractual obligations the university started the TU/e Holding B.V. in 1997. The Holding is a limited company, dedicated to the commercial exploitation of scientific knowledge.
Education.
Departments.
The scientific departments (or faculties; Dutch: "faculteiten") are the primary vehicles for teaching and research in the university. They employ the majority of the academic staff, are responsible for teaching and sponsor the research schools and institutions.
The vast majority of education is undergraduate education provided by the departments to students, who are adults with no other academic qualifications than a secondary education diploma. Some education is also provided to members of the postgraduate designer programs, but they are employed by the university and do not count as part of the student body.
Undergraduate education was given in four- or five-year programs until 2002, styled along the lines of the German system of education; graduates of these programs were granted an engineering title and allowed to prefix their name with the title "ir." (an abbreviation of ingenieur; not to be confused with graduates of technical "hogescholen", who were engineers abbreviated "ing."). Starting in 2002, following the entry into force of the Bologna Accords, the university switched to the bachelor/master structure (students graduating in 2002 were given both an old-style engineering title and a new master's title). The undergraduate programs are now split into two programs each, a three-year bachelor program and a two-year master program. These programs are completely independent, in the sense that a bachelor can leave the university with his title and go to work, can enter a master's program at another university or continue on to the master's program of his department at the university. Of course bachelors from other universities can also enroll in the new master's programs.
The departments also offer Ph.D programs (Dutch: "promotiefase") whereby a qualified master may earn a Ph.D. Unlike in anglo-saxon countries these are not educational programs, however; rather, a person working towards promotion is a research assistant, employed by the department, with teaching responsibilities in addition to his research work.
The TU/e has nine departments:
Honors programs.
The university offers two honors programs for "top students": students who have proven to have a knack for studying, have the capacity to handle a higher academic load and have an interest in more depth in their programs. There are two honors programs, both aimed at the bachelor students:
Qualifying students can choose between the Honors programs, or follow both at the same time.
Shared minors.
As of September 2010 the university offers bachelors in their third year a minor on sustainable energy. This minor will focus on providing students with skills needed to work in the sustainable energy industry, or to start their own company in this industry. For that reason the minor is driven entirely by problems supplied by industry and knowledge institutes, which minor students must solve in multidisciplinary teams. In addition, students must come up with an innovative plan and develop a start-up company.
Postgraduate designer programs.
The university started a number of postgraduate designer programs together with the other Dutch technical universities in 1986. These programs are currently managed by the Stan Ackermans Institute on behalf of the 3TU Federation. Each program is two years in length and graduates earn a Professional Doctorate in Engineering and may call themselves "technical designers". There are a total of eleven program active, of which eight are available at the TU/e:
The post-MSc program as a whole graduated its 3.000th technical designer (Dipl.-Eng. Sissy Papatheologou, PDEng) on 16 September 2010.
Other educational programs.
The university hosts a number of other educational programs that are in some way related to the main educational programs. These include the teacher's program and an MBA program.
Research.
The TU/e does not only host research in its departments. The TU/e participates in a large number of research institutes which balance in different ways between pure science and applied science research. Some of these institutes are bound strictly to the university, others combine research across different universities. Some have even been designated to be of national importance.
Top in research partnerships with industry.
The TU/e is among the world’s ten best-performing research universities in terms of research cooperation with industry in 2011 (Number 1 in 2009). Ten to 20 percent of the scientific publications of these ten universities in the period 2006–2008 were the result of partnerships with researchers in industry. As well as TU/e and Delft University of Technology, the top 10 also includes two universities in Japan (Tokyo Institute of Technology and Keio University in Tokyo), two in Sweden (CTH Chalmers University of Technology and KTH Royal Institute of Technology in Stockholm), and one each in Denmark (DTU Technical University of Denmark in Lyngby), Finland (University of Helsinki), Norway (Norwegian University of Science and Technology in Trondheim) and the USA (Rensselaer Polytechnic Institute in Troy, New York).
Technological Topinstitutes.
A Technological Topinstitute is a research institute that is a combined effort of different universities, commercial entities and the government. The Dutch government has identified a number of areas of research as "key areas" of vital, national interest and has commissioned a Top Institute for each of them. The TU/e hosts and manages two of them:
Research schools.
The TU/e is commissioner and participant of a number of research schools:
Off-campus activities.
The TU/e plays a central role in the academic, economic and social life of Eindhoven and the surrounding region. In addition the university maintains relations with institutions far beyond that region as well and participates in national and international events (sometimes through the student body).
Economic and research motor.
The TU/e is enormously important to the economy of the Eindhoven region, as well as the wider areas of BrabantStad and the Samenwerkingsverband Regio Eindhoven. It provides highly skilled labor for the local knowledge economy and is a knowledge and research partner for technology companies in the area.
The historic basis for the university's role as an economy and research motor was the interaction with Philips. The university was founded primarily to address the need of Philips for local personnel with academic levels of education in electronics, physics, chemistry and later computer science. Later that interest spread to DAF and Royal Dutch Shell (which became the primary employer for graduates of the chemistry department). There was also a synergy with these companies in that senior personnel were hired from them to form the academic staff of the university (which led to the Eindhoven joke that the university trains the engineers and Philips trains the professors).
Changing economic times and business strategies changed the relationship during the 1980s and 1990s. As Philips started moving away from the region, its importance to the region and the university decreased. A struggle for economic survival forced the university to seek closer ties with the city and region of Eindhoven in the 1989–1995 period, resulting in the creation of the Brainport initiative to draw high tech business and industry to the region. The university started expending more effort in knowledge valorisation, in incubating technology startups, in providing direct knowledge support for local technology companies. Also the academic interests of the research shifted with the times, with more effort going into energy efficiency research, green technologies, and other areas of interest driven by social relevance (the call for better technology in the medical field, for example, led to cooperation with the Catharina Hospital and the University of Maastricht medical department and finally the creation of the Biomedical Technology department).
The TU/e is host (and in some cases also commissioner) of a number of highly successful research schools, including the ESI and the DPI. These research institutes are a source of high-tech knowledge for high-tech companies in the area, such as ASML, NXP and FEI. The university also plays a large role as knowledge and personnel supplier to other companies in the High Tech Campus Eindhoven and helps incubate startups through the Eindhoven Twinning Center. It is also a knowledge supporter of the automotive industry in the Helmond region.
In the extended region, the TU/e is part of the backbone of the Eindhoven-Leuven-Aachen triangle. This economic cooperation agreement between three cities in three countries has created one of the most innovative regions in the European Union (measured in terms of money invested in technology and knowledge economy); the agreement is based on the cooperative triangle that connects the three technical universities in those cities.
Eindhoven Energy Institute.
As of the summer of 2010, the TU/e is host to the Eindhoven Energy Institute (EEI). The EEI is a virtual research institute (meaning that it doesn't have any actual offices or facilities), which manages and coordinates the activities of a large number of groups and subinstitutes in the general area of sustainable and alternative energy technologies.
The scientific director of the institute is prof.dr.ir. David Smeulders. He is pro forma head of the research department, which is split into four key areas: "Built Environment" (energy usage and patterns in building, headed by prof.dr.ir. Jan Hensen from the Department of the Built Environment), "Future Fuels" (headed by prof.dr. Philip de Goey of Mechanical Engineering), "Energy Conversion" (headed by prof.dr.ir. René Janssen from Chemical Engineering) and "Fusion and Plasma" (headed by prof.dr. Niek Lopes Cardozo from Physics). The EEI also incorporates the Graduate School on Sustainable Energy, which the TU/e had already established together with the TU Munich and DTU Lyngby. Secretarial services will be provided by the Center Technology for Sustainable Development (TDO) which also already existed at the TU/e (since 1994).
Energy research at the TU/e is among the best in academic Europe (a February 2010 study by Reed Elsevier puts it second only to Imperial College London). This fact, as well as the unique attention to energy in the built-up environment, drew the attention of the European Institute of Innovation and Technology. The EEI is now a full co-location of EIT's KIC on Sustainable Energy (InnoEnergy).
International cooperation and appeal.
The TU/e sets a lot of store by international contacts and cooperation. The university maintains active, academic cooperation with sister institutions in several different countries, for example:
The TU/e also provides education to an increasing number of foreign students and graduates. According to the 2009 annual report in the academic year 2008–2009 there were 490 exchange students, 103 foreign nationals registered in a bachelor program, 430 in a master program, 158 in a professional doctorate program (79% of the total). In 2009 the university employed 37 foreign professors (15.9% of the total) and 16 foreign associate professors (12.8%). Overall, 29.5% of the university staff was non-Dutch.
In 2011/2012, the TU/e has Erasmus bilateral agreements with many universities in 30 countries across Europe in a diverse range of subjects for student exchange.
Technological sports.
In addition to the "regular" types of sports practiced among the student body and by the staff, the TU/e collaborates with the student body in a number of "technology sporting efforts". These usually take the form of cross-department projects, which makes them multidisciplinary efforts. Some examples include:
Student organizations and facilities.
The university offers many different facilities for its student body and hosts many different student organizations<ref name="TU Eindhoven Student/Study/Sport/Culture/International associations"></ref> on campus as well.
Student and study associations.
There are two main types of student clubs at Dutch universities: student associations (Dutch: "studentenvereniging") and study associations (Dutch: "studievereninging"). The first are somewhat analogous to fraternities and sororities in the United States, except that they tend to be coed. The second are linked to the departments and educational programs.
Student associations.
There are three main student associations associated with the TU/e, plus a number of independent clubs:
Culture, international association and spirituality.
There are several associations, clubs and circles associated with the TU/e, which are meant to help students and staff develop themselves in non-academic areas. Such areas include cultural development, building international contacts and investigating spiritual beliefs.
Cultural activities.
The following associations organize activities with cultural or social/societal relevance:
Sport associations.
There are many sports associations within the university. They are overseen by the general sports council ESSF.
Service organizations.
There university is more than just the departments, research bodies and the students. There are several ancillary activities necessary to the running of the university, activities that cross the boundaries and interests of the different departments. These activities are carried out by the universities' service organizations.
The university has the following service organizations:
Spinoffs.
Over the years several spin off companies have been started by TU/e graduates, based on some research done at the university. Examples include:
International acclaim.
On the 2009 THE–QS World University Rankings (From 2010 two separate rankings will be produced by the Times Higher Education World University Rankings and the QS World University Rankings)
list, the Eindhoven University of Technology was ranked inside the top 200 for the fifth consecutive year. An overview of the 2005–2009 rankings can be seen below. In 2010 the QS World University Rankings ranked the university 126th in the world. On the Times Higher Education ranking of 2010 Eindhoven University of Technology is the highest ranked (#114) University of the Netherlands. They are followed by Leiden University (#124).
In a 2003 European Commission report, TU/e was ranked as third among European research universities (after Cambridge and Oxford, at equality with TU Munich and thus making it the highest ranked Technical University in Europe), based on the impact of its scientific researches.
In 2011 Academic Ranking of World Universities (ARWU) rankings, TU/e was placed at the 52-75 bucket internationally in Engineering/Technology and Computer Science ( ENG ) category and at 34th place internationally in the Computer Science subject field.

</doc>
<doc id="9707" url="https://en.wikipedia.org/wiki?curid=9707" title="Electronegativity">
Electronegativity

Electronegativity, symbol χ, is a chemical property that describes the tendency of an atom or a functional group to attract electrons (or electron density) towards itself. An atom's electronegativity is affected by both its atomic number and the distance at which its valence electrons reside from the charged nucleus. The higher the associated electronegativity number, the more an element or compound attracts electrons towards it.
The term "electronegativity" was introduced by Jöns Jacob Berzelius in 1811,
though the concept was known even before that and was studied by many chemists including Avogadro.
In spite of its long history, an accurate scale of electronegativity had to wait till 1932, when Linus Pauling proposed an electronegativity scale, which depends on bond energies, as a development of valence bond theory. It has been shown to correlate with a number of other chemical properties. Electronegativity cannot be directly measured and must be calculated from other atomic or molecular properties. Several methods of calculation have been proposed, and although there may be small differences in the numerical values of the electronegativity, all methods show the same periodic trends between elements.
The most commonly used method of calculation is that originally proposed by Linus Pauling. This gives a dimensionless quantity, commonly referred to as the 
, on a relative scale running from around 0.7 to 3.98 (hydrogen = 2.20). When other methods of calculation are used, it is conventional (although not obligatory) to quote the results on a scale that covers the same range of numerical values: this is known as an electronegativity in Pauling units.
As it is usually calculated, electronegativity is not a property of an atom alone, but rather a property of an atom in a molecule. Properties of a free atom include ionization energy and electron affinity. It is to be expected that the electronegativity of an element will vary with its chemical environment, but it is usually considered to be a transferable property, that is to say that similar values will be valid in a variety of situations.
On the most basic level, electronegativity is determined by factors like the nuclear charge (the more protons an atom has, the more "pull" it will have on electrons) and the number/location of other electrons present in the atomic shells (the more electrons an atom has, the farther from the nucleus the valence electrons will be, and as a result the less positive charge they will experience—both because of their increased distance from the nucleus, and because the other electrons in the lower energy core orbitals will act to shield the valence electrons from the positively charged nucleus).
The opposite of electronegativity is electropositivity: a measure of an element's ability to donate electrons.
Caesium is the least electronegative element in the periodic table (=0.79), while fluorine is most electronegative (=3.98). (Francium and caesium were originally both assigned 0.7; caesium's value was later refined to 0.79, but no experimental data allows a similar refinement for francium. However, francium's ionization energy is known to be slightly higher than caesium's, in accordance with the relativistic stabilization of the 7s orbital, and this in turn implies that caesium is in fact more electronegative than francium.)
Methods of calculation.
Pauling electronegativity.
Pauling first proposed the concept of electronegativity in 1932 as an explanation of the fact that the covalent bond between two different atoms (A–B) is stronger than would be expected by taking the average of the strengths of the A–A and B–B bonds. According to valence bond theory, of which Pauling was a notable proponent, this "additional stabilization" of the heteronuclear bond is due to the contribution of ionic canonical forms to the bonding.
The difference in electronegativity between atoms A and B is given by:
where the dissociation energies, "E", of the A–B, A–A and B–B bonds are expressed in electronvolts, the factor (eV) being included to ensure a dimensionless result. Hence, the difference in Pauling electronegativity between hydrogen and bromine is 0.73 (dissociation energies: H–Br, 3.79 eV; H–H, 4.52 eV; Br–Br 2.00 eV)
As only differences in electronegativity are defined, it is necessary to choose an arbitrary reference point in order to construct a scale. Hydrogen was chosen as the reference, as it forms covalent bonds with a large variety of elements: its electronegativity was fixed first at 2.1, later revised to 2.20. It is also necessary to decide which of the two elements is the more electronegative (equivalent to choosing one of the two possible signs for the square root). This is usually done using "chemical intuition": in the above example, hydrogen bromide dissolves in water to form H and Br ions, so it may be assumed that bromine is more electronegative than hydrogen. However, in principle, since the same electronegativities should be obtained for any two bonding compounds, the data are in fact overdetermined, and the signs are unique once a reference point is fixed (usually, for H or F).
To calculate Pauling electronegativity for an element, it is necessary to have data on the dissociation energies of at least two types of covalent bond formed by that element. A. L. Allred updated Pauling's original values in 1961 to take account of the greater availability of thermodynamic data, and it is these "revised Pauling" values of the electronegativity that are most often used.
The essential point of Pauling electronegativity is that there is an underlying, quite accurate, semi-empirical formula for dissociation energies, namely:
or sometimes, a more accurate fit
This is an approximate equation, but holds with good accuracy. Pauling obtained it by noting that a bond can be approximately represented as a quantum mechanical superposition of a covalent bond and two ionic bond-states. The covalent energy of a bond is approximately, by quantum mechanical calculations, the geometric mean of the two energies of covalent bonds of the same molecules, and there is an additional energy that comes from ionic factors, i.e. polar character of the bond.
The geometric mean is approximately equal to the arithmetic mean - which is applied in the first formula above - when the energies are of the similar value, e.g., except for the highly electropositive elements, where there is a larger difference of two dissociation energies; the geometric mean is more accurate and almost always gives a positive excess energy, due to ionic bonding. The square root of this excess energy, Pauling notes, is approximately additive, and hence one can introduce the electronegativity. Thus, it is this semi-empirical formula for bond energy that underlies Pauling electronegativity concept.
The formulas are approximate, but this rough approximation is in fact relatively good and gives the right intuition, with the notion of polarity of the bond and some theoretical grounding in quantum mechanics. The electronegativities are then determined to best fit the data.
In more complex compounds, there is additional error since electronegativity depends on the molecular environment of an atom. Also, the energy estimate can be only used for single, not for multiple bonds. The energy of formation of a molecule containing only single bonds then can be approximated from an electronegativity table, and depends on the constituents and sum of squares of differences of electronegativities of all pairs of bonded atoms. Such a formula for estimating energy typically has relative error of order of 10%, but can be used to get a rough qualitative idea and understanding of a molecule.
Mulliken electronegativity.
Robert S. Mulliken proposed that the arithmetic mean of the first ionization energy (E) and the electron affinity (E) should be a measure of the tendency of an atom to attract electrons. As this definition is not dependent on an arbitrary relative scale, it has also been termed absolute electronegativity, with the units of kilojoules per mole or electronvolts.
However, it is more usual to use a linear transformation to transform these absolute values into values that resemble the more familiar Pauling values. For ionization energies and electron affinities in electronvolts,
and for energies in kilojoules per mole,
The Mulliken electronegativity can only be calculated for an element for which the electron affinity is known, fifty-seven elements as of 2006.
The Mulliken electronegativity of an atom is sometimes said to be the negative of the chemical potential. By inserting the energetic definitions of the ionization potential and electron affinity into the Mulliken electronegativity, it is possible to show that the Mulliken chemical potential is a finite difference approximation of the electronic energy with respect to the number of electrons., i.e.,
Allred–Rochow electronegativity.
A. Louis Allred and Eugene G. Rochow considered that electronegativity should be related to the charge experienced by an electron on the "surface" of an atom: The higher the charge per unit area of atomic surface the greater the tendency of that atom to attract electrons. The effective nuclear charge, "Z", experienced by valence electrons can be estimated using Slater's rules, while the surface area of an atom in a molecule can be taken to be proportional to the square of the covalent radius, "r". When "r" is expressed in picometres,
Sanderson electronegativity equalization.
R.T. Sanderson has also noted the relationship between Mulliken electronegativity and atomic size, and has proposed a method of calculation based on the reciprocal of the atomic volume. With a knowledge of bond lengths, Sanderson's model allows the estimation of bond energies in a wide range of compounds. Sanderson's model has also been used to calculate molecular geometry, "s"-electrons energy, NMR spin-spin constants and other parameters for organic compounds. This work underlies the concept of electronegativity equalization, which suggests that electrons distribute themselves around a molecule to minimize or to equalize the Mulliken electronegativity. This behavior is analogous to the equalization of chemical potential in macroscopic thermodynamics.
Allen electronegativity.
Perhaps the simplest definition of electronegativity is that of Leland C. Allen, who has proposed that it is related to the average energy of the valence electrons in a free atom,
where ε are the one-electron energies of s- and p-electrons in the free atom and "n" are the number of s- and p-electrons in the valence shell. It is usual to apply a scaling factor, 1.75×10 for energies expressed in kilojoules per mole or 0.169 for energies measured in electronvolts, to give values that are numerically similar to Pauling electronegativities.
The one-electron energies can be determined directly from spectroscopic data, and so electronegativities calculated by this method are sometimes referred to as spectroscopic electronegativities. The necessary data are available for almost all elements, and this method allows the estimation of electronegativities for elements that cannot be treated by the other methods, e.g. francium, which has an Allen electronegativity of 0.67. However, it is not clear what should be considered to be valence electrons for the d- and f-block elements, which leads to an ambiguity for their electronegativities calculated by the Allen method.
In this scale neon has the highest electronegativity of all elements, followed by fluorine, helium, and oxygen.
Correlation of electronegativity with other properties.
The wide variety of methods of calculation of electronegativities, which all give results that correlate well with one another, is one indication of the number of chemical properties which might be affected by electronegativity. The most obvious application of electronegativities is in the discussion of bond polarity, for which the concept was introduced by Pauling. In general, the greater the difference in electronegativity between two atoms the more polar the bond that will be formed between them, with the atom having the higher electronegativity being at the negative end of the dipole. Pauling proposed an equation to relate "ionic character" of a bond to the difference in electronegativity of the two atoms, although this has fallen somewhat into disuse.
Several correlations have been shown between infrared stretching frequencies of certain bonds and the electronegativities of the atoms involved: however, this is not surprising as such stretching frequencies depend in part on bond strength, which enters into the calculation of Pauling electronegativities. More convincing are the correlations between electronegativity and chemical shifts in NMR spectroscopy or isomer shifts in Mössbauer spectroscopy (see figure). Both these measurements depend on the s-electron density at the nucleus, and so are a good indication that the different measures of electronegativity really are describing "the ability of an atom in a molecule to attract electrons to itself".
Trends in electronegativity.
Periodic trends.
In general, electronegativity increases on passing from left to right along a period, and decreases on descending a group. Hence, fluorine is the most electronegative of the elements (not counting noble gases), whereas caesium is the least electronegative, at least of those elements for which substantial data is available. This would lead one to believe that caesium fluoride is the compound with the strongest ionic bond, which is correct.
There are some exceptions to this general rule. Gallium and germanium have higher electronegativities than aluminium and silicon, respectively, because of the d-block contraction. Elements of the fourth period immediately after the first row of the transition metals have unusually small atomic radii because the 3d-electrons are not effective at shielding the increased nuclear charge, and smaller atomic size correlates with higher electronegativity (see Allred-Rochow electronegativity, Sanderson electronegativity above). The anomalously high electronegativity of lead, in particular when compared to thallium and bismuth, appears to be an artifact of data selection (and data availability)—methods of calculation other than the Pauling method show the normal periodic trends for these elements.
Variation of electronegativity with oxidation number.
In inorganic chemistry it is common to consider a single value of the electronegativity to be valid for most "normal" situations. While this approach has the advantage of simplicity, it is clear that the electronegativity of an element is "not" an invariable atomic property and, in particular, increases with the oxidation state of the element.
Allred used the Pauling method to calculate separate electronegativities for different oxidation states of the handful of elements (including tin and lead) for which sufficient data was available. However, for most elements, there are not enough different covalent compounds for which bond dissociation energies are known to make this approach feasible. This is particularly true of the transition elements, where quoted electronegativity values are usually, of necessity, averages over several different oxidation states and where trends in electronegativity are harder to see as a result.
The chemical effects of this increase in electronegativity can be seen both in the structures of oxides and halides and in the acidity of oxides and oxoacids. Hence CrO and MnO are acidic oxides with low melting points, while CrO is amphoteric and MnO is a completely basic oxide.
The effect can also be clearly seen in the dissociation constants of the oxoacids of chlorine. The effect is much larger than could be explained by the negative charge being shared among a larger number of oxygen atoms, which would lead to a difference in p"K" of log(¼) = –0.6 between hypochlorous acid and perchloric acid. As the oxidation state of the central chlorine atom increases, more electron density is drawn from the oxygen atoms onto the chlorine, reducing the partial negative charge on the oxygen atoms and increasing the acidity.
Group electronegativity.
In organic chemistry, electronegativity is associated more with different functional groups than with individual atoms. The terms group electronegativity and substituent electronegativity are used synonymously. However, it is common to distinguish between the inductive effect and the resonance effect, which might be described as σ- and π-electronegativities, respectively. There are a number of linear free-energy relationships that have been used to quantify these effects, of which the Hammett equation is the best known. Kabachnik parameters are group electronegativities for use in organophosphorus chemistry.
Electropositivity.
Electropositivity is a measure of an element's ability to donate electrons, and therefore form positive ions; thus, it is opposed to electronegativity.
Mainly, this is an attribute of metals, meaning that, in general, the greater the metallic character of an element the greater the electropositivity. Therefore, the alkali metals are most electropositive of all. This is because they have a single electron in their outer shell and, as this is relatively far from the nucleus of the atom, it is easily lost; in other words, these metals have low ionization energies.
While electronegativity increases along periods in the periodic table, and decreases down groups, electropositivity "decreases" along periods (from left to right) and "increases" down groups.

</doc>
<doc id="9708" url="https://en.wikipedia.org/wiki?curid=9708" title="European Charter for Regional or Minority Languages">
European Charter for Regional or Minority Languages

The European Charter for Regional or Minority Languages (ECRML) is a European treaty (CETS 148) adopted in 1992 under the auspices of the Council of Europe to protect and promote historical regional and minority languages in Europe. The preparation for the charter was undertaken by the predecessor to the current Congress of Local and Regional Authorities, the Standing Conference of Local and Regional Authorities of Europe because involvement of local and regional government was essential. The actual charter was written in the Parliamentary Assembly based on the Congress' Recommendations. It only applies to languages traditionally used by the nationals of the State Parties (thus excluding languages used by recent immigrants from other states, see immigrant languages), which significantly differ from the majority or official language (thus excluding what the state party wishes to consider as mere local dialects of the official or majority language) and that either have a territorial basis (and are therefore traditionally spoken by populations of regions or areas within the State) or are used by linguistic minorities within the State as a whole (thereby including such languages as Yiddish and Romani, which are used over a wide geographic area).
Some states, such as the Ukraine and Sweden, have tied the status of minority language to the recognized national minorities, which are defined by ethnic, cultural and/or religious criteria, thereby circumventing the Charter's notion of linguistic minority. 
Languages that are official within regions, provinces or federal units within a State (for example Catalan in Spain) are not classified as official languages of the State and may therefore benefit from the Charter. On the other hand, Ireland has not been able to sign the Charter on behalf of the Irish language (although a minority language) as it is defined as the first official language of the state. The United Kingdom has ratified the Charter in respect to (among other languages) Welsh in Wales and Irish in Northern Ireland. France, although a signatory, has been constitutionally blocked from ratifying the Charter in respect to the languages of France.
The charter provides a large number of different actions state parties can take to protect and promote historical regional and minority languages. There are two levels of protection—all signatories must apply the lower level of protection to qualifying languages. Signatories may further declare that a qualifying language or languages will benefit from the higher level of protection, which lists a range of actions from which states must agree to undertake at least 35.
Protections.
Countries can ratify the charter in respect of its minority languages based on Part II or Part III of the charter, which contain varying principles. Countries can treat languages differently under the charter, for example, in the United Kingdom, the Welsh language is ratified under the general Part II principles as well as the more specific Part III commitments, while the Cornish language is ratified only under Part II.
Part II.
Part II of the Charter details eight main principles and objectives upon which States must base their policies and legislation. They are seen as a framework for the preservation of the languages concerned.
Part III.
Part III details comprehensive rules across a number of sectors, that states agree to abide by. Each language to which Part III of the Charter is applied must be specifically named by the government. States must select at least thirty-five of the undertakings in respect of each language. Many provisions contain several options, of varying degrees of stringency, one of which has to be chosen “according to the situation of each language”. The areas from which these specific undertakings must be chosen are as follows:
Languages protected under the Charter.
Countries that have ratified the Charter, and languages for which the ratification was made:

</doc>
<doc id="9709" url="https://en.wikipedia.org/wiki?curid=9709" title="English Civil War">
English Civil War

The English Civil War (1642–1651) was a series of armed conflicts and political machinations between Parliamentarians ("Roundheads") and Royalists ("Cavaliers") in the Kingdom of England over, principally, the manner of its government. The first (1642–46) and second (1648–49) wars pitted the supporters of King Charles I against the supporters of the Long Parliament, while the third (1649–51) saw fighting between supporters of King Charles II and supporters of the Rump Parliament. The war ended with the Parliamentarian victory at the Battle of Worcester on 3 September 1651.
The overall outcome of the war was threefold: the trial and execution of Charles I; the exile of his son, Charles II; and the replacement of English monarchy with, at first, the Commonwealth of England (1649–53) and then the Protectorate (1653–59) under Oliver Cromwell's personal rule. The monopoly of the Church of England on Christian worship in England ended with the victors consolidating the established Protestant Ascendancy in Ireland. Constitutionally, the wars established the precedent that an English monarch cannot govern without Parliament's consent, although the idea of parliament as the ruling power of England was legally established as part of the Glorious Revolution in 1688.
Terminology.
The term "English Civil War" appears most often in the singular form, although historians often divide the conflict into two or three separate wars. Nor were these wars restricted to England. Wales was then a part of the Kingdom of England, and was affected accordingly; from the outset, moreover, the conflicts involved wars with and civil wars within both Scotland and Ireland (see Wars of the Three Kingdoms for an overview).
Unlike other civil wars in England, which focused on who should rule, this war was more concerned with the manner in which the kingdoms of England, Scotland and Ireland were governed. The 1911 Encyclopædia Britannica called the series of conflicts the "Great Rebellion", while some historiansespecially Marxists such as Christopher Hill (1912–2003)have long favoured the term "English Revolution".
Strategy and tactics.
Many of the officers and veteran soldiers of the English Civil war studied and implemented war strategies that had been learned and perfected in other wars across Europe, namely by the Spanish and the Dutch during the Dutch war for independence which began in 1568.
The main battle tactic came to be known as pike and shot infantry, in which both sides would line up facing each other with infantry brigades of musketeers in the centre, carrying matchlock muskets; these muskets were inaccurate, but could be lethal from up to 300 yards. The brigades would arrange themselves in lines of musketeers, three deep, where the first row would kneel, the second would crouch, and the third would stand, allowing all three to fire a volley simultaneously. At times there would be two groups of three lines allowing one group to reload while the other group arranged themselves and fired. Mixed in among the musketeers were pikemen carrying pikes that were between and long, whose primary purpose was to protect the musketeers from cavalry charges. Positioned on each side of the infantry were the cavalry, with a right-wing led by the lieutenant-general, and a left-wing by the commissary general; the main goal of the cavalry was to rout the opponent’s cavalry and then turn and overpower their infantry.
The Royalist cavaliers' skill and speed on horseback led to many early victories. Prince Rupert, the leader of the king’s cavalry, learned a tactic while fighting in the Dutch army where the cavalry would charge at full speed into the opponent’s infantry firing their pistols just before impact.
However, with Oliver Cromwell and the introduction of the more disciplined New Model Army, a group of disciplined pikemen would stand their ground in the face of charging cavalry and could have a devastating effect. While the Parliamentarian cavalry were slower than the cavaliers, they were also better disciplined. The Royalists had a tendency to chase down individual targets after the initial charge leaving their forces scattered and tired; Cromwell’s cavalry, on the other hand, trained to operate as a single unit, which led to many decisive victories.
Background.
The King's rule.
The English Civil War broke out fewer than forty years after the death of Queen Elizabeth I in 1603. Elizabeth's death had resulted in the accession of her first cousin twice-removed, King James VI of Scotland, to the English throne as James I of England, creating the first personal union of the Scottish and English kingdoms. As King of Scots, James had become accustomed to Scotland's weak parliamentary tradition since assuming control of the Scottish government in 1583, so that upon assuming power south of the border, the new King of England was genuinely affronted by the constraints the English Parliament attempted to place on him in exchange for money. In spite of this, James' personal extravagance meant he was perennially short of money and had to resort to extra-Parliamentary sources of income.
James' personal extravagance was tempered by his peaceful disposition, so that by the succession of his son Charles I to the English and Scottish thrones in 1625 the two kingdoms had both experienced relative peace, both internally and in their relations with each other, for as long as anyone could remember. Charles hoped to unite the kingdoms of England, Scotland and Ireland into a new single kingdom, fulfilling the dream of his father. Many English Parliamentarians had suspicions regarding such a move because they feared that setting up a new kingdom might destroy the old English traditions which had bound the English monarchy. As Charles shared his father's position on the power of the crown (James had described kings as "little gods on Earth", chosen by God to rule in accordance with the doctrine of the "Divine Right of Kings"), the suspicions of the Parliamentarians had some justification.
Parliament in the English constitutional framework.
At the time, the Parliament of England did not have a large permanent role in the English system of government. Instead, Parliament functioned as a temporary advisory committee and was summoned only if and when the monarch saw fit to summon it. Once summoned, a parliament's continued existence was at the king's pleasure, since it was subject to dissolution by him at any time.
Yet in spite of this limited role, Parliament had, over the preceding centuries, acquired "de facto" powers of enough significance that monarchs could not simply ignore them indefinitely. Without question, for a monarch, Parliament's most indispensable power was its ability to raise tax revenues far in excess of all other sources of revenue at the Crown's disposal. By the seventeenth century, Parliament's tax-raising powers had come to be derived from the fact that the gentry was the only stratum of society with the ability and authority to actually collect and remit the most meaningful forms of taxation then available at the local level. This meant that if the king wanted to ensure a smooth collection of revenue, he needed the co-operation of the gentry. For all of the Crown's legal authority, by any modern standard, its resources were limited to the extent that, if and when the gentry refused to collect the king's taxes on a national scale, the Crown lacked any practical means with which to compel them.
Therefore, in order to secure their co-operation, monarchs permitted the gentry (and only the gentry) to elect representatives to sit in the House of Commons. When assembled along with the House of Lords, these elected representatives formed a Parliament. Parliaments therefore, allowed representatives of the gentry to meet, primarily (at least in the opinion of the monarch) so that they could give their sanction to whatever taxes the monarch expected their electorate to collect. In the process, the representatives could also confer and send policy proposals to the king in the form of bills. However, Parliament lacked any legal means of forcing its will upon the monarch; its only leverage with the king was the threat of its withholding the financial means required to execute his plans.
Parliamentary concerns and the Petition of Right.
Many concerns were raised over Charles's marriage to a Roman Catholic, French princess Henrietta Maria, in 1625. The Parliament refused to assign him the traditional right to collect customs duties for his entire reign, deciding instead to grant it only on a provisional basis and negotiate with him.
Charles, meanwhile, decided to send an expeditionary force to relieve the French Huguenots whom French royal troops held besieged in La Rochelle. Military support for Protestants on the Continent was, in itself, popular both in Parliament and with the Protestant majority in general, and it had the potential to alleviate concerns brought about by the King's marriage to a Catholic. However, Charles's insistence on having his unpopular royal favourite George Villiers, the Duke of Buckingham, assume command of the English force undermined that support. Unfortunately for Charles and Buckingham, the relief expedition proved a fiasco (1627), and Parliament, already hostile to Buckingham for his monopoly on royal patronage, opened impeachment proceedings against him. Charles responded by dissolving Parliament. This move, while saving Buckingham, reinforced the impression that Charles wanted to avoid Parliamentary scrutiny of his ministers.
Having dissolved Parliament and unable to raise money without it, the king assembled a new one in 1628. (The elected members included Oliver Cromwell and Edward Coke.) The new Parliament drew up the Petition of Right, and Charles accepted it as a concession in order to obtain his subsidy. Amongst other things, the Petition referred to the Magna Carta. However, it did not grant him the right of tonnage and poundage, which Charles had been collecting without Parliamentary authorisation since 1625. Several of the more active members of the opposition were imprisoned, which caused some outrage; one, John Eliot, subsequently died in prison, becoming regarded as a martyr for the rights of Parliament.
Personal rule.
Charles I avoided calling a Parliament for the next decade, a period known as the "personal rule of Charles I", or the "Eleven Years' Tyranny". During this period, Charles's lack of money determined policies. First and foremost, to avoid Parliament, the King needed to avoid war. Charles made peace with France and Spain, effectively ending England's involvement in the Thirty Years' War. However, that in itself was far from enough to balance the Crown's finances.
Unable to raise revenue without Parliament, and unwilling to convene it, Charles resorted to other means. One method was reviving certain conventions, often long-outdated. For example, a failure to attend and to receive knighthood at Charles's coronation was a finable offence with the fine paid to the Crown. The King also tried to raise revenue through the ship money tax, by exploiting a naval war-scare in 1635, demanding that the inland English counties pay the tax for the Royal Navy. Established law supported this policy, but authorities had ignored it for centuries, and many regarded it as yet another extra-Parliamentary (and therefore illegal) tax. Some prominent men refused to pay ship money, arguing that the tax was illegal, but they lost in court, and the fines imposed on them for refusing to pay ship money (and for standing against the tax's legality) aroused widespread indignation.
During the "Personal Rule," Charles aroused most antagonism through his religious measures: he believed in High Anglicanism, a sacramental version of the Church of England, theologically based upon Arminianism, a creed shared with his main political advisor, Archbishop William Laud. In 1633, Charles appointed Laud as Archbishop of Canterbury and started making the Church more ceremonial, replacing the wooden communion tables with stone altars. Puritans accused Laud of reintroducing Catholicism; when they complained, he had them arrested. In 1637 John Bastwick, Henry Burton, and William Prynne had their ears cut off for writing pamphlets attacking Laud's views—a rare penalty for gentlemen, and one that aroused anger. Moreover, the Church authorities revived the statutes passed in the time of Elizabeth I about church attendance, and fined Puritans for not attending Anglican church services.
Rebellion in Scotland.
The end of Charles's independent governance came when he attempted to apply the same religious policies in Scotland. The Church of Scotland, reluctantly episcopal in structure, had independent traditions. Charles, however, wanted one uniform Church throughout Britain and introduced a new, High Anglican version of the English Book of Common Prayer to Scotland in the middle of 1637. This was violently resisted; a riot broke out in Edinburgh, which may have been started in St Giles' Cathedral, according to legend, by Jenny Geddes. In February 1638, the Scots formulated their objections to royal policy in the National Covenant. This document took the form of a "loyal protest," rejecting all innovations not first having been tested by free parliaments and General Assemblies of the Church.
In the spring of 1639, King Charles I accompanied his forces to the Scottish border to end the rebellion known as the Bishops' War. But, after an inconclusive military campaign, he accepted the offered Scottish truce: the Pacification of Berwick. The truce proved temporary, and a second war followed in the middle of 1640. This time, a Scots army defeated Charles's forces in the north, then captured Newcastle. Charles eventually agreed not to interfere with Scotland's religion and paid the Scots' war-expenses.
Recall of the English Parliament.
Charles needed to suppress the rebellion in Scotland. He had insufficient funds, however, and needed to seek money from a newly elected English Parliament in 1640. The majority faction in the new Parliament, led by John Pym, took this appeal for money as an opportunity to discuss grievances against the Crown and opposed the idea of an English invasion of Scotland. Charles took exception to this "lèse-majesté" (offence against the ruler) and dissolved the Parliament after only a few weeks; hence the name "the Short Parliament".
Without Parliament's support, Charles attacked Scotland again, breaking the truce at Berwick, and suffered a comprehensive defeat. The Scots went on to invade England, occupying Northumberland and Durham. Meanwhile, another of Charles' chief advisors, Thomas Wentworth, 1st Viscount Wentworth, had risen to the role of Lord Deputy of Ireland in 1632 and brought in much-needed revenue for Charles by persuading the Irish Catholic gentry to pay new taxes in return for promised religious concessions.
In 1639, Charles had recalled Wentworth to England and in 1640 made him Earl of Strafford, attempting to have him achieve similar results in Scotland. This time he proved less successful and the English forces fled the field in their second encounter with the Scots in 1640. Almost the entirety of Northern England was occupied and Charles was forced to pay £850 per day to keep the Scots from advancing. If he did not, they would "take" the money by pillaging and burning the cities and towns of Northern England.
All this put Charles in a desperate financial position. As King of Scots, he had to find money to pay the Scottish army in England; as King of England, he had to find money to pay and equip an English army to defend England. His means of raising English revenue without an English Parliament fell critically short of achieving this. Against this backdrop, and according to advice from the Magnum Concilium (the House of Lords, but without the Commons, so not a Parliament), Charles finally bowed to pressure and summoned another English Parliament in November 1640.
The Long Parliament.
The new Parliament proved even more hostile to Charles than its predecessor. It immediately began to discuss grievances against Charles and his government and with Pym and Hampden (of ship money fame) in the lead, took the opportunity presented by the King's troubles to force various reforming measures—including many with strong 'anti-Papist' themes—upon him. The legislators passed a law which stated that a new Parliament should convene at least once every three years—without the King's summons, if necessary. Other laws passed by the Parliament made it illegal for the king to impose taxes without Parliamentary consent and later gave Parliament control over the king's ministers. Finally, the Parliament passed a law forbidding the King to dissolve it without its consent, even if the three years were up. Ever since, this Parliament has been known as the "Long Parliament". However, Parliament did attempt to avert conflict by requiring all adults to sign The Protestation, an oath of allegiance to Charles.
Early in the Long Parliament's proceedings the house overwhelmingly accused Thomas Wentworth, Earl of Strafford of high treason and other crimes and misdemeanours.
Henry Vane the Younger supplied evidence in relation to Strafford's claimed improper use of the army in Ireland, alleging that Strafford was encouraging the King to use his army raised in Ireland to threaten England into compliance. This evidence was obtained from Vane's father, Henry Vane the Elder, a member of the King's Privy council, who refused to confirm it in Parliament out of loyalty to Charles. On 10 April 1641, Pym's case collapsed, but Pym made a direct appeal to Henry Vane the Younger to produce a copy of the notes from the King's Privy council, discovered by the younger Vane and secretly turned over to Pym, to the great anguish of the Elder Vane. These notes from the King's Privy Council contained evidence Strafford had told the King, "Sir, you have done your duty, and your subjects have failed in theirs; and therefore you are absolved from the rules of government, and may supply yourself by extraordinary ways; you have an army in Ireland, with which you may reduce the kingdom."
Pym immediately launched a Bill of Attainder, stating Strafford's guilt and demanding that the Earl be put to death. Unlike a guilty finding in a court case, attainder did not require a legal burden of proof, but it did require the king's approval. Charles, however, guaranteed Strafford that he would not sign the attainder, without which the bill could not be passed. Furthermore, the Lords were opposed to the severity of the sentence of death imposed upon Strafford. Yet, increased tensions and a plot in the army to support Strafford began to sway the issue. On 21 April, the Commons passed the Bill (204 in favour, 59 opposed, and 250 abstained), and the Lords acquiesced. Charles, still incensed over the Commons' handling of Buckingham, refused. Strafford himself, hoping to head off the war he saw looming, wrote to the king and asked him to reconsider. Charles, fearing for the safety of his family, signed on 10 May. Strafford was beheaded two days later. In the meantime both Parliament and the King agreed to an independent investigation into the king's involvement in Strafford's plot.
The Long Parliament then passed the Triennial Act, also known as the Dissolution Act in May 1641, to which the Royal Assent was readily granted. The Triennial Act required that Parliament be summoned at least once every three years, and that when the King failed to issue proper summons, the members could assemble on their own. This act also forbade ship money without Parliament's consent, fines in destraint of knighthood and forced loans. Monopolies were cut back severely, and the Courts of Star Chamber and High Commission were abolished by the Habeas Corpus Act 1640 and the Triennial Act respectively. All remaining forms of taxation were legalised and regulated by the Tonnage and Poundage Act. On 3 May, Parliament decreed The Protestation, attacking the 'wicked counsels' of Charles's government, whereby those who signed the petition undertook to defend 'the true reformed religion', parliament, and the king's person, honour and estate. Throughout May, the House of Commons launched several bills attacking bishops and episcopalianism in general, each time defeated in the Lords.
It was hoped by both Charles and Parliament that the execution of Strafford and the Protestation would end the drift towards war; in fact, they encouraged it. Charles and his supporters continued to resent Parliament's demands, while Parliamentarians continued to suspect Charles of wanting to impose episcopalianism and unfettered royal rule by military force. Within months, the Irish Catholics, fearing a resurgence of Protestant power, struck first, and all Ireland soon descended into chaos. Rumours circulated that the King supported the Irish, and Puritan members of the Commons soon started murmuring that this exemplified the fate that Charles had in store for them all.
In early January 1642, accompanied by 400 soldiers, Charles attempted to arrest five members of the House of Commons on a charge of treason. This attempt failed. When the troops marched into Parliament, Charles enquired of William Lenthall, the Speaker, as to the whereabouts of the five. Lenthall replied, "May it please your Majesty, I have neither eyes to see nor tongue to speak in this place but as the House is pleased to direct me, whose servant I am here." In other words, the Speaker proclaimed himself a servant of Parliament, rather than of the King.
Local grievances.
In the summer of 1642 these national troubles helped to polarise opinion, ending indecision about which side to support or what action to take. Opposition to Charles also arose owing to many local grievances. For example, the imposition of drainage-schemes in The Fens negatively affected the livelihood of thousands of people after the King awarded a number of drainage-contracts. Many regarded the King as indifferent to the welfare of the people, and this played a role in bringing a large part of eastern England into Parliament’s camp. This sentiment brought with it people such as the Earl of Manchester and Oliver Cromwell, each a notable wartime adversary of the King. Conversely, one of the leading drainage contractors, the Earl of Lindsey, was to die fighting for the King at the Battle of Edgehill.
First English Civil War (1642–1646).
In early January 1642, a few days after his failure to capture five members of the House of Commons, fearing for the safety of his family and retinue, Charles left the London area for the north of the country. Further negotiations by frequent correspondence between the King and the Long Parliament through to early summer proved fruitless. As the summer progressed, cities and towns declared their sympathies for one faction or the other: for example, the garrison of Portsmouth under the command of Sir George Goring declared for the King, but when Charles tried to acquire arms for his cause from Kingston upon Hull, the depository for the weapons used in the previous Scottish campaigns, Sir John Hotham, the military governor appointed by Parliament in January, refused to let Charles enter Hull, and when Charles returned with more men later, Hotham drove them off. Charles issued a warrant for Hotham to be arrested as a traitor but was powerless to enforce it. Throughout the summer months, tensions rose and there was brawling in a number of places, with the first death from the conflict taking place in Manchester.
At the outset of the conflict, much of the country remained neutral, though the Royal Navy and most English cities favoured Parliament, while the King found considerable support in rural communities. Historians estimate that between them, both sides had only about 15,000 men. However, the war quickly spread and eventually involved every level of society. Many areas attempted to remain neutral. Some formed bands of Clubmen to protect their localities against the worst excesses of the armies of both sides, but most found it impossible to withstand both the King and Parliament. On one side, the King and his supporters fought for traditional government in Church and state. On the other, most supporters of the Parliamentary cause initially took up arms to defend what they thought of as the traditional balance of government in Church and state, which the bad advice the King had received from his advisers had undermined before and during the "Eleven Years' Tyranny." The views of the Members of Parliament ranged from unquestioning support of the King – at one point during the First Civil War, more members of the Commons and Lords gathered in the King's Oxford Parliament than at Westminster – through to radicals, who wanted major reforms in favour of religious independence and the redistribution of power at the national level. However, even the most radical supporters of the Parliamentarian cause still favoured the retention of Charles on the throne.
After the debacle at Hull, Charles moved on to Nottingham, where on 22 August 1642, he raised the royal standard. When he raised his standard, Charles had with him about 2,000 cavalry and a small number of Yorkshire infantry-men, and using the archaic system of a Commission of Array, Charles's supporters started to build a larger army around the standard. Charles moved in a south-westerly direction, first to Stafford, and then on to Shrewsbury, because the support for his cause seemed particularly strong in the Severn valley area and in North Wales. While passing through Wellington, in what became known as the "Wellington Declaration," he declared that he would uphold the "Protestant religion, the laws of England, and the liberty of Parliament."
The Parliamentarians who opposed the King had not remained passive during this pre-war period. As in the case of Kingston upon Hull, they had taken measures to secure strategic towns and cities by appointing to office men sympathetic to their cause, and on 9 June they had voted to raise an army of 10,000 volunteers and appointed Robert Devereux, 3rd Earl of Essex commander three days later. He received orders "to rescue His Majesty's person, and the persons of the Prince [of Wales] and the Duke of York out of the hands of those desperate persons who were about them." The Lords Lieutenant, whom Parliament appointed, used the Militia Ordinance to order the militia to join Essex's army.
Two weeks after the King had raised his standard at Nottingham, Essex led his army north towards Northampton, picking up support along the way (including a detachment of Cambridgeshire cavalry raised and commanded by Oliver Cromwell). By the middle of September Essex's forces had grown to 21,000 infantry and 4,200 cavalry and dragoons. On 14 September he moved his army to Coventry and then to the north of the Cotswolds, a strategy which placed his army between the Royalists and London. With the size of both armies now in the tens of thousands, and only Worcestershire between them, it was inevitable that cavalry reconnaissance units would sooner or later meet. This happened in the first major skirmish of the Civil War, when a cavalry troop of about 1,000 Royalists commanded by Prince Rupert, a German nephew of the King and one of the outstanding cavalry commanders of the war, defeated a Parliamentary cavalry detachment under the command of Colonel John Brown in the Battle of Powick Bridge, at a bridge across the River Teme close to Worcester.
Rupert withdrew to Shrewsbury, where a council-of-war discussed two courses of action: whether to advance towards Essex's new position near Worcester, or to march along the now opened road towards London. The Council decided to take the London route, but not to avoid a battle, for the Royalist generals wanted to fight Essex before he grew too strong, and the temper of both sides made it impossible to postpone the decision. In the Earl of Clarendon's words: "it was considered more counsellable to march towards London, it being morally sure that Essex would put himself in their way". Accordingly, the army left Shrewsbury on 12 October, gaining two days' start on the enemy, and moved south-east. This had the desired effect, as it forced Essex to move to intercept them.
The first pitched battle of the war, fought at Edgehill on 23 October 1642, proved inconclusive, and both the Royalists and Parliamentarians claimed it as a victory. The second field action of the war, the stand-off at Turnham Green, saw Charles forced to withdraw to Oxford. This city would serve as his base for the remainder of the war.
In 1643 the Royalist forces won at Adwalton Moor, and gained control of most of Yorkshire. In the Midlands, a Parliamentary force under Sir John Gell besieged and captured the cathedral city of Lichfield, after the death of the original commander, Lord Brooke. This group subsequently joined forces with Sir John Brereton to fight the inconclusive Battle of Hopton Heath (19 March 1643), where the Royalist commander, the Earl of Northampton, was killed. Subsequent battles in the west of England at Lansdowne and at Roundway Down also went to the Royalists. Prince Rupert could then take Bristol. In the same year, Oliver Cromwell formed his troop of "Ironsides", a disciplined unit that demonstrated his military leadership ability. With their assistance, he won a victory at the Battle of Gainsborough in July.
At this stage, from 7 to 9 August 1643, there were some popular demonstrations in London—both pro and against war. They were protesting at Westminster. A peace demonstration by London women, which turned violent, was suppressed by William Waller's regiment of horse. Some women were beaten and even killed, and many arrested.
Following these events of August, the representative of Venice in England reported to the doge that the London government took considerable measures to stifle dissent.
In general, the early part of the war went well for the Royalists. The turning point came in the late summer and early autumn of 1643, when the Earl of Essex's army forced the king to raise the siege of Gloucester and then brushed the Royalist army aside at the First Battle of Newbury (20 September 1643), in order to return triumphantly to London. Other Parliamentarian forces won the Battle of Winceby, giving them control of Lincoln. Political manoeuvring to gain an advantage in numbers led Charles to negotiate a ceasefire in Ireland, freeing up English troops to fight on the Royalist side in England, while Parliament offered concessions to the Scots in return for aid and assistance.
With the help of the Scots, Parliament won at Marston Moor (2 July 1644), gaining York and the north of England. Cromwell's conduct in this battle proved decisive, and demonstrated his potential as both a political and an important military leader. The defeat at the Battle of Lostwithiel in Cornwall, however, marked a serious reverse for Parliament in the south-west of England. Subsequent fighting around Newbury (27 October 1644), though tactically indecisive, strategically gave another check to Parliament.
In 1645 Parliament reaffirmed its determination to fight the war to a finish. It passed the Self-denying Ordinance, by which all members of either House of Parliament laid down their commands, and re-organized its main forces into the New Model Army ("Army"), under the command of Sir Thomas Fairfax, with Cromwell as his second-in-command and Lieutenant-General of Horse. In two decisive engagements—the Battle of Naseby on 14 June and the Battle of Langport on 10 July—the Parliamentarians effectively destroyed Charles' armies.
In the remains of his English realm Charles attempted to recover a stable base of support by consolidating the Midlands. He began to form an axis between Oxford and Newark on Trent in Nottinghamshire. Those towns had become fortresses and showed more reliable loyalty to him than to others. He took Leicester, which lies between them, but found his resources exhausted. Having little opportunity to replenish them, in May 1646 he sought shelter with a Presbyterian Scottish army at Southwell in Nottinghamshire. Charles was eventually handed over to the English Parliament by the Scots and was imprisoned. This marked the end of the First English Civil War.
Second English Civil War (1648–1649).
Charles I took advantage of the deflection of attention away from himself to negotiate a secret treaty with the Scots, again promising church reform, on 28 December 1647. Under the agreement, called the "Engagement", the Scots undertook to invade England on Charles' behalf and restore him to the throne on condition of the establishment of Presbyterianism for three years.
A series of Royalist uprisings throughout England and a Scottish invasion occurred in the summer of 1648. Forces loyal to Parliament put down most of the uprisings in England after little more than skirmishes, but uprisings in Kent, Essex and Cumberland, the rebellion in Wales, and the Scottish invasion involved the fighting of pitched battles and prolonged sieges.
In the spring of 1648 unpaid Parliamentarian troops in Wales changed sides. Colonel Thomas Horton defeated the Royalist rebels at the Battle of St Fagans (8 May) and the rebel leaders surrendered to Cromwell on 11 July after the protracted two-month siege of Pembroke. Sir Thomas Fairfax defeated a Royalist uprising in Kent at the Battle of Maidstone on 1 June. Fairfax, after his success at Maidstone and the pacification of Kent, turned northward to reduce Essex, where, under their ardent, experienced and popular leader Sir Charles Lucas, the Royalists had taken up arms in great numbers. Fairfax soon drove the enemy into Colchester, but his first attack on the town met with a repulse and he had to settle down to a long siege.
In the North of England, Major-General John Lambert fought a very successful campaign against a number of Royalist uprisings—the largest that of Sir Marmaduke Langdale in Cumberland. Thanks to Lambert's successes, the Scottish commander, the Duke of Hamilton, had perforce to take the western route through Carlisle in his pro-Royalist Scottish invasion of England. The Parliamentarians under Cromwell engaged the Scots at the Battle of Preston (17–19 August). The battle took place largely at Walton-le-Dale near Preston in Lancashire, and resulted in a victory by the troops of Cromwell over the Royalists and Scots commanded by Hamilton. This Parliamentarian victory marked the end of the Second English Civil War.
Nearly all the Royalists who had fought in the First Civil War had given their parole not to bear arms against the Parliament, and many of these, like Lord Astley, refused to break their word by taking any part in the second war. So the victors in the Second Civil War showed little mercy to those who had brought war into the land again. On the evening of the surrender of Colchester, Parliamentarians had Sir Charles Lucas and Sir George Lisle shot. Parliamentary authorities sentenced the leaders of the Welsh rebels, Major-General Rowland Laugharne, Colonel John Poyer and Colonel Rice Powel to death, but executed Poyer alone (25 April 1649), having selected him by lot. Of five prominent Royalist peers who had fallen into the hands of Parliament, three, the Duke of Hamilton, the Earl of Holland, and Lord Capel, one of the Colchester prisoners and a man of high character, were beheaded at Westminster on 9 March.
Trial of Charles I for treason.
Charles' secret pacts and encouragement of his supporters to break their parole caused Parliament to debate whether to return the King to power at all. Those who still supported Charles' place on the throne, such as the army leader and moderate Fairfax, tried once more to negotiate with him. Furious that Parliament continued to countenance Charles as a ruler, the Army marched on Parliament and conducted "Pride's Purge" (named after the commanding officer of the operation, Thomas Pride) in December 1648. Troops arrested 45 Members of Parliament and kept 146 out of the chamber. They allowed only 75 Members in, and then only at the Army's bidding. This Rump Parliament received orders to set up, in the name of the people of England, a High Court of Justice for the trial of Charles I for treason. Fairfax, a constitutional monarchist and moderate, refused to participate whatsoever in the trial and resigned as head of the army, allowing Oliver Cromwell to ascend in power.
At the end of the trial the 59 Commissioners (judges) found Charles I guilty of high treason, as a "tyrant, traitor, murderer and public enemy". His beheading took place on a scaffold in front of the Banqueting House of the Palace of Whitehall on 30 January 1649. After the Restoration in 1660, of the surviving regicides not living in exile, nine were executed and most of the rest sentenced to life imprisonment.
Following the execution, Charles, the eldest son was in Jersey where, on 17 February 1649 in the Royal Square in St. Helier, he was publicly proclaimed King Charles II (following the first public proclamation in Edinburgh on 5 February 1649)
Third English Civil War (1649–1651).
Ireland.
Ireland had known continuous war since the rebellion of 1641, with most of the island controlled by the Irish Confederates. Increasingly threatened by the armies of the English Parliament after Charles I's arrest in 1648, the Confederates signed a treaty of alliance with the English Royalists. The joint Royalist and Confederate forces under the Duke of Ormonde attempted to eliminate the Parliamentary army holding Dublin, but their opponents routed them at the Battle of Rathmines (2 August 1649). As the former Member of Parliament Admiral Robert Blake blockaded Prince Rupert's fleet in Kinsale, Oliver Cromwell could land at Dublin on 15 August 1649 with an army to quell the Royalist alliance in Ireland.
Cromwell's suppression of the Royalists in Ireland during 1649 still has a strong resonance for many Irish people. After the siege of Drogheda, the massacre of nearly 3,500 people — comprising around 2,700 Royalist soldiers and 700 others, including civilians, prisoners, and Catholic priests (Cromwell claimed all the men were carrying arms) — became one of the historical memories that has driven Irish-English and Catholic-Protestant strife during the last three centuries. The Parliamentarian conquest of Ireland ground on for another four years until 1653, when the last Irish Confederate and Royalist troops surrendered. The victors confiscated almost all Irish Catholic-owned land in the wake of the conquest and distributed it to the Parliament's creditors, to the Parliamentary soldiers who served in Ireland, and to English people who had settled there before the war.
Scotland.
The execution of Charles I altered the dynamics of the Civil War in Scotland, which had raged between Royalists and Covenanters since 1644. By 1649, the struggle had left the Royalists there in disarray and their erstwhile leader, the Marquess of Montrose, had gone into exile. At first, Charles II encouraged Montrose to raise a Highland army to fight on the Royalist side. However, when the Scottish Covenanters (who did not agree with the execution of Charles I and who feared for the future of Presbyterianism under the new Commonwealth) offered him the crown of Scotland, Charles abandoned Montrose to his enemies. However, Montrose, who had raised a mercenary force in Norway, had already landed and could not abandon the fight. He did not succeed in raising many Highland clans and the Covenanters defeated his army at the Battle of Carbisdale in Ross-shire on 27 April 1650. The victors captured Montrose shortly afterwards and took him to Edinburgh. On 20 May the Scottish Parliament sentenced him to death and had him hanged the next day.
Charles II landed in Scotland at Garmouth in Morayshire on 23 June 1650 and signed the 1638 National Covenant and the 1643 Solemn League and Covenant shortly after coming ashore. With his original Scottish Royalist followers and his new Covenanter allies, King Charles II became the greatest threat facing the new English republic. In response to the threat, Cromwell left some of his lieutenants in Ireland to continue the suppression of the Irish Royalists and returned to England.
He arrived in Scotland on 22 July 1650 and proceeded to lay siege to Edinburgh. By the end of August disease and a shortage of supplies had reduced his army, and he had to order a retreat towards his base at Dunbar. A Scottish army, assembled under the command of David Leslie, tried to block the retreat, but Cromwell defeated them at the Battle of Dunbar on 3 September. Cromwell's army then took Edinburgh, and by the end of the year his army had occupied much of southern Scotland.
In July 1651, Cromwell's forces crossed the Firth of Forth into Fife and defeated the Scots at the Battle of Inverkeithing (20 July 1651). The New Model Army advanced towards Perth, which allowed Charles, at the head of the Scottish army, to move south into England. Cromwell followed Charles into England, leaving George Monck to finish the campaign in Scotland. Monck took Stirling on 14 August and Dundee on 1 September. The next year, 1652, saw the mopping up of the remnants of Royalist resistance, and under the terms of the "Tender of Union", the Scots received 30 seats in a united Parliament in London, with General Monck appointed as the military governor of Scotland.
England.
Although Cromwell's New Model Army had defeated a Scottish army at Dunbar, Cromwell could not prevent Charles II from marching from Scotland deep into England at the head of another Royalist army. The Royalists marched to the west of England because English Royalist sympathies were strongest in that area, but although some English Royalists joined the army, they came in far fewer numbers than Charles and his Scottish supporters had hoped. Cromwell finally engaged and defeated the new king at Worcester on 3 September 1651.
Immediate aftermath.
After the Royalist defeat at Worcester, Charles II escaped, via safe houses and a famous oak tree, to France, and Parliament was left in "de facto" control of England. Resistance continued for a time in the Channels Islands, Ireland and Scotland, but with the pacification of England the resistance elsewhere did not threaten the military supremacy of the New Model Army and its parliamentary paymasters.
Political control.
During the Wars, the Parliamentarians established a number of successive committees to oversee the war-effort. The first of these, the Committee of Safety, set up in July 1642, comprised 15 Members of Parliament. Following the Anglo-Scottish alliance against the Royalists, the Committee of Both Kingdoms replaced the Committee of Safety between 1644 and 1648. Parliament dissolved the Committee of Both Kingdoms when the alliance ended, but its English members continued to meet and became known as the Derby House Committee. A second Committee of Safety then replaced that committee.
Episcopacy.
During the period of the English Civil War, the role of bishops as wielders of political power and as upholders of the established church became a matter of heated political controversy. John Calvin formulated a doctrine of Presbyterianism, which held that in the New Testament the offices of "presbyter" and "episkopos" were identical; he rejected the doctrine of apostolic succession. Calvin's follower John Knox brought Presbyterianism to Scotland when the Scottish church was reformed in 1560. In practice, Presbyterianism meant that committees of lay elders had a substantial voice in church government, as opposed to merely being subjects to a ruling hierarchy.
This vision of at least partial democracy in ecclesiology paralleled the struggles between Parliament and the King. A body within the Puritan movement in the Church of England sought to abolish the office of bishop and remake the Church of England along Presbyterian lines. The Martin Marprelate tracts (1588–89), applying the pejorative name of "prelacy" to the church hierarchy, attacked the office of bishop with satire that deeply offended Elizabeth I and her Archbishop of Canterbury John Whitgift. The vestments controversy also related to this movement, seeking further reductions in church ceremony, and labelling the use of elaborate vestments as "unedifying" and even idolatrous.
King James I, reacting against the perceived contumacy of his Presbyterian Scottish subjects, adopted "No Bishop, no King" as a slogan; he tied the hierarchical authority of the bishop to the absolute authority he sought as king, and viewed attacks on the authority of the bishops as attacks on his own authority. Matters came to a head when King Charles I appointed William Laud as the Archbishop of Canterbury; Laud aggressively attacked the Presbyterian movement and sought to impose the full Anglican liturgy. The controversy eventually led to Laud's impeachment for treason by a bill of attainder in 1645, and subsequent execution. Charles also attempted to impose episcopacy on Scotland; the Scots' violent rejection of bishops and liturgical worship sparked the Bishops' Wars in 1639–1640.
During the height of Puritan power in the Commonwealth and the Protectorate, episcopacy was formally abolished in the Church of England on 9 October 1646. The Church of England remained Presbyterian until the Restoration of the monarchy with Charles II in 1660.
English overseas possessions.
During the period of the English Civil War, the English overseas possessions were highly involved. In the Channel Islands, the island of Jersey and Castle Cornet in Guernsey supported the King until in December 1651 they surrendered with honour.
Casualties.
As usual in wars of this era, disease caused more deaths than combat. There are no accurate figures for these periods, and it is not possible to give a precise overall figure for those killed in battle, as opposed to those who died from disease, or even from a natural decline in population.
Figures for casualties during this period are unreliable, but some attempt has been made to provide rough estimates.
In England, a conservative estimate is that roughly 100,000 people died from war-related disease during the three civil wars. Historical records count 84,830 dead from the wars themselves. Counting in accidents and the two Bishops' wars, an estimate of 190,000 dead is achieved, out of a total population of about five million.
Figures for Scotland are more unreliable and should be treated with greater caution. Casualties include the deaths of prisoners-of-war in conditions that accelerated their deaths, with estimates of 10,000 prisoners not surviving or not returning home (8,000 captured during and immediately after the Battle of Worcester were deported to New England, Bermuda and the West Indies to work for landowners as indentured labourers). There are no figures to calculate how many died from war-related diseases, but if the same ratio of disease to battle deaths from English figures is applied to the Scottish figures, a not unreasonable estimate of 60,000 people is achieved, from a population of about one million.
Figures for Ireland are described as "miracles of conjecture". Certainly the devastation inflicted on Ireland was massive, with the best estimate provided by Sir William Petty, the father of English demography. Petty estimates that 112,000 Protestants and 504,000 Catholics were killed through plague, war and famine, giving an estimated total of 616,000 dead, from a pre-war population of about one and a half million. Although Petty's figures are the best available, they are still acknowledged as being tentative; they do not include the estimate of 40,000 driven into exile, some of whom served as soldiers in European continental armies, while others were sold as indentured servants to New England and the West Indies. Many of those sold to landowners in New England eventually prospered, but many of those sold to landowners in the West Indies were worked to death.
These estimates indicate that England suffered a 3.7% loss of population, Scotland a loss of 6%, while Ireland suffered a loss of 41% of its population. Putting these numbers into the context of other catastrophes helps to understand the devastation to Ireland in particular. The Great Hunger of 1845–1852 resulted in a loss of 16% of the population, while during the Second World War the population of the Soviet Union fell by 16%.
Popular gains.
Ordinary people took advantage of the dislocation of civil society during the 1640s to derive advantages for themselves. The contemporary guild democracy movement won its greatest successes among London's transport workers, notably the Thames watermen. Rural communities seized timber and other resources on the sequestrated estates of royalists and Catholics, and on the estates of the royal family and the church hierarchy. Some communities improved their conditions of tenure on such estates. The old "status quo" began a retrenchment after the end of the First Civil War in 1646, and more especially after the restoration of monarchy in 1660. But some gains were long-term. The democratic element introduced in the watermen's company in 1642, for example, survived, with vicissitudes, until 1827.
Aftermath.
The wars left England, Scotland, and Ireland among the few countries in Europe without a monarch. In the wake of victory, many of the ideals (and many of the idealists) became sidelined. The republican government of the Commonwealth of England ruled England (and later all of Scotland and Ireland) from 1649 to 1653 and from 1659 to 1660. Between the two periods, and due to in-fighting amongst various factions in Parliament, Oliver Cromwell ruled over the Protectorate as Lord Protector (effectively a military dictator) until his death in 1658.
Upon his death, Oliver Cromwell's son Richard became Lord Protector, but the Army had little confidence in him. After seven months the Army removed Richard, and in May 1659 it re-installed the Rump. However, since the Rump Parliament acted as though nothing had changed since 1653 and as though it could treat the Army as it liked, military force shortly afterwards dissolved this, as well. After the second dissolution of the Rump, in October 1659, the prospect of a total descent into anarchy loomed as the Army's pretence of unity finally dissolved into factions.
Into this atmosphere General George Monck, Governor of Scotland under the Cromwells, marched south with his army from Scotland. On 4 April 1660, in the Declaration of Breda, Charles II made known the conditions of his acceptance of the Crown of England. Monck organised the Convention Parliament, which met for the first time on 25 April 1660. On 8 May 1660, it declared that King Charles II had reigned as the lawful monarch since the execution of Charles I in January 1649. Charles returned from exile on 23 May 1660. On 29 May 1660, the populace in London acclaimed him as king. His coronation took place at Westminster Abbey on 23 April 1661. These events became known as the "Restoration".
Although the monarchy was restored, it was still only with the consent of Parliament; therefore, the civil wars effectively set England and Scotland on course to adopt a parliamentary monarchy form of government. This system would result in the outcome that the future Kingdom of Great Britain, formed in 1707 under the Acts of Union, would manage to forestall the kind of often-bloody revolution, typical of European republican movements that followed the Jacobin revolution in 18th century France and the later success of Napoleon, which generally resulted in the total abolition of monarchy. It was no coincidence that the United Kingdom was spared the wave of revolutions that occurred in Europe in the 1840s. Specifically, future monarchs became wary of pushing Parliament too hard, and Parliament effectively chose the line of royal succession in 1688 with the Glorious Revolution and in the 1701 Act of Settlement. After the Restoration, Parliament's factions became political parties (later becoming the Tories and Whigs) with competing views and varying abilities to influence the decisions of their monarchs.
Historiography and explanations.
In the early decades of the 20th century the Whig school was the dominant theoretical view. They explained the Civil War as resulting from a centuries-long struggle between Parliament (especially the House of Commons) and the Monarchy, with Parliament defending the traditional rights of Englishmen, while the Stuart monarchy continually attempted to expand its right to arbitrarily dictate law. The most important Whig historian, S.R. Gardiner, popularised the English Civil War as a 'Puritan Revolution': challenging the repressive Stuart Church, and preparing the way for religious toleration in the Restoration. Thus, Puritanism was the natural ally of a people preserving their traditional rights against arbitrary monarchical power.
The Whig view was challenged and largely superseded by the Marxist school, which became popular in the 1940s, and which interpreted the English Civil War as a bourgeois revolution. According to Marxist historian Christopher Hill:
In the 1970s, revisionist historians challenged both the Whig and the Marxist theories, notably in the 1973 anthology "The Origins of the English Civil War" (Conrad Russell ed.). These historians produced work focused on the minutiae of the years immediately preceding the civil war, thereby returning to the contingency-based historiography of Clarendon's famous contemporary history "History of the Rebellion and Civil Wars in England". This, it was claimed, demonstrated that factional war-allegiance patterns did not fit either Whig or Marxist history. Parliament was not inherently progressive, with the events of 1640 a precursor for the Glorious Revolution, nor did Puritans necessarily ally themselves with Parliamentarians. Many members of the bourgeoisie fought for the King, while many landed aristocrats supported Parliament. Thus, revisionist historians claim to have discredited some Whig and Marxist interpretations of the English Civil War.
From the 1990s, a number of historians discarded and replaced the historical title "English Civil War" with the titles the Wars of the Three Kingdoms and the "British Civil Wars", positing that the civil war in England cannot be understood isolated from events in other parts of Great Britain and Ireland; King Charles I remains crucial, not just as King of England, but also because of his relationship with the peoples of his other realms. For example, the wars began when King Charles I tried imposing an Anglican Prayer Book upon Scotland, and when this was met with resistance from the Covenanters, he needed an army to impose his will. However, this forced him to call an English Parliament to raise new taxes to pay for the army. The English Parliaments were not willing to grant Charles the revenue he needed to pay for the Scottish expeditionary army unless he addressed their grievances. By the early 1640s, Charles was left in a state of near permanent crisis management; often he was not willing to concede enough ground to any one faction to neutralise the threat, and in some circumstances to do so would only antagonise another faction. For example, Charles finally agreed upon terms with the Covenanters in August 1641, but although this might have weakened the position of the English Parliament, the Irish Rebellion of 1641 broke out in October 1641, largely negating the political advantage he had obtained by relieving himself of the cost of the Scottish invasion.
Thomas Hobbes gives a much earlier historical account of the English Civil War in his essay Behemoth, written in 1668 and published in 1681. He reports that the causes of the war were the doctrines of politics and conflicts that arose from science that disputed those political doctrines.
Behemoth offered a uniquely historical and philosophical approach to naming the catalysts for the war. It also served as a political statement to explain why King Charles I was incapable of holding his place of power and maintaining peace in his kingdom.
Specifically, Hobbes analyses the following aspects of English thought during the war (listed in order of his discussions in Behemoth): the opinions of divinity and politics that spurred rebellion; rhetoric and doctrine used by the rebels against the king; and how opinions about "taxation, the conscription of soldiers, and military strategy" affected the outcomes of battles and shifts of sovereignty.
Hobbes offered a unique contribution to historical interpretation of the civil war through his Behemoth by connecting the civil war to the motivations of intellectuals who Hobbes reports caused it by trying to spread certain ideas throughout the nation, largely for the sake of displaying their own wisdom and learning.
Hobbes held the belief that clerical pretensions had contributed significantly to the trouble during the civil war—"whether those of puritan fundamentalists, papal supremacists or divine right Episcopalians" (Sommerville). Hobbes wanted to revoke all of independent power of the clergy and to change the civil system such that they were controlled by the state.
Some scholars suggest that Behemoth has not received its due respect as an academic work, being comparatively overlooked and underrated in the shadow of Leviathan. One factor that may have contributed to its lack of reception as a historical work is that it takes the form of a dialogue. While philosophical dialogues are common, historical ones are not. Other factors that hindered its success include King Charles II's refusing its publication and Hobbes’ chiefly interpretive approach to the historical narrative.
Much can be gleaned about Hobbes as a person from looking at the difficulties he faced while seeking an audience for Behemoth. The essay illuminates a flaw shared by most of Hobbes's political philosophy as well, which is his lack of ability or willingness to empathize with perspectives that largely differed from his own. As his perspective was so much at odds with other views, Hobbes struggled to understand the thinking of most of his potential audience and people in general. For instance, he accredits the Presbyterians and Parliamentarians with "improbably long-planned and wicked ambitions". What’s more, "he hardly understands the orthodox Royalists (he was himself a highly unorthodox Royalist) any better, and he makes only limited concessions of sincerity to the religious feelings of the various parties".
Re-enactments.
Two large historical societies exist, The Sealed Knot and The English Civil War Society, which regularly re-enact events and battles of the Civil War in full period costume.

</doc>
<doc id="9710" url="https://en.wikipedia.org/wiki?curid=9710" title="Elementary algebra">
Elementary algebra

Elementary algebra encompasses some of the basic concepts of algebra, one of the main branches of mathematics. It is typically taught to secondary school students and builds on their understanding of arithmetic. Whereas arithmetic deals with specified numbers, algebra introduces quantities without fixed values, known as variables. This use of variables entails a use of algebraic notation and an understanding of the general rules of the operators introduced in arithmetic. Unlike abstract algebra, elementary algebra is not concerned with algebraic structures outside the realm of real and complex numbers.
The use of variables to denote quantities allows general relationships between quantities to be formally and concisely expressed, and thus enables solving a broader scope of problems. Most quantitative results in science and mathematics are expressed as algebraic equations.
Algebraic notation.
Algebraic notation describes how algebra is written. It follows certain rules and conventions, and has its own terminology. For example, the expression formula_1 has the following components:
<br>
1 : Exponent (power), 2 : Coefficient, 3 : term, 4 : operator, 5 : constant, formula_2 : variables
A "coefficient" is a numerical value, or letter representing a numerical constant, that multiplies a variable (the operator is omitted). A "term" is an addend or a summand, a group of coefficients, variables, constants and exponents that may be separated from the other terms by the plus and minus operators. Letters represent variables and constants. By convention, letters at the beginning of the alphabet (e.g. formula_3) are typically used to represent constants, and those toward the end of the alphabet (e.g. formula_2 and formula_5) are used to represent variables. They are usually written in italics.
Algebraic operations work in the same way as arithmetic operations, such as addition, subtraction, multiplication, division and exponentiation. and are applied to algebraic variables and terms. Multiplication symbols are usually omitted, and implied when there is no space between two variables or terms, or when a coefficient is used. For example, formula_6 is written as formula_7, and formula_8 may be written formula_9.
Usually terms with the highest power (exponent), are written on the left, for example, formula_10 is written to the left of formula_11. When a coefficient is one, it is usually omitted (e.g. formula_12 is written formula_10). Likewise when the exponent (power) is one, (e.g. formula_14 is written formula_15). When the exponent is zero, the result is always 1 (e.g. formula_16 is always rewritten to formula_17). However formula_18, being undefined, should not appear in an expression, and care should be taken in simplifying expressions in which variables may appear in exponents.
Alternative notation.
Other types of notation are used in algebraic expressions when the required formatting is not available, or can not be implied, such as where only letters and symbols are available. For example, exponents are usually formatted using superscripts, e.g. formula_10. In plain text, and in the TeX mark-up language, the caret symbol "^" represents exponents, so formula_10 is written as "x^2". In programming languages such as Ada, Fortran, Perl, Python and Ruby, a double asterisk is used, so formula_10 is written as "x**2". Many programming languages and calculators use a single asterisk to represent the multiplication symbol, and it must be explicitly used, for example, formula_15 is written "3*x".
Concepts.
Variables.
Elementary algebra builds on and extends arithmetic by introducing letters called variables to represent general (non-specified) numbers. This is useful for several reasons.
Evaluating expressions.
Algebraic expressions may be evaluated and simplified, based on the basic properties of arithmetic operations (addition, subtraction, multiplication, division and exponentiation). For example,
Equations.
An equation states that two expressions are equal using the symbol for equality, formula_42 (the equals sign). One of the most well-known equations describes Pythagoras' law relating the length of the sides of a right angle triangle:
This equation states that formula_44, representing the square of the length of the side that is the hypotenuse (the side opposite the right angle), is equal to the sum (addition) of the squares of the other two sides whose lengths are represented by formula_45 and formula_46.
An equation is the claim that two expressions have the same value and are equal. Some equations are true for all values of the involved variables (such as formula_47); such equations are called identities. Conditional equations are true for only some values of the involved variables, e.g. formula_48 is true only for formula_49 and formula_50. The values of the variables which make the equation true are the solutions of the equation and can be found through equation solving.
Another type of equation is an inequality. Inequalities are used to show that one side of the equation is greater, or less, than the other. The symbols used for this are: formula_51 where formula_52 represents 'greater than', and formula_53 where formula_54 represents 'less than'. Just like standard equality equations, numbers can be added, subtracted, multiplied or divided. The only exception is that when multiplying or dividing by a negative number, the inequality symbol must be flipped.
Properties of equality.
By definition, equality is an equivalence relation, meaning it has the properties (a) reflexive (i.e. formula_55), (b) symmetric (i.e. if formula_56 then formula_57) (c) transitive (i.e. if formula_56 and formula_59 then formula_60). It also satisfies the important property that if two symbols are used for equal things, then one symbol can be substituted for the other in any true statement about the first and the statement will remain true. This implies the following properties:
Properties of inequality.
The relations "less than" formula_54 and greater than formula_52 have the property of transitivity:
By reversing the inequation, formula_54 and formula_52 can be swapped, for example:
Substitution.
Substitution is replacing the terms in an expression to create a new expression. Substituting 3 for a in the expression a*5 makes a new expression 3*5 with meaning 15. Substituting the terms of a statement makes a new statement. When the original statement is true independent of the values of the terms, the statement created by substitutions is also true. Hence definitions can be made in symbolic terms and interpreted through substitution: if formula_88, where := means "is defined to equal", substituting 3 for formula_45 informs the reader of this statement that formula_90 means 3*3=9. Often it's not known whether the statement is true independent of the values of the terms, and substitution allows one to derive restrictions on the possible values, or show what conditions the statement holds under. For example, taking the statement x+1=0, if x is substituted with 1, this imples 1+1=2=0, which is false, which implies that if x+1=0 then x can't be 1.
If "x" and "y" are integers, rationals, or real numbers, then "xy"=0 implies "x"=0 or "y"=0. Suppose "abc"=0. Then, substituting "a" for "x" and "bc" for "y", we learn "a"=0 or "bc"=0. Then we can substitute again, letting "x"="b" and "y"="c", to show that if "bc"=0 then "b"=0 or "c"=0. Therefore, if "abc"=0, then "a"=0 or ("b"=0 or "c"=0), so "abc"=0 implies "a"=0 or "b"=0 or "c"=0.
Consider if the original fact were stated as ""ab"=0 implies "a"=0 or "b"=0." Then when we say "suppose "abc"=0," we have a conflict of terms when we substitute. Yet the above logic is still valid to show that if "abc"=0 then "a"=0 or "b"=0 or "c"=0 if instead of letting "a"="a" and "b"="bc" we substitute "a" for "a" and "b" for "bc" (and with "bc"=0, substituting "b" for "a" and "c" for "b"). This shows that substituting for the terms in a statement isn't always the same as letting the terms from the statement equal the substituted terms. In this situation it's clear that if we substitute an expression "a" into the "a" term of the original equation, the "a" substituted does not refer to the "a" in the statement ""ab"=0 implies "a"=0 or "b"=0."
Solving algebraic equations.
The following sections lay out examples of some of the types of algebraic equations that may be encountered.
Linear equations with one variable.
Linear equations are so-called, because when they are plotted, they describe a straight line. The simplest equations to solve are linear equations that have only one variable. They contain only constant numbers and a single variable without an exponent. As an example, consider:
To solve this kind of equation, the technique is add, subtract, multiply, or divide both sides of the equation by the same number in order to isolate the variable on one side of the equation. Once the variable is isolated, the other side of the equation is the value of the variable. This problem and its solution are as follows:
In words: my son's age is 4.
The general form of a linear equation with one variable, can be written as: formula_93
Following the same procedure (i.e. subtract formula_46 from both sides, and then divide by formula_45), the general solution is given by formula_96
Linear equations with two variables.
A linear equation with two variables has many (i.e. an infinite number of) solutions. For example:
This can not be worked out by itself. If I told you my son's age, then there would no longer be two unknowns (variables), and the problem becomes a linear equation with just one variable, that can be solved as described above.
To solve a linear equation with two variables (unknowns), requires two related equations. For example, if I also revealed that:
Now there are two related linear equations, each with two unknowns, which lets us produce a linear equation with just one variable, by subtracting one from the other (called the elimination method):
In other words, my son is aged 12, and as I am 22 years older, I must be 34. In 10 years time, my son will be 22, and I will be twice his age, 44. This problem is illustrated on the associated plot of the equations.
For other ways to solve this kind of equations, see below, System of linear equations.
Quadratic equations.
A quadratic equation is one which includes a term with an exponent of 2, for example, formula_34, and no term with higher exponent. The name derives from the Latin "quadrus", meaning square. In general, a quadratic equation can be expressed in the form formula_101, where formula_45 is not zero (if it were zero, then the equation would not be quadratic but linear). Because of this a quadratic equation must contain the term formula_103, which is known as the quadratic term. Hence formula_104, and so we may divide by formula_45 and rearrange the equation into the standard form
where formula_107 and formula_108. Solving this, by a process known as completing the square, leads to the quadratic formula
where the symbol "±" indicates that both
are solutions of the quadratic equation.
Quadratic equations can also be solved using factorization (the reverse process of which is expansion, but for two linear terms is sometimes denoted foiling). As an example of factoring:
which is the same thing as
It follows from the zero-product property that either formula_113 or formula_114 are the solutions, since precisely one of the factors must be equal to zero. All quadratic equations will have two solutions in the complex number system, but need not have any in the real number system. For example,
has no real number solution since no real number squared equals −1.
Sometimes a quadratic equation has a root of multiplicity 2, such as:
For this equation, −1 is a root of multiplicity 2. This means −1 appears two times, since the equation can be rewritten in factored form as
Complex numbers.
All quadratic equations have two solutions in complex numbers, a category that includes real numbers, imaginary numbers, and sums of real and imaginary numbers. Complex numbers first arise in the teaching of quadratic equations and the quadratic formula. For example, the quadratic equation
has solutions
Since formula_120 is not any real number, both of these solutions for "x" are complex numbers.
Exponential and logarithmic equations.
An exponential equation is one which has the form formula_121 for formula_122, which has solution
when formula_124. Elementary algebraic techniques are used to rewrite a given equation in the above way before arriving at the solution. For example, if
then, by subtracting 1 from both sides of the equation, and then dividing both sides by 3 we obtain
whence
or
A logarithmic equation is an equation of the form formula_129 for formula_122, which has solution
For example, if
then, by adding 2 to both sides of the equation, followed by dividing both sides by 4, we get
whence
from which we obtain
Radical equations.
A radical equation is one that includes a radical sign, which includes square roots, formula_136, cube roots, formula_136, and "n"th roots, formula_136. Recall that an "n"th root can be rewritten in exponential format, so that formula_136 is equivalent to formula_140. Combined with regular exponents (powers), then formula_141 (the square root of formula_11 cubed), can be rewritten as formula_143. So a common form of a radical equation is formula_144 (equivalent to formula_145) where formula_146 and formula_147 are integers. It has real solution(s):
For example, if:
then
System of linear equations.
There are different methods to solve a system of linear equations with two variables.
Elimination method.
An example of solving a system of linear equations is by using the elimination method:
Multiplying the terms in the second equation by 2:
Adding the two equations together to get:
which simplifies to
Since the fact that formula_113 is known, it is then possible to deduce that formula_156 by either of the original two equations (by using "2" instead of formula_11 ) The full solution to this problem is then
Note that this is not the only way to solve this specific system; formula_98 could have been solved before formula_11.
Substitution method.
Another way of solving the same system of linear equations is by substitution.
An equivalent for formula_98 can be deduced by using one of the two equations. Using the second equation:
Subtracting formula_164 from each side of the equation:
and multiplying by −1:
Using this formula_98 value in the first equation in the original system:
Adding "2" on each side of the equation:
which simplifies to
Using this value in one of the equations, the same solution as in the previous method is obtained.
Note that this is not the only way to solve this specific system; in this case as well, formula_98 could have been solved before formula_11.
Other types of systems of linear equations.
Inconsistent systems.
In the above example, a solution exists. However, there are also systems of equations which do not have any solution. Such a system is called inconsistent. An obvious example is
As 0≠2, the second equation in the system has no solution. Therefore, the system has no solution.
However, not all inconsistent systems are recognized at first sight. As an example, let us consider the system 
Multiplying by 2 both sides of the second equation, and adding it to the first one results in
which has clearly no solution.
Undetermined systems.
There are also systems which have infinitely many solutions, in contrast to a system with a unique solution (meaning, a unique pair of values for formula_11 and formula_98) For example:
Isolating formula_98 in the second equation:
And using this value in the first equation in the system:
The equality is true, but it does not provide a value for formula_11. Indeed, one can easily verify (by just filling in some values of formula_11) that for any formula_11 there is a solution as long as formula_186. There is an infinite number of solutions for this system.
Over- and underdetermined systems.
Systems with more variables than the number of linear equations are called underdetermined. Such a system, if it has any solutions, does not have a unique one but rather an infinitude of them. An example of such a system is
When trying to solve it, one is led to express some variables as functions of the other ones if any solutions exist, but cannot express "all" solutions numerically because there are an infinite number of them if there are any.
A system with a greater number of equations than variables is called overdetermined. If an overdetermined system has any solutions, necessarily some equations are linear combinations of the others.

</doc>
<doc id="9712" url="https://en.wikipedia.org/wiki?curid=9712" title="ERP">
ERP

ERP or Erp may refer to:

</doc>
<doc id="9713" url="https://en.wikipedia.org/wiki?curid=9713" title="Ernest Thayer">
Ernest Thayer

Ernest Lawrence Thayer (August 14, 1863 – August 21, 1940) was an American writer and poet who wrote "Casey at the Bat," "the single most famous baseball poem ever written" according to the Baseball Almanac, and "the nation’s best-known piece of comic verse—a ballad that began a native legend as colorful and permanent as that of Johnny Appleseed or Paul Bunyan."
Biography.
Thayer was born in Lawrence, Massachusetts and raised in Worcester. He graduated "magna cum laude" in philosophy from Harvard in 1885, where he was editor of the "Harvard Lampoon" and member of the Hasty Pudding theatrical club. William Randolph Hearst, a friend from both the Pudding and Lampoon, hired Thayer as humor columnist for the "San Francisco Examiner" 1886–88.
Thayer's last piece, dated June 3, 1888, was a ballad entitled "Casey" ("Casey at the Bat") which made him "a prize specimen of the one-poem poet" according to American Heritage.
It took several months after its publication for the poem to make Thayer famous, since he was hardly the boastful type and had signed the June 24 poem with the nickname "Phin" which he had used since his time on the "Lampoon". Two mysteries remain about the poem: whether anyone or anyplace was the real-life Casey and Mudville, and, if so, their actual identities. On March 31, 2007, Katie Zezima of "The New York Times" penned an article called "In 'Casey' Rhubarb, 2 Cities Cry 'Foul!'" on the competing claims of two towns to such renown: Stockton, California, and Holliston, Massachusetts.
On the possible model for Casey, Thayer dismissed the notion that any single living baseball player was an influence. However, late 1880s Boston star Mike "King" Kelly is odds-on the most likely model for Casey's baseball situations. Besides being a native of a town close to Boston, Thayer, as a "San Francisco Examiner" baseball reporter in the offseason of 1887–88, covered exhibition games featuring Kelly. In November 1887, some of his reportage about a Kelly at-bat has the same ring as Casey's famous at-bat in the poem. A 2004 book by Howard W. Rosenberg, "Cap Anson 2: The Theatrical and Kingly Mike Kelly: U.S. Team Sport's First Media Sensation and Baseball's Original Casey at the Bat," reprints a 1905 Thayer letter to a Baltimore scribe who was asking about the poem's roots. In the letter, Thayer singled out Kelly (d. 1894), as having shown "impudence" in claiming to have written it. Rosenberg argues that if Thayer still felt offended, Thayer may have steered later comments away from connecting Kelly to it. Kelly had also performed in vaudeville, and recited the poem dozens of times, possibly, to Thayer's dismay, butchering it. Incidentally, the first public performance of the poem was on August 14, 1888, by actor De Wolf Hopper, on Thayer's 25th birthday.
Thayer's recitation of it at a Harvard class reunion in 1895 may seem trivial except that it helps solve the mystery, which lingered into the 20th century, of who had written it. In the mid-1890s, Thayer contributed several other comic poems for Hearst's "New York Journal" and then turned to overseeing his family's mills in Worcester full-time.
Thayer moved to Santa Barbara in 1912, where he married Rosalind Buel Hammett and retired. He died in 1940, at age 77.
The New York Times' obituary of Thayer on August 22, 1940, p. 19 quotes comedian DeWolf Hopper, who helped make the poem famous: 

</doc>
<doc id="9714" url="https://en.wikipedia.org/wiki?curid=9714" title="List of English-language poets">
List of English-language poets

This is a list of English-language poets, who wrote or write much of their poetry in English. 

</doc>
<doc id="9717" url="https://en.wikipedia.org/wiki?curid=9717" title="Excalibur">
Excalibur

Excalibur or Caliburn is the legendary sword of King Arthur, sometimes attributed with magical powers or associated with the rightful sovereignty of Great Britain. Sometimes Excalibur and the Sword in the Stone (the proof of Arthur's lineage) are said to be the same weapon, but in most versions they are considered separate. The sword was associated with the Arthurian legend very early. In Welsh, the sword is called Caledfwlch; in Cornish, the sword is called Calesvol; in Breton, the sword is called Kaledvoulc'h; in Latin, the sword is called Caliburnus.
Forms and etymologies.
The name "Excalibur" ultimately comes from the ancestor of Welsh "Caledfwlch" (and Breton "Kaledvoulc'h", Middle Cornish "Calesvol") which is a compound of ' "hard" and ' "breach, cleft". Caledfwlch appears in several early Welsh works, including the poem "Preiddeu Annwfn" (though it is not directly named - but only alluded to - here) and the prose tale "Culhwch and Olwen", a work associated with the "Mabinogion" and written perhaps around 1100. The name was later used in Welsh adaptations of foreign material such as the "Brut"s (chronicles), which were based on Geoffrey of Monmouth. It is often considered to be related to the phonetically similar "Caladbolg", a sword borne by several figures from Irish mythology, although a borrowing of "Caledfwlch" from Irish "Caladbolg" has been considered unlikely by Rachel Bromwich and D. Simon Evans. They suggest instead that both names "may have similarly arisen at a very early date as generic names for a sword"; this sword then became exclusively the property of Arthur in the British tradition. 
Geoffrey of Monmouth, in his "Historia Regum Britanniae" ("The History of the Kings of Britain", c. 1136), Latinised the name of Arthur's sword as "Caliburnus" (potentially influenced by the Medieval Latin spelling "calibs" of Classical Latin "chalybs", from Greek "chályps" 'χάλυψ' "steel") and states that it was forged in the Isle of Avalon. Most Celticists consider Geoffrey's "Caliburnus" to be derivative of a lost Old Welsh text in which "bwlch" had not yet been lenited to "fwlch". In Old French sources this then became "Escalibor", "Excalibor" and finally the familiar "Excalibur".
Geoffrey Gaimar, in his Old French "L'Estoire des Engles" (1134-1140), mentions Arthur and his sword: "this Constantine was the nephew of Arthur, who had the sword Caliburc" ("Cil Costentin li niès Artur, Ki out l'espée Caliburc").
In Wace's "Roman de Brut" (c. 1150-1155), an Old French translation and versification of Geoffrey of Monmouth's "Historia Regum Britanniae", the sword is called "Calabrum", "Callibourc", "Chalabrun", and "Calabrun" (with alternate spellings such as "Chalabrum", "Calibore", "Callibor", "Caliborne", "Calliborc", and "Escaliborc", found in various manuscripts of the "Brut").
In Chrétien de Troyes' late 12th century Old French "Perceval", Gawain carries the sword "Escalibor" and it is stated, "for at his belt hung Excalibor, the finest sword that there was, which sliced through iron as through wood" ("Qu'il avoit cainte Escalibor, la meillor espee qui fust, qu'ele trenche fer come fust"). This statement was probably picked up by the author of the "Estoire Merlin", or Vulgate Merlin, where the author (who was fond of fanciful folk etymologies) asserts that Escalibor "is a Hebrew name which means in French 'cuts iron, steel, and wood'" ("c'est non Ebrieu qui dist en franchois trenche fer & achier et fust"; note that the word for "steel" here, achier, also means "blade" or "sword" and comes from medieval Latin "aciarium", a derivative of "acies" "sharp", so there is no direct connection with Latin "chalybs" in this etymology). It is from this fanciful etymological musing that Thomas Malory got the notion that Excalibur meant "cut steel" ("'the name of it,' said the lady, 'is Excalibur, that is as moche to say, as Cut stele'").
Excalibur and the Sword in the Stone.
In Arthurian romance, a number of explanations are given for Arthur's possession of Excalibur. In Robert de Boron's "Merlin", Arthur obtained the British throne by pulling a sword from a stone. (The story of the Sword in the Stone has an analogue in some versions of the story of Sigurd, the Norse proto-Siegfried, whose father, Sigmund, draws the sword Gram out of the tree Barnstokkr where it is embedded by the Norse god Odin.) In this account, the act could not be performed except by "the true king," meaning the divinely appointed king or true heir of Uther Pendragon. This sword is thought by many to be the famous Excalibur, and its identity is made explicit in the later Prose "Merlin", part of the Lancelot-Grail cycle. This version also appears in the 1938 Arthurian novel "The Sword in the Stone" by British author T. H. White, and the Disney adaptation. They both quote the line from Thomas Malory in the 15th century; "Whoso Pulleth Out This Sword of this Stone and Anvil, is Rightwise King Born of all England". The challenge of drawing a sword from a stone also appears in the Arthurian legends of Galahad, whose achievement of the task indicates that he is destined to find the Holy Grail. 
However, in what is called the Post-Vulgate Cycle, Excalibur was given to Arthur by the Lady of the Lake sometime after he began to reign. She calls the sword "Excalibur, that is as to say as Cut-steel." In the Vulgate "Mort Artu", Arthur orders Griflet to throw the sword into the enchanted lake. After two failed attempts (as he felt such a great sword should not be thrown away), he finally complies with the wounded king's request and a hand emerges from the lake to catch it, a tale which becomes attached to Bedivere instead in Malory and the English tradition. Malory records both versions of the legend in his "Le Morte d'Arthur", naming both swords as Excalibur. 
History.
In Welsh legend, Arthur's sword is known as "Caledfwlch". In "Culhwch and Olwen", it is one of Arthur's most valuable possessions and is used by Arthur's warrior Llenlleawg the Irishman to kill the Irish king Diwrnach while stealing his magical cauldron. Irish mythology mentions a weapon "Caladbolg", the sword of Fergus mac Róich. Caladbolg was also known for its incredible power and was carried by some of Ireland's greatest heroes. The name, which can also mean "hard cleft" in Irish, appears in the plural, "caladbuilc", as a generic term for "great swords" in "Togail Troi" ("The Destruction of Troy"), the 10th century Irish translation of the classical tale.
Though not named as Caledfwlch, Arthur's sword is described vividly in "The Dream of Rhonabwy" one of the tales associated with the "Mabinogion":
In the late 15th/early 16th-century Middle Cornish play Beunans Ke, Arthur's sword is called "Calesvol", which is etymologically an exact Middle Cornish cognate of the Welsh "Caledfwlch". It is unclear if the name was borrowed from the Welsh (if so, it must have been an early loan, for phonological reasons), or represents an early, pan-Brittonic traditional name for Arthur's sword.
Geoffrey's "Historia" is the first non-Welsh source to speak of the sword. Geoffrey says the sword was forged in Avalon and Latinises the name "Caledfwlch" as "Caliburnus". When his influential pseudo-history made it to Continental Europe, writers altered the name further until it finally took on the popular form "Excalibur" (various spellings in the medieval Arthurian Romance and Chronicle tradition include: Calabrun, Calabrum, Calibourne, Callibourc, Calliborc, Calibourch, Escaliborc, and Escalibor). The legend was expanded upon in the Vulgate Cycle, also known as the Lancelot-Grail Cycle, and in the Post-Vulgate Cycle which emerged in its wake. Both included the work known as the "Prose Merlin", but the Post-Vulgate authors left out the "Merlin" continuation from the earlier cycle, choosing to add an original account of Arthur's early days including a new origin for Excalibur.
In several early French works, such as Chrétien de Troyes' "Perceval, the Story of the Grail" and the Vulgate "Lancelot Proper" section, Excalibur is used by Gawain, Arthur's nephew and one of his best knights. This is in contrast to later versions, where Excalibur belongs solely to the king.
Attributes.
In many versions, Excalibur's blade was engraved with phrases on opposite sides: "Take me up" and "Cast me away" (or similar). In addition, when Excalibur was first drawn, in the first battle testing Arthur's sovereignty, its blade blinded his enemies. Thomas Malory writes: "thenne he drewe his swerd Excalibur, but it was so breyght in his enemyes eyen that it gaf light lyke thirty torchys."
Excalibur's scabbard was said to have powers of its own. Loss of blood from injuries, for example, would not kill the bearer. In some tellings, wounds received by one wearing the scabbard did not bleed at all. The scabbard is stolen by Morgan le Fay in revenge for the death of her beloved Accolon and thrown into a lake, never to be found again.
Nineteenth century poet Alfred, Lord Tennyson, described the sword in full Romantic detail in his poem "Morte d'Arthur", later rewritten as "The Passing of Arthur", one of the "Idylls of the King":
Arthur's other weapons.
Excalibur is by no means the only weapon associated with Arthur, nor the only sword. Welsh tradition also knew of a dagger named Carnwennan and a spear named Rhongomyniad that belonged to him. Carnwennan ("Little White-Hilt") first appears in "Culhwch and Olwen", where it was used by Arthur to slice the Black Witch in half. Rhongomyniad ("spear" + "striker, slayer") is also first mentioned in "Culhwch", although only in passing; it appears as simply "Ron" ("spear") in Geoffrey's "Historia".
In the "Alliterative Morte Arthure", a Middle English poem, there is mention of Clarent, a sword of peace meant for knighting and ceremonies as opposed to battle, which was stolen and then used to kill Arthur by Mordred. The "Prose Lancelot" of the "Vulgate Cycle" mentions a sword called Seure, which belonged to the king but was used by Lancelot in one battle.
Similar weapons.
There are other similar weapons described in other mythologies. In particular, Claíomh Solais, which is an Irish term meaning "Sword of Light", or "Shining Sword", which appears in a number of orally transmitted Irish folk-tales.

</doc>
<doc id="9719" url="https://en.wikipedia.org/wiki?curid=9719" title="Eight-bar blues">
Eight-bar blues

In music, an eight-bar blues is a typical blues chord progression, "the second most common blues form," "common to folk, rock, and jazz forms of the blues," taking eight 4/4 or 12/8 bars to the verse.
Examples include "Sitting on Top of the World" and "Key to the Highway", "Trouble in Mind" and "Stagolee". "Heartbreak Hotel", "How Long Blues", "Ain't Nobody's Business", "Cherry Red", and "Get a Haircut" are all eight-bar blues standards.
One variant using this progression is to couple one eight-bar blues melody with a different eight-bar blues bridge to create a blues variant of the standard 32-bar song. "Walking By Myself", "I Want a Little Girl" and "(Romancing) In The Dark" are examples of this form. See also blues ballad.
Eight bar blues progressions have more variations than the more rigidly defined twelve bar format. The move to the IV chord usually happens at bar 3 (as opposed to 5 in twelve bar). However, "the I chord moving to the V chord right away, in the second measure, is a characteristic of the eight-bar blues."
In the following examples each box represents a 'bar' of music (the specific time signature is not relevant). The chord in the box is played for the full bar. If two chords are in the box they are each played for half a bar, etc. The chords are represented as scale degrees in Roman numeral analysis. Roman numerals are used so the musician may understand the progression of the chords regardless of the key it is played in.
"Worried Life Blues" (probably the most common eight bar blues progression):
"Heartbreak Hotel" (variation with the I on the first half):
J. B. Lenoir's "Slow Down" and "Key to the Highway" (variation with the V at bar 2):
"Get a Haircut" by George Thorogood (simple progression):
Jimmy Rogers' "Walkin' By Myself" (somewhat unorthodox example of the form):
Howlin Wolf's version of "Sitting on Top of the World" is actually a 9 bar blues that adds an extra "V" chord at the end of the progression. The song uses movement between major and dominant 7th and major and minor fourth:
The first four bar progression used by Wolf is also used in Nina Simone's 1965 version of Trouble in Mind, but with a more uptempo beat than Sitting on Top of the World:
The progression may be created by dropping the first four bars from the twelve-bar blues, as in the solo section of Bonnie Raitt's "Love Me Like a Man" and Buddy Guy's "Mary Had a Little Lamb":
There are at least a few very successful songs using somewhat unique chord progressions as well. For example, the song Ain't Nobody's business as performed by Freddie King at least, uses a I III IV iv progression in each of the first four bars. The same four bar progression is used by the band Radiohead to make up the bulk of the song 'Creep'.
(The same chord progression can also be called a sixteen-bar blues, if each symbol above is taken to be a half note in 2/2 or 4/4 time—blues has not traditionally been associated with notation, so its form becomes a bit slippery when written down.) For example "Nine Pound Hammer". Ray Charles's original instrumental "Sweet Sixteen Bars" is another example.

</doc>
<doc id="9720" url="https://en.wikipedia.org/wiki?curid=9720" title="Echidna (disambiguation)">
Echidna (disambiguation)

Echidna is the common name for a family of Australian mammals.
Other.
Echidna may also refer to:

</doc>
<doc id="9723" url="https://en.wikipedia.org/wiki?curid=9723" title="Edward Waring">
Edward Waring

Edward Waring (15 August 1798) was an English mathematician. He entered Magdalene College, Cambridge as a sizar and became Senior wrangler in 1757. He was elected a Fellow of Magdalene and in 1760 Lucasian Professor of Mathematics, holding the chair until his death. He made the assertion known as Waring's problem without proof in his writings "Meditationes Algebraicae". Waring was elected a Fellow of the Royal Society in 1763 and awarded the Copley Medal in 1784.
Early years.
Waring was the eldest son of John and Elizabeth Waring, a prosperous farming couple. He received his early education in Shrewsbury School under a Mr Hotchkin and was admitted as a sizar at Magdalene College, Cambridge, on 24 March 1753, being also Millington exhibitioner. His extraordinary talent for mathematics was recognised from his early years in Cambridge. In 1757 he graduated BA as senior wrangler and on 24 April 1758 was elected to a fellowship at Magdalene. He belonged to the Hyson Club, whose members included William Paley.
Career.
At the end of 1759 Waring published the first chapter of "Miscellanea Analytica". On 28 January the next year he was appointed Lucasian professor of mathematics, one of the highest positions in Cambridge. William Samuel Powell, then tutor in St John's College, Cambridge opposed Waring's election and instead supported the candidacy of William Ludlam. In the polemic with Powell, Waring was backed by John Wilson. In fact Waring was very young and did not hold the MA, necessary for qualifying for the Lucasian chair, but this was granted him in 1760 by royal mandate. In 1762 he published the full "Miscellanea Analytica", mainly devoted to the theory of numbers and algebraic equations. In 1763 he was elected to the Royal Society. He was awarded its Copley Medal in 1784 but withdrew from the society in 1795, after he had reached sixty, 'on account of i age'. Waring was also a member of the academies of sciences of Göttingen and Bologna. In 1767 he took an MD degree, but his activity in medicine was quite limited. He carried out dissections with Richard Watson, professor of chemistry and later bishop of Llandaff. From about 1770 he was physician at Addenbrooke's Hospital at Cambridge, and he also practised at St Ives, Huntingdonshire, where he lived for some years after 1767. His career as a physician was not very successful since he was seriously short-sighted and a very shy man.
Personal life.
Waring had a younger brother, Humphrey, who obtained a fellowship at Magdalene in 1775. In 1776 Waring married Mary Oswell, sister of a draper in Shrewsbury; they moved to Shrewsbury and then retired to Plealey, 8 miles out of the town, where Waring owned an estate of 215 acres in 1797
Work.
Waring wrote a number of papers in the "Philosophical Transactions of the Royal Society", dealing with the resolution of algebraic equations, number theory, series, approximation of roots, interpolation, the geometry of conic sections, and dynamics. The "Meditationes Algebraicae" (1770), where many of the results published in "Miscellanea Analytica" were reworked and expanded, was described by Joseph-Louis Lagrange as 'a work full of excellent researches'. In this work Waring published many theorems concerning the solution of algebraic equations which attracted the attention of continental mathematicians, but his best results are in number theory. Included in this work was the so-called Goldbach conjecture (every even integer is the sum of two primes), and also the following conjecture: every odd integer is a prime or the sum of three primes. Lagrange had proved that every positive integer is the sum of not more than four squares; Waring suggested that every positive integer is either a cube or the sum of not more than nine cubes. He also advanced the hypothesis that every positive integer is either a biquadrate or the sum of not more than nineteen biquadrates. These hypotheses form what is known as Waring's problem. He also published a theorem, due to his friend John Wilson, concerning prime numbers; it was later proved rigorously by Lagrange.
In "Proprietates Algebraicarum Curvarum" (1772) Waring reissued in a much revised form the first four chapters of the second part of "Miscellanea Analytica". He devoted himself to the classification of higher plane curves, improving results obtained by Isaac Newton, James Stirling, Leonhard Euler, and Gabriel Cramer. In 1794 he published a few copies of a philosophical work entitled "An Essay on the Principles of Human Knowledge", which were circulated among his friends.
Waring's mathematical style is highly analytical. In fact he criticised those British mathematicians who adhered too strictly to geometry. It is indicative that he was one of the subscribers of John Landen's "Residual Analysis" (1764), one of the works in which the tradition of the Newtonian fluxional calculus was more severely criticised. In the preface of "Meditationes Analyticae" Waring showed a good knowledge of continental mathematicians such as Alexis Clairaut, Jean le Rond d'Alembert, and Euler. He lamented the fact that in Great Britain mathematics was cultivated with less interest than on the continent, and clearly desired to be considered as highly as the great names in continental mathematics—there is no doubt that he was reading their work at a level never reached by any other eighteenth-century British mathematician. Most notably, at the end of chapter three of "Meditationes Analyticae" Waring presents some partial fluxional equations (partial differential equations in Leibnizian terminology); such equations are a mathematical instrument of great importance in the study of continuous bodies which was almost completely neglected in Britain before Waring's researches. One of the most interesting results in "Meditationes Analyticae" is a test for the convergence of series generally attributed to d'Alembert (the 'ratio test'). The theory of convergence of series (the object of which is to establish when the summation of an infinite number of terms can be said to have a finite 'sum') was not much advanced in the eighteenth century.
Waring's work was known both in Britain and on the continent, but it is difficult to evaluate his impact on the development of mathematics. His work on algebraic equations contained in "Miscellanea Analytica" was translated into Italian by Vincenzo Riccati in 1770. Waring's style is not systematic and his exposition is often obscure. It seems that he never lectured and did not habitually correspond with other mathematicians. After Jérôme Lalande in 1796 observed, in "Notice sur la vie de Condorcet", that in 1764 there was not a single first-rate analyst in England, Waring's reply, published after his death as 'Original letter of Dr Waring' in the "Monthly Magazine", stated that he had given 'somewhere between three and four hundred new propositions of one kind or another'.
Death.
During his last years he sank into a deep religious melancholy, and a violent cold caused his death, in Plealey, on 15 August 1798. He was buried in the churchyard at Fitz, Shropshire.

</doc>
<doc id="9724" url="https://en.wikipedia.org/wiki?curid=9724" title="Eden Phillpotts">
Eden Phillpotts

Eden Phillpotts (4 November 1862 – 29 December 1960) was an English author, poet and dramatist. He was born in Mount Abu, British India, educated in Plymouth, Devon, and worked as an insurance officer for 10 years before studying for the stage and eventually becoming a writer.
He co-wrote two plays with his daughter Adelaide Phillpotts, "The Farmer's Wife" (1924) and "Yellow Sands" (1926); but is best known as the author of many novels, plays and poems about Dartmoor. His Dartmoor cycle of 18 novels and two volumes of short stories still has many avid readers despite the fact that many titles are out of print.
Life and character.
Phillpotts was for many years the President of the Dartmoor Preservation Association and cared passionately about the conservation of Dartmoor. He was also an agnostic and a supporter of the Rationalist Press Association.
Phillpotts was a friend of Agatha Christie, who was an admirer of his work and a regular visitor to his home. Jorge Luis Borges was another admirer. Borges mentioned him numerous times, wrote at least two reviews of his novels, and included him in his "Personal Library", a collection of works selected to reflect his personal literary preferences.
Phillpotts died in Broadclyst.
Writings.
Phillpotts wrote a great many books with a Dartmoor setting. One of his novels, "Widecombe Fair", inspired by an annual fair at the village of Widecombe-in-the-Moor, provided the scenario for his comic play "The Farmer's Wife". It went on to become a silent movie of the same name, directed by Alfred Hitchcock and filmed in 1927. The cast included Jameson Thomas, Lillian Hall-Davis, Gordon Harker and Gibb McLaughlin.
He also wrote a series of novels, each set against the background of a different trade or industry. Titles include: "Brunel's Tower" (a pottery) and "Storm in a Teacup" (hand-papermaking).
Among his other works is "The Grey Room, " the plot of which is centered on a haunted room in an English manor house. He also wrote a number of other mystery novels, both under his own name and the pseudonym Harrington Hext. Titles include: "The Thing at Their Heels", "The Red Redmaynes", "The Monster", "The Clue from the Stars", and "The Captain's Curio".
"The Human Boy" was a collection of schoolboy stories in the same genre as Rudyard Kipling's Stalky & Co., though different in mood and style.
Although mainly a novelist, he also wrote several plays.
Late in his long writing career he wrote a few books of interest to science fiction and fantasy readers, the most noteworthy being "Saurus", which involves an alien reptilian observing human life.
Quality of writing.
Eric Partridge praised the immediacy and impact of his dialect writing.

</doc>
<doc id="9725" url="https://en.wikipedia.org/wiki?curid=9725" title="Ecuador–United States relations">
Ecuador–United States relations

The Republic of Ecuador and the United States of America maintained close ties based on mutual interests in maintaining democratic institutions; combating cannabis and cocaine; building trade, investment, and financial ties; cooperating in fostering Ecuador's economic development; and participating in inter-American organizations. Ties are further strengthened by the presence of an estimated 150,000-200,000 Ecuadorians living in the United States and by 24,000 U.S. citizens visiting Ecuador annually, and by approximately 15,000 U.S. citizens residing in Ecuador. The United States assists Ecuador's economic development directly through the Agency for International Development (USAID) program in Ecuador and through multilateral organizations such as the Inter-American Development Bank and the World Bank. In addition, the U.S. Peace Corps operates a sizable program in Ecuador. More than 100 U.S. companies are doing business in Ecuador.
Relations between the two nations have been strained following Julian Assange's bid to seek political asylum in the Ecuadorian embassy in London following repeated claims that the US government was pursuing his extradition due to his work with Wikileaks.
History.
Both nations are signatories of the Inter-American Treaty of Reciprocal Assistance (the "Rio Treaty") of 1947, the Western Hemisphere's regional mutual security treaty. Ecuador shares U.S. concern over increasing narcotrafficking and international terrorism and has energetically condemned terrorist actions, whether directed against government officials or private citizens. The government has maintained Ecuador virtually free of coca production since the mid-1980s and is working to combat money laundering and the transshipment of drugs and chemicals essential to the processing of cocaine.
Ecuador and the U.S. agreed in 1999 to a 10-year arrangement whereby U.S. military surveillance aircraft could use the airbase at Manta, Ecuador, as a "Forward Operating Location" to detect drug trafficking flights through the region. The arrangement expired in 2009; current president Rafael Correa vowed not to renew it, and since then the Ecuador has not had any foreign military facilities in the country.
In fisheries issues, the United States claims jurisdiction for the management of coastal fisheries up to 200 mile (370 km) from its coast, but excludes highly migratory species; Ecuador, on the other hand, claims a 200-mile (370-km) territorial sea, and imposes license fees and fines on foreign fishing vessels in the area, making no exceptions for catches of migratory species. In the early 1970s, Ecuador seized about 100 foreign-flag vessels (many of them U.S.) and collected fees and fines of more than $6 million. After a drop-off in such seizures for some years, several U.S. tuna boats were again detained and seized in 1980 and 1981.
The U.S. Magnuson Fishery Conservation and Management Act then triggered an automatic prohibition of U.S. imports of tuna products from Ecuador. The prohibition was lifted in 1983, and although fundamental differences between U.S. and Ecuadorian legislation still exist, there is no current conflict. During the period that has elapsed since seizures which triggered the tuna import ban, successive Ecuadorian governments have declared their willingness to explore possible solutions to this problem with mutual respect for longstanding positions and principles of both sides. The election of Rafael Correa in October 2006, has strained relations between the two countries and relations have since been fraught with tension. Rafael Correa is quite critical of U.S. foreign polcy.
In April 2011, relations between Ecuador and the United States soured particularly after Ecuador expelled the U.S. ambassador after a leaked diplomatic cable was shown accusing president Correa of knowingly ignoring police corruption. In reciprocation, the Ecuadorian ambassador Luis Gallegos was expelled from the United States.
In 2013, when Ecuador unilaterally pulled out of a preferential trade pact with the United States over claiming the U.S. used it as blackmail in regards to the asylum request of Edward Snowden, relations between Ecuador and the United States reached an all-time low. The pact offered Ecuador 23 million USD, which it offered to the U.S. for human rights training. Tariff free imports had been offered to Ecuador in exchange for drug elimination efforts.
Education.
American schools in Ecuador:
 

</doc>
<doc id="9727" url="https://en.wikipedia.org/wiki?curid=9727" title="Eight-ball">
Eight-ball

Eight-ball (often spelled 8-ball or eightball, and sometimes called solids and stripes, spots and stripes in the UK or, more rarely, bigs and littles/smalls or highs and lows) is a pool (pocket billiards) game popular in much of the world, and the subject of international professional and amateur competition. Played on a pool table with six pockets, the game is so universally known in some countries that beginners are often unaware of other pool games and believe the word "pool" itself refers to eight-ball. The game has numerous variations, including Alabama eight-ball, crazy eight, last pocket, misery, Missouri, 1 and 15 in the sides, rotation eight ball, soft eight, and others. Standard eight-ball is the second most competitive professional pool game, after nine-ball and for the last several decades ahead of straight pool.
Eight-ball is played with cue sticks and 16 balls: a , and 15 consisting of seven striped balls, seven solid-colored balls and the black 8 ball. After the balls are scattered with a , the players are assigned either the group of solid balls or the stripes once a ball from a particular group is legally pocketed. The ultimate object of the game is to legally pocket the eight ball in a called pocket, which can only be done after all of the balls from a player's assigned group have been cleared from the table.
History.
The game of eight-ball is derived from an earlier game invented around 1900 (first recorded in 1908) in the United States and initially popularized under the name "B.B.C. Co. Pool" (a name that was still in use as late as 1925) by the Brunswick-Balke-Collender Company. This forerunner game was played with seven and seven , a , and the cue ball. Today, numbered and are preferred in most of the world, though the British-style offshoot, blackball, uses the traditional colors (as did early televised "casino" tournaments in the U.S.). The game had relatively simple rules compared to today and was not added (under any name) to an official rule book (i.e., one published by a national or international sport governing body) until 1940.
Standardized "Rules of Play".
American-style eight-ball rules are played around the world by professionals, and in many amateur leagues. Nevertheless, the rules for eight-ball may be the most contested of any billiard game. There are several competing sets of "official" rules. The non-profit World Pool-Billiard Association (WPA) – with national affiliates around the world, some of which long pre-date the WPA, such as the Billiard Congress of America (BCA) – promulgates standardized rules as "Pool Billiards – The Rules of Play" for amateur and professional play.
Meanwhile, many amateur leagues, such as the American Poolplayers Association (APA) / Canadian Poolplayers Association (CPA), the Valley National Eight-ball Association (VNEA, international in scope despite its historic name), and the BCA Pool League (BCAPL), use their own rulesets (most of them at least loosely based on the WPA/BCA version), while millions of individuals play informally using colloquial rules which vary not only from area to area but even from venue to venue ("house rules").
A summary of the international rules follows "(see the WPA/BCA or other leagues' published rules, which conflict on minor points, for more details)".
Equipment.
The table's playing surface is approximately (regulation size), though some leagues and tournaments using the World Standardized Rules may allow smaller sizes, down to , and early-20th-century models are occasionally also used. WPA professional competition is generally on regulation tables, while the amateur league championships of various leagues, including ACS, BCAPL, VNEA, and APA, use the 7-foot tables, to fit hundreds of them into the hosting venue.
There are seven numbered 1 through 7, seven numbered 9 through 15, an , and a . The balls are usually colored as follows:
Setup.
To start the game, the are placed in a triangular rack. The base of the rack is parallel to the (the short end of the pool table) and positioned so the apex ball of the rack is located on the . The balls in the rack are ideally placed so that they are all in contact with one another; this is accomplished by pressing the balls together from the back of the rack toward the apex ball. The order of the balls should be random, with the exceptions of the 8 ball, which must be placed in the center of the rack (i.e., the middle of the third row), and the two back corner balls one of which must be a stripe and the other a solid. The cue ball is placed anywhere the breaker desires inside the . 
Break.
One person is chosen (by a predetermined method, e.g., coin flip, win or loss of previous game, or ) to shoot first, using the cue ball to the object-ball rack apart. If the shooter who breaks fails to make a legal break (usually defined as at least four balls hitting cushions or an object ball being pocketed), then the opponent can call for a and become the breaker, or elect to play from the current position of the balls.
According to World Standardized Rules, if the 8 ball is pocketed on the break without , the breaker may ask for a re-rack and break again, or have the 8 ball and continue shooting with the balls as they . If the breaker scratches (pockets the cue ball) while pocketing the 8 ball on the break, the incoming player may call for a re-rack and break, or have the 8 ball spotted and begin shooting with behind the , with the balls as they lie. "(For regional amateur variations, such as pocketing the 8 on the break being an instant win or loss, see "Informal rule variations", below.)"
Turns.
A player (or team) will continue to shoot until committing a , or failing to legally pocket an object ball on a non-foul shot (whether or not). Thereupon it is the turn of the opposing player(s). Play alternates in this manner for the remainder of the game. Following a foul, the incoming player has anywhere on the table, unless the foul occurred on the break shot, as noted previously.
Selection of the target group.
At some point in the game, one of the players can select balls 1–7 (the "solids") or balls 9–15 (the "stripes") as their group of object balls by legally pocketing a ball from one category. The other player is assigned to the other group. Once the target groups have been assigned, they remain fixed throughout the remainder of the game.
Pocketing the 8 ball.
Once all of a player's or team's group of object balls are pocketed, they may attempt to sink the 8 ball. To win, the player (or team) must first designate which pocket they plan to sink the 8 ball into and then successfully pot the 8 ball in that called pocket. If the 8 ball falls into any pocket other than the one designated or is knocked off the table, or a foul (see below) occurs and the 8 ball is pocketed, this results in loss of game. Otherwise, the shooter's turn is simply over, including when a foul such as a scratch occurs on an "unsuccessful" attempt to pocket the 8 ball. In short, a World Standardized Rules game of eight-ball, like a game of nine-ball, is "not" over until the "" is no longer on the table. This rule is unusual to some bar and league players, because in American, Canadian and many other varieties of , and in some leagues, such as APA, such a foul is a loss of game. This is not the case in World Standardized Rules, nor in some other leagues that use those rules or a variant of them, e.g. VNEA beginning with the season, BCAPL, and USAPL.
Winning.
Any of the following results in a game win:
Informal rule variations.
Canada.
In Canada there are similar levels and types of variation as in the US "(see below)". One particularly common feature of Canadian bar pool is the "hooked yourself on the 8" rule — failure to hit the 8 ball when one is shooting for the 8 is a loss of game, "unless" one was hooked (ed) by one's opponent (even then, if for the 8, as opposed to "just a shot", i.e. a , failure to hit the 8 is an instant loss). Pocketing an opponent's while shooting for the 8, even if the shot was otherwise legal, is also a game-loser, often even in local league play. It is also an automatic loss if a player es while shooting for the 8, regardless of whether the eight ball is pocketed or not. "" shots, where the appears to simultaneously strike a legal and an opponent's object balls, are generally considered legal shots in informal games, as long as they are called as split shots, and the hit is in fact simultaneous to the human eye. A further Canadian bar-pool rule is that a shot is a -ending (but not ) foul if one pockets one's called shot but also pockets another ball incidentally, even if it is one's own (however, if that secondary pocketing was also called, the shot is legal, regardless of the order in which the balls were dropped). Finally, it is also a visit-ending foul if the cue ball touches any ball that is ultimately pocketed more than once on the same shot (this is colloquially known as a "double kiss"), even if the player called for the ball to enter that particular pocket on the shot.
Latin America.
The ed balls are often loose, crooked and/or not exactly on the (it is not considered to matter), and the rack itself may be made of rubber, and flexible, making a tight rack physically impossible to achieve. Other than the 8 ball, other balls may be placed far more randomly than players in other areas would tolerate, with large clusters of solids together, and stripes with each other.
In most of Latin America, including Mexico, shots are un-ed, as in British pool (i.e. shots count, a concept foreign to most American players other than APA league members). In many if not most areas (Brazil being an exception), fouls result in behind the only, as in American bar pool (allowing for intentional scratches that leave the opponent a very difficult shot if all opponent balls are "in the ", behind the headstring).
A common Latin American variant of "" is that each player is allowed either one (or even two) cue ball scratches when shooting for the 8, which "must" be pocketed in the same pocket as the shooter's final object ball. Such fouls simply end the shooter's turn at the table and give the opponent ball-in-hand behind the head string; only the second (or third, respectively) such scratch is a loss of game (though scratching the 8 ball itself off the table or into the wrong pocket is an instant loss). This version is common even in US pool bars that are dominated by recent Latino immigrants. This requirement has a profound effect upon game strategy – it is effectively 5 times harder to – and most North American (and British, etc.) players are completely unprepared for it, unless they are last-pocket players. Players must be very mindful what they do with their last few balls, and common failure to get that allows for the last object-ball shot to set the player up for an easy 8 ball shot into the same pocket leads to long games with many , and shots on the 8.
In some parts of Latin America, especially South America, the 1 ball often must be pocketed in the right side pocket (relative to the end of the table one breaks from), and the 15 ball must be pocketed in the other side pocket (left). This rule probably developed to make it harder to run out after the first shot. Position play takes a larger role in this variation, and a player's strategy must necessarily initially revolve around getting the 1 or 15 in and preventing this opponent from doing likewise. When racking the balls for this variation, the 1 and 15 balls are placed behind the 8 ball at the center of the rack, the 1 ball on the left and the 15 ball on the right (from the "racker's" perspective). Latino last-pocket is virtually the only version of eight-ball played in Mexico, other than in the Mexico–United States border area.
In Mexico, a minority of players rack with the 8 ball rather than the apex ball on the foot spot, a trait in common with British-style blackball/8-ball pool. Pocketing the 8 ball on the shot is an instant win, as it usually is in American bar pool, but is not in the international rules. The only ball-in-hand (behind the head string) foul in Mexican pool is ing the cue ball into a pocket; other fouls are simply loss-of-turn. Because Mexican pool, except near the US border, is almost always played on open-pocket pool-hall-style tables, rather than coin-operated tables that trap object balls, any of one's own balls pocketed on a foul are ted (but how they are spotted varies widely, with the balls often placed against the on the , and adjacent to nearby if more than one must be spotted, instead of on the foot spot, but sometimes even to the "side" at diamonds, due to the influence of coyote, a Mexican variant of Chicago; foot-spot spotting is neither common nor uncommon.) Pool itself is not considered a very serious game in the country other than in the northern states; in most of Mexico, three-cushion billiards is the serious game, while pool is mostly played by youths, by groups of friends (including many young women) as a bar game to pass the time, and by older working-class men as an after-work activity. In many recreation halls, dominoes is more popular than pool.
In many bars in Brazil (and not an official rule), a foul is generally punished by pocketing the lowest-numbered ball of the opponent. In that case, the cue ball remains where it stopped, as ball-in-hand is not commonly used. Additionally, in the case of scratching the cue ball, the opponent places the cue ball in the , on the , or most commonly anywhere inside , indicating some British snooker and/or blackball influence.
New Zealand.
New Zealand eight-ball in many respects is closer to British blackball, but with numbered balls being used. is typically drawn on the table above the (as on a snooker table) and the shooting player is required to place the cue ball within it on the and after an opponent . The shooting player can shoot the ball in any direction from within the "D". If no "D" is drawn on the table then the "forward play" rule is followed: After a scratch, the player with ball-in-hand must shoot forward of the baulk line, i.e. towards the rack area, even if all legal balls are behind the baulk line. The "" of blackball may or may not be followed; this depends on individual players and/or pubs.
The "nomination" rule is unique to New Zealand: A player ed on the 8 ball may nominate one of the opponent's balls (if any remain) to hit as an alternative, legal ". However, the shooter is not permitted to pot (pocket) such a nominated ball – doing so results in a loss of game.
North Africa.
In North African countries (as in Latin America, but reversed), both the 1 and 15 balls must be pocketed in the sides, the 15 on the right and 1 on the left (relative to the end of the table one breaks from). The North African version of the informal game is always played ". is not taken on fouls, and "" is a very common rule in addition to last-pocket.
United Kingdom & Ireland.
Pool is popularly played in two forms. Traditionally it is played with smaller balls than the internationally standardized version, on a 4.5 by 7 foot pub-sized table, with differently shaped, smaller pockets. The is also slightly smaller than the . "American-style" pool tables are also common in the UK, especially for nine-ball competition; the tables themselves are often referred to as "nine-ball tables", with that game being played only rarely on the more common, smaller traditional British-style tables. The two most common competitive rule sets used on the traditional tables are WEPF world rules (replacing old EPA rules) and WPA world-standardized blackball rules. Most amateurs play "pub rules", meaning the local rule variation established at that venue.
The two main rule sets have features about them which most amateurs find offensive. WEPF rules permit intentional fouls; despite opponents being awarded two visits for a standard foul causing an intentional foul, or not trying to play a legal shot, it is seen as unfair play and distasteful. In WPA rules, two shots following a foul do not carry, meaning the first shot is a "free shot" rather than two visits, a player takes their free shot and then play returns to normal. As most pub rules are based around old EPA rules, in which two visits are awarded (rather than a free shot) amateurs are often unhappy with this difference in blackball, although it is in no way as offensive as intentional fouls which are illegal in blackball and result in loss of frame. Both WEPF rules and WPA require a player to either pot on their visit, or drive any ball, including the white, into a cushion after hitting a legal object ball, or else they give a foul. Although this rule, and the precise specifics of it are somewhat a mouthful, amateur players usually find the rule acceptable and see it primarily as a way to prevent "tucking up", whereby a player does not attempt to pot and instead just rolls up to their object ball to use it to snooker their opponent; tucking up is seen as unsporting, so being forced to play harder shots is quite welcomed.
There are several sets of rules which use a combination of many others in an attempt to find a balance between WPA rules, which are seen as more aggressive, and WEPF rules which are often referred to, pejoratively, as "chess".
Pakistan.
During game play, if the player fails to hit a ball of his designated group or he hits the opponent's ball with the cue ball, then the opponent receives 2 shots unless the opponent has pocketed all his balls and only the 8 ball remains, then the opponent will only get one shot. In case of such a foul, the game continues with the player playing the cue ball at the place where it stopped. If a Scratch occurs, then the opponent plays Ball-in-Hand but he is only allowed to place it anywhere in the D however he can play the cue ball in any direction. Knocking a ball (apart from the cue ball) off the table carries no penalty. Instead the misplaced ball is returned to its original place and the game continues.
India.
If a scratch or other foul occurs while playing the 8 ball, as long as the opponent has at least one ball of his group present on the table and the 8 ball is not pocketed, the game continues. In both cases of this foul-on-the-8 situation, the opponent gets two chances (regardless of whether any balls are potted on the first chance) before the fouling player may shoot again. In these circumstances, treatment of the cue ball depends on the type of foul. If the cue ball had been scratched, the cue ball must be placed behind the break line. If it was some other foul which had occurred while playing the 8 ball, the cue ball is not moved. If the incoming opponent scratches, the player who originally fouled now receives two chances. When the 8 ball is the only ball on the table, any kind of foul ends the game, and the opponent of the fouling player wins.
United States.
Most commonly of all in American , it is sometimes required that all shots be "in detail", as to what balls and bank/kick cushions will be involved in the shot, with the shot considered a turn-ending (but not ball-in-hand) foul if not executed precisely as planned (and a loss of game if the "foul" shot pocketed the 8 ball). Contrariwise, some Americans hold that nothing other than the 8 ball has to be called in any way — "" counts.
In informal amateur play in most areas, the table will only be considered open if no balls were pocketed, or an equal number of stripes and solids were pocketed, or the cue ball was ed (into a pocket or off the table), on the ; if an odd number of balls were legally pocketed, such as one solid and two stripes, or no solids and one stripe, the breaker must shoot the balls that were pocketed in the greatest quantity (stripes in these examples). The table is almost never considered so as for it to be legal to use a ball of the opposite , much less the 8 ball, as the first ball in a combination shot while the table is open (despite this being perfectly legal in WPA World Standardized and many US league rules). In non- it is fairly common for a foul break in which the rack was not struck at all (e.g., due to a ) to be re-shot by the original breaker.
Fouls, in common bar pool, that are not cue ball scratches generally only cause loss of turn, with cue ball left in place (even if it is ed). Even in the case of a scratch, this only results in "behind the ". Regionally, there is a great deal of bar pool variation in the handling of fouls while shooting at and/or pocketing the 8 ball. In some cases any foul while shooting at but not pocketing the 8 is a loss of game, in others only a foul while otherwise successfully pocketing the 8, and in yet others only certain fouls, such as also sinking an opponent's ball or touching the 8 ball and scratching.
What is considered a foul further diverges from established, published rulesets. Scoop-under s are usually considered valid (these are fouls in WPA and most league rules, as they are s, though few players realize it). When a cue ball is frozen or near-frozen to an object ball, shooting it dead-on, in line with both balls, is a foul in formal rulesets (as another kind of double-hit), but is generally tolerated in bar pool.
Other US bar pool oddities varying from area to area include: Knocking the cue ball off the table on the break may be an instant loss; scratching on the break may be an instant loss; pocketing the 8 ball on the break (without scratching) may be either an instant win or instant loss (the latter being a rare variant); no safeties may be allowed at all – all shots may be required to be at least vaguely plausible attempts to pocket a legal ball; all jump shots may be banned; shots may be banned; it may be illegal to use the 8 ball in any way in combinations, caroms or kisses; the 8 ball may be required to be pocketed "cleanly" in the sense of no contact with other object balls (even if the shot can be accurately called); failure to hit one of one's own object balls (or the 8 if shooting for the 8) may be considered a "table scratch" that gives the opponent a shot in-hand from behind the head string; failure to hit the 8 if shooting for the 8 may be a loss of game; and a "split" shot, where the cue ball appears to simultaneously strike a legal ball and an opponent's object ball, may be considered a legal shot, as long as it is called as a split shot, and the hit is in fact simultaneous to the naked eye.
" is a common American amateur variation, especially on coin-operated (because it usually makes the game last longer), in which the 8 ball must be off one or more (s may also qualify in some versions), into the ; either player may suggest bank-the-eight at any time before or during the game, and the other may accept or refuse; all other rules apply as usual. Playing bank-the-eight may be considered rude if there is a long line of players waiting to use the table.
A similarly motivated variant is ", in which the 8 ball must be pocketed in the same pocket as the shooting player's last object ball (i.e., each player may be said to eventually "own" a pocket in which their 8 ball shot must be played if they have already run out their ); all other rules apply as usual. This variant is popular in Mexico.
Due probably to the influence of nine-ball, in which the 1 ball "must" be the apex ball of the rack, most American bar players traditionally rack a game of eight-ball with the 1 ball in this position. Racking is also typically done solid-stripe-solid-stripe-solid along the two sides of the rack, resulting in solids being on all three corners. This is not a legal rack in World Standardized Rules, nor any other notable league ruleset other than APA.
Derivative games and variants.
British-style variant.
In the United Kingdom, eight-ball pool (and its internationally standardized variant blackball) has evolved into an overall rather different game, influenced by English billiards and snooker, and has become popular in amateur competition in Britain, Ireland, Australia, and some other countries. As with American-style eight-ball, there are multiple competing standards bodies that have issued international rules. Aside from using unnumbered object balls (except for the 8), UK-style tables have pockets just larger than the balls. More than one type of is typically used. Most tables do not have s on the rails, and consequently the racking spot is on the not at the second-diamond . The rules significantly differ in numerous ways, including the handling of fouls, which may give the opponent two shots, racking (the 8 ball, not the apex ball, goes on the spot), selection of which group of balls will be shot by which player, handling of balls and s, and many other details.
The English Pool Association is recognized by the Sports Council as the governing body for pool including blackball in England.
Eight-ball rotation.
The hybrid game eight-ball rotation is a combination of eight-ball and rotation, in which the players must pocket their balls (other than the 8, which remains last) in numerical order.

</doc>
<doc id="9728" url="https://en.wikipedia.org/wiki?curid=9728" title="Earned value management">
Earned value management

Earned value management (EVM), or Earned value project/performance management (EVPM) is a project management technique for measuring project performance and progress in an objective manner.
Overview.
Earned value management is a project management technique for measuring project performance and progress. It has the ability to combine measurements of the project management triangle:
In a single integrated system, Earned Value Management is able to provide accurate forecasts of project performance problems, which is an important contribution for project management.
Early EVM research showed that the areas of planning and control are significantly impacted by its use; and similarly, using the methodology improves both scope definition as well as the analysis of overall project performance. More recent research studies have shown that the principles of EVM are positive predictors of project success. Popularity of EVM has grown in recent years beyond government contracting, a sector in which its importance continues to rise (e.g., recent new DFARS rules), in part because EVM can also surface in and help substantiate contract disputes.
Essential features of any EVM implementation include
EVM implementations for large or complex projects include many more features, such as indicators and forecasts of cost performance (over budget or under budget) and schedule performance (behind schedule or ahead of schedule). However, the most basic requirement of an EVM system is that it quantifies progress using PV and EV.
Application example.
Project A has been approved for a duration of 1 year and with the budget of X. It was also planned, that the project spends 50% of the approved budget in the first 6 months. If now 6 months after the start of the project a Project Manager would report that he has spent 50% of the budget, one can initially think, that the project is perfectly on plan. However, in reality the provided information is not sufficient to come to such a conclusion. The project can spend 50% of the budget, whilst finishing only 25% of the work, which would mean the project is not doing well; or the project can spend 50% of the budget, whilst completing 75% of the work, which would mean that project is doing better than planned. EVM is meant to address such and similar issues.
History.
EVM emerged as a financial analysis specialty in United States Government programs in the 1960s, but it has since become a significant branch of project management and cost engineering. Project management research investigating the contribution of EVM to project success suggests a moderately strong positive relationship.
Implementations of EVM can be scaled to fit projects of all sizes and complexities.
The genesis of EVM occurred in industrial manufacturing at the turn of the 20th century, based largely on the principle of "earned time" popularized by Frank and Lillian Gilbreth, but the concept took root in the United States Department of Defense in the 1960s. The original concept was called PERT/COST, but it was considered overly burdensome (not very adaptable) by contractors who were mandated to use it, and many variations of it began to proliferate among various procurement programs. In 1967, the DoD established a criterion-based approach, using a set of 35 criteria, called the Cost/Schedule Control Systems Criteria (C/SCSC). In the 1970s and early 1980s, a subculture of C/SCSC analysis grew, but the technique was often ignored or even actively resisted by project managers in both government and industry. C/SCSC was often considered a financial control tool that could be delegated to analytical specialists.
In 1979, EVM was introduced to the architecture and engineering industry in a "Public Works Magazine" article by David Burstein, a project manager with a national engineering firm. This technique has been taught ever since as part of the project management training program presented by PSMJ Resources, an international training and consulting firm that specializes in the engineering and architecture industry.
In the late 1980s and early 1990s, EVM emerged as a project management methodology to be understood and used by managers and executives, not just EVM specialists. In 1989, EVM leadership was elevated to the Undersecretary of Defense for Acquisition, thus making EVM an element of program management and procurement. In 1991, Secretary of Defense Dick Cheney canceled the Navy A-12 Avenger II Program because of performance problems detected by EVM. This demonstrated conclusively that EVM mattered to secretary-level leadership. In the 1990s, many U.S. Government regulations were eliminated or streamlined. However, EVM not only survived the acquisition reform movement, but became strongly associated with the acquisition reform movement itself. Most notably, from 1995 to 1998, ownership of EVM criteria (reduced to 32) was transferred to industry by adoption of ANSI EIA 748-A standard.
The use of EVM expanded beyond the U.S. Department of Defense. It was adopted by the National Aeronautics and Space Administration, United States Department of Energy and other technology-related agencies. Many industrialized nations also began to utilize EVM in their own procurement programs.
An overview of EVM was included in the Project Management Institute's first PMBOK Guide in 1987 and was expanded in subsequent editions. In the most recent edition of the PMBOK guide, EVM is listed among the general tools and techniques for processes to control project costs.
The construction industry was an early commercial adopter of EVM. Closer integration of EVM with the practice of project management accelerated in the 1990s. In 1999, the Performance Management Association merged with the Project Management Institute (PMI) to become PMI’s first college, the College of Performance Management. The United States Office of Management and Budget began to mandate the use of EVM across all government agencies, and, for the first time, for certain internally managed projects (not just for contractors). EVM also received greater attention by publicly traded companies in response to the Sarbanes-Oxley Act of 2002.
In Australia EVM has been codified as standards AS 4817-2003 and AS 4817-2006.
Earned value management topics.
Project tracking.
 It is helpful to see an example of project tracking that does not include earned value performance management. Consider a project that has been planned in detail, including a time-phased spend plan for all elements of work. Figure 1 shows the cumulative budget (cost) for this project as a function of time (the blue line, labeled PV). It also shows the cumulative actual cost of the project (red line, labeled AC) through week 8. To those unfamiliar with EVM, it might appear that this project was over budget through week 4 and then under budget from week 6 through week 8. However, what is missing from this chart is any understanding of how much work has been accomplished during the project. If the project was actually completed at week 8, then the project would actually be well under budget and well ahead of schedule. If, on the other hand, the project is only 10% complete at week 8, the project is significantly over budget and behind schedule. A method is needed to measure technical performance objectively and quantitatively, and that is what EVM accomplishes.
Project tracking with EVM.
Consider the same project, except this time the project plan includes pre-defined methods of quantifying the accomplishment of work. At the end of each week, the project manager identifies every detailed element of work that has been completed, and sums the EV for each of these completed elements. Earned value may be accumulated monthly, weekly, or as progress is made.
Earned value (EV).
formula_1
Figure 2 shows the EV curve (in green) along with the PV curve from Figure 1. The chart indicates that technical performance (i.e., progress) started more rapidly than planned, but slowed significantly and fell behind schedule at week 7 and 8. This chart illustrates the schedule performance aspect of EVM. It is complementary to critical path or critical chain schedule management.
Figure 3 shows the same EV curve (green) with the actual cost data from Figure 1 (in red). It can be seen that the project was actually under budget, relative to the amount of work accomplished, since the start of the project. This is a much better conclusion than might be derived from Figure 1.
Figure 4 shows all three curves together – which is a typical EVM line chart. The best way to read these three-line charts is to identify the EV curve first, then compare it to PV (for schedule performance) and AC (for cost performance). It can be seen from this illustration that a true understanding of cost performance and schedule performance "relies first on measuring technical performance objectively." This is the "foundational principle" of EVM.
Scaling EVM from simple to advanced implementations.
The "foundational principle" of EVM, mentioned above, does not depend on the size or complexity of the project. However, the "implementations" of EVM can vary significantly depending on the circumstances. In many cases, organizations establish an all-or-nothing threshold; projects above the threshold require a full-featured (complex) EVM system and projects below the threshold are exempted. Another approach that is gaining favor is to scale EVM implementation according to the project at hand and skill level of the project team.
Simple implementations (emphasizing only technical performance).
There are many more small and simple projects than there are large and complex ones, yet historically only the largest and most complex have enjoyed the benefits of EVM. Still, lightweight implementations of EVM are achievable by any person who has basic spreadsheet skills. In fact, spreadsheet implementations are an excellent way to learn basic EVM skills.
The "first step" is to define the work. This is typically done in a hierarchical arrangement called a work breakdown structure (WBS) although the simplest projects may use a simple list of tasks. In either case, it is important that the WBS or list be comprehensive. It is also important that the elements be mutually exclusive, so that work is easily categorized in one and only one element of work. The most detailed elements of a WBS hierarchy (or the items in a list) are called activities (or tasks).
The "second step" is to assign a value, called planned value (PV), to each activity. For large projects, PV is almost always an allocation of the total project budget, and may be in units of currency (e.g., dollars or euros) or in labor hours, or both. However, in very simple projects, each activity may be assigned a weighted “point value" which might not be a budget number. Assigning weighted values and achieving consensus on all PV quantities yields an important benefit of EVM, because it exposes misunderstandings and miscommunications about the scope of the project, and resolving these differences should always occur as early as possible. Some terminal elements can not be known (planned) in great detail in advance, and that is expected, because they can be further refined at a later time.
The "third step" is to define “earning rules” for each activity. The simplest method is to apply just one earning rule, such as the 0/100 rule, to all activities. Using the 0/100 rule, no credit is earned for an element of work until it is finished. A related rule is called the 50/50 rule, which means 50% credit is earned when an element of work is started, and the remaining 50% is earned upon completion. Other fixed earning rules such as a 25/75 rule or 20/80 rule are gaining favor, because they assign more weight to finishing work than for starting it, but they also motivate the project team to identify when an element of work is started, which can improve awareness of work-in-progress. These simple earning rules work well for small or simple projects because generally each activity tends to be fairly short in duration.
These initial three steps define the minimal amount of planning for simplified EVM. The "final step" is to execute the project according to the plan and measure progress. When activities are started or finished, EV is accumulated according to the earning rule. This is typically done at regular intervals (e.g., weekly or monthly), but there is no reason why EV cannot be accumulated in near real-time, when work elements are started/completed. In fact, waiting to update EV only once per month (simply because that is when cost data are available) only detracts from a primary benefit of using EVM, which is to create a technical performance scoreboard for the project team.
Intermediate implementations (integrating technical and schedule performance).
In many projects, schedule performance (completing the work on time) is equal in importance to technical performance. For example, some new product development projects place a high premium on finishing quickly. It is not that cost is unimportant, but finishing the work later than a competitor may cost a great deal more in lost market share. It is likely that these kinds of projects will not use the lightweight version of EVM described in the previous section, because there is no planned timescale for measuring schedule performance. A second layer of EVM skill can be very helpful in managing the schedule performance of these “intermediate” projects. The project manager may employ a critical path or critical chain to build a project schedule model. As in the lightweight implementation, the project manager must define the work comprehensively, typically in a WBS hierarchy. He/she will construct a project schedule model that describes the precedence links between elements of work. This schedule model can then be used to develop the PV curve (or baseline), as shown in Figure 2.
It should be noted that measuring schedule performance using EVM does not replace the need to understand schedule performance versus the project's schedule model (precedence network). However, EVM schedule performance, as illustrated in Figure 2 provides an additional indicator — one that can be communicated in a single chart. Although it is theoretically possible that detailed schedule analysis will yield different conclusions than broad schedule analysis, in practice there tends to be a high correlation between the two. Although EVM schedule measurements are not necessarily conclusive, they provide useful diagnostic information.
Although such intermediate implementations do not require units of currency (e.g., dollars), it is common practice to use budgeted dollars as the scale for PV and EV. It is also common practice to track labor hours in parallel with currency. The following EVM formulas are for schedule management, and do not require accumulation of actual cost (AC). This is important because it is common in small and intermediate size projects for true costs to be unknown or unavailable.
However, Schedule Variance (SV) measured through EVM method is indicative only. To know whether a project is really behind or ahead of schedule (on time completion), Project Manager has to perform critical path analysis based on precedence and inter-dependencies of the project activities.
Making earned value schedule metrics concordant with the CPM schedule.
The actual critical path is ultimately the determining factor of every project's duration. Because earned value schedule metrics take no account of critical path data, big budget activities that are not on the critical path have the potential to dwarf the impact of performing small budget critical path activities. This can lead to "gaming" the SV and SPI metrics by ignoring critical path activities in favor of big budget activities that may have lots of float. This can sometimes even lead to performing activities out-of-sequence just to improve the schedule tracking metrics, which can cause major problems with quality.
A simple two-step process has been suggested to fix this:
1. Create a second earned value baseline strictly for schedule, with the weighted activities/milestones on the as-late-as-possible dates of the backward pass of the critical path algorithm, where there is no float.
2. Allow earned value credit for schedule metrics to be taken no earlier than the reporting period during which the activity is scheduled unless it is on the project's current critical path.
In this way, the distorting aspect of float would be eliminated. There would be no benefit to performing a non-critical activity with lots of float until it is due in proper sequence. Also, an activity would not generate a negative schedule variance until it had used up its float. Under this method, one way of gaming the schedule metrics would be eliminated. The only way of generating a positive schedule variance (or SPI over 1.0) would be by completing work on the current critical path ahead of schedule, which is in fact the only way for a project to get ahead of schedule.
Advanced implementations (integrating cost, schedule and technical performance).
In addition to managing technical and schedule performance, large and complex projects require that cost performance be monitored and reviewed at regular intervals. To measure cost performance, planned value (or BCWS - Budgeted Cost of Work Scheduled) and earned value (or BCWP - Budgeted Cost of Work Performed) must be in units of currency (the same units that actual costs are measured.) In large implementations, the planned value curve is commonly called a Performance Measurement Baseline (PMB) and may be arranged in control accounts, summary-level planning packages, planning packages and work packages. In large projects, establishing control accounts is the primary method of delegating responsibility and authority to various parts of the performing organization. Control accounts are cells of a responsibility assignment (RACI) matrix, which is the intersection of the project WBS and the organizational breakdown structure (OBS). Control accounts are assigned to Control Account Managers (CAMs). Large projects require more elaborate processes for controlling baseline revisions, more thorough integration with subcontractor EVM systems, and more elaborate management of procured materials.
In the United States, the primary standard for full-featured EVM systems is the ANSI/EIA-748A standard, published in May 1998 and reaffirmed in August 2002. The standard defines 32 criteria for full-featured EVM system compliance. As of the year 2007, a draft of ANSI/EIA-748B, a revision to the original is available from ANSI. Other countries have established similar standards.
In addition to using BCWS and BCWP, prior to 1998 implementations often use the term Actual Cost of Work Performed (ACWP) instead of AC. Additional acronyms and formulas include:
Agile EVM.
In complex environments like software development, an iterative and incremental or Agile approach is often used to deliver complex products more successfully. Agile EVM is used as trend burndown/burnup graphs to make forecasts of progress towards a completion date transparent. However, EVM techniques are always used for the underlying calculations.
Preparation.
Setting up Agile EVM is similar to a simple implementation of EVM with the following preparation steps:
Practices.
 Agile EVM is now all about executing the project and tracking the accumulated EV according to the simple earning rule. Because Agile EVM has been evolving for many years the following practices are well-established:
Calculations.
Agile EVM is based on transparency and therefore graphically used in various trend charts. However, all EVM formulas (CPI, SPI, EAC, etc.) can still be used in Agile EVM by expressing the input variables like EV, PV and AC as:
Agile embraces change and therefore scope is considered variable (i.e. not fixed). Instead of using the INITIAL estimate in total number of Story Points, in Agile EVM calculations always the LATEST estimate in total number of Story Points is used to calculate CPI, SPI, EAC, etc.
Schedule Performance.
The use of SPI in EVM is rather limited in forecasting schedule performance problems because it is dependent on the completion of earned value on the Critical Time Path(CTP).
Because Agile EVM is used in a complex environment, any earned value is more likely to be on the CTP. The latest estimate for the number of fixed time intervals can be calculated in Agile EVM as:
Limitations.
Proponents of EVM note a number of issues with implementing it
, and further limitations may be inherent to the concept itself.
Because EVM requires quantification of a project plan, it is often perceived to be inapplicable to discovery-driven or Agile software development projects. For example, it may be impossible to plan certain research projects far in advance, because research itself uncovers some opportunities (research paths) and actively eliminates others. However, another school of thought holds that all work can be planned, even if in weekly timeboxes or other short increments. Thus, the challenge is to create agile or discovery-driven "implementations" of the EVM principle, and not simply to reject the notion of measuring technical performance objectively. (See the lightweight implementation for small projects, described above). Applying EVM in fast-changing work environments is, in fact, an area of project management research.
Traditional EVM is not intended for non-discrete (continuous) effort. In traditional EVM standards, non-discrete effort is called “level of effort" (LOE). If a project plan contains a significant portion of LOE, and the LOE is intermixed with discrete effort, EVM results will be contaminated. This is another area of EVM research.
Traditional definitions of EVM typically assume that project accounting and project network schedule management are prerequisites to achieving any benefit from EVM. Many small projects don't satisfy either of these prerequisites, but they too can benefit from EVM, as described for simple implementations, above. Other projects can be planned with a project network, but do not have access to true and timely actual cost data. The systems that feed the data required by earned value management are usually in silos rather than interfaced and integrated. In practice, the collection of true and timely actual cost data can be the most difficult aspect of EVM. Such projects can benefit from EVM, as described for intermediate implementations, above, and Earned Schedule.
As a means of overcoming objections to EVM's lack of connection to qualitative performance issues, the Naval Air Systems Command (NAVAIR) PEO(A) organization initiated a project in the late 1990s to integrate true technical achievement into EVM projections by utilizing risk profiles. These risk profiles anticipate opportunities that may be revealed and possibly be exploited as development and testing proceeds. The published research resulted in a Technical Performance Management (TPM) methodology and software application that is still used by many DoD agencies in informing EVM estimates with technical achievement.
The research was peer-reviewed and was the recipient of the Defense Acquisition University Acquisition Research Symposium 1997 Acker Award for excellence in the exchange of information in the field of acquisition research.
There is the difficulty inherent for any periodic monitoring of synchronizing data timing: actual deliveries, actual invoicing, and the date the EVM analysis is done are all independent, so that some items have arrived but their invoicing has not and by the time analysis is delivered the data will likely be weeks behind events. This may limit EVM to a less tactical or less definitive role where use is combined with other forms to explain why or add recent news and manage future expectations.
There is a measurement limitation for how precisely EVM can be used, stemming from classic conflict between accuracy and precision, as the mathematics can calculate deceptively far beyond the precision of the measurements of data and the approximation that is the plan estimation. The limitation on estimation is commonly understood (such as the ninety-ninety rule in software) but is not visible in any margin of error. The limitations on measurement are largely a form of digitization error as EVM measurements ultimately can be no finer than by item, which may be the Work Breakdown Structure terminal element size, to the scale of reporting period, typically end summary of a month, and by the means of delivery measure. (The delivery measure may be actual deliveries, may include estimates of partial work done at the end of month subject to estimation limits, and typically does not include QC check or risk offsets.)

</doc>
<doc id="9730" url="https://en.wikipedia.org/wiki?curid=9730" title="Electron microscope">
Electron microscope

An electron microscope is a microscope that uses a beam of accelerated electrons as a source of illumination. Because the wavelength of an electron can be up to 100,000 times shorter than that of visible light photons, the electron microscope has a higher resolving power than a light microscope and can reveal the structure of smaller objects. A transmission electron microscope can achieve better than 50 pm resolution and magnifications of up to about 10,000,000x whereas most light microscopes are limited by diffraction to about 200 nm resolution and useful magnifications below 2000x.
The transmission electron microscope uses electrostatic and electromagnetic lenses to control the electron beam and focus it to form an image. These electron optical lenses are analogous to the glass lenses of an optical light microscope.
Electron microscopes are used to investigate the ultrastructure of a wide range of biological and inorganic specimens including microorganisms, cells, large molecules, biopsy samples, metals, and crystals. Industrially, the electron microscope is often used for quality control and failure analysis. Modern electron microscopes produce electron micrographs using specialized digital cameras and frame grabbers to capture the image.
History.
The first electromagnetic lens was developed in 1926 by Hans Busch.
According to Dennis Gabor, the physicist Leó Szilárd tried in 1928 to convince Busch to build an electron microscope, for which he had filed a patent.
German physicist Ernst Ruska and the electrical engineer Max Knoll constructed the prototype electron microscope in 1931, capable of four-hundred-power magnification; the apparatus was the first demonstration of the principles of electron microscopy. Two years later, in 1933, Ruska built an electron microscope that exceeded the resolution attainable with an optical (light) microscope. Moreover, Reinhold Rudenberg, the scientific director of Siemens-Schuckertwerke, obtained the patent for the electron microscope in May 1931.
In 1932, Ernst Lubcke of Siemens & Halske built and obtained images from a prototype electron microscope, applying concepts described in the Rudenberg patent applications. Five years later (1937), the firm financed the work of Ernst Ruska and Bodo von Borries, and employed Helmut Ruska (Ernst’s brother) to develop applications for the microscope, especially with biological specimens. Also in 1937, Manfred von Ardenne pioneered the scanning electron microscope. The first "practical" electron microscope was constructed in 1938, at the University of Toronto, by Eli Franklin Burton and students Cecil Hall, James Hillier, and Albert Prebus; and Siemens produced the first "commercial" transmission electron microscope (TEM) in 1939. Although contemporary electron microscopes are capable of two million-power magnification, as scientific instruments, they remain based upon Ruska’s prototype.
Types.
Transmission electron microscope (TEM).
The original form of electron microscope, the transmission electron microscope (TEM) uses a high voltage electron beam to create an image. The electron beam is produced by an electron gun, commonly fitted with a tungsten filament cathode as the electron source. The electron beam is accelerated by an anode typically at +100 keV (40 to 400 keV) with respect to the cathode, focused by electrostatic and electromagnetic lenses, and transmitted through the specimen that is in part transparent to electrons and in part scatters them out of the beam. When it emerges from the specimen, the electron beam carries information about the structure of the specimen that is magnified by the objective lens system of the microscope. The spatial variation in this information (the "image") may be viewed by projecting the magnified electron image onto a fluorescent viewing screen coated with a phosphor or scintillator material such as zinc sulfide. Alternatively, the image can be photographically recorded by exposing a photographic film or plate directly to the electron beam, or a high-resolution phosphor may be coupled by means of a lens optical system or a fibre optic light-guide to the sensor of a CCD (charge-coupled device) camera. The image detected by the CCD may be displayed on a monitor or computer.
Resolution of the TEM is limited primarily by spherical aberration, but a new generation of aberration correctors have been able to partially overcome spherical aberration to increase resolution. Hardware correction of spherical aberration for the high-resolution transmission electron microscopy (HRTEM) has allowed the production of images with resolution below 0.5 angstrom (50 picometres) and magnifications above 50 million times. The ability to determine the positions of atoms within materials has made the HRTEM an important tool for nano-technologies research and development.
Transmission electron microscopes are often used in electron diffraction mode. The advantages of electron diffraction over X-ray crystallography are that the specimen need not be a single crystal or even a polycrystalline powder, and also that the Fourier transform reconstruction of the object's magnified structure occurs physically and thus avoids the need for solving the phase problem faced by the X-ray crystallographers after obtaining their X-ray diffraction patterns of a single crystal or polycrystalline powder. The major disadvantage of the transmission electron microscope is the need for extremely thin sections of the specimens, typically about 100 nanometers. Biological specimens are typically required to be chemically fixed, dehydrated and embedded in a polymer resin to stabilize them sufficiently to allow ultrathin sectioning. Sections of biological specimens, organic polymers and similar materials may require special treatment with heavy atom labels in order to achieve the required image contrast.
Scanning electron microscope (SEM).
The SEM produces images by probing the specimen with a focused electron beam that is scanned across a rectangular area of the specimen (raster scanning). When the electron beam interacts with the specimen, it loses energy by a variety of mechanisms. The lost energy is converted into alternative forms such as heat, emission of low-energy secondary electrons and high-energy backscattered electrons, light emission (cathodoluminescence) or X-ray emission, all of which provide signals carrying information about the properties of the specimen surface, such as its topography and composition. The image displayed by an SEM maps the varying intensity of any of these signals into the image in a position corresponding to the position of the beam on the specimen when the signal was generated. In the SEM image of an ant shown below and to the right, the image was constructed from signals produced by a secondary electron detector, the normal or conventional imaging mode in most SEMs.
Generally, the image resolution of an SEM is at least an order of magnitude poorer than that of a TEM. However, because the SEM image relies on surface processes rather than transmission, it is able to image bulk samples up to many centimeters in size and (depending on instrument design and settings) has a great depth of field, and so can produce images that are good representations of the three-dimensional shape of the sample. Another advantage of SEM is its variety called environmental scanning electron microscope (ESEM) can produce images of sufficient quality and resolution with the samples being wet or contained in low vacuum or gas. This greatly facilitates imaging biological samples that are unstable in the high vacuum of conventional electron microscopes.
Color.
In their most common configurations, electron microscopes produce images with a single brightness value per pixel, with the results usually rendered in grayscale. However, often these images are then colorized through the use of feature-detection software, or simply by hand-editing using a graphics editor. This is usually for aesthetic effect or for clarifying structure, and generally does not add information about the specimen.
In some configurations more information about specimen properties is gathered per pixel, usually by the use of multiple detectors. In SEM, the attributes of topography and material contrast can be obtained by a pair of backscattered electron detectors and such attributes can be superimposed in a single color image by assigning a different primary color to each attribute. Similarly, a combination of backscattered and secondary electron signals can be assigned to different colors and superimposed on a single color micrograph displaying simultaneously the properties of the specimen.
Some types of detectors used in SEM have analytical capabilities, and can provide several items of data at each pixel. Examples are the Energy-dispersive X-ray spectroscopy (EDS) detectors used in elemental analysis and Cathodoluminescence microscope (CL) systems that analyse the intensity and spectrum of electron-induced luminescence in (for example) geological specimens. In SEM systems using these detectors it is common to color code the signals and superimpose them in a single color image, so that differences in the distribution of the various components of the specimen can be seen clearly and compared. Optionally, the standard secondary electron image can be merged with the one or more compositional channels, so that the specimen's structure and composition can be compared. Such images can be made while maintaining the full integrity of the original signal, which is not modified in any way.
Reflection electron microscope (REM).
In the reflection electron microscope (REM) as in the TEM, an electron beam is incident on a surface but instead of using the transmission (TEM) or secondary electrons (SEM), the reflected beam of elastically scattered electrons is detected. This technique is typically coupled with reflection high energy electron diffraction (RHEED) and "reflection high-energy loss spectroscopy (RHELS)". Another variation is spin-polarized low-energy electron microscopy (SPLEEM), which is used for looking at the microstructure of magnetic domains.
Scanning transmission electron microscope (STEM).
The STEM rasters a focused incident probe across a specimen that (as with the TEM) has been thinned to facilitate detection of electrons scattered "through" the specimen. The high resolution of the TEM is thus possible in STEM. The focusing action (and aberrations) occur before the electrons hit the specimen in the STEM, but afterward in the TEM. The STEMs use of SEM-like beam rastering simplifies annular dark-field imaging, and other analytical techniques, but also means that image data is acquired in serial rather than in parallel fashion. Often TEM can be equipped with the scanning option and then it can function both as TEM and STEM.
Sample preparation.
Materials to be viewed under an electron microscope may require processing to produce a suitable sample. The technique required varies depending on the specimen and the analysis required:
Disadvantages.
Electron microscopes are expensive to build and maintain, but the capital and running costs of confocal light microscope systems now overlaps with those of basic electron microscopes. Microscopes designed to achieve high resolutions must be housed in stable buildings (sometimes underground) with special services such as magnetic field cancelling systems.
The samples largely have to be viewed in vacuum, as the molecules that make up air would scatter the electrons. One exception is the environmental scanning electron microscope, which allows hydrated samples to be viewed in a low-pressure (up to ) and/or wet environment.
Scanning electron microscopes operating in conventional high-vacuum mode usually image conductive specimens; therefore non-conductive materials require conductive coating (gold/palladium alloy, carbon, osmium, etc.). The low-voltage mode of modern microscopes makes possible the observation of non-conductive specimens without coating. Non-conductive materials can be imaged also by a variable pressure (or environmental) scanning electron microscope.
Small, stable specimens such as carbon nanotubes, diatom frustules and small mineral crystals (asbestos fibres, for example) require no special treatment before being examined in the electron microscope. Samples of hydrated materials, including almost all biological specimens have to be prepared in various ways to stabilize them, reduce their thickness (ultrathin sectioning) and increase their electron optical contrast (staining). These processes may result in "artifacts", but these can usually be identified by comparing the results obtained by using radically different specimen preparation methods. It is generally believed by scientists working in the field that as results from various preparation techniques have been compared and that there is no reason that they should all produce similar artifacts, it is reasonable to believe that electron microscopy features correspond with those of living cells. Since the 1980s, analysis of cryofixed, vitrified specimens has also become increasingly used by scientists, further confirming the validity of this technique.
Biology and life sciences
External links.
History.
John H L Watson's recollections at the University of Toronto when he worked with Hillier and Prebus: 

</doc>
<doc id="9731" url="https://en.wikipedia.org/wiki?curid=9731" title="List of recently extinct birds">
List of recently extinct birds

Since 1500, over 190 species of birds have become extinct, and this rate of extinction seems to be increasing. The situation is exemplified by Hawaii, where 30% of all known recently extinct bird taxa originally lived. Other areas, such as Guam, have also been hit hard; Guam has lost over 60% of its native bird taxa in the last 30 years, many of them due to the introduced brown tree snake.
Currently there are approximately 10,000 species of birds, with an estimated 1,200 considered to be under threat of extinction.
Island species in general, and flightless island species in particular are most at risk. The disproportionate number of rails in the list reflects the tendency of that family to lose the ability to fly when geographically isolated. Even more rails became extinct before they could be described by scientists; these taxa are listed in Late Quaternary prehistoric birds.
The extinction dates given below are usually approximations of the actual date of extinction. In some cases, more exact dates are given as it is sometimes possible to pinpoint the date of extinction to a specific year or even day (the San Benedicto rock wren is possibly the most extreme example—its extinction could be timed with an accuracy of maybe half an hour). Extinction dates in the literature are usually the dates of the last verified record (credible observation or specimen taken); in many Pacific birds which became extinct shortly after European contact, however, this leaves an uncertainty period of over a century because the islands on which they used to occur were only rarely visited by scientists.
Extinct bird species.
Struthioniformes.
The ostrich and related ratites
Anseriformes.
Ducks, geese and swans
Galliformes.
Quails and relatives
See also Bokaak "bustard" under Gruiformes below
Charadriiformes.
Shorebirds, gulls and auks
Gruiformes.
Rails and allies - probably paraphyletic
Podicipediformes.
Grebes
Ciconiiformes.
Herons and related birds - possibly paraphyletic
Pelecaniformes.
Cormorants and related birds
Procellariiformes.
Petrels, shearwaters, albatrosses and storm petrels.
Sphenisciformes.
Penguins
Columbiformes.
Pigeons, doves and dodos
For the "Réunion solitaire", see Réunion sacred ibis.
Psittaciformes.
Parrots
Cuculiformes.
Cuckoos
Falconiformes.
Birds of prey
Strigiformes.
Typical owls and barn-owls.
Caprimulgiformes.
Caprimulgidae - nightjars and nighthawks
Reclusive ground-nesting birds that sally out at night to hunt for large insects and similar prey. They are easily located by the males' song, but this is not given all year. Habitat destruction represents currently the biggest threat, while island populations are threatened by introduced mammalian predators, notably dogs, cats, pigs and mongoose.
Apodiformes.
Swifts and hummingbirds
Coraciiformes.
Kingfishers and related birds
Piciformes.
Woodpeckers and related birds
Passeriformes.
Perching birds
Acanthisittidae– New Zealand "wrens"
Formicariidae – antpittas and antthrushes
Mohoidae – Hawaiian "honeyeaters". Family established in 2008, previously in Meliphagidae.
Meliphagidae – honeyeaters and Australian chats
Acanthizidae – scrubwrens, thornbills, and gerygones
Pachycephalidae – whistlers, shrike-thrushes, pitohuis and allies
Dicruridae – monarch flycatchers and allies
†Turnagridae – piopios
Callaeidae – New Zealand wattlebirds
Hirundinidae – swallows and martins
Acrocephalidae – marsh and tree warblers
Muscicapidae – Old World flycatchers and chats
Megaluridae – megalurid warblers or grass warblers
Cisticolidae – cisticolas and allies
Zosteropidae – white-eyes - probably belonging to Timaliidae
Timaliidae – Old World babblers
Pycnonotidae – bulbuls
Sylvioidea "incertae sedis"
Sturnidae – starlings
Turdidae – thrushes and allies
Mimidae – mockingbirds and thrashers
Estrildidae– estrildid finches (waxbills, munias, etc.)
Icteridae – grackles
Parulidae – New World warblers
Ploceidae – weavers
Fringillidae – true finches and Hawaiian honeycreepers
Emberizidae – buntings and American sparrow
(Probably) extinct subspecies of birds.
Extinction of subspecies is a subject very dependent on guesswork. National and international conservation projects and research publications such as redlists usually focus on species as a whole. Reliable information on the status of threatened subspecies usually has to be assembled piecemeal from published observations such as regional checklists. Therefore, the following listing contains a high proportion of taxa that may still exist, but are listed here due to any combination of absence of recent records, a known threat such as habitat destruction, or an observed decline.
Struthioniformes.
The ostrich and related ratites
Tinamiformes.
Tinamous
Anseriformes.
Ducks, geese and swans
Galliformes.
Quails and relatives
Charadriiformes.
Shorebirds, gulls and auks
Gruiformes.
Rails and allies - probably paraphyletic
Ciconiiformes.
Herons and related birds - possibly paraphyletic
Pteroclidiformes.
Sandgrouses
Columbiformes.
Pigeons, doves and dodos
Psittaciformes.
Parrots
Cuculiformes.
Cuckoos
Falconiformes.
Birds of prey
Strigiformes.
Typical owls and barn-owls
Caprimulgiformes.
Nightjars and allies
Apodiformes.
Swifts and hummingbirds
Coraciiformes.
Kingfishers and related birds
Piciformes.
Woodpeckers and related birds
Passeriformes.
Perching birds
Pittidae – pittas
Tyrannidae – tyrant flycatchers
Furnariidae – ovenbirds
Formicariidae – antpittas and antthrushes
Maluridae – Australasian "wrens"
Pardalotidae – pardalotes, scrubwrens, thornbills, and gerygones
PetroicidaeAustralasian "robins"
Cinclosomatidae – whipbirds and allies
Artamidae – woodswallows, currawongs and allies
Monarchidae – monarch flycatchers
Rhipiduridae – fantails
Campephagidae – cuckoo-shrikes and trillers
Oriolidae – orioles and figbird
Corvidae – crows, ravens, magpies and jays
Callaeidae – New Zealand wattlebirds
Regulidae – kinglets
Hirundinidae – swallows and martins
Phylloscopidae – phylloscopid warblers or leaf-warblers
Cettiidae – cettiid warblers or typical bush-warblers
Acrocephalidae – acrocephalid warblers or marsh- and tree warblers
Pycnonotidae – bulbuls
Cisticolidae – cisticolas and allies
Sylviidae – sylviid ("true") warblers and parrotbills
Zosteropidae – white-eyes. Probably belong into Timaliidae
Timaliidae – Old World babblers
"African warblers"
Sylvioidea "incertae sedis"
Troglodytidae – wrens
Paridae – tits, chickadees and titmice
Cinclidae – dippers
Muscicapidae – Old World flycatchers and chats
Turdidae – thrushes and allies
Mimidae – mockingbirds and thrashers
Estrildidae – Estrildid finches (waxbills, munias, etc.)
Fringillidae – True finches and Hawaiian honeycreepers
Icteridae – grackles
Parulidae – New World warblers
Thraupidae – tanagers
Emberizidae – buntings and American sparrows

</doc>
<doc id="9732" url="https://en.wikipedia.org/wiki?curid=9732" title="Eli Whitney">
Eli Whitney

Eli Whitney (December 8, 1765 – January 8, 1825) was an American inventor best known for inventing the cotton gin. This was one of the key inventions of the Industrial Revolution and shaped the economy of the Antebellum South. Whitney's invention made upland short cotton into a profitable crop, which strengthened the economic foundation of slavery in the United States. Despite the social and economic impact of his invention, Whitney lost many profits in legal battles over patent infringement for the cotton gin. Thereafter, he turned his attention into securing contracts with the government in the manufacture of muskets for the newly formed United States Army. He continued making arms and inventing until his death in 1825.
Early life and education.
Whitney was born in Westborough, Massachusetts, on December 8, 1765, the eldest child of Eli Whitney Sr., a prosperous farmer, and his wife Elizabeth Fay, also of Westborough.
Although the younger Eli, born in 1765, could technically be called a "Junior", history has never known him as such. He was famous during his lifetime and afterward by the name "Eli Whitney". His son, born in 1820, also named Eli, was well known during his lifetime and afterward by the name "Eli Whitney, Jr."
Whitney's mother, Elizabeth Fay, died in 1777, when he was 11. At age 14 he operated a profitable nail manufacturing operation in his father's workshop during the Revolutionary War.
Because his stepmother opposed his wish to attend college, Whitney worked as a farm laborer and school teacher to save money. He prepared for Yale at Leicester Academy (now Becker College) and under the tutelage of Rev. Elizur Goodrich of Durham, Connecticut, he entered the class of 1789 and graduated Phi Beta Kappa in 1792. Whitney expected to study law but, finding himself short of funds, accepted an offer to go to South Carolina as a private tutor.
Instead of reaching his destination, he was convinced to visit Georgia. In the closing years of the 18th century, Georgia was a magnet for New Englanders seeking their fortunes (its Revolutionary-era governor had been Lyman Hall, a migrant from Connecticut). When he initially sailed for South Carolina, among his shipmates were the widow and family of the Revolutionary hero Gen. Nathanael Greene of Rhode Island. Mrs. Greene invited Whitney to visit her Georgia plantation, Mulberry Grove. Her plantation manager and husband-to-be was Phineas Miller, another Connecticut migrant and Yale graduate (class of 1785), who would become Whitney's business partner.
Whitney is most famous for two innovations which later divided the United States in the mid-19th century: the cotton gin (1793) and his advocacy of interchangeable parts. In the South, the cotton gin revolutionized the way cotton was harvested and reinvigorated slavery. In the North the adoption of interchangeable parts revolutionized the manufacturing industry, and contributed greatly to the U.S. victory in the Civil War.
Career.
Cotton gin.
The cotton gin is a mechanical device that removes the seeds from cotton, a process that had previously been extremely labor-intensive. The word "gin" is short for "engine." The cotton gin was a wooden drum stuck with hooks that pulled the cotton fibers through a mesh. The cotton seeds would not fit through the mesh and fell outside. Whitney occasionally told a story wherein he was pondering an improved method of seeding the cotton when he was inspired by observing a cat attempting to pull a chicken through a fence, and could only pull through some of the feathers.
A single cotton gin could generate up to of cleaned cotton daily. This contributed to the economic development of the Southern states of the United States, a prime cotton growing area; some historians believe that this invention allowed for the African slavery system in the Southern United States to become more sustainable at a critical point in its development.
Whitney received a patent (later numbered as X72) for his cotton gin on March 14, 1794, but it was not validated until 1807. Whitney and his partner, Miller, did not intend to sell the gins. Rather, like the proprietors of grist and sawmills, they expected to charge farmers for cleaning their cotton – two-fifths of the value, paid in cotton. Resentment at this scheme, the mechanical simplicity of the device and the primitive state of patent law, made infringement inevitable. Whitney and Miller could not build enough gins to meet demand, so gins from other makers found ready sale. Ultimately, patent infringement lawsuits consumed the profits and their cotton gin company went out of business in 1797. One oft-overlooked point is that there were drawbacks to Whitney's first design. There is significant evidence that the design flaws were solved by a plantation owner, Catherine Littlefield Greene, wife of the American Revolutionary War general Nathanael Greene; Whitney gave her no public credit or recognition.
While the cotton gin did not earn Whitney the fortune he had hoped for, it did give him fame.
It has been argued by some historians that Whitney's cotton gin was an important if unintended cause of the American Civil War. After Whitney's invention, the plantation slavery industry was rejuvenated, eventually culminating in the Civil War.
And the cotton gin transformed Southern agriculture and the national economy. Southern cotton found ready markets in Europe and in the burgeoning textile mills of New England. Cotton exports from the U.S. boomed after the cotton gin's appearance – from less than in 1793 to by 1810. Cotton was a staple that could be stored for long periods and shipped long distances, unlike most agricultural products. It became the U.S.'s chief export, representing over half the value of U.S. exports from 1820 to 1860.
Paradoxically, the cotton gin, a labor-saving device, helped preserve slavery in the U.S. Before the 1790s, slave labor was primarily employed in growing rice, tobacco, and indigo, none of which were especially profitable any more. Neither was cotton, due to the difficulty of seed removal. But with the gin, growing cotton with slave labor became highly profitable – the chief source of wealth in the American South, and the basis of frontier settlement from Georgia to Texas. "King Cotton" became a dominant economic force, and slavery was sustained as a key institution of Southern society.
Interchangeable parts.
Eli Whitney has often been incorrectly credited with inventing the idea of interchangeable parts, which he championed for years as a maker of muskets; however, the idea predated Whitney, and Whitney's role in it was one of promotion and popularizing, not invention. Successful implementation of the idea eluded Whitney until near the end of his life, occurring first in others' armories.
Attempts at interchangeability of parts can be traced back as far as the Punic Wars through both archaeological remains of boats now in Museo Archeologico Baglio Anselmi and contemporary written accounts. In modern times the idea developed over decades among many people. An early leader was Jean-Baptiste Vaquette de Gribeauval, an 18th-century French artillerist who created a fair amount of standardization of artillery pieces, although not true interchangeability of parts. He inspired others, including Honoré Blanc and Louis de Tousard, to work further on the idea, and on shoulder weapons as well as artillery. In the 19th century these efforts produced the "armory system," or American system of manufacturing. Certain other New Englanders, including Captain John H. Hall and Simeon North, arrived at successful interchangeability before Whitney's armory did. The Whitney armory finally succeeded not long after his death in 1825.
The motives behind Whitney's acceptance of a contract to manufacture muskets in 1798 were mostly monetary. By the late 1790s, Whitney was on the verge of bankruptcy and the cotton gin litigation had left him deeply in debt. His New Haven cotton gin factory had burned to the ground, and litigation sapped his remaining resources. The French Revolution had ignited new conflicts between Great Britain, France, and the United States. The new American government, realizing the need to prepare for war, began to rearm. The War Department issued contracts for the manufacture of 10,000 muskets. Whitney, who had never made a gun in his life, obtained a contract in January 1798 to deliver 10,000 to 15,000 muskets in 1800. He had not mentioned interchangeable parts at that time. Ten months later, the Treasury Secretary, Oliver Wolcott, Jr., sent him a "foreign pamphlet on arms manufacturing techniques," possibly one of Honoré Blanc's reports, after which Whitney first began to talk about interchangeability.
In May 1798, Congress voted for legislation that would use eight hundred thousand dollars in order to pay for small arms and cannons in case war with France erupted. They offered a 5,000 dollar incentive with an additional 5,000 dollars once that money was exhausted for the person that was able to accurately produce arms for the government. Because the cotton gin had not brought Whitney the rewards he believed he would get, he accepted the contract. Although the contract was for one year, Whitney did not deliver the arms until eight years later in 1809 using multiple excuses for the delay of such. Recently, historians have found that during 1801–1806, Whitney took the money and headed into South Carolina in order to profit from the cotton gin.
Although Whitney's demonstration of 1801 appeared to show the ingenuity of interchangeable parts, Merritt Roe Smith concludes that Whitney's demonstration was "staged" and "duped government authorities" into believing that he had created interchangeable parts. The charade was only useful in order to gain more time and resources for the project but not to create interchangeable parts.
When the government complained that Whitney's price per musket compared unfavorably with those produced in government armories, Whitney was able to calculate an actual price per musket by including fixed costs such as insurance and machinery, which the government had not included. He thus made early contributions to both the concept of cost accounting, and the concept of the efficiency of private industry.
Milling machine.
Machine tool historian Joseph W. Roe credited Whitney with inventing the first milling machine circa 1818. Subsequent work by other historians (Woodbury; Smith; Muir; Battison ited by Baida

</doc>
<doc id="9734" url="https://en.wikipedia.org/wiki?curid=9734" title="The American Prisoner">
The American Prisoner

The American Prisoner is a British novel written by Eden Phillpotts and published in 1904 and adapted into a film by the same name in 1929. The story concerns an English woman who lives at Fox Tor farm, and an American captured during the American Revolutionary War and held at the prison at Princetown on Dartmoor.
The heroine's father, Maurice Malherb, is based on Thomas Windeatt.
In the novel "Malherb" is a miscreant who destroys Childe's tomb and beats his servant. He is depicted as a victim of his own bad temper rather than a sadist.
Malherb is introduced as the younger son of a noble family and he builds the Fox Tor house to be the impressive gentleman's residence suggested by William Crossing rather than the humble cottage which it actually is.

</doc>
<doc id="9735" url="https://en.wikipedia.org/wiki?curid=9735" title="Electromagnetic field">
Electromagnetic field

An electromagnetic field (also EM field) is a physical field produced by electrically charged objects. It affects the behavior of charged objects in the vicinity of the field. The electromagnetic field extends indefinitely throughout space and describes the electromagnetic interaction. It is one of the four fundamental forces of nature (the others are gravitation, weak interaction and strong interaction).
The field can be viewed as the combination of an electric field and a magnetic field. The electric field is produced by stationary charges, and the magnetic field by moving charges (currents); these two are often described as the sources of the field. The way in which charges and currents interact with the electromagnetic field is described by Maxwell's equations and the Lorentz force law.
From a classical perspective in the history of electromagnetism, the electromagnetic field can be regarded as a smooth, continuous field, propagated in a wavelike manner; whereas from the perspective of quantum field theory, the field is seen as quantized, being composed of individual particles.
Structure.
The electromagnetic field may be viewed in two distinct ways: a continuous structure or a discrete structure.
Continuous structure.
Classically, electric and magnetic fields are thought of as being produced by smooth motions of charged objects. For example, oscillating charges produce electric and magnetic fields that may be viewed in a 'smooth', continuous, wavelike fashion. In this case, energy is viewed as being transferred continuously through the electromagnetic field between any two locations. For instance, the metal atoms in a radio transmitter appear to transfer energy continuously. This view is useful to a certain extent (radiation of low frequency), but problems are found at high frequencies (see ultraviolet catastrophe).
Discrete structure.
The electromagnetic field may be thought of in a more 'coarse' way. Experiments reveal that in some circumstances electromagnetic energy transfer is better described as being carried in the form of packets called quanta (in this case, photons) with a fixed frequency. Planck's relation links the energy "E" of a photon to its frequency ν through the equation:
where "h" is Planck's constant, and ν is the frequency of the photon . Although modern quantum optics tells us that there also is a semi-classical explanation of the photoelectric effect—the emission of electrons from metallic surfaces subjected to electromagnetic radiation—the photon was historically (although not strictly necessarily) used to explain certain observations. It is found that increasing the intensity of the incident radiation (so long as one remains in the linear regime) increases only the number of electrons ejected, and has almost no effect on the energy distribution of their ejection. Only the frequency of the radiation is relevant to the energy of the ejected electrons.
This quantum picture of the electromagnetic field (which treats it as analogous to harmonic oscillators) has proved very successful, giving rise to quantum electrodynamics, a quantum field theory describing the interaction of electromagnetic radiation with charged matter. It also gives rise to quantum optics, which is different from quantum electrodynamics in that the matter itself is modelled using quantum mechanics rather than quantum field theory.
Dynamics.
In the past, electrically charged objects were thought to produce two different, unrelated types of field associated with their charge property. An electric field is produced when the charge is stationary with respect to an observer measuring the properties of the charge, and a magnetic field as well as an electric field is produced when the charge moves, creating an electric current with respect to this observer. Over time, it was realized that the electric and magnetic fields are better thought of as two parts of a greater whole — the electromagnetic field. Recall that until 1820 (when the Danish physicist H.C. Ørsted discovered the effect of electricity through a wire on a compass needle), electricity and magnetism had been viewed as unrelated phenomena . In 1831, Michael Faraday, one of the great thinkers of his time, made the seminal observation that time-varying magnetic fields could induce electric currents and then, in 1864, James Clerk Maxwell published his famous paper on a dynamical theory of the electromagnetic field.
Once this electromagnetic field has been produced from a given charge distribution, other charged objects in this field will experience a force in a similar way that planets experience a force in the gravitational field of the sun. If these other charges and currents are comparable in size to the sources producing the above electromagnetic field, then a new net electromagnetic field will be produced. Thus, the electromagnetic field may be viewed as a dynamic entity that causes other charges and currents to move, and which is also affected by them. These interactions are described by Maxwell's equations and the Lorentz force law. This discussion ignores the radiation reaction force.
Feedback loop.
The behavior of the electromagnetic field can be divided into four different parts of a loop:
A common misunderstanding is that (a) the quanta of the fields act in the same manner as (b) the charged particles that generate the fields. In our everyday world, charged particles, such as electrons, move slowly through matter with a drift velocity of a fraction of a centimeter (or inch) per second, but fields propagate at the speed of light - approximately 300 thousand kilometers (or 186 thousand miles) a second. The mundane speed difference between charged particles and field quanta is on the order of one to a million, more or less. Maxwell's equations relate (a) the presence and movement of charged particles with (b) the generation of fields. Those fields can then affect the force on, and can then move other slowly moving charged particles. Charged particles can move at relativistic speeds nearing field propagation speeds, but, as Einstein showed, this requires enormous field energies, which are not present in our everyday experiences with electricity, magnetism, matter, and time and space.
The feedback loop can be summarized in a list, including phenomena belonging to each part of the loop:
Mathematical description.
There are different mathematical ways of representing the electromagnetic field. The first one views the electric and magnetic fields as three-dimensional vector fields. These vector fields each have a value defined at every point of space and time and are thus often regarded as functions of the space and time coordinates. As such, they are often written as E(x, y, z, t) (electric field) and B(x, y, z, t) (magnetic field).
If only the electric field (E) is non-zero, and is constant in time, the field is said to be an electrostatic field. Similarly, if only the magnetic field (B) is non-zero and is constant in time, the field is said to be a magnetostatic field. However, if either the electric or magnetic field has a time-dependence, then both fields must be considered together as a coupled electromagnetic field using Maxwell's equations.
With the advent of special relativity, physical laws became susceptible to the formalism of tensors. Maxwell's equations can be written in tensor form, generally viewed by physicists as a more elegant means of expressing physical laws.
The behaviour of electric and magnetic fields, whether in cases of electrostatics, magnetostatics, or electrodynamics (electromagnetic fields), is governed by Maxwell's equations. In the vector field formalism, these are:
where formula_6 is the charge density, which can (and often does) depend on time and position, formula_7 is the permittivity of free space, formula_8 is the permeability of free space, and J is the current density vector, also a function of time and position. The units used above are the standard SI units. Inside a linear material, Maxwell's equations change by switching the permeability and permittivity of free space with the permeability and permittivity of the linear material in question. Inside other materials which possess more complex responses to electromagnetic fields, these terms are often represented by complex numbers, or tensors.
The Lorentz force law governs the interaction of the electromagnetic field with charged matter.
When a field travels across to different media, the properties of the field change according to the various boundary conditions. These equations are derived from Maxwell's equations.
The tangential components of the electric and magnetic fields as they relate on the boundary of two media are as follows:
The angle of refraction of an electric field between media is related to the permittivity formula_13 of each medium:
The angle of refraction of a magnetic field between media is related to the permeability formula_15 of each medium:
Properties of the field.
Reciprocal behavior of electric and magnetic fields.
The two Maxwell equations, Faraday's Law and the Ampère-Maxwell Law, illustrate a very practical feature of the electromagnetic field. Faraday's Law may be stated roughly as 'a changing magnetic field creates an electric field'. This is the principle behind the electric generator.
Ampere's Law roughly states that 'a changing electric field creates a magnetic field'. Thus, this law can be applied to generate a magnetic field and run an electric motor.
Light as an electromagnetic disturbance.
Maxwell's equations take the form of an electromagnetic wave in a volume of space not containing charges or currents (free space) – that is, where formula_6 and J are zero. Under these conditions, the electric and magnetic fields satisfy the electromagnetic wave equation:
James Clerk Maxwell was the first to obtain this relationship by his completion of Maxwell's equations with the addition of a displacement current term to Ampere's Circuital law.
Relation to and comparison with other physical fields.
Being one of the four fundamental forces of nature, it is useful to compare the electromagnetic field with the gravitational, strong and weak fields. The word 'force' is sometimes replaced by 'interaction' because modern particle physics models electromagnetism as an exchange of particles known as gauge bosons.
Electromagnetic and gravitational fields.
Sources of electromagnetic fields consist of two types of charge – positive and negative. This contrasts with the sources of the gravitational field, which are masses. Masses are sometimes described as "gravitational charges", the important feature of them being that there are only positive masses and no negative masses. Further, gravity differs from electromagnetism in that positive masses attract other positive masses whereas same charges in electromagnetism repel each other.
The relative strengths and ranges of the four interactions and other information are tabulated below:
Applications.
Static E and M fields and static EM fields.
When an EM field (see electromagnetic tensor) is not varying in time, it may be seen as a purely electrical field or a purely magnetic field, or a mixture of both. However the general case of a static EM field with both electric and magnetic components present, is the case that appears to most observers. Observers who see only an electric or magnetic field component of a static EM field, have the other (electric or magnetic) component suppressed, due to the special case of the immobile state of the charges that produce the EM field in that case. In such cases the other component becomes manifest in other observer frames.
A consequence of this, is that any case that seems to consist of a "pure" static electric or magnetic field, can be converted to an EM field, with both E and M components present, by simply moving the observer into a frame of reference which is moving with regard to the frame in which only the “pure” electric or magnetic field appears. That is, a pure static electric field will show the familiar magnetic field associated with a current, in any frame of reference where the charge moves. Likewise, any new motion of a charge in a region that seemed previously to contain only a magnetic field, will show that that the space now contains an electric field as well, which will be found to produces an additional Lorentz force upon the moving charge.
Thus, electrostatics, as well as magnetism and magnetostatics, are now seen as studies of the static EM field when a particular frame has been selected to suppress the other type of field, and since an EM field with both electric and magnetic will appear in any other frame, these "simpler" effects are merely the observer's. The "applications" of all such non-time varying (static) fields are discussed in the main articles linked in this section.
Time-varying EM fields in Maxwell’s equations.
An EM field that varies in time has two “causes” in Maxwell’s equations. One is charges and currents (so-called “sources”), and the other cause for an E or M field is a change in the other type of field (this last cause also appears in “free space” very far from currents and charges).
An electromagnetic field very far from currents and charges (sources) is called electromagnetic radiation (EMR) since it radiates from the charges and currents in the source, and has no "feedback" effect on them, and is also not affected directly by them in the present time (rather, it is indirectly produced by a sequences of changes in fields radiating out from them in the past). EMR consists of the radiations in the electromagnetic spectrum, including radio waves, microwave, infrared, visible light, ultraviolet light, X-rays, and gamma rays. The many commercial applications of these radiations are discussed in the named and linked articles.
A notable application of visible light is that this type of energy from the Sun powers all life on Earth that either makes or uses oxygen.
A changing electromagnetic field which is physically close to currents and charges (see near and far field for a definition of “close”) will have a dipole characteristic that is dominated by either a changing electric dipole, or a changing magnetic dipole. This type of dipole field near sources is called an electromagnetic "near-field".
Changing "electric" dipole fields, as such, are used commercially as near-fields mainly as a source of dielectric heating. Otherwise, they appear parasitically around conductors which absorb EMR, and around antennas which have the purpose of generating EMR at greater distances.
Changing "magnetic" dipole fields (i.e., magnetic near-fields) are used commercially for many types of magnetic induction devices. These include motors and electrical transformers at low frequencies, and devices such as metal detectors and MRI scanner coils at higher frequencies. Sometimes these high-frequency magnetic fields change at radio frequencies without being far-field waves and thus radio waves; see RFID tags.
See also near-field communication.
Further uses of near-field EM effects commercially, may be found in the article on virtual photons, since at the quantum level, these fields are represented by these particles. Far-field effects (EMR) in the quantum picture of radiation, are represented by ordinary photons.
Health and safety.
The potential health effects of the very low frequency EMFs surrounding power lines and electrical devices are the subject of on-going research and a significant amount of public debate. The US National Institute for Occupational Safety and Health (NIOSH) and other US government agencies do not consider EMFs a proven health hazard. NIOSH has issued some cautionary advisories but stresses that the data are currently too limited to draw good conclusions.
The potential effects of electromagnetic fields on human health vary widely depending on the frequency and intensity of the fields. For more information on the health effects due to specific parts of the electromagnetic spectrum, see the following articles:

</doc>
<doc id="9736" url="https://en.wikipedia.org/wiki?curid=9736" title="Empire State Building">
Empire State Building

The Empire State Building is a 102-story skyscraper located in Midtown Manhattan, New York City, on Fifth Avenue between West 33rd and 34th Streets. It has a roof height of 1,250 feet (381 m), and with its antenna spire included, it stands a total of high. Its name is derived from the nickname for New York, the Empire State. It stood as the world's tallest building for nearly 40 years, from its completion in early 1931 until the topping out of the original World Trade Center's North Tower in late 1970. Following the September 11 attacks in 2001, the Empire State Building was again the tallest building in New York, until One World Trade Center reached a greater height in April 2012. The Empire State Building is currently the fifth-tallest completed skyscraper in the United States and the 29th-tallest in the world. It is also the fifth-tallest freestanding structure in the Americas. When measured by pinnacle height, it is the fourth-tallest building in the United States.
The Empire State Building is an American cultural icon. It is designed in the distinctive Art Deco style and has been named as one of the Seven Wonders of the Modern World by the American Society of Civil Engineers. The building and its street floor interior are designated landmarks of the New York City Landmarks Preservation Commission, and confirmed by the New York City Board of Estimate. It was designated as a National Historic Landmark in 1986. In 2007, it was ranked number one on the AIA's List of America's Favorite Architecture.
The building is owned by the Empire State Realty Trust, of which Anthony Malkin serves as Chairman, CEO and President. In 2010, the Empire State Building underwent a $550 million renovation, with $120 million spent to transform the building into a more energy efficient and eco-friendly structure. The Empire State Building is the tallest Leadership in Energy and Environmental Design (LEED)-certified building in the United States, having received a gold LEED rating in September 2011.
History.
The site of the Empire State Building was first developed as the John Thompson Farm in the late 18th century. At the time, a stream ran across the site, emptying into Sunfish Pond, located a block away. Beginning in the late 19th century, the block was occupied by the Waldorf-Astoria Hotel, frequented by The Four Hundred, the social elite of New York.
The limestone for the Empire State Building came from the Empire Mill in Sanders, Indiana which is an unincorporated town adjacent to Bloomington, Indiana. The Empire Mill Land office is near State Road 37 and Old State Road 37 just south of Bloomington. The Bloomington, Bedford, and Oolitic area is known locally as the limestone capital of the world.
Design and construction.
The Empire State Building was designed by William F. Lamb from the architectural firm Shreve, Lamb and Harmon, which produced the building drawings in just two weeks, using its earlier designs for the Reynolds Building in Winston-Salem, North Carolina, and the Carew Tower in Cincinnati, Ohio (designed by the architectural firm W. W. Ahlschlager & Associates) as a basis. Every year the staff of the Empire State Building sends a Father's Day card to the staff at the Reynolds Building in Winston-Salem to pay homage to its role as predecessor to the Empire State Building. The building was designed from the top down. The general contractors were The Starrett Brothers and Eken, and the project was financed primarily by John J. Raskob and Pierre S. du Pont. The construction company was chaired by Alfred E. Smith, a former Governor of New York and James Farley's General Builders Supply Corporation supplied the building materials. John W. Bowser was project construction superintendent.
Excavation of the site began on January 22, 1930, and construction on the building itself started on March 17—St. Patrick's Day—per Al Smith's influence as Empire State, Inc. president. The project involved 3,400 workers, mostly immigrants from Europe, along with hundreds of Mohawk iron workers, many from the Kahnawake reserve near Montreal. According to official accounts, five workers died during the construction. Governor Smith's grandchildren cut the ribbon on May 1, 1931. Lewis Wickes Hine's photography of the construction provides not only invaluable documentation of the construction, but also a glimpse into common day life of workers in that era.
The construction was part of an intense competition in New York for the title of "world's tallest building". Two other projects fighting for the title, 40 Wall Street and the Chrysler Building, were still under construction when work began on the Empire State Building. Each held the title for less than a year, as the Empire State Building surpassed them upon its completion, on April 11, 1931, 12 days ahead of schedule, just 410 days after construction commenced. The building was officially opened on May 1, 1931 in dramatic fashion, when United States President Herbert Hoover turned on the building's lights with the push of a button from Washington, D.C. Ironically, the first use of tower lights atop the Empire State Building, the following year, was for the purpose of signaling the victory of Franklin D. Roosevelt over Hoover in the presidential election of November 1932.
Opening.
The building's opening coincided with the Great Depression in the United States, and as a result much of its office space was initially unrented. The building's vacancy was exacerbated by its poor location on 34th Street, which placed it relatively far from public transportation, as Grand Central Terminal and Penn Station, built decades beforehand, are several blocks away, as is the more recently built Port Authority Bus Terminal. Other more successful skyscrapers, such as the Chrysler Building, did not have this problem. In its first year of operation, the observation deck took in approximately 2 million dollars, as much money as its owners made in rent that year. The lack of renters led New Yorkers to deride the building as the "Empty State Building". The building only became profitable in 1950. The 1951 sale of the Empire State Building to Roger L. Stevens and his business partners was brokered by the prominent upper Manhattan real-estate firm Charles F. Noyes & Company for a record $51 million. At the time, that was the highest price paid for a single structure in real-estate history.
Incidents.
1945 plane crash.
At 9:40 am on Saturday, July 28, 1945, a B-25 Mitchell bomber, piloted in thick fog by Lieutenant Colonel William Franklin Smith, Jr., crashed into the north side of the Empire State Building, between the 79th and 80th floors, where the offices of the National Catholic Welfare Council were located. One engine shot through the side opposite the impact and flew as far as the next block, where it landed on the roof of a nearby building, starting a fire that destroyed a penthouse. The other engine and part of the landing gear plummeted down an elevator shaft. The resulting fire was extinguished in 40 minutes. Fourteen people were killed in the incident. Elevator operator Betty Lou Oliver survived a plunge of 75 stories inside an elevator, which still stands as the Guinness World Record for the longest survived elevator fall recorded. Despite the damage and loss of life, the building was open for business on many floors on the following Monday. The crash helped spur the passage of the long-pending Federal Tort Claims Act of 1946, as well as the insertion of retroactive provisions into the law, allowing people to sue the government for the incident.
A year later, another aircraft narrowly missed striking the building.
Suicide attempts.
Over the years, more than 30 people have attempted suicide, most successfully, by jumping from the upper parts of the building. The first suicide occurred even before its completion, by a worker who had been laid off. The fence around the observatory terrace was put up in 1947 after five people tried to jump during a three-week span.
On May 1, 1947, 23-year-old Evelyn McHale leapt to her death from the 86th floor observation deck and landed on a limousine parked at the curb. Photography student Robert Wiles took a photo of McHale's oddly intact corpse a few minutes after her death. The police found a suicide note among possessions she left on the observation deck: "He is much better off without me ... I wouldn’t make a good wife for anybody". The photo ran in the edition of May 12, 1947 of "Life" magazine, and is often referred to as "The Most Beautiful Suicide". It was later used by visual artist Andy Warhol in one of his prints entitled "Suicide (Fallen Body)".
In December 1943, ex-United States Navy gunner's mate William Lloyd Rambo jumped to his death, landing amidst Christmas shoppers on the street below.
Only one person has jumped from the upper observatory: on November 3, 1932, Frederick Eckert, of Astoria, ran past a guard in the enclosed 102nd floor gallery and jumped a gate leading to an outdoor catwalk intended for dirigible passengers. Eckert's body landed on the roof of the 86th floor observation promenade.
Two people have survived jumps, in both cases by not managing to fall more than a floor: On December 2, 1979, Elvita Adams jumped from the 86th floor, only to be blown back onto a ledge on the 85th floor by a gust of wind and left with a broken hip. On April 25, 2013, a man, who is presumed to have jumped, fell from the 86th floor observation deck but landed alive on an 85th floor ledge – where security guards managed to bring him inside; he suffered only minor injuries.
Shootings.
Two major shooting incidents have occurred at or in front of the Empire State Building.
On February 23, 1997, at about 5 p.m. EST, a gunman shot seven people on the 86th floor observation deck. Abu Kamal, a 69-year-old Palestinian teacher, killed one person and wounded six others, supposedly in response to events happening in Palestine and Israel, before committing suicide.
On August 24, 2012 at about 9 a.m. EDT, on the sidewalk at the Fifth Avenue side of the building, a gunman shot and killed a former co-worker from a workplace that had laid him off in 2011. When two police officers confronted the gunman, 58-year-old Jeffrey T. Johnson, he aimed his firearm at them. They responded by firing 16 shots at Johnson, killing him but also wounding nine bystanders, most of whom were hit by fragments, although three took direct hits from bullets.
Architecture.
Interior.
The Empire State Building rises to at the 102nd floor, and including the pinnacle, its full height reaches . The building has 85 stories of commercial and office space representing of rentable space. It has an indoor and outdoor observation deck on the 86th floor. The remaining 16 stories represent the Art Deco tower, which is capped by a 102nd-floor observatory. Atop the tower is the pinnacle, much of which is covered by broadcast antennas, with a lightning rod at the very top.
The Empire State Building was the first building to have more than 100 floors. It has 6,500 windows and 73 elevators, and there are 1,860 steps from street level to the 102nd floor. It has a total floor area of ; the base of the Empire State Building is about . The building houses 1,000 businesses and has its own ZIP code, 10118. As of 2007, approximately 21,000 employees work in the building each day, making the Empire State Building the second-largest single office complex in America, after the Pentagon. The building was completed in one year and 45 days. Its original 64 elevators are located in a central core; today, the Empire State Building has 73 elevators in all, including service elevators. It takes less than one minute by elevator to get to the 80th floor, which contains a gift shop and an exhibit detailing the building's construction. From there, visitors can take another elevator or climb the stairs to the 86th floor, where an outdoor observation deck is located. The building has of pipe, of electrical wire, and about 9,000 faucets. It is heated by low-pressure steam; despite its height, the building only requires between of steam pressure for heating. It weighs approximately . The exterior of the building is clad in Indiana limestone panels.
The Empire State Building cost $40,948,900 to build (equivalent to $ in 2016). Long-term forecasting of the life cycle of the structure was implemented at the design phase to ensure that the building's future intended uses were not restricted by the requirements of previous generations. This is particularly evident in the over-design of the building's electrical system.
The building's art deco design is typical of pre–World War II architecture in New York. The modernistic stainless steel canopies of the entrances on 33rd and 34th Streets lead to two story-high corridors around the elevator core, crossed by stainless steel and glass-enclosed bridges at the second-floor level. The elevator core contains 67 elevators.
The lobby is three stories high and features an aluminum relief of the skyscraper without the antenna, which was not added to the spire until 1952. The north corridor contained eight illuminated panels, created by Roy Sparkia and Renée Nemorov in 1963 in time for the 1964 World's Fair, which depicts the building as the Eighth Wonder of the World, alongside the traditional seven. These panels were eventually moved near a ticketing line for the observation deck.
Until the 1960s, the ceilings in the lobby had a shiny art deco mural inspired by both the sky and the Machine Age, until it was covered with ceiling tiles and fluorescent lighting. Because the original murals, designed by an artist named Leif Neandross, were damaged, reproductions were installed. Over 50 artists and workers used 15,000 square feet of aluminum and 1,300 square feet of 23-carat gold leaf to re-create the mural. Renovations to the lobby alluded to original plans for the building; replacing the clock over the information desk in the Fifth Avenue lobby with an anemometer, as well as installing two chandeliers originally intended to be part of the building when it first opened. In 2000, the building's owners installed a series of paintings by the New York artist Kysa Johnson in the concourse level. In January 2014 the artist filed suit in federal court in New York under the Visual Artists Rights Act, alleging the negligent destruction of the paintings and damage to her reputation as an artist.
The building's lobbies and common areas received a $550 million renovation in 2009, which included new air conditioning, waterproofing, and renovating the observation deck; moving the gift shop to the 80th floor. Of this, $120 million was spent in an effort to transform the building into a more energy efficient and eco-friendly structure. For example, the 6,500 windows were remanufactured onsite into superwindows which block heat but pass light. Air conditioning operating costs on hot days were reduced and this saved $17 million of the project's capital cost immediately, partly funding other retrofitting. Receiving a gold Leadership in Energy and Environmental Design (LEED) rating in September 2011, the Empire State Building is the tallest LEED certified building in the United States.
Features.
Above the 102nd floor.
On the 102nd floor of the Empire State Building there is a door with stairs ascending to the 103rd floor. This was built as a disembarkation floor for airships tethered to the building's spire, and has a circular balcony outside. It is now a hot spot for celebrities, and an access point to reach the spire for maintenance. The room now contains electrical equipment. Above the 103rd floor, there is a set of stairs and a ladder to reach the spire for maintenance work.
The building's Art Deco spire was designed to be a mooring mast and depot for dirigibles. An elevator between the 86th and 102nd floors would carry passengers after they checked in on the 86th floor. The idea proved impractical and dangerous, due to the powerful updrafts caused by the building itself, as well as the lack of mooring lines tying the other end of the craft to the ground. The building's design was expanded to include the mooring mast as part of a competition for the world's tallest building.
A large broadcast tower was added atop the spire in the early 1950s, to support the transmission antennas of several television and FM stations. Until then, NBC had exclusive rights to the site, and – beginning in 1931 – built various, smaller antennas for their television transmissions.
Broadcast stations.
New York City is the largest media market in the United States. Since the September 11 attacks, nearly all of the city's commercial broadcast stations (both television and FM radio) have transmitted from the top of the Empire State Building, although a few FM stations are located at the nearby Condé Nast Building. Most New York City AM stations broadcast from sites across the Hudson River in New Jersey or from other surrounding areas.
Broadcasting began at the Empire State Building on December 22, 1931, when RCA began transmitting experimental television broadcasts from a small antenna erected atop the spire. They leased the 85th floor and built a laboratory there, and—in 1934—RCA was joined by Edwin Howard Armstrong in a cooperative venture to test his FM system from the building's antenna. When Armstrong and RCA fell out in 1935 and his FM equipment was removed, the 85th floor became the home of RCA's New York television operations, first as experimental station W2XBS channel 1, which eventually became (on July 1, 1941) commercial station WNBT, channel 1 (now WNBC-TV channel 4). NBC's FM station (WEAF-FM, now WQHT) began transmitting from the antenna in 1940. NBC retained exclusive use of the top of the building until 1950, when the FCC ordered the exclusive deal broken, based on consumer complaints that a common location was necessary for the (now) seven New York-area television stations (five licensed to New York City, NY, one licensed to Newark, NJ, and one licensed to Secaucus, NJ) to transmit from so that receiving antennas would not have to be constantly adjusted. Construction on a giant tower began. Other television broadcasters then joined RCA at the building, on the 83rd, 82nd, and 81st floors, frequently bringing sister FM stations along for the ride. Multiple transmissions of TV and FM began from the new tower in 1951. In 1965, a separate set of FM antennas was constructed ringing the 103rd floor observation area to act as a master antenna. When the World Trade Center was being constructed, it caused serious reception problems for the television stations, most of which then moved to the World Trade Center as soon as it was completed. This made it possible to renovate the antenna structure and the transmitter facilities for the benefit of the FM stations remaining there, which were soon joined by other FMs and UHF TVs moving in from elsewhere in the metropolitan area. The destruction of the World Trade Center necessitated a great deal of shuffling of antennas and transmitter rooms to accommodate the stations moving back uptown.
As of 2012, the Empire State Building is home to the following stations:
Observation decks.
The Empire State Building has one of the most popular outdoor observatories in the world, having been visited by over 110 million people. The 86th-floor observation deck offers impressive 360-degree views of the city. There is a second observation deck on the 102nd floor that is open to the public. It was closed in 1999, but reopened in November 2005. It is completely enclosed and much smaller than the first one; it may be closed on high-traffic days. Tourists may pay to visit the observation deck on the 86th floor and an additional amount for the 102nd floor. The lines to enter the observation decks, according to Concierge.com, are "as legendary as the building itself:" there are five of them: the sidewalk line, the lobby elevator line, the ticket purchase line, the second elevator line, and the line to get off the elevator and onto the observation deck. For an extra fee tourists can skip to the front of the line. The Empire State Building makes more money from tickets sales for its observation decks than it does from renting office space.
The skyscraper's observation deck plays host to several cinematic, television, and literary classics including, "An Affair To Remember", "On the Town", "Love Affair" and "Sleepless in Seattle". In the Latin American literary classic, Giannina Braschi's Empire of Dreams the observation deck is the site of a pastoral revolution; shepherds take over the City of New York. The deck was also the site of a publicity-stunt Martian invasion in an episode of "I Love Lucy" ("Lucy Is Envious", season 3, episode 25).
New York Skyride.
The Empire State Building also has a motion simulator attraction located on the 2nd floor. Opened in 1994 as a complement to the observation deck, the New York Sky ride (or NY Sky ride) is a simulated aerial tour over the city. The cinematic presentation lasts approximately 25 minutes. As of May 2013, tickets are Adults $57, Children $42, Seniors $49.
Since its opening, the ride has gone through two incarnations. The original version, which ran from 1994 until around 2002, featured James Doohan, "" Scotty, as the airplane's pilot, who humorously tried to keep the flight under control during a storm, with the tour taking an unexpected route through the subway, Coney Island, and FAO Schwartz, among other places. After the September 11 attacks in 2001, however, the ride was closed, and an updated version debuted in mid-2002 with actor Kevin Bacon as the pilot. The new version of the narration attempted to make the attraction more educational, and included some minor post-9/11 patriotic undertones with retrospective footage of the World Trade Center. The new flight also goes haywire, but this segment is much shorter than in the original.
Lights.
In 1964, floodlights were added to illuminate the top of the building at night. Since 1976 the spire has been lit in colors chosen to match seasonal and other events, such as St. Patrick's Day, Christmas, Independence Day and Bastille Day. After the eightieth birthday and subsequent death of Frank Sinatra, for example, the building was bathed in blue light to represent the singer's nickname "Ol' Blue Eyes". After the death of actress Fay Wray ("King Kong") in late 2004, the building stood in complete darkness for 15 minutes.
The floodlights bathed the building in red, white, and blue for several months after the destruction of the World Trade Center, then reverted to the standard schedule. On June 4, 2002, the Empire State Building donned purple and gold (the royal colors of Elizabeth II), in thanks for the United Kingdom playing the Star Spangled Banner during the Changing of the Guard at Buckingham Palace on September 12, 2001 (a show of support after the September 11 attacks). This would also be shown after the Westminster Dog Show. Traditionally, in addition to the standard schedule, the building will be lit in the colors of New York's sports teams on the nights they have home games (orange, blue and white for the New York Knicks, red, white and blue for the New York Rangers, and so on). The first weekend in June finds the building bathed in green light for the Belmont Stakes held in nearby Belmont Park. The building is illuminated in tennis-ball yellow during the US Open tennis tournament in late August and early September. It was twice lit in scarlet to support nearby Rutgers University: once for a football game against the University of Louisville on November 9, 2006, and again on April 3, 2007 when the women's basketball team played in the national championship game. On January 13, 2012, the building was lit in red, orange, and yellow to honor the 60th anniversary of NBC's "The Today Show" making it the first time the building was illuminated to honor a television program. From June 1 to 3, 2012, the building was lit in blue and white, the colors of the Israeli flag, in honor of the 49th annual Celebrate Israel Parade.
During 2012, the building's metal halide lamps and floodlights were replaced with LED fixtures, increasing the available colors from nine to over 16 million. The computer-controlled system allows the building to be illuminated in ways that were unable to be done previously with plastic gels. For instance, on November 6, 2012, CNN used the top of the Empire State Building as a scoreboard for the 2012 United States presidential election. When incumbent president Barack Obama had reached the 270 electoral votes necessary to win re-election, the lights turned blue. Had Republican challenger Mitt Romney won, the building would have been lit red. Also, on November 26, 2012, the building had its first ever synchronized light show, using music from recording artist Alicia Keys. Those wishing to hear the music could tune to certain radio stations in the New York area. A video of the performance was posted online the next day. In 2013 the lights were changed to "Financial Times" pink. In the run-up week to Super Bowl XLVIII held at MetLife Stadium on February 2, 2014, the building was lit in a contest sponsored by the National Football League's wireless partner, Verizon Wireless to determine both the winner and fan support for the two teams via their team colors in the game through the #WhosGonnaWin Twitter hashtag, either the "action green" and navy blue of the Seattle Seahawks or orange and blue of the Denver Broncos, along with a light show during the game's halftime.
Height records and comparisons.
The Empire State Building remained the tallest man-made structure in the world for 23 years before it was surpassed by the Griffin Television Tower Oklahoma (KWTV Mast) in 1954. It was also the tallest free-standing structure in the world for 36 years before it was surpassed by the Ostankino Tower in 1967.
The longest world record held by the Empire State Building was for the tallest skyscraper (to structural height), which it held for 42 years until it was surpassed by the North Tower of the World Trade Center in 1972. An early-1970s proposal to dismantle the spire and replace it with an additional 11 floors, which would have brought the building's height to 1,494 feet (455 m) and made it once again the world's tallest at the time, was considered but ultimately rejected.
With the destruction of the World Trade Center in the September 11 attacks, the Empire State Building again became the tallest building in New York City, and the second-tallest building in the Americas, surpassed only by the Willis Tower in Chicago. It is currently the fifth-tallest, surpassed by the Willis Tower, the Trump International Hotel and Tower (Chicago), 432 Park Avenue and the new One World Trade Center. One World Trade Center surpassed the roof height of the Empire State Building on April 30, 2012, and became the tallest building in New York City—on the way toward becoming the tallest building in the Americas at a planned 1,776 feet (541 m).
The Empire State Building is currently the fifth-tallest completed skyscraper in the United States, after the One World Trade Center, 432 Park Avenue in New York City, the Willis Tower and Trump International Hotel and Tower, both in Chicago. It is also the 25th-tallest in the world, the tallest now is Burj Khalifa, located in Dubai. It is also the fifth-tallest freestanding structure in the Americas.
On clear days, the building can be seen from much of the New York Metropolitan Area, and as far away as New Haven, Connecticut and Morristown, New Jersey.
Neighboring Midtown Manhattan landmarks.
The Empire State Building anchors an area of Midtown which features other major Manhattan landmarks as well, including Macy's Herald Square, Koreatown, Penn Station, Madison Square Garden, and the Flower District. Together, these sites contribute to a significant volume of commuter and tourist pedestrian traffic traversing the southern portion of Midtown Manhattan.
Notable tenants.
Current
Former
References.
Notes
Citations
Bibliography

</doc>
<doc id="9737" url="https://en.wikipedia.org/wiki?curid=9737" title="Eugenics">
Eugenics

Eugenics (; from Greek εὐγενής "eugenes" "well-born" from εὖ "eu", "good, well" and γένος "genos", "race, stock, kin") is a set of beliefs and practices that aims at improving the genetic quality of the human population. It is a social philosophy advocating the improvement of human genetic traits through the promotion of higher rates of sexual reproduction for people with desired traits (positive eugenics), or reduced rates of sexual reproduction and sterilization of people with less-desired or undesired traits (negative eugenics), or both. Alternatively, gene selection rather than "people selection" has recently been made possible through advances in gene editing (e.g. CRISPR). The exact definition of "eugenics" has been a matter of debate since the term was coined. The definition of it as a "social philosophy"—that is, a philosophy with implications for social order—is not universally accepted, and was taken from Frederick Osborn's 1937 journal article "Development of a Eugenic Philosophy".
While eugenic principles have been practiced as far back in world history as Ancient Greece, the modern history of eugenics began in the early 20th century when a popular eugenics movement emerged in the United Kingdom and spread to many countries, including the United States and most European countries. In this period, eugenic ideas were espoused across the political spectrum. Consequently, many countries adopted eugenic policies meant to improve the genetic stock of their countries. Such programs often included both "positive" measures, such as encouraging individuals deemed particularly "fit" to reproduce, and "negative" measures such as marriage prohibitions and forced sterilization of people deemed unfit for reproduction. People deemed unfit to reproduce often included people with mental or physical disabilities, people who scored in the low ranges of different IQ tests, criminals and deviants, and members of disfavored minority groups. The eugenics movement became negatively associated with Nazi Germany and the Holocaust—the murder by the German state of approximately 11 million people—when many of the defendants at the Nuremberg trials attempted to justify their human rights abuses by claiming there was little difference between the Nazi eugenics programs and the US eugenics programs.
In the decades following World War II, with the institution of human rights, many countries gradually abandoned eugenics policies, although some Western countries, among them Sweden and the US, continued to carry out forced sterilizations for several decades.
Since the 1980s and 1990s when new assisted reproductive technology procedures became available, such as gestational surrogacy (available since 1985), preimplantation genetic diagnosis (available since 1989) and cytoplasmic transfer (first performed in 1996), fear about a possible future revival of eugenics and a widening of the gap between the rich and the poor has emerged.
A major criticism of eugenics policies is that, regardless of whether "negative" or "positive" policies are used, they are vulnerable to abuse because the criteria of selection are determined by whichever group is in political power. Furthermore, negative eugenics in particular is considered by many to be a violation of basic human rights, which include the right to reproduction. Another criticism is that eugenic policies eventually lead to a loss of genetic diversity, resulting in inbreeding depression instead due to a low genetic variation.
History.
The idea of eugenics to produce better human beings has existed at least since Plato suggested selective mating to produce a guardian class. The idea of eugenics to decrease the birth of inferior human beings has existed at least since William Goodell (1829-1894) advocated the castration and spaying of the insane.
However, the term "eugenics" to describe the modern concept of improving the quality of human beings born into the world was originally developed by Francis Galton. Galton had read his half-cousin Charles Darwin's theory of evolution, which sought to explain the development of plant and animal species, and desired to apply it to humans. Galton believed that desirable traits were hereditary based on biographical studies, Darwin strongly disagreed with his interpretation of the book. In 1883, one year after Darwin's death, Galton gave his research a name: "eugenics". Throughout its recent history, eugenics has remained a controversial concept.
Eugenics became an academic discipline at many colleges and universities and received funding from many sources. Organisations formed to win public support, and modify opinion towards responsible eugenic values in parenthood, included the British Eugenics Education Society of 1907, and the American Eugenics Society of 1921. Both sought support from leading clergymen, and modified their message to meet religious ideals. Three International Eugenics Conferences presented a global venue for eugenists with meetings in 1912 in London, and in 1921 and 1932 in New York. Eugenic policies were first implemented in the early 1900s in the United States. It has roots in France, Germany, Great Britain, and the United States. Later, in the 1920s and 30s, the eugenic policy of sterilizing certain mental patients was implemented in other countries, including Belgium, Brazil, Canada, Japan and Sweden.
The scientific reputation of eugenics started to decline in the 1930s, a time when Ernst Rüdin used eugenics as a justification for the racial policies of Nazi Germany. Nevertheless, in Sweden the eugenics program continued until 1975. In addition to being practised in a number of countries, eugenics was internationally organized through the International Federation of Eugenics Organizations. Its scientific aspects were carried on through research bodies such as the Kaiser Wilhelm Institute of Anthropology, Human Heredity, and Eugenics, the Cold Spring Harbour Carnegie Institution for Experimental Evolution, and the Eugenics Record Office. Its political aspects involved advocating laws allowing the pursuit of eugenic objectives, such as sterilization laws. Its moral aspects included rejection of the doctrine that all human beings are born equal, and redefining morality purely in terms of genetic fitness. Its racist elements included pursuit of a pure "Nordic race" or "Aryan" genetic pool and the eventual elimination of "less fit" races.
As a social movement, eugenics reached its greatest popularity in the early decades of the 20th century. At this point in time, eugenics was practiced around the world and was promoted by governments and influential individuals and institutions. Many countries enacted various eugenics policies and programmes, including: genetic screening, birth control, promoting differential birth rates, marriage restrictions, segregation (both racial segregation and segregation of the mentally ill from the rest of the population), compulsory sterilization, forced abortions or forced pregnancies, and genocide. Most of these policies were later regarded as coercive or restrictive, and now few jurisdictions implement policies that are explicitly labelled as eugenic or unequivocally eugenic in substance. The methods of implementing eugenics varied by country; however, some early 20th century methods involved identifying and classifying individuals and their families, including the poor, mentally ill, blind, deaf, developmentally disabled, promiscuous women, homosexuals, and racial groups (such as the Roma and Jews in Nazi Germany) as "degenerate" or "unfit", the segregation or institutionalization of such individuals and groups, their sterilization, euthanasia, and their mass murder. The practice of euthanasia was carried out on hospital patients in the Aktion T4 centers such as Hartheim Castle.
By the end of World War II, many of the discriminatory eugenics laws were largely abandoned, having become associated with Nazi Germany. After World War II, the practice of "imposing measures intended to prevent births within populatio group" fell within the definition of the new international crime of genocide, set out in the Convention on the Prevention and Punishment of the Crime of Genocide. The Charter of Fundamental Rights of the European Union also proclaims "the prohibition of eugenic practices, in particular those aiming at selection of persons". In spite of the decline in discriminatory eugenics laws, government practices of compulsive sterilization continued into the 21st century. During the ten years President Alberto Fujimori led Peru from 1990 to 2000, allegedly 2,000 persons were involuntarily sterilized. China maintained its coercive one-child policy until 2015 as well as a suite of other eugenics based legislation in order to reduce population size and manage fertility rates of different populations. In 2007 the United Nations reported coercive sterilisations and hysterectomies in Uzbekistan. During the years 2005–06 to 2012–13, nearly one-third of the 144 California prison inmates who were sterilized did not give lawful consent to the operation.
Developments in genetic, genomic, and reproductive technologies at the end of the 20th century are raising numerous questions regarding the ethical status of eugenics, effectively creating a resurgence of interest in the subject.
Some, such as UC Berkeley sociologist Troy Duster, claim that modern genetics is a back door to eugenics. This view is shared by White House Assistant Director for Forensic Sciences, Tania Simoncelli, who stated in a 2003 publication by the Population and Development Program at Hampshire College that advances in pre-implantation genetic diagnosis (PGD) are moving society to a "new era of eugenics", and that, unlike the Nazi eugenics, modern eugenics is consumer driven and market based, "where children are increasingly regarded as made-to-order consumer products". In a 2006 newspaper article, Richard Dawkins said that discussion regarding eugenics was inhibited by the shadow of Nazi misuse, to the extent that some scientists would not admit that breeding humans for certain abilities is at all possible. He believes that it is not physically different from breeding domestic animals for traits such as speed or herding skill. Dawkins felt that enough time had elapsed to at least ask just what the ethical differences were between breeding for ability versus training athletes or forcing children to take music lessons, though he could think of persuasive reasons to draw the distinction.
Some, such as Nathaniel C. Comfort from Johns Hopkins University, claim that the change from state-led reproductive-genetic decision-making to individual choice has moderated the worst abuses of eugenics by transferring the decision-making from the state to the patient and their family. Comfort suggests that "he eugenic impulse drives us to eliminate disease, live longer and healthier, with greater intelligence, and a better adjustment to the conditions of society; and the health benefits, the intellectual thrill and the profits of genetic bio-medicine are too great for us to do otherwise." Others, such as bioethicist Stephen Wilkinson of Keele University and Honorary Research Fellow Eve Garrard at the University of Manchester, claim that some aspects of modern genetics can be classified as eugenics, but that this classification does not inherently make modern genetics immoral. In a co-authored publication by Keele University, they stated that "ugenics doesn't seem always to be immoral, and so the fact that PGD, and other forms of selective reproduction, might sometimes technically be eugenic, isn't sufficient to show that they're wrong."
In October 2015, the United Nations' International Bioethics Committee wrote that the ethical problems of human genetic engineering should not be confused with the ethical problems of the 20th century eugenics movements; however, it is still problematic because it challenges the idea of human equality and opens up new forms of discrimination and stigmatization for those who do not want or cannot afford the enhancements.
Meanings and types.
The term eugenics and its modern field of study were first formulated by Francis Galton in 1883, drawing on the recent work of his half-cousin Charles Darwin. Galton published his observations and conclusions in his book "Inquiries into Human Faculty and Its Development".
The origins of the concept began with certain interpretations of Mendelian inheritance, and the theories of August Weismann. The word "eugenics" is derived from the Greek word "eu" ("good" or "well") and the suffix "-genēs" ("born"), and was coined by Galton in 1883 to replace the word "stirpiculture", which he had used previously but which had come to be mocked due to its perceived sexual overtones. Galton defined eugenics as "the study of all agencies under human control which can improve or impair the racial quality of future generations". Galton did not understand the mechanism of inheritance.
Eugenics has, from the very beginning, meant many different things. Historically, the term has referred to everything from prenatal care for mothers to forced sterilization and euthanasia. To population geneticists, the term has included the avoidance of inbreeding without altering allele frequencies; for example, J. B. S. Haldane wrote that "the motor bus, by breaking up inbred village communities, was a powerful eugenic agent." Debate as to what exactly counts as eugenics has continued to the present day.
Edwin Black, journalist and author of "War Against the Weak", claims eugenics is often deemed a pseudoscience because what is defined as a genetic improvement of a desired trait is often deemed a cultural choice rather than a matter that can be determined through objective scientific inquiry. The most disputed aspect of eugenics has been the definition of "improvement" of the human gene pool, such as what is a beneficial characteristic and what is a defect. This aspect of eugenics has historically been tainted with scientific racism.
Early eugenists were mostly concerned with perceived intelligence factors that often correlated strongly with social class. Some of these early eugenists include Karl Pearson and Walter Weldon, who worked on this at the University College London.
Eugenics also had a place in medicine. In his lecture "Darwinism, Medical Progress and Eugenics", Karl Pearson said that everything concerning eugenics fell into the field of medicine. He basically placed the two words as equivalents. He was supported in part by the fact that Francis Galton, the father of eugenics, also had medical training.
Eugenic policies have been conceptually divided into two categories. Positive eugenics is aimed at encouraging reproduction among the genetically advantaged; for example, the reproduction of the intelligent, the healthy, and the successful. Possible approaches include financial and political stimuli, targeted demographic analyses, "in vitro" fertilization, egg transplants, and cloning. The movie Gattaca provides a fictional example of positive eugenics done voluntarily. Negative eugenics aimed to eliminate, through sterilization or segregation, those deemed physically, mentally, or morally "undesirable". This includes abortions, sterilization, and other methods of family planning. Both positive and negative eugenics can be coercive; abortion for fit women, for example, was illegal in Nazi Germany.
Jon Entine claims that eugenics simply means "good genes" and using it as synonym for genocide is an "all-too-common distortion of the social history of genetics policy in the United States." According to Entine, eugenics developed out of the Progressive Era and not "Hitler's twisted Final Solution".
Implementation methods.
According to Richard Lynn, eugenics may be divided into two main categories based on the ways in which the methods of eugenics can be applied.
Arguments.
Doubts on traits triggered by inheritance.
The first major challenge to conventional eugenics based upon genetic inheritance was made in 1915 by Thomas Hunt Morgan, who demonstrated the event of genetic mutation occurring outside of inheritance involving the discovery of the hatching of a fruit fly ("Drosophila melanogaster") with white eyes from a family of red-eyes. Morgan claimed that this demonstrated that major genetic changes occurred outside of inheritance and that the concept of eugenics based upon genetic inheritance was not completely scientifically accurate. Additionally, Morgan criticized the view that subjective traits, such as intelligence and criminality, were caused by heredity because he believed that the definitions of these traits varied and that accurate work in genetics could only be done when the traits being studied were accurately defined. In spite of Morgan's public rejection of eugenics, much of his genetic research was absorbed by eugenics.
Ethics.
A common criticism of eugenics is that "it inevitably leads to measures that are unethical". Historically, this statement is evidenced by the obvious control of one group imposing its agenda on minority groups. This includes programs in England, Germany, and America targeting various groups, including Jews, homosexuals, Muslims, Romani, the homeless, and those with intellectual disabilities.
Many of the ethical concerns from eugenics arise from the controversial past, prompting a discussion on what place, if any, it should have in the future. Advances in science have changed eugenics. In the past, eugenics has had more to do with sterilization and enforced reproduction laws (i.e. no inter-racial marriage and marriage restrictions based on land ownership). Now, in the age of a progressively mapped genome, embryos can be tested for susceptibility to disease, gender, and genetic defects, and alternative methods of reproduction such as in vitro fertilization are becoming more common. In short, eugenics is no longer ex post facto regulation of the living but instead preemptive action on the unborn.
With this change, however, there are ethical concerns which lack adequate attention, and which must be addressed before eugenic policies can be properly implemented in the future. Sterilized individuals, for example, could volunteer for the procedure, albeit under incentive or duress, or at least voice their opinion. The unborn fetus on which these new eugenic procedures are performed cannot speak out, as the fetus lacks the voice to consent or to express his or her opinion. The ability to manipulate a fetus and determine who the child will be is something questioned by many of the opponents of, and even proponents for, eugenic policies.
Societal and political consequences of eugenics call for a place in the discussion on the ethics behind the eugenics movement. Public policy often focuses on issues related to race and gender, both of which could be controlled by manipulation of embryonic genes; eugenics and political issues are interconnected and the political aspect of eugenics must be addressed. Laws controlling the subjects, the methods, and the extent of eugenics will need to be considered in order to prevent the repetition of the unethical events of the past.
Most of the ethical concerns about eugenics involve issues of morality and power. Decisions about the morality and the control of this new science (and the subsequent results of the science) will need to be made as eugenics continue to influence the development of the science and medical fields.
Losing genetic diversity by classifying traits as diseases.
Eugenic policies could also lead to loss of genetic diversity, in which case a culturally accepted "improvement" of the gene pool could very likely—as evidenced in numerous instances in isolated island populations (e.g., the dodo, "Raphus cucullatus", of Mauritius)—result in extinction due to increased vulnerability to disease, reduced ability to adapt to environmental change, and other factors both known and unknown. A long-term species-wide eugenics plan might lead to a scenario similar to this because the elimination of traits deemed undesirable would reduce genetic diversity by definition.
Edward M. Miller claims that, in any one generation, any realistic program should make only minor changes in a fraction of the gene pool, giving plenty of time to reverse direction if unintended consequences emerge, reducing the likelihood of the elimination of desirable genes. Miller also argues that any appreciable reduction in diversity is so far in the future that little concern is needed for now.
While the science of genetics has increasingly provided means by which certain characteristics and conditions can be identified and understood, given the complexity of human genetics, culture, and psychology there is at this point no agreed objective means of determining which traits might be ultimately desirable or undesirable. Some diseases such as sickle-cell disease and cystic fibrosis respectively confer immunity to malaria and resistance to cholera when a single copy of the recessive allele is contained within the genotype of the individual. Reducing the instance of sickle-cell disease genes in Africa where malaria is a common and deadly disease could indeed have extremely negative net consequences.
However, some genetic diseases such as haemochromatosis can increase susceptibility to illness, cause physical deformities, and other dysfunctions, which provides some incentive for people to re-consider some elements of eugenics.
Autistic people have advocated a shift in perception of autism spectrum disorders as complex syndromes rather than diseases that must be cured. Proponents of this view reject the notion that there is an "ideal" brain configuration and that any deviation from the norm is pathological; they promote tolerance for what they call neurodiversity. Baron-Cohen argues that the genes for Asperger's combination of abilities have operated throughout recent human evolution and have made remarkable contributions to human history. The possible reduction of autism rates through selection against the genetic predisposition to autism is a significant political issue in the autism rights movement, which claims that autism is a part of neurodiversity.
Many culturally Deaf people oppose attempts to cure deafness, believing instead deafness should be considered a defining cultural characteristic not a disease. Some people have started advocating the idea that deafness brings about certain advantages, often termed "Deaf Gain."
Heterozygous recessive traits.
The heterozygote test is used for the early detection of recessive hereditary diseases, allowing for couples to determine if they are at risk of passing genetic defects to a future child. The goal of the test is to estimate the likelihood of passing the hereditary disease to future descendants.
Recessive traits can be severely reduced, but never eliminated unless the complete genetic makeup of all members of the pool was known, as aforementioned. As only very few undesirable traits, such as Huntington's disease, are dominant, it could be argued from certain perspectives that the practicality of "eliminating" traits is quite low.
There are examples of eugenic acts that managed to lower the prevalence of recessive diseases, although not influencing the prevalence of heterozygote carriers of those diseases. The elevated prevalence of certain genetically transmitted diseases among the Ashkenazi Jewish population (Tay–Sachs, cystic fibrosis, Canavan's disease, and Gaucher's disease), has been decreased in current populations by the application of genetic screening.
Pleiotropic genes.
Pleiotropy occurs when one gene influences multiple, seemingly unrelated phenotypic traits, an example being phenylketonuria, which is a human disease that affects multiple systems but is caused by one gene defect. Andrzej Pękalski, from the University of Wrocław, argues that eugenics can cause harmful loss of genetic diversity if a eugenics program selects for a pleiotropic gene that is also associated with a positive trait. Pekalski uses the example of a coercive government eugenics program that prohibits people with myopia from breeding but has the unintended consequence of also selecting against high intelligence since the two go together.
Supporters and critics.
At its peak of popularity, eugenics was supported by a wide variety of prominent people, including Winston Churchill, Margaret Sanger, Marie Stopes, H. G. Wells, Norman Haire, Havelock Ellis, Theodore Roosevelt, Herbert Hoover, George Bernard Shaw, John Maynard Keynes, John Harvey Kellogg, Robert Andrews Millikan, Linus Pauling, Sidney Webb, and W. E. B. Du Bois.
In 1909 the Anglican clergymen William Inge and James Peile both wrote for the British Eugenics Education Society. Inge was an invited speaker at the 1921 International Eugenics Conference, which was also endorsed by the Roman Catholic Archbishop of New York Patrick Joseph Hayes.
In 1925 Adolf Hitler praised and incorporated eugenic ideas in "Mein Kampf" and emulated eugenic legislation for the sterilization of "defectives" that had been pioneered in the United States.
Early critics of the philosophy of eugenics included the American sociologist Lester Frank Ward, the English writer G. K. Chesterton, the German-American anthropologist Franz Boas, and Scottish tuberculosis pioneer and author Halliday Sutherland. Ward's 1913 article "Eugenics, Euthenics, and Eudemics", Chesterton's 1917 book "", and Boas' 1916 article "Eugenics" (published in "The Scientific Monthly") were all harshly critical of the rapidly growing movement. Sutherland identified eugenists as a major obstacle to the eradication and cure of tuberculosis in his 1917 address "Consumption: Its Cause and Cure", and criticism of eugenists and Neo-Malthusians in his 1921 book "Birth Control" led to a writ for libel from the eugenist Marie Stopes. Several biologists were also antagonistic to the eugenics movement, including Lancelot Hogben. Other biologists such as J. B. S. Haldane and R. A. Fisher expressed skepticism that sterilization of "defectives" would lead to the disappearance of undesirable genetic traits.
Some supporters of eugenics later reversed their positions on it. For example, H. G. Wells, who had called for "the sterilization of failures" in 1904, stated in his 1940 book "The Rights of Man: Or What are we fighting for?" that among the human rights he believed should be available to all people was "a prohibition on mutilation, sterilization, torture, and any bodily punishment".
Among institutions, the Catholic Church was an opponent of state-enforced sterilizations. Attempts by the Eugenics Education Society to persuade the British government to legalise voluntary sterilisation were opposed by Catholics and by the Labour Party. The American Eugenics Society initially gained some Catholic supporters, but Catholic support declined following the 1930 papal encyclical "Casti connubii". In this, Pope Pius XI explicitly condemned sterilization laws: "Public magistrates have no direct power over the bodies of their subjects; therefore, where no crime has taken place and there is no cause present for grave punishment, they can never directly harm, or tamper with the integrity of the body, either for the reasons of eugenics or for any other reason."

</doc>
<doc id="9738" url="https://en.wikipedia.org/wiki?curid=9738" title="Email">
Email

Electronic mail, most commonly called email or e-mail since around 1993, is a method of exchanging digital messages from an author to one or more recipients. Email operates across the Internet or other computer networks .
Some early email systems required the author and the recipient to both be online at the same time, in common with instant messaging. Today's email systems are based on a store-and-forward model. Email servers accept, forward, deliver, and store messages. Neither the users nor their computers are required to be online simultaneously; they need connect only briefly, typically to a mail server, for as long as it takes to send or receive messages.
Historically, the term "electronic mail" was used generically for any electronic document transmission. For example, several writers in the early 1970s used the term to describe fax document transmission. As a result, it is difficult to find the first citation for the use of the term with the more specific meaning it has today.
An Internet email message consists of three components, the message "envelope", the message "header", and the message "body". The message header contains control information, including, minimally, an originator's email address and one or more recipient addresses. Usually descriptive information is also added, such as a subject header field and a message submission date/time stamp.
Originally an ASCII text-only communications medium, Internet email was extended by Multipurpose Internet Mail Extensions (MIME)
to carry text in other character sets and multi-media content attachments. International email, with internationalized email addresses using UTF-8, has been standardized, but not yet widely adopted.
Electronic mail predates the inception of the Internet and was in fact a crucial tool in creating it, but the history of modern, global Internet email services reaches back to the early ARPANET. Standards for encoding email messages were proposed as early as 1973 (RFC 561). Conversion from ARPANET to the Internet in the early 1980s produced the core of the current services. An email message sent in the early 1970s looks quite similar to a basic text message sent on the Internet today.
Email is an information and communications technology. It uses technology to communicate a digital message over the Internet. Users use email differently, based on how they think about it. There are many software platforms available to send and receive. Popular email platforms include Gmail, Hotmail, Yahoo! Mail, Outlook, and many others.
Network-based email was initially exchanged on the ARPANET in extensions to the File Transfer Protocol (FTP), but is now carried by the Simple Mail Transfer Protocol (SMTP), first published as Internet standard 10 (RFC 821) in 1982. In the process of transporting email messages between systems, SMTP communicates delivery parameters using a message "envelope" separate from the message (header and body) itself.
Spelling.
Electronic mail has several English spelling options:
Origin.
The AUTODIN network, first operational in 1962, provided a message service between 1,350 terminals, handling 30 million messages per month, with an average message length of approximately 3,000 characters. Autodin was supported by 18 large computerized switches, and was connected to the United States General Services Administration Advanced Record System, which provided similar services to roughly 2,500 terminals.
Host-based mail systems.
With the introduction of MIT's Compatible Time-Sharing System (CTSS) in 1961 multiple users were able to log into a central system from remote dial-up terminals, and to store and share files on the central disk. Informal methods of using this to pass messages were developed and expanded :
Developers of other early systems developed similar email applications:
These original messaging systems had widely different features and ran on systems that were incompatible with each other. Most of them only allowed communication between users logged into the same host or "mainframe", although there might be hundreds or thousands of users within an organization.
LAN email systems.
In the early 1980s, networked personal computers on LANs became increasingly important. Server-based systems similar to the earlier mainframe systems were developed. Again, these systems initially allowed communication only between users logged into the same server infrastructure. Examples include:
Eventually these systems too could link different organizations as long as they ran the same email system and proprietary protocol.
Email networks.
To facilitate electronic mail exchange between remote sites and with other organizations, telecommunication links, such as dialup modems or leased lines, provided means to transport email globally, creating local and global networks. This was challanging for a number of reasons, including the widely different email address formats in use. 
Attempts at interoperability.
Early interoperability among independent systems included:
From SNDMSG to MSG.
In the early 1970s, Ray Tomlinson updated an existing utility called SNDMSG so that it could copy messages (as files) over the network. Lawrence Roberts The project manager for the ARPANET development, took the idea of READMAIL, which dumped all "recent" messages onto the user's terminal, and wrote a programme for TENEX in TECO macros called "RD", which permitted access to individual messages. Barry Wessler then updated RD and called it "NRD".
Marty Yonke rewrote NRD to include reading, access to SNDMSG for sending, and a help system, and called the utility "WRD", which was later known as "BANANARD". John Vittal then updated this version to include three important commands: "Move" (combined save/delete command), "Answer" (determined to whom a reply should be sent) and "Forward" (sent an email to a person who was not already a recipient). The system was called "MSG". With inclusion of these features, MSG is considered to be the first integrated modern email programme, from which many other applications have descended.
ARPANET mail.
Experimental email transfers between separate computer systems began shortly after the creation of the ARPANET in 1969. Ray Tomlinson is generally credited as having sent the first email across a network, initiating the use of the "@" sign to separate the names of the user and the user's machine in 1971, when he sent a message from one Digital Equipment Corporation DEC-10 computer to another DEC-10. The two machines were placed next to each other. Tomlinson's work was quickly adopted across the ARPANET, which significantly increased the popularity of email.
Initially addresses were of the form, "username@hostname" but were extended to "username@host.domain" with the development of the Domain Name System (DNS).
As the influence of the ARPANET spread across academic communities, gateways were developed to pass mail to and from other networks such as CSNET, JANET, BITNET, X.400, and FidoNet. This often involved addresses such as:
which routes mail to a user with a "bang path" address at a UUCP host.
Operation.
The diagram to the right shows a typical sequence of events that takes place when sender Alice transmits a message using a mail user agent (MUA) addressed to the email address of the recipient.
In addition to this example, alternatives and complications exist in the email system:
Many MTAs used to accept messages for any recipient on the Internet and do their best to deliver them. Such MTAs are called "open mail relays". This was very important in the early days of the Internet when network connections were unreliable. However, this mechanism proved to be exploitable by originators of unsolicited bulk email and as a consequence open mail relays have become rare, and many MTAs do not accept messages from open mail relays.
Message format.
The Internet email message format is now defined by RFC 5322, with multi-media content attachments being defined in RFC 2045 through RFC 2049, collectively called "Multipurpose Internet Mail Extensions" or "MIME". RFC 5322 replaced the earlier RFC 2822 in 2008, and in turn RFC 2822 in 2001 replaced RFC 822 – which had been the standard for Internet email for nearly 20 years. Published in 1982, RFC 822 was based on the earlier RFC 733 for the ARPANET.
Internet email messages consist of two major sections, the message header and the message body. The header is structured into fields such as From, To, CC, Subject, Date, and other information about the email. The body contains the message, as unstructured text, sometimes containing a signature block at the end. The header is separated from the body by a blank line.
Message header.
Each message has exactly one header, which is structured into fields. Each field has a name and a value. RFC 5322 specifies the precise syntax.
Informally, each line of text in the header that begins with a printable character begins a separate field. The field name starts in the first character of the line and ends before the separator character ":". The separator is then followed by the field value (the "body" of the field). The value is continued onto subsequent lines if those lines have a space or tab as their first character. Field names and values are restricted to 7-bit ASCII characters. Non-ASCII values may be represented using MIME encoded words.
Header fields.
Email header fields can be multi-line, and each line should be at most 78 characters long and in no event more than 998 characters long. Header fields defined by RFC 5322 can only contain US-ASCII characters; for encoding characters in other sets, a syntax specified in RFC 2047 can be used. Recently the IETF EAI working group has defined some standards track extensions, replacing previous experimental extensions, to allow UTF-8 encoded Unicode characters to be used within the header. In particular, this allows email addresses to use non-ASCII characters. Such characters must only be used by servers that support these extensions.
The message header must include at least the following fields:
The message header should include at least the following fields:
RFC 3864 describes registration procedures for message header fields at the IANA; it provides for permanent and provisional message header field names, including also fields defined for MIME, netnews, and http, and referencing relevant RFCs. Common header fields for email include:
Note that the "To:" field is not necessarily related to the addresses to which the message is delivered. The actual delivery list is supplied separately to the transport protocol, SMTP, which may or may not originally have been extracted from the header content. The "To:" field is similar to the addressing at the top of a conventional letter which is delivered according to the address on the outer envelope. In the same way, the "From:" field does not have to be the real sender of the email message. Some mail servers apply email authentication systems to messages being relayed. Data pertaining to server's activity is also part of the header, as defined below.
SMTP defines the "trace information" of a message, which is also saved in the header using the following two fields:
Other header fields that are added on top of the header by the receiving server may be called "trace fields", in a broader sense.
Message body.
Content encoding.
Email was originally designed for 7-bit ASCII. Most email software is 8-bit clean but must assume it will communicate with 7-bit servers and mail readers. The MIME standard introduced character set specifiers and two content transfer encodings to enable transmission of non-ASCII data: quoted printable for mostly 7 bit content with a few characters outside that range and base64 for arbitrary binary data. The 8BITMIME and BINARY extensions were introduced to allow transmission of mail without the need for these encodings, but many mail transport agents still do not support them fully. In some countries, several encoding schemes coexist; as the result, by default, the message in a non-Latin alphabet language appears in non-readable form (the only exception is coincidence, when the sender and receiver use the same encoding scheme). Therefore, for international character sets, Unicode is growing in popularity.
Plain text and HTML.
Most modern graphic email clients allow the use of either plain text or HTML for the message body at the option of the user. HTML email messages often include an automatically generated plain text copy as well, for compatibility reasons.
Advantages of HTML include the ability to include in-line links and images, set apart previous messages in block quotes, wrap naturally on any display, use emphasis such as underlines and italics, and change font styles. Disadvantages include the increased size of the email, privacy concerns about web bugs, abuse of HTML email as a vector for phishing attacks and the spread of malicious software.
Some web based mailing lists recommend that all posts be made in plain-text, with 72 or 80 characters per line for all the above reasons, but also because they have a significant number of readers using text-based email clients such as Mutt.
Some Microsoft email clients allow rich formatting using their proprietary Rich Text Format (RTF), but this should be avoided unless the recipient is guaranteed to have a compatible email client.
Servers and client applications.
Messages are exchanged between hosts using the Simple Mail Transfer Protocol with software programs called mail transfer agents (MTAs); and delivered to a mail store by programs called mail delivery agents (MDAs, also sometimes called local delivery agents, LDAs). Users can retrieve their messages from servers using standard protocols such as POP or IMAP, or, as is more likely in a large corporate environment, with a proprietary protocol specific to Novell Groupwise, Lotus Notes or Microsoft Exchange Servers. Webmail interfaces allow users to access their mail with any standard web browser, from any computer, rather than relying on an email client. Programs used by users for retrieving, reading, and managing email are called mail user agents (MUAs).
Mail can be stored on the client, on the server side, or in both places. Standard formats for mailboxes include Maildir and mbox. Several prominent email clients use their own proprietary format and require conversion software to transfer email between them. Server-side storage is often in a proprietary format but since access is through a standard protocol such as IMAP, moving email from one server to another can be done with any MUA supporting the protocol.
Accepting a message obliges an MTA to deliver it, and when a message cannot be delivered, that MTA must send a bounce message back to the sender, indicating the problem.
Filename extensions.
Upon reception of email messages, email client applications save messages in operating system files in the file system. Some clients save individual messages as separate files, while others use various database formats, often proprietary, for collective storage. A historical standard of storage is the "mbox" format. The specific format used is often indicated by special filename extensions:
Some applications (like Apple Mail) leave attachments encoded in messages for searching while also saving separate copies of the attachments. Others separate attachments from messages and save them in a specific directory.
URI scheme mailto.
The URI scheme, as registered with the IANA, defines the mailto: scheme for SMTP email addresses. Though its use is not strictly defined, URLs of this form are intended to be used to open the new message window of the user's mail client when the URL is activated, with the address as defined by the URL in the "To:" field.
Types.
Web-based email.
Many email providers have a web-based email client (e.g. AOL Mail, Gmail, Outlook.com and Yahoo! Mail). This allows users to log into the email account by using any compatible web browser to send and receive their email. Mail is typically not downloaded to the client, so can't be read without a current Internet connection.
POP3 email services.
The Post Office Protocol 3 (POP3) is a mail access protocol used by a client application to read messages from the mail server. Received messages are often deleted from the server. POP supports simple download-and-delete requirements for access to remote mailboxes (termed maildrop in the POP RFC's).
IMAP email servers.
The Internet Message Access Protocol (IMAP) provides features to manage a mailbox from multiple devices. Small portable devices like smartphones are increasingly used to check email while travelling, and to make brief replies, larger devices with better keyboard access being used to reply at greater length. IMAP shows the headers of messages, the sender and the subject and the device needs to request to download specific messages. Usually mail is left in folders in the mail server.
MAPI email servers.
Messaging Application Programming Interface (MAPI) is a messaging architecture and an API based on the Component Object Model (COM) for Microsoft Windows.
Use.
Flaming.
Flaming occurs when a person sends a message with angry or antagonistic content. The term is derived from the use of the word Incendiary to describe particularly heated email discussions. Flaming is assumed to be more common today because of the ease and impersonality of email communications: confrontations in person or via telephone require direct interaction, where social norms encourage civility, whereas typing a message to another person is an indirect interaction, so civility may be forgotten.
Email bankruptcy.
Also known as "email fatigue", email bankruptcy is when a user ignores a large number of email messages after falling behind in reading and answering them. The reason for falling behind is often due to information overload and a general sense there is so much information that it is not possible to read it all. As a solution, people occasionally send a boilerplate message explaining that the email inbox is being cleared out. Harvard University law professor Lawrence Lessig is credited with coining this term, but he may only have popularized it.
In business.
Email was been widely accepted by business as one of the key parts of an 'e-revolution' in business communication. It is now the most widely used medium of communication within the business world. A 2010 study on workplace communication by Paytronics found 83% of U.S. knowledge workers felt email was critical to their success and productivity at work.
It has some key benefits to business, including:
Email marketing.
Email marketing via "opt-in" is often successfully used to send special sales offerings and new product information, but offering hyperlinks or generic information on consumer trends is less useful - and email sent without permission such as "opt-in" is likely to be viewed as unwelcome "email spam".
Mobile.
Email has become widely used on smart phones. Mobile apps for email increase accessibility to the medium. While before users could only access email on computers, it is now possible for users to check their email out of the home and out of the library while on the go. Alerts can also be sent to the phone to notify them immediately of new messages. This has given email the ability to be used for more frequent communication between users and allowed them to check their email and write messages throughout the day. Today, there are an estimated 1.4 billion email users worldwide and 50 billion non-spam emails that are sent daily.
It was found that US adults check their email more than they browse the web or check their Facebook accounts, making email the most popular activity for users to do on their smart phones. 78% of the respondents in the study revealed that they check their email on their phone. It was also found that 30% of consumers use only their smartphone to check their email, and 91% were likely to check their email at least once per day on their smartphone. However, the percentage of consumers using email on smartphone ranges and differs dramatically across different countries. For example, in comparison to 75% of those consumers in the US who used it, only 17% in India did.
Problems.
Attachment size limitation.
Email messages may have one or more attachments, i.e. MIME parts intended to provide copies of files. Attachments serve the purpose of delivering binary or text files of unspecified size. In principle there is no technical intrinsic restriction in the InternetMessage Format, SMTP protocol or MIME limiting the size or number of attachments. In practice, however, email service providers implement various limitations on the permissible size of files or the size of an entire message.
Furthermore, due to technical reasons, often a small attachment can increase in size when sent, which can be confusing to senders when trying to assess whether they can or cannot send a file by email, and this can result in their message being rejected.
As larger and larger file sizes are being created and traded, many users are either forced to upload and download their files using an FTP server, or more popularly, use online file sharing facilities or services, usually over web-friendly HTTP, in order to send and receive them.
Information overload.
A December 2007 New York Times blog post described information overload as "a $650 Billion Drag on the Economy", and the New York Times reported in April 2008 that "E-MAIL has become the bane of some people's professional lives" due to information overload, yet "none of the current wave of high-profile Internet start-ups focused on email really eliminates the problem of email overload because none helps us prepare replies". GigaOm posted a similar article in September 2010, highlighting research that found 57% of knowledge workers were overwhelmed by the volume of email they received. Technology investors reflect similar concerns.
In October 2010, CNN published an article titled "Happy Information Overload Day" that compiled research about "email overload" from IT companies and productivity experts. According to Basex, the average knowledge worker receives 93 messages per day. Subsequent studies have reported higher numbers. Marsha Egan, an email productivity expert, called email technology both a blessing and a curse in the article. She stated, "Everyone just learns that they have to have it dinging and flashing and open just in case the boss e-mails," she said. "The best gift any group can give each other is to never use e-mail urgently. If you need it within three hours, pick up the phone."
Spamming and computer viruses.
The usefulness of email is being threatened by four phenomena: email bombardment, spamming, phishing, and email worms.
Spamming is unsolicited commercial (or bulk) email. Because of the minuscule cost of sending email, spammers can send hundreds of millions of email messages each day over an inexpensive Internet connection. Hundreds of active spammers sending this volume of mail results in information overload for many computer users who receive voluminous unsolicited email each day.
Email worms use email as a way of replicating themselves into vulnerable computers. Although the first email worm affected UNIX computers, the problem is most common today on the Microsoft Windows operating system.
The combination of spam and worm programs results in users receiving a constant drizzle of junk email, which reduces the usefulness of email as a practical tool.
A number of anti-spam techniques mitigate the impact of spam. In the United States, U.S. Congress has also passed a law, the Can Spam Act of 2003, attempting to regulate such email. Australia also has very strict spam laws restricting the sending of spam from an Australian ISP, but its impact has been minimal since most spam comes from regimes that seem reluctant to regulate the sending of spam.
Email spoofing.
Email spoofing occurs when the email message header is designed to make the message appear to come from a known or trusted source. Email spam and phishing methods typically use spoofing to mislead the recipient about the true message origin.
Email bombing.
Email bombing is the intentional sending of large volumes of messages to a target address. The overloading of the target email address can render it unusable and can even cause the mail server to crash.
Privacy concerns.
Today it can be important to distinguish between Internet and internal email systems. Internet email may travel and be stored on networks and computers without the sender's or the recipient's control. During the transit time it is possible that third parties read or even modify the content. Internal mail systems, in which the information never leaves the organizational network, may be more secure, although information technology personnel and others whose function may involve monitoring or managing may be accessing the email of other employees.
Email privacy, without some security precautions, can be compromised because:
There are cryptography applications that can serve as a remedy to one or more of the above. For example, Virtual Private Networks or the Tor anonymity network can be used to encrypt traffic from the user machine to a safer network while GPG, PGP, SMEmail, or S/MIME can be used for end-to-end message encryption, and SMTP STARTTLS or SMTP over Transport Layer Security/Secure Sockets Layer can be used to encrypt communications for a single mail hop between the SMTP client and the SMTP server.
Additionally, many mail user agents do not protect logins and passwords, making them easy to intercept by an attacker. Encrypted authentication schemes such as SASL prevent this.
Finally, attached files share many of the same hazards as those found in peer-to-peer filesharing. Attached files may contain trojans or viruses.
Tracking of sent mail.
The original SMTP mail service provides limited mechanisms for tracking a transmitted message, and none for verifying that it has been delivered or read. It requires that each mail server must either deliver it onward or return a failure notice (bounce message), but both software bugs and system failures can cause messages to be lost. To remedy this, the IETF introduced Delivery Status Notifications (delivery receipts) and Message Disposition Notifications (return receipts); however, these are not universally deployed in production. (A complete Message Tracking mechanism was also defined, but it never gained traction; see RFCs 3885 through 3888.)
Many ISPs now deliberately disable non-delivery reports (NDRs) and delivery receipts due to the activities of spammers:
In the absence of standard methods, a range of system based around the use of web bugs have been developed. However, these are often seen as underhand or raising privacy concerns, and only work with e-mail clients that support rendering of HTML. Many mail clients now default to not showing "web content". Webmail providers can also disrupt web bugs by pre-caching images.
U.S. government.
The U.S. state and federal governments have been involved in electronic messaging and the development of email in several different ways.
Starting in 1977, the U.S. Postal Service (USPS) recognized that electronic messaging and electronic transactions posed a significant threat to First Class mail volumes and revenue. The USPS explored an electronic messaging initiative in 1977 and later disbanded it. Twenty years later, in 1997, when email volume overtook postal mail volume, the USPS was again urged to embrace email, and the USPS declined to provide email as a service. The USPS initiated an experimental email service known as E-COM. E-COM provided a method for the simple exchange of text messages. In 2011, shortly after the USPS reported its state of financial bankruptcy, the USPS Office of Inspector General (OIG) began exploring the possibilities of generating revenue through email servicing. Electronic messages were transmitted to a post office, printed out, and delivered as hard copy. To take advantage of the service, an individual had to transmit at least 200 messages. The delivery time of the messages was the same as First Class mail and cost 26 cents. Both the Postal Regulatory Commission and the Federal Communications Commission opposed E-COM. The FCC concluded that E-COM constituted common carriage under its jurisdiction and the USPS would have to file a tariff. Three years after initiating the service, USPS canceled E-COM and attempted to sell it off.
The early ARPANET dealt with multiple email clients that had various, and at times incompatible, formats. For example, in the Multics, the "@" sign meant "kill line" and anything before the "@" sign was ignored, so Multics users had to use a command-line option to specify the destination system. The Department of Defense DARPA desired to have uniformity and interoperability for email and therefore funded efforts to drive towards unified inter-operable standards. This led to David Crocker, John Vittal, Kenneth Pogran, and Austin Henderson publishing RFC 733, "Standard for the Format of ARPA Network Text Message" (November 21, 1977), a subset of which provided a stable base for common use on the ARPANET, but which was not fully effective, and in 1979, a meeting was held at BBN to resolve incompatibility issues. Jon Postel recounted the meeting in RFC 808, "Summary of Computer Mail Services Meeting Held at BBN on 10 January 1979" (March 1, 1982), which includes an appendix listing the varying email systems at the time. This, in turn, led to the release of David Crocker's RFC 822, "Standard for the Format of ARPA Internet Text Messages" (August 13, 1982). RFC 822 is a small adaptation of RFC 733's details, notably enhancing the host portion, to use Domain Names, that were being developed at the same time.
The National Science Foundation took over operations of the ARPANET and Internet from the Department of Defense, and initiated NSFNet, a new backbone for the network. A part of the NSFNet AUP forbade commercial traffic. In 1988, Vint Cerf arranged for an interconnection of MCI Mail with NSFNET on an experimental basis. The following year Compuserve email interconnected with NSFNET. Within a few years the commercial traffic restriction was removed from NSFNETs AUP, and NSFNET was privatised.
In the late 1990s, the Federal Trade Commission grew concerned with fraud transpiring in email, and initiated a series of procedures on spam, fraud, and phishing. In 2004, FTC jurisdiction over spam was codified into law in the form of the CAN SPAM Act. Several other U.S. federal agencies have also exercised jurisdiction including the Department of Justice and the Secret Service.
NASA has provided email capabilities to astronauts aboard the Space Shuttle and International Space Station since 1991 when a Macintosh Portable was used aboard Space Shuttle mission STS-43 to send the first email via AppleLink. Today astronauts aboard the International Space Station have email capabilities via the wireless networking throughout the station and are connected to the ground at 10 Mbit/s Earth to station and 3 Mbit/s station to Earth, comparable to home DSL connection speeds.

</doc>
