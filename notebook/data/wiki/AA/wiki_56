<doc id="7903" url="https://en.wikipedia.org/wiki?curid=7903" title="Diffie–Hellman key exchange">
Diffie–Hellman key exchange

Diffie–Hellman key exchange (D–H) is a specific method of securely exchanging cryptographic keys over a public channel and was one of the first public-key protocols as originally conceptualized by Ralph Merkle and named after Whitfield Diffie and Martin Hellman. D–H is one of the earliest practical examples of public key exchange implemented within the field of cryptography. Traditionally, secure encrypted communication between two parties required that they first exchange keys by some secure physical channel, such as paper key lists transported by a trusted courier. The Diffie–Hellman key exchange method allows two parties that have no prior knowledge of each other to jointly establish a shared secret key over an insecure channel. This key can then be used to encrypt subsequent communications using a symmetric key cipher.
Diffie–Hellman is used to secure a variety of Internet services. However, research published in October 2015 suggests that the parameters in use for many D-H Internet applications at that time are not strong enough to prevent compromise by very well-funded attackers, such as the security services of large governments.
The scheme was first published by Whitfield Diffie and Martin Hellman in 1976. By 1975, James H. Ellis, Clifford Cocks and Malcolm J. Williamson within GCHQ, the British signals intelligence agency, had previously shown how public-key cryptography could be achieved; however, their work was kept secret until 1997.
Although Diffie–Hellman key agreement itself is a non-authenticated key-agreement protocol, it provides the basis for a variety of authenticated protocols, and is used to provide forward secrecy in Transport Layer Security's ephemeral modes (referred to as EDH or DHE depending on the cipher suite).
The method was followed shortly afterwards by RSA, an implementation of public-key cryptography using asymmetric algorithms.
, from 1977, is now expired and describes the now-public domain algorithm. It credits Hellman, Diffie, and Merkle as inventors.
Name.
In 2002, Hellman suggested the algorithm be called Diffie–Hellman–Merkle key exchange in recognition of Ralph Merkle's contribution to the invention of public-key cryptography (Hellman, 2002), writing:
Description.
General overview.
Diffie–Hellman Key Exchange establishes a shared secret between two parties that can be used for secret communication for exchanging data over a public network. The following conceptual diagram illustrates the general idea of the key exchange by using colors instead of very large numbers.
The process begins by having the two parties, Alice and Bob, agree on an arbitrary starting color that does not need to be kept secret (but should be different every time); in this example the color is yellow. Each of them selects a secret color–red and aqua respectively–that they keep to themselves. The crucial part of the process is that Alice and Bob now mix their secret color together with their mutually shared color, resulting in orange and blue mixtures respectively, then publicly exchange the two mixed colors. Finally, each of the two mix together the color they received from the partner with their own private color. The result is a final color mixture (brown) that is identical to the partner's color mixture.
If another party (usually named "Eve" in cryptology publications, Eve being a third-party who is considered to be an eavesdropper) had been listening in on the exchange, it would be computationally difficult for that person to determine the common secret color; in fact, when using large numbers rather than colors, this action is impossible for modern supercomputers to do in a reasonable amount of time.
Cryptographic explanation.
The simplest and the original implementation of the protocol uses the multiplicative group of integers modulo "p", where "p" is prime, and "g" is a primitive root modulo "p". These two values are chosen in this way to ensure that the resulting shared secret can take on any value from 1 to "p"–1. Here is an example of the protocol, with non-secret values in blue, and secret values in red.
Both Alice and Bob have arrived at the same value s, because, under mod p,
formula_1
Note that only "a", "b", and "(g" mod "p" = "g" mod "p)" are kept secret. All the other values – "p", "g", "g" mod "p", and "g" mod "p" – are sent in the clear. Once Alice and Bob compute the shared secret they can use it as an encryption key, known only to them, for sending messages across the same open communications channel.
Of course, much larger values of "a", "b", and "p" would be needed to make this example secure, since there are only 23 possible results of "n" mod 23. However, if "p" is a prime of at least 600 digits, then even the fastest modern computers cannot find "a" given only "g", "p" and "g" mod "p". Such a problem is called the discrete logarithm problem. The computation of "g" mod "p" is known as modular exponentiation and can be done efficiently even for large numbers.
Note that "g" need not be large at all, and in practice is usually a small integer (like 2, 3, ...).
Generalization to finite cyclic groups.
Here is a more general description of the protocol:
Both Alice and Bob are now in possession of the group element "g", which can serve as the shared secret key. The group "G" satisfies the requisite condition for secure communication if there is not an efficient algorithm for determining whether "g" = "g" given "g", "g", and "g" for some "c" ∈ "G".
Secrecy chart.
The chart below depicts who knows what, again with non-secret values in blue, and secret values in red. Here Eve is an eavesdropper—she watches what is sent between Alice and Bob, but she does not alter the contents of their communications.
Now s is the shared secret key and it is known to both Alice and Bob, but "not" to Eve.
Note: It should be difficult for Alice to solve for Bob's private key or for Bob to solve for Alice's private key. If it is not difficult for Alice to solve for Bob's private key (or vice versa), Eve may simply substitute her own private / public key pair, plug Bob's public key into her private key, produce a fake shared secret key, and solve for Bob's private key (and use that to solve for the shared secret key. Eve may attempt to choose a public / private key pair that will make it easy for her to solve for Bob's private key).
Another demonstration of Diffie-Hellman (also using numbers too small for practical use) is given here.
Operation with more than two parties.
Diffie–Hellman key agreement is not limited to negotiating a key shared by only two participants. Any number of users can take part in an agreement by performing iterations of the agreement protocol and exchanging intermediate data (which does not itself need to be kept secret). For example, Alice, Bob, and Carol could participate in a Diffie–Hellman agreement as follows, with all operations taken to be modulo "p":
An eavesdropper has been able to see "g", "g", "g", "g", "g", and "g", but cannot use any combination of these to efficiently reproduce "g".
To extend this mechanism to larger groups, two basic principles must be followed:
These principles leave open various options for choosing in which order participants contribute to keys. The simplest and most obvious solution is to arrange the "N" participants in a circle and have "N" keys rotate around the circle, until eventually every key has been contributed to by all "N" participants (ending with its owner) and each participant has contributed to "N" keys (ending with their own). However, this requires that every participant perform "N" modular exponentiations.
By choosing a more optimal order, and relying on the fact that keys can be duplicated, it is possible to reduce the number of modular exponentiations performed by each participant to using a divide-and-conquer-style approach, given here for eight participants:
Once this operation has been completed all participants will possess the secret "g", but each participant will have performed only four modular exponentiations, rather than the eight implied by a simple circular arrangement.
Security.
The protocol is considered secure against eavesdroppers if "G" and "g" are chosen properly. In particular, the order of the group G must be large, particularly if the same group is used for large amounts of traffic. The eavesdropper ("Eve") has to solve the Diffie–Hellman problem to obtain "g". This is currently considered difficult for groups whose order is large enough. An efficient algorithm to solve the discrete logarithm problem would make it easy to compute "a" or "b" and solve the Diffie–Hellman problem, making this and many other public key cryptosystems insecure. Fields of small characteristic may be less secure.
The order of "G" should have a large prime factor to prevent use of the Pohlig–Hellman algorithm to obtain "a" or "b". For this reason, a Sophie Germain prime "q" is sometimes used to calculate , called a safe prime, since the order of "G" is then only divisible by 2 and "q". "g" is then sometimes chosen to generate the order "q" subgroup of "G", rather than "G", so that the Legendre symbol of "g" never reveals the low order bit of "a". A protocol using such a choice is for example IKEv2.
"g" is often a small integer such as 2. Because of the random self-reducibility of the discrete logarithm problem a small "g" is equally secure as any other generator of the same group.
If Alice and Bob use random number generators whose outputs are not completely random and can be predicted to some extent, then Eve's task is much easier.
In the original description, the Diffie–Hellman exchange by itself does not provide authentication of the communicating parties and is thus vulnerable to a man-in-the-middle attack. Mallory may establish two distinct key exchanges, one with Alice and the other with Bob, effectively masquerading as Alice to Bob, and vice versa, allowing her to decrypt, then re-encrypt, the messages passed between them. Note that Mallory must continue to be in the middle, transferring messages every time Alice and Bob communicate. If she is ever absent, her previous presence is then revealed to Alice and Bob. They will know that all of their private conversations had been intercepted and decoded by someone in the channel.
A method to authenticate the communicating parties to each other is generally needed to prevent this type of attack. Variants of Diffie–Hellman, such as STS protocol, may be used instead to avoid these types of attacks.
Practical attacks on Internet traffic.
The number field sieve algorithm, which is generally the most effective in solving the discrete logarithm problem, consists of four computational steps. The first three steps only depend on the order of the group G, not on the specific number whose finite log is desired. It turns out that much Internet traffic uses one of a handful of groups that are of order 1024-bits or less. By precomputing the first three steps of the number field sieve for the most common groups, an attacker need only carry out the last step, which is much less computationally expensive than the first three steps, to obtain a specific logarithm. The Logjam attack used this vulnerability to compromise a variety of Internet services that allowed the use of groups whose order was a 512-bit prime number, so called export grade. The authors needed several thousand CPU cores for a week to precompute data for a single 512-bit prime. Once that was done, however, individual logarithms could be solved in about a minute using two 18-core Intel Xeon CPUs.
As estimated by the authors behind the Logjam attack, the much more difficult precomputation needed to solve the discrete log problem for a 1024-bit prime would cost on the order of $100 million, well within the budget of large national intelligence agency such as the U.S. National Security Agency (NSA). The Logjam authors speculate that precomputation against widely reused 1024 DH primes is behind claims in leaked NSA documents that NSA is able break much of current crypto.
To avoid these vulnerabilities, authors recommend use of elliptic curve cryptography, for which no similar attack is known. Failing that, they recommend that the order, "p", of the Diffie–Hellman group should be at least 2048 bits. They estimate that the pre-computation required for a 2048-bit prime is 10 more difficult than for 1024-bit primes.
If NSA is breaking Diffie–Hellman, but has not pushed for US sites to upgrade to longer keys, then it would be an example of NSA's NOBUS policy of not closing security holes that NSA believes only they can exploit.
Other uses.
Encryption.
Public key encryption schemes based on the Diffie–Hellman key exchange have been proposed. The first such scheme is the ElGamal encryption. A more modern variant is the Integrated Encryption Scheme.
Forward secrecy.
Protocols that achieve forward secrecy generate new key pairs for each session and discard them at the end of the session.
The Diffie–Hellman key exchange is a frequent choice for such protocols, because of its fast key generation.
Password-authenticated key agreement.
When Alice and Bob share a password, they may use a password-authenticated key agreement (PK) form of Diffie–Hellman to prevent man-in-the-middle attacks. One simple scheme is to compare the hash of s concatenated with the password calculated independently on both ends of channel. A feature of these schemes is that an attacker can only test one specific password on each iteration with the other party, and so the system provides good security with relatively weak passwords. This approach is described in ITU-T Recommendation X.1035, which is used by the G.hn home networking standard.
Public key.
It is also possible to use Diffie–Hellman as part of a public key infrastructure, allowing Bob to encrypt a message so that only Alice will be able to decrypt it, with no prior communication between them other than Bob having trusted knowledge of Alice's public key. Alice's public key is formula_2. To send her a message, Bob chooses a random "b" and then sends Alice formula_3 (un-encrypted) together with the message encrypted with symmetric key formula_4. Only Alice can determine the symmetric key and hence decrypt the message because only she has "a" (the private key). A pre-shared public key also prevents man-in-the-middle attacks.
In practice, Diffie–Hellman is not used in this way, with RSA being the dominant public key algorithm. This is largely for historical and commercial reasons, namely that RSA Security created a certificate authority for key signing that became Verisign. Diffie–Hellman cannot be used to sign certificates. However, the ElGamal and DSA signature algorithms are mathematically related to it, as well as MQV, STS and the IKE component of the IPsec protocol suite for securing Internet Protocol communications.

</doc>
<doc id="7906" url="https://en.wikipedia.org/wiki?curid=7906" title="Destry Rides Again">
Destry Rides Again

Destry Rides Again is a 1939 western starring Marlene Dietrich and James Stewart, and directed by George Marshall. The supporting cast includes Mischa Auer, Charles Winninger, Brian Donlevy, Allen Jenkins, Irene Hervey, Billy Gilbert, Bill Cody, Jr., Lillian Yarbo, and Una Merkel. It bears no relation to Max Brand's popular novel; the characters and story are completely different and unrelated.
In 1996, "Destry Rides Again" was selected for preservation in the United States National Film Registry by the Library of Congress as being "culturally, historically, or aesthetically significant".
Plot.
Saloon owner Kent (Brian Donlevy), the unscrupulous boss of the fictional Western town of Bottleneck, has the town's sheriff, Keogh, killed when Keogh asks one too many questions about a rigged poker game. Kent and "Frenchy" (Marlene Dietrich), his girlfriend and the dance hall queen, now have a stranglehold over the local cattle ranchers. The crooked town's mayor, Hiram J. Slade (Samuel S. Hinds), who is in collusion with Kent, appoints the town drunk, Washington Dimsdale (Charles Winninger), as the new sheriff, assuming that he will be easy to control and manipulate. But what the mayor does not know is that Dimsdale was a deputy under the famous lawman, Tom Destry, and is able to call upon the latter's equally formidable son, Tom Destry, Jr. (James Stewart), to help him make Bottleneck a lawful, respectable town. 
Destry confounds the townsfolk by refusing to strap on a gun in spite of demonstrating that he is an expert marksman. He still carries out the "letter of the law", as deputy sheriff, and earns their respect. A final confrontation between Destry and Kent's gang is inevitable, but "Frenchy" is won over by Destry and changes sides. A final gunfight ensues where Frenchy is killed in the crossfire, and the rule of law wins the day.
Cast.
As appearing in screen credits:
Songs.
Dietrich sings "See What the Boys in the Back Room Will Have" and "You've Got That Look", written by Frank Loesser, set to music by Frederick Hollander, which have become classics.
Production.
Famed Western writer Max Brand contributed the novel, "Destry Rides Again", but the film also owes its origins to Brand's serial "Twelve Peers", published in a pulp-magazine. In the original work, Harrison (or "Harry") Destry was not a pacifist. As filmed in 1932, with Tom Mix in the starring role, the central character differed in that Destry did wear six-guns in that version.
The film was James Stewart's first western (he would not return to the genre until 1950, with "Broken Arrow" and "Winchester 73"), and was also notable for a ferocious cat-fight between Marlene Dietrich and Una Merkel, which apparently caused a mild censorship problem at the time of release.
According to writer/director Peter Bogdanovich, Marlene Dietrich told him during an aircraft flight that she and James Stewart had an affair during shooting and that she became pregnant and had the baby surreptitiously aborted without telling Stewart.
Internationally, the film was released under the alternate titles "" in French and "Arizona" in Spanish.
Reception.
"Destry Rides Again" was generally well accepted by the public, as well as critics. It was reviewed by Frank S. Nugent in "The New York Times," who noted that the film did not follow the usual Hollywood type-casting. On Dietrich's role, he characterized, "It's difficult to reconcile Miss Dietrich's Frenchy, the cabaret girl of the Bloody Gulch Saloon, with the posed and posturing Dietrich we last saw in Mr. Lubitsch's 'Angel'." Stewart's contribution was similarly treated, "turning in an easy, likable, pleasantly humored performance."
References.
Notes
Bibliography

</doc>
<doc id="7921" url="https://en.wikipedia.org/wiki?curid=7921" title="Derivative">
Derivative

The derivative of a function of a real variable measures the sensitivity to change of a quantity (a function value or dependent variable) which is determined by another quantity (the independent variable). Derivatives are a fundamental tool of calculus. For example, the derivative of the position of a moving object with respect to time is the object's velocity: this measures how quickly the position of the object changes when time is advanced. 
The derivative of a function of a single variable at a chosen input value, when it exists, is the slope of the tangent line to the graph of the function at that point. The tangent line is the best linear approximation of the function near that input value. For this reason, the derivative is often described as the "instantaneous rate of change", the ratio of the instantaneous change in the dependent variable to that of the independent variable.
Derivatives may be generalized to functions of several real variables. In this generalization, the derivative is reinterpreted as a linear transformation whose graph is (after an appropriate translation) the best linear approximation to the graph of the original function. The Jacobian matrix is the matrix that represents this linear transformation with respect to the basis given by the choice of independent and dependent variables. It can be calculated in terms of the partial derivatives with respect to the independent variables. For a real-valued function of several variables, the Jacobian matrix reduces to the gradient vector.
The process of finding a derivative is called differentiation. The reverse process is called "antidifferentiation". The fundamental theorem of calculus states that antidifferentiation is the same as integration. Differentiation and integration constitute the two fundamental operations in single-variable calculus.
Differentiation and derivative.
"Differentiation" is the action of computing a derivative. The derivative of a function of a variable is a measure of the rate at which the value of the function changes with respect to the change of the variable. It is called the "derivative" of with respect to . If and are real numbers, and if the graph of is plotted against , the derivative is the slope of this graph at each point.
The simplest case, apart from the trivial case of a constant function, is when is a linear function of , meaning that the graph of divided by is a line. In this case, , for real numbers and , and the slope is given by
where the symbol (Delta) is an abbreviation for "change in." This formula is true because
Thus, since
it follows that
This gives an exact value for the slope of a line.
If the function is not linear (i.e. its graph is not a straight line), however, then the change in divided by the change in varies: differentiation is a method to find an exact value for this rate of change at any given value of .
The idea, illustrated by Figures 1 to 3, is to compute the rate of change as the limit value of the ratio of the differences as becomes infinitely small.
Notation.
Two distinct notations are commonly used for the derivative, one deriving from Leibniz and the other from Joseph Louis Lagrange.
In Leibniz's notation, an infinitesimal change in is denoted by , and the derivative of with respect to is written
suggesting the ratio of two infinitesimal quantities. (The above expression is read as "the derivative of "y" with respect to "x"", "d y by d x", or "d y over d x". The oral form "d y d x" is often used conversationally, although it may lead to confusion.)
In Lagrange's notation, the derivative with respect to of a function is denoted (read as "f prime of x") or (read as "f prime x of x"), in case of ambiguity of the variable implied by the derivation. Lagrange's notation is sometimes incorrectly attributed to Newton.
Rigorous definition.
The most common approach to turn this intuitive idea into a precise definition is to define the derivative as a limit of difference quotients of real numbers. This is the approach described below.
Let be a real valued function defined in an open neighborhood of a real number . In classical geometry, the tangent line to the graph of the function at was the unique line through the point that did "not" meet the graph of transversally, meaning that the line did not pass straight through the graph. The derivative of with respect to at is, geometrically, the slope of the tangent line to the graph of at . The slope of the tangent line is very close to the slope of the line through and a nearby point on the graph, for example . These lines are called secant lines. A value of close to zero gives a good approximation to the slope of the tangent line, and smaller values (in absolute value) of will, in general, give better approximations. The slope of the secant line is the difference between the values of these points divided by the difference between the values, that is, 
This expression is Newton's difference quotient. Passing from an approximation to an exact answer is done using a limit. Geometrically, the limit of the secant lines is the tangent line. Therefore, the limit of the difference quotient as approaches zero, if it exists, should represent the slope of the tangent line to . This limit is defined to be the derivative of the function at :
When the limit exists, is said to be "differentiable" at . Here is one of several common notations for the derivative (see below).
Equivalently, the derivative satisfies the property that
which has the intuitive interpretation (see Figure 1) that the tangent line to at gives the "best linear approximation"
to near (i.e., for small ). This interpretation is the easiest to generalize to other settings (see below).
Substituting 0 for in the difference quotient causes division by zero, so the slope of the tangent line cannot be found directly using this method. Instead, define to be the difference quotient as a function of :
In practice, the existence of a continuous extension of the difference quotient to is shown by modifying the numerator to cancel in the denominator. Such manipulations can make the limit value of for small clear even though is still not defined at . This process can be long and tedious for complicated functions, and many shortcuts are commonly used to simplify the process.
Definition over the hyperreals.
Relative to a hyperreal extension of the real numbers, the derivative of a real function at a real point can be defined as the shadow of the quotient for infinitesimal , where . Here the natural extension of to the hyperreals is still denoted . Here the derivative is said to exist if the shadow is independent of the infinitesimal chosen.
Example.
The squaring function is differentiable at , and its derivative there is 6. This result is established by calculating the limit as approaches zero of the difference quotient of :
The last expression shows that the difference quotient equals when and is undefined when , because of the definition of the difference quotient. However, the definition of the limit says the difference quotient does not need to be defined when . The limit is the result of letting go to zero, meaning it is the value that tends to as becomes very small:
Hence the slope of the graph of the squaring function at the point is 6, and so its derivative at is .
More generally, a similar computation shows that the derivative of the squaring function at is .
Continuity and differentiability.
If is differentiable at , then must also be continuous at . As an example, choose a point and let be the step function that returns a value, say 1, for all less than , and returns a different value, say 10, for all greater than or equal to . cannot have a derivative at . If is negative, then is on the low part of the step, so the secant line from to is very steep, and as tends to zero the slope tends to infinity. If is positive, then is on the high part of the step, so the secant line from to has slope zero. Consequently, the secant lines do not approach any single slope, so the limit of the difference quotient does not exist.
However, even if a function is continuous at a point, it may not be differentiable there. For example, the absolute value function is continuous at , but it is not differentiable there. If is positive, then the slope of the secant line from 0 to is one, whereas if is negative, then the slope of the secant line from 0 to is negative one. This can be seen graphically as a "kink" or a "cusp" in the graph at . Even a function with a smooth graph is not differentiable at a point where its tangent is vertical: For instance, the function is not differentiable at .
In summary: for a function to have a derivative it is "necessary" for the function to be continuous, but continuity alone is not "sufficient".
Most functions that occur in practice have derivatives at all points or at almost every point. Early in the history of calculus, many mathematicians assumed that a continuous function was differentiable at most points. Under mild conditions, for example if the function is a monotone function or a Lipschitz function, this is true. However, in 1872 Weierstrass found the first example of a function that is continuous everywhere but differentiable nowhere. This example is now known as the Weierstrass function. In 1931, Stefan Banach proved that the set of functions that have a derivative at some point is a meager set in the space of all continuous functions. Informally, this means that hardly any continuous functions have a derivative at even one point.
The derivative as a function.
Let be a function that has a derivative at every point in the domain of . Because every point has a derivative, there is a function that sends the point to the derivative of at . This function is written and is called the "derivative function" or the "derivative" of . The derivative of collects all the derivatives of at all the points in the domain of .
Sometimes has a derivative at most, but not all, points of its domain. The function whose value at equals whenever is defined and elsewhere is undefined is also called the derivative of . It is still a function, but its domain is strictly smaller than the domain of .
Using this idea, differentiation becomes a function of functions: The derivative is an operator whose domain is the set of all functions that have derivatives at every point of their domain and whose range is a set of functions. If we denote this operator by , then is the function . Since is a function, it can be evaluated at a point . By the definition of the derivative function, .
For comparison, consider the doubling function ; is a real-valued function of a real number, meaning that it takes numbers as inputs and has numbers as outputs:
The operator , however, is not defined on individual numbers. It is only defined on functions:
Because the output of is a function, the output of can be evaluated at a point. For instance, when is applied to the squaring function, , outputs the doubling function , which we named . This output function can then be evaluated to get , , and so on.
Higher derivatives.
Let be a differentiable function, and let be its derivative. The derivative of (if it has one) is written and is called the "second derivative of ". Similarly, the derivative of a second derivative, if it exists, is written and is called the "third derivative of ". Continuing this process, one can define, if it exists, the th derivative as the derivative of the th derivative. These repeated derivatives are called "higher-order derivatives". The th derivative is also called the derivative of order .
If represents the position of an object at time , then the higher-order derivatives of have physical interpretations. The second derivative of is the derivative of , the velocity, and by definition this is the object's acceleration. The third derivative of is defined to be the jerk, and the fourth derivative is defined to be the jounce.
A function need not have a derivative, for example, if it is not continuous. Similarly, even if does have a derivative, it may not have a second derivative. For example, let
Calculation shows that is a differentiable function whose derivative is
On the real line, every polynomial function is infinitely differentiable. By standard differentiation rules, if a polynomial of degree is differentiated times, then it becomes a constant function. All of its subsequent derivatives are identically zero. In particular, they exist, so polynomials are smooth functions.
The derivatives of a function at a point provide polynomial approximations to that function near . For example, if is twice differentiable, then
in the sense that
If is infinitely differentiable, then this is the beginning of the Taylor series for evaluated at around .
Inflection point.
A point where the second derivative of a function changes sign is called an "inflection point". At an inflection point, the second derivative may be zero, as in the case of the inflection point of the function , or it may fail to exist, as in the case of the inflection point of the function . At an inflection point, a function switches from being a convex function to being a concave function or vice versa.
Notation (details).
Leibniz's notation.
The notation for derivatives introduced by Gottfried Leibniz is one of the earliest. It is still commonly used when the equation is viewed as a functional relationship between dependent and independent variables. Then the first derivative is denoted by
and was once thought of as an infinitesimal quotient. Higher derivatives are expressed using the notation
for the "n"th derivative of (with respect to "x"). These are abbreviations for multiple applications of the derivative operator. For example,
With Leibniz's notation, we can write the derivative of "y" at the point in two different ways:
Leibniz's notation allows one to specify the variable for differentiation (in the denominator). This is especially relevant for partial differentiation. It also makes the chain rule easy to remember:
Lagrange's notation.
Sometimes referred to as "prime notation", one of the most common modern notation for differentiation is due to Joseph-Louis Lagrange and uses the prime mark, so that the derivative of a function "f"("x") is denoted "f"′("x") or simply "f"′. Similarly, the second and third derivatives are denoted
To denote the number of derivatives beyond this point, some authors use Roman numerals in superscript, whereas others place the number in parentheses:
The latter notation generalizes to yield the notation "f" for the "n"th derivative of "f" – this notation is most useful when we wish to talk about the derivative as being a function itself, as in this case the Leibniz notation can become cumbersome.
Newton's notation.
Newton's notation for differentiation, also called the dot notation, places a dot over the function name to represent a time derivative. If , then
denote, respectively, the first and second derivatives of "y" with respect to "t". This notation is used exclusively for time derivatives, meaning that the independent variable of the function represents time. It is very common in physics and in mathematical disciplines connected with physics such as differential equations. While the notation becomes unmanageable for high-order derivatives, in practice only very few derivatives are needed.
Fluent and fluxions.
Newton tried to explain calculus using fluent and fluxions. He said that the rate of generation is the fluxion of the fluent, which is denoted by the variable with a dot over it. Then the rate of the fluxion is the second fluxion, which has two dots over it. These fluxions were thought of, as very close to zero but not quite zero. But when you multiply two fluxions together you get something that is so close to zero that it is treated as zero. Newton took derivatives by replacing all the "x" values with formula_30 and all the y values with formula_31 and then used derivative rules to take the derivative and solve for formula_32 
Here is an example:
formula_33
Using the fact that formula_34 we can see formula_35 and formula_36 so formula_37.
Newton described mathematical quantities to be like continuous motion. This motion, he said, could be thought of in the same way that a point traces a curve. He defined this quantity and called it a “fluent”. He went on to name the rate at which these quantities change. Newton called this the “fluxion of the fluent” and he represented it by formula_38.
So, if the fluent was represented by "x", Newton denoted its fluxion by formula_38, the second fluxion by formula_40, and so on. This can be related to the modern language we use to describe derivatives. In modern language, the fluxion of the variable "x" relative to an independent time-variable "t" would be its velocity . In other words, the derivative of "f"("x") with respect to time, "t", is .
Moment of the fluent.
Newton called "o" the moment of the fluent. The moment of the fluent represents the infinitely small part by which a fluent was increased in a small time interval. Once he allowed himself to divide through by "o" (although "o" can not be treated as zero because that would make the division illegitimate). Newton decided it was justifiable to drop all terms containing "o".
Euler's notation.
Euler's notation uses a differential operator "D", which is applied to a function "f" to give the first derivative "Df". The second derivative is denoted "D""f", and the "n"th derivative is denoted "D""f".
If is a dependent variable, then often the subscript "x" is attached to the "D" to clarify the independent variable "x".
Euler's notation is then written
although this subscript is often omitted when the variable "x" is understood, for instance when this is the only variable present in the expression.
Euler's notation is useful for stating and solving linear differential equations.
Rules of computation.
The derivative of a function can, in principle, be computed from the definition by considering the difference quotient, and computing its limit. In practice, once the derivatives of a few simple functions are known, the derivatives of other functions are more easily computed using "rules" for obtaining derivatives of more complicated functions from simpler ones.
Rules for basic functions.
Most derivative computations eventually require taking the derivative of some common functions. The following incomplete list gives some of the most frequently used functions of a single real variable and their derivatives.
where "r" is any real number, then
wherever this function is defined. For example, if formula_45, then
and the derivative function is defined only for positive "x", not for . When , this rule implies that "f"′("x") is zero for , which is almost the constant rule (stated below).
Rules for combined functions.
In many cases, complicated limit calculations by direct application of Newton's difference quotient can be avoided using differentiation rules. Some of the most basic rules are the following.
Computation example.
The derivative of
is
Here the second term was computed using the chain rule and third using the product rule. The known derivatives of the elementary functions "x", "x", sin("x"), ln("x") and , as well as the constant 7, were also used.
Derivatives in higher dimensions.
Derivatives of vector valued functions.
A vector-valued function y("t") of a real variable sends real numbers to vectors in some vector space R. A vector-valued function can be split up into its coordinate functions "y"("t"), "y"("t"), …, "y"("t"), meaning that . This includes, for example, parametric curves in R or R. The coordinate functions are real valued functions, so the above definition of derivative applies to them. The derivative of y("t") is defined to be the vector, called the tangent vector, whose coordinates are the derivatives of the coordinate functions. That is,
Equivalently,
if the limit exists. The subtraction in the numerator is subtraction of vectors, not scalars. If the derivative of y exists for every value of "t", then y′ is another vector valued function.
If e, …, e is the standard basis for R, then y("t") can also be written as . If we assume that the derivative of a vector-valued function retains the linearity property, then the derivative of y("t") must be
because each of the basis vectors is a constant.
This generalization is useful, for example, if y("t") is the position vector of a particle at time "t"; then the derivative y′("t") is the velocity vector of the particle at time "t".
Partial derivatives.
Suppose that "f" is a function that depends on more than one variable—for instance,
"f" can be reinterpreted as a family of functions of one variable indexed by the other variables:
In other words, every value of "x" chooses a function, denoted "f", which is a function of one real number. That is,
Once a value of "x" is chosen, say "a", then determines a function "f" that sends "y" to :
In this expression, "a" is a "constant", not a "variable", so "f" is a function of only one real variable. Consequently, the definition of the derivative for a function of one variable applies:
The above procedure can be performed for any choice of "a". Assembling the derivatives together into a function gives a function that describes the variation of "f" in the "y" direction:
This is the partial derivative of "f" with respect to "y". Here ∂ is a rounded "d" called the partial derivative symbol. To distinguish it from the letter "d", ∂ is sometimes pronounced "der", "del", or "partial" instead of "dee".
In general, the partial derivative of a function in the direction "x" at the point ("a" …, "a") is defined to be:
In the above difference quotient, all the variables except "x" are held fixed. That choice of fixed values determines a function of one variable
and, by definition,
In other words, the different choices of "a" index a family of one-variable functions just as in the example above. This expression also shows that the computation of partial derivatives reduces to the computation of one-variable derivatives.
An important example of a function of several variables is the case of a scalar-valued function on a domain in Euclidean space R (e.g., on R or R). In this case "f" has a partial derivative ∂"f"/∂"x" with respect to each variable "x". At the point "a", these partial derivatives define the vector
This vector is called the gradient of "f" at "a". If "f" is differentiable at every point in some domain, then the gradient is a vector-valued function ∇"f" that takes the point "a" to the vector ∇"f"("a"). Consequently, the gradient determines a vector field.
Directional derivatives.
If "f" is a real-valued function on R, then the partial derivatives of "f" measure its variation in the direction of the coordinate axes. For example, if "f" is a function of "x" and "y", then its partial derivatives measure the variation in "f" in the "x" direction and the "y" direction. They do not, however, directly measure the variation of "f" in any other direction, such as along the diagonal line . These are measured using directional derivatives. Choose a vector
The directional derivative of "f" in the direction of v at the point x is the limit
In some cases it may be easier to compute or estimate the directional derivative after changing the length of the vector. Often this is done to turn the problem into the computation of a directional derivative in the direction of a unit vector. To see how this works, suppose that . Substitute into the difference quotient. The difference quotient becomes:
This is λ times the difference quotient for the directional derivative of "f" with respect to u. Furthermore, taking the limit as "h" tends to zero is the same as taking the limit as "k" tends to zero because "h" and "k" are multiples of each other. Therefore, . Because of this rescaling property, directional derivatives are frequently considered only for unit vectors.
If all the partial derivatives of "f" exist and are continuous at x, then they determine the directional derivative of "f" in the direction v by the formula:
This is a consequence of the definition of the total derivative. It follows that the directional derivative is linear in v, meaning that .
The same definition also works when "f" is a function with values in R. The above definition is applied to each component of the vectors. In this case, the directional derivative is a vector in R.
Total derivative, total differential and Jacobian matrix.
When "f" is a function from an open subset of R to R, then the directional derivative of "f" in a chosen direction is the best linear approximation to "f" at that point and in that direction. But when , no single directional derivative can give a complete picture of the behavior of "f". The total derivative gives a complete picture by considering all directions at once. That is, for any vector v starting at a, the linear approximation formula holds:
Just like the single-variable derivative, is chosen so that the error in this approximation is as small as possible.
If "n" and "m" are both one, then the derivative is a number and the expression is the product of two numbers. But in higher dimensions, it is impossible for to be a number. If it were a number, then would be a vector in R while the other terms would be vectors in R, and therefore the formula would not make sense. For the linear approximation formula to make sense, must be a function that sends vectors in R to vectors in R, and must denote this function evaluated at v.
To determine what kind of function it is, notice that the linear approximation formula can be rewritten as
Notice that if we choose another vector w, then this approximate equation determines another approximate equation by substituting w for v. It determines a third approximate equation by substituting both w for v and for a. By subtracting these two new equations, we get
If we assume that v is small and that the derivative varies continuously in a, then is approximately equal to , and therefore the right-hand side is approximately zero. The left-hand side can be rewritten in a different way using the linear approximation formula with substituted for v. The linear approximation formula implies:
This suggests that is a linear transformation from the vector space R to the vector space R. In fact, it is possible to make this a precise derivation by measuring the error in the approximations. Assume that the error in these linear approximation formula is bounded by a constant times ||v||, where the constant is independent of v but depends continuously on a. Then, after adding an appropriate error term, all of the above approximate equalities can be rephrased as inequalities. In particular, is a linear transformation up to a small error term. In the limit as v and w tend to zero, it must therefore be a linear transformation. Since we define the total derivative by taking a limit as v goes to zero, must be a linear transformation.
In one variable, the fact that the derivative is the best linear approximation is expressed by the fact that it is the limit of difference quotients. However, the usual difference quotient does not make sense in higher dimensions because it is not usually possible to divide vectors. In particular, the numerator and denominator of the difference quotient are not even in the same vector space: The numerator lies in the codomain R while the denominator lies in the domain R. Furthermore, the derivative is a linear transformation, a different type of object from both the numerator and denominator. To make precise the idea that is the best linear approximation, it is necessary to adapt a different formula for the one-variable derivative in which these problems disappear. If , then the usual definition of the derivative may be manipulated to show that the derivative of "f" at "a" is the unique number such that
This is equivalent to
because the limit of a function tends to zero if and only if the limit of the absolute value of the function tends to zero. This last formula can be adapted to the many-variable situation by replacing the absolute values with norms.
The definition of the total derivative of "f" at a, therefore, is that it is the unique linear transformation such that
Here h is a vector in R, so the norm in the denominator is the standard length on R. However, "f"′(a)h is a vector in R, and the norm in the numerator is the standard length on R. If "v" is a vector starting at "a", then is called the pushforward of v by "f" and is sometimes written .
If the total derivative exists at a, then all the partial derivatives and directional derivatives of "f" exist at a, and for all v, is the directional derivative of "f" in the direction v. If we write "f" using coordinate functions, so that , then the total derivative can be expressed using the partial derivatives as a matrix. This matrix is called the Jacobian matrix of "f" at a:
The existence of the total derivative "f"′(a) is strictly stronger than the existence of all the partial derivatives, but if the partial derivatives exist and are continuous, then the total derivative exists, is given by the Jacobian, and depends continuously on a.
The definition of the total derivative subsumes the definition of the derivative in one variable. That is, if "f" is a real-valued function of a real variable, then the total derivative exists if and only if the usual derivative exists. The Jacobian matrix reduces to a 1×1 matrix whose only entry is the derivative "f"′("x"). This 1×1 matrix satisfies the property that is approximately zero, in other words that
Up to changing variables, this is the statement that the function formula_97 is the best linear approximation to "f" at "a".
The total derivative of a function does not give another function in the same way as the one-variable case. This is because the total derivative of a multivariable function has to record much more information than the derivative of a single-variable function. Instead, the total derivative gives a function from the tangent bundle of the source to the tangent bundle of the target.
The natural analog of second, third, and higher-order total derivatives is not a linear transformation, is not a function on the tangent bundle, and is not built by repeatedly taking the total derivative. The analog of a higher-order derivative, called a jet, cannot be a linear transformation because higher-order derivatives reflect subtle geometric information, such as concavity, which cannot be described in terms of linear data such as vectors. It cannot be a function on the tangent bundle because the tangent bundle only has room for the base space and the directional derivatives. Because jets capture higher-order information, they take as arguments additional coordinates representing higher-order changes in direction. The space determined by these additional coordinates is called the jet bundle. The relation between the total derivative and the partial derivatives of a function is paralleled in the relation between the "k"th order jet of a function and its partial derivatives of order less than or equal to "k".
By repeatedly taking the total derivative, one obtains higher versions of the Fréchet derivative, specialized to R. The "k"th order total derivative may be interpreted as a map
which takes a point x in R and assigns to it an element of the space of "k"-linear maps from R to R – the "best" (in a certain precise sense) "k"-linear approximation to "f" at that point. By precomposing it with the diagonal map Δ, , a generalized Taylor series may be begun as
where f(a) is identified with a constant function, are the components of the vector , and and are the components of and as linear transformations.
Generalizations.
The concept of a derivative can be extended to many other settings. The common thread is that the derivative of a function at a point serves as a linear approximation of the function at that point.

</doc>
<doc id="7922" url="https://en.wikipedia.org/wiki?curid=7922" title="Dravidian languages">
Dravidian languages

The Dravidian languages are a language family spoken mainly in southern India and parts of eastern and central India, as well as in northeastern Sri Lanka, Pakistan, Nepal, Bangladesh and Bhutan, and overseas in other countries such as Malaysia and Singapore. The Dravidian languages with the most speakers are Telugu, Tamil, Kannada and Malayalam. There are also small groups of Dravidian-speaking scheduled tribes, who live beyond the mainstream communities, such as the Kurukh and Gond tribes. It is often considered that Dravidian languages are native to India. Epigraphically the Dravidian languages have been attested since the 2nd century BCE. Only two Dravidian languages are exclusively spoken outside India: Brahui in Pakistan and Dhangar, a dialect of Kurukh, in Nepal.
Dravidian place names along the Arabian Sea coast and the Dravidian grammatical influence such as clusivity in the Indo-Aryan languages, namely Marathi, Konkani, Gujarati, Marwari and Sindhi languages, suggest that Dravidian languages were once spoken more widely across the Indian subcontinent.
Etymology.
Alexander D. Campbell first suggested the existence of a Dravidian language family in 1816 in his "Grammar of the Teloogoo Language", in which he and Francis W. Ellis argued that Tamil and Telugu descended from a common, non-Indo-European ancestor. In 1856 Robert Caldwell published his "Comparative Grammar of the Dravidian or South-Indian Family of Languages", which considerably expanded the Dravidian umbrella and established Dravidian as one of the major language groups of the world. Caldwell coined the term "Dravidian" for this family of languages, based on the usage of the Sanskrit word "" in the work "Tantravārttika" by . In his own words, Caldwell says,
As for the origin of the Sanskrit word ' itself, researchers have proposed various theories. Basically the theories deal with the direction of derivation between ' and '. There is no definite philological and linguistic basis for asserting unilaterally that the name "Dravida" also forms the origin of the word "Tamil" (Dravida → Dramila → Tamizha or Tamil). Kamil Zvelebil cites the forms such as "dramila" (in 's Sanskrit work "Avanisundarīkathā") ' (found in the Sri Lankan (Ceylonese) chronicle "Mahavamsa") and then goes on to say, "The forms "damiḷa"/"damila" almost certainly provide a connection of ' " and "... ' < ' ...whereby the further development might have been *' > *' > '- / "damila"- and further, with the intrusive, 'hypercorrect' (or perhaps analogical) -"r"-, into "". The -"m"-/-"v"- alternation is a common enough phenomenon in Dravidian phonology"
Zvelebil in his earlier treatise states, "It is obvious that the Sanskrit ', Pali "damila", ' and Prakrit ' are all etymologically connected with '", and further remarks, "The "r" in ' → ' is a hypercorrect insertion, cf. an analogical case of DED 1033 Ta. "kamuku", Tu. "kangu" "areca nut": Skt. "kramu(ka)"."
Further, another Dravidian linguist, Bhadriraju Krishnamurti, in his book "Dravidian Languages" states, 
Based on what Krishnamurti states (referring to a scholarly paper published in the "International Journal of Dravidian Linguistics"), the Sanskrit word ' itself is later than ' since the dates for the forms with -r- are centuries later than the dates for the forms without -r- (', '-, "damela"- etc.). The "Monier-Williams Sanskrit Dictionary" lists for the Sanskrit word "" a meaning of "collective Name for 5 peoples, viz. the and ".
Classification.
The Dravidian languages form a close-knit family. They are descended from the Proto-Dravidian language. There is reasonable agreement on how they are related to each other. Most scholars agree on four groups: North, Central (Kolami–Parji), South-Central (Telugu–Kui) and South Dravidian. Earlier classifications grouped Central and South-Central Dravidian in a single branch. The classification below follows Krishnamurti in grouping South-Central and South Dravidian. Languages recognized as official languages of India appear here in boldface.
In addition, "Ethnologue" lists several unclassified Dravidian languages: Allar, Bazigar, Bharia, Malankuravan (possibly a dialect of Malayalam), Vishavan, as well as the otherwise unclassified Southern Dravidian languages Mala Malasar, Malasar, Thachanadan, Ullatan, Kalanadi, Kumbaran, Kunduvadi, Kurichiya, Attapady Kurumba, Muduga, Pathiya and Wayanad Chetti to Tamil-Kannada.
North Dravidian.
Some authors deny that North Dravidian forms a valid subgroup, splitting it into Northeast (Kurukh–Malto) and Northwest (Brahui). Their affiliation has been proposed primarily based on a small number of common phonetic developments, including:
McAlpin (2003) notes that no exact conditioning can be established for the first two changes, and proposes that distinct Proto-Dravidian *q and *kʲ should be reconstructed behind these correspondences; and that Brahui, Kurukh-Malto and the rest of Dravidian may be three coordinate branches, possibly with Brahui being the earliest language to split off. A few morphological parallels between Brahui and Kurukh-Malto are also known, but according to McAlpin they are analyzable as shared archaisms rather than shared innovations.
Distribution.
Approximately 29% of India's population spoke Dravidian languages in 1981. The proportion has fallen due to lower birth rates compared to the Indo-Aryan speakers and according to 2001 census, about 21.5% or 220 million of total population of 1.02 billion were Dravidian speakers.
History.
The origins of the Dravidian languages, as well as their subsequent development and the period of their differentiation are unclear, partially due to the lack of comparative linguistic research into the Dravidian languages. Among Indian languages, Tamil has the most ancient non-Sanskritised Indian literature. The earliest records in Old Tamil are short inscriptions from around the 2nd century BCE in caves and on pottery. These inscriptions are written in a variant of the Brahmi script called Tamil Brahmi. The earliest long text in Old Tamil is the Tolkāppiyam, an early work on Tamil grammar and poetics, whose oldest layers could be as old as the 1st century BCE.
Although in modern times speakers of the various Dravidian languages have mainly occupied the southern portion of India, nothing definite is known about the ancient domain of the Dravidian parent speech. The Brahui, Kurukh and Malto have myths about external origins. The Kurukh have traditionally claimed to be from the Deccan Peninsula, more specifically Karnataka. The same tradition has existed of the Brahui, who call themselves immigrants. Many scholars hold this same view of the Brahui such as L. H. Horace Perera and M. Ratnasabapathy. Proto-Dravidian is thought to have differentiated into Proto-North Dravidian, Proto-Central Dravidian, Proto South-Central Dravidian and Proto-South Dravidian around 500 BCE, although some linguists have argued that the degree of differentiation between the sub-families points to an earlier split.
Relationship to other language families.
Despite many proposals, scholars have not shown a systematic relationship between the Dravidian languages and any other language family. Nonetheless, while there are no readily detectable genealogical connections, Dravidian shares strong areal features with the Indo-Aryan languages, which have been attributed to a substratum influence from Dravidian. The earliest known Dravidian inscriptions are 76 Old Tamil inscriptions on cave walls in Madurai and Tirunelveli districts in Tamil Nadu, dating from the 2nd century BCE.
Proposed larger groupings.
The Dravidian family has defied all of the attempts to show a connection with other languages, including Indo-European, Hurrian, Basque, Sumerian, and Korean. Comparisons have been made not just with the other language families of the Indian subcontinent (Indo-European, Austroasiatic, Sino-Tibetan, and Nihali), but with all typologically similar language families of the Old World.
Dravidian languages display typological similarities with the Uralic language group, suggesting to some a prolonged period of contact in the past. This idea is popular amongst Dravidian linguists and has been supported by a number of scholars, including Robert Caldwell, Thomas Burrow, Kamil Zvelebil, and Mikhail Andronov. This hyphothesis has, however, been rejected by some specialists in Uralic languages, and has in recent times also been criticised by other Dravidian linguists such as Bhadriraju Krishnamurti.
Dravidian is one of the primary language families in the Nostratic proposal, which would link most languages in North Africa, Europe and Western Asia into a family with its origins in the Fertile Crescent sometime between the last Ice Age and the emergence of proto-Indo-European 4–6 thousand years BCE. However, the general consensus is that such deep connections are not, or not yet, demonstrable. McAlpin (1975) proposed linking Dravidian languages with the ancient Elamite language of what is now southwestern Iran. However, despite decades of research, this Elamo-Dravidian language family has not been demonstrated to the satisfaction of other historical linguists.
Dravidian influence on Sanskrit.
Dravidian languages show extensive lexical (vocabulary) borrowing, but only a few traits of structural (either phonological or grammatical) borrowing from Indo-Aryan, whereas Indo-Aryan shows more structural than lexical borrowings from the Dravidian languages.
Many of these features are already present in the oldest known Indo-Aryan language, the language of the "Rigveda" (c. 1500 BCE), which also includes over a dozen words borrowed from Dravidian.
Vedic Sanskrit has retroflex consonants (/, ) with about 88 words in the "Rigveda" having unconditioned retroflexes. Some sample words are ', ',', ', ' and '.
Since other Indo-European languages, including other Indo-Iranian languages, lack retroflex consonants, their presence in Indo-Aryan is often cited as evidence of substrate influence from close contact of the Vedic speakers with speakers of a foreign language family rich in retroflex consonants. The Dravidian family is a serious candidate since it is rich in retroflex phonemes reconstructible back to the Proto-Dravidian stage.
In addition, a number of grammatical features of Vedic Sanskrit not found in its sister Avestan language appear to have been borrowed from Dravidian languages. These include the gerund, which has the same function as in Dravidian, and the quotative marker "iti". Some linguists explain this asymmetrical borrowing by arguing that Middle Indo-Aryan languages were built on a Dravidian substratum. These scholars argue that the most plausible explanation for the presence of Dravidian structural features in Indic is language shift, that is, native Dravidian speakers learning and adopting Indic languages. Although each of the innovative traits in Indic could be accounted for by internal explanations, early Dravidian influence is the only explanation that can account for all of the innovations at once; moreover, it accounts for the several of the innovative traits in Indic better than any internal explanation that has been proposed.
The Brahui population of Balochistan has been taken by some as the linguistic equivalent of a relict population, perhaps indicating that Dravidian languages were formerly much more widespread and were supplanted by the incoming Indo-Aryan languages. However it has been argued that the absence of any Old Iranian (Avestan) loanwords in Brahui suggests that the Brahui migrated to Balochistan from central India less than 1000 years ago. The main Iranian contributor to Brahui vocabulary, Balochi, is a western Iranian language like Kurdish, and arrived in the area from the west only around 1000 CE.
Sound changes shared with Kurukh and Malto also suggest that Brahui was originally spoken near them in central India.
Grammar.
The most characteristic grammatical features of Dravidian languages are:
Phonology.
Dravidian languages are noted for the lack of distinction between aspirated and unaspirated stops. While some Dravidian languages have accepted large numbers of loan words from Sanskrit and other Indo-Iranian languages in addition to their already vast vocabulary, in which the orthography shows distinctions in voice and aspiration, the words are pronounced in Dravidian according to different rules of phonology and phonotactics: aspiration of plosives is generally absent, regardless of the spelling of the word. This is not a universal phenomenon and is generally avoided in formal or careful speech, especially when reciting. For instance, Tamil does not distinguish between voiced and voiceless stops. In fact, the Tamil alphabet lacks symbols for voiced and aspirated stops. Dravidian languages are also characterized by a three-way distinction between dental, alveolar, and retroflex places of articulation as well as large numbers of liquids.
Proto-Dravidian.
Proto-Dravidian had five short and long vowels: "*a", "*ā", "*i", "*ī", "*u", "*ū", "*e", "*ē", "*o", "*ō". There were no diphthongs; "ai" and "au" are treated as *"ay" and *"av" (or *"aw").
The five-vowel system is largely preserved in the descendent subgroups.
The following consonantal phonemes are reconstructed:
Numerals.
The numerals from 1 to 10 in various Dravidian and Indo-Aryan languages (here exemplified by Hindi, Sanskrit and Marathi).

</doc>
<doc id="7923" url="https://en.wikipedia.org/wiki?curid=7923" title="Dracula">
Dracula

Dracula is an 1897 Gothic horror novel by Irish author Bram Stoker.
Famous for introducing the character of the vampire Count Dracula, the novel tells the story of Dracula's attempt to move from Transylvania to England so he may find new blood and spread the undead curse, and the battle between Dracula and a small group of men and women led by Professor Abraham Van Helsing.
"Dracula" has been assigned to many literary genres including vampire literature, horror fiction, the gothic novel and invasion literature. Although Stoker did not invent the vampire, he defined its modern form, and the novel has spawned numerous theatrical, film and television interpretations.
Plot summary.
The story is told in epistolary format, as a series of letters, diary entries, and ships' log entries, whose narrators are the novel's protagonists, and occasionally supplemented with newspaper clippings relating events not directly witnessed. The events portrayed in the novel take place chronologically and largely in England and Transylvania during the 1890s and all transpire within the same year between the 3rd of May and the 6th of November. A short note is located at the end of the final chapter written 7 years after the events outlined in the novel.
The tale begins with Jonathan Harker, a newly qualified English solicitor, visiting Count Dracula in the Carpathian Mountains on the border of Transylvania, Bukovina, and Moldavia, to provide legal support for a real estate transaction overseen by Harker's employer. At first enticed by Dracula's gracious manners, Harker soon realizes that he is Dracula's prisoner. Wandering the Count's castle against Dracula's admonition, Harker encounters three female vampires, called "the sisters", from whom he is rescued by Dracula. After the preparations are made, Dracula leaves Transylvania and abandons Harker to the sisters. Harker barely escapes from the castle with his life.
Not long afterward, a Russian ship, the "Demeter", having weighed anchor at Varna, runs aground on the shores of Whitby. The captain's log narrates the gradual disappearance of the entire crew, until the captain alone remained, himself bound to the helm to maintain course. An animal resembling "a large dog" is seen leaping ashore. The ship's cargo is described as silver sand and 50 boxes of "mould", or earth, from Transylvania. It is later learned that Dracula successfully purchased multiple estates under the alias 'Count De Ville' throughout London and devised to distribute the 50 boxes to each of them utilizing transportation services as well as moving them himself. He does this to secure for himself "lairs" and the 50 boxes of earth would be used as his graves which would grant safety and rest during times of feeding and replenishing his strength.
Soon Dracula is recorded stalking Lucy Westenra, who happens to live in Whitby. As time passes she begins suffering from episodes of sleepwalking and dementia. Lucy receives three marriage proposals from Dr. John Seward, Quincey Morris, and Arthur Holmwood (The son of Lord Godalming who later obtains the title himself). Lucy accepts Holmwood's proposal while turning down Seward and Morris, but all remain friends. Dracula communicates with Seward's patient Renfield, an insane man who wishes to consume insects, spiders, birds, and rats to absorb their "life force", and therefore assimilate to Dracula himself. Renfield is able to detect Dracula's presence and supplies clues accordingly.
When Lucy begins to waste away suspiciously, Seward invites his old teacher, Abraham Van Helsing, who immediately determines the true cause of Lucy's condition. He refuses to disclose it but diagnoses her with acute blood-loss. Helsing prescribes numerous blood transfusions to which Dr. Seward, Helsing, Quincy and Arthur all contribute over time. Helsing also prescribes flowers to be placed throughout her room and weaves a necklace of withered Garlic Blossoms for her to wear as well. She however continues to waste away - appearing to lose blood every night. While both doctors are absent, Lucy and her mother are attacked by a wolf; Mrs. Westenra, who has a heart condition, dies of fright. Van Helsing attempts to protect her with garlic but fate thwarts him each night, whether Lucy's mother removes the garlic from her room, or Lucy herself does so in her restless sleep. The doctors have found two small puncture marks about her neck, which Dr Seward is at a loss to understand. Helsing then places a crucifix around her neck, but soon after she is discovered dead with the crucifix missing. Helsing discovers that one of the nurses stole it the night before.
Following Lucy's death, the newspapers report children being stalked in the night by, in their words, a "bloofer lady" (i.e., "beautiful lady"). Van Helsing, knowing Lucy has become a vampire, confides in Seward, Lord Godalming, and Morris. The suitors and Van Helsing track her down and, after a confrontation with her, stake her heart, behead her, and fill her mouth with garlic. Around the same time, Jonathan Harker arrives from Budapest, where Mina marries him after his escape, and he and Mina join the coalition against Dracula.
All of the individuals directly affected by Dracula's cruelty team up and vow to destroy him and rid the Earth of his evil. To do so, they all stay at Dr. Seward's residence, holding nightly meetings and providing reports based on each of their various tasks. Mina discovers that each of their journals and letters collectively contain clues to which they can track him down. She tasks herself with collecting them, researching newspaper clippings, fitting the most relevant entries into chronological order and typing out copies to distribute to each of the party which they are to study. Jonathan Harker travels throughout London to track down the shipments of boxed graves and the estates which Dracula has purchased in order to store them. Van Helsing conducts research along with Dr. Seward to analyze the behavior of their patient Renfield who they learn is directly influenced by Dracula. They also research historical events, folklore, and superstitions from various cultures to create an understanding of Dracula's powers and weaknesses. Van Helsing also establishes a criminal profile on Dracula in order to better understand his actions and predict his movements. Arthur Holmwood's fortune assists in funding the entire operation and expenses. As they learn the various properties Dracula had purchased, the male protagonists team up to raid each property and are several times confronted by Dracula. As they discover each of the boxed graves scattered throughout London, they pry them open to place and seal wafers of sacramental bread within. This act renders the boxes of earth completely useless to Dracula as he is unable to open, enter or further transport them.
After Dracula learns of the group's plot against him, he attacks Mina on three occasions, and feeds Mina his own blood to control her. This curses Mina with vampirism and changes her but does not completely turn her into a vampire. Van Helsing attempts to bless Mina through prayer and by placing a wafer of sacrament against her forehead, although it burns her upon contact leaving a wretched scar. Under this curse, Mina oscillates from consciousness to a semi-trance during which she perceives Dracula's surroundings and actions. Van Helsing is able to use hypnotism at the hour of dawn and put her into this trance to further track his movements. Mina, afraid of Dracula's link with her, urges the team not to tell her their plans out of fear that Dracula will be listening. After the protagonists discover and sterilize 49 boxes found throughout his lairs in London, they learn that Dracula has fled with the missing 50th box back to his castle in Transylvania. They pursue him under the guidance of Mina. They split up into teams once they reach Europe; Van Helsing and Mina team up to locate the castle of Dracula while the others attempt to ambush the boat Dracula is using to reach his home. Van Helsing raids the castle and destroys the vampire "sisters". Upon discovering Dracula being transported by Gypsies, Harker shears Dracula through the throat with a kukri while the mortally wounded Quincey stabs the Count in the heart with a Bowie knife. Dracula crumbles to dust, and Mina is freed from her curse of vampirism.
The book closes with a note left by Jonathan Harker seven years after the events of the novel, detailing his married life with Mina and the birth of their son, whom they name after all four members of the party, but address as "Quincey". Quincey is depicted sitting on the knee of Van Helsing as they recount their adventure.
"Dracula's Guest".
In 1914, two years after Stoker's death, the short story "Dracula's Guest" was posthumously published. It was, according to most contemporary critics, the deleted first (or second) chapter from the original manuscript and the one which gave the volume its name, but which the original publishers deemed unnecessary to the overall story.
"Dracula's Guest" follows an unnamed Englishman traveller as he wanders around Munich before leaving for Transylvania. It is Walpurgis Night, and in spite of the coachman's warnings, the young Englishman foolishly leaves his hotel and wanders through a dense forest alone. Along the way he feels he is being watched by a tall and thin stranger (possibly Count Dracula).
The short story climaxes in an old graveyard, where in a marble tomb (with a large iron stake driven into it), the Englishman encounters a sleeping female vampire called "Countess Dolingen". This malevolent and beautiful vampire awakens from her marble bier to conjure a snowstorm before being struck by lightning and returning to her eternal prison. However, the Englishman's troubles are not quite over, as he is dragged away by an unseen force and rendered unconscious. He awakes to find a "gigantic" wolf lying on his chest and licking at his throat; however, the wolf merely keeps him warm and protects him until help arrives.
When the Englishman is finally taken back to his hotel, a telegram awaits him from his expectant host Dracula, with a warning about "dangers from snow and wolves and night".
Deleted ending.
An excerpt from the original final chapter was removed, in which Dracula's castle falls apart as he dies, hiding the fact that vampires were ever there.
Background.
Between 1879 and 1898, Stoker was a business manager for the world-famous Lyceum Theatre in London, where he supplemented his income by writing a large number of sensational novels, his most famous being the vampire tale "Dracula" published on 26 May 1897. Parts of it are set around the town of Whitby, where he spent summer holidays. Throughout the 1880s and 1890s, authors such as H. Rider Haggard, Rudyard Kipling, Robert Louis Stevenson, Arthur Conan Doyle, and H. G. Wells wrote many tales in which fantastic creatures threatened the British Empire. Invasion literature was at a peak, and Stoker's formula of an invasion of England by continental European influences was by 1897 very familiar to readers of fantastic adventure stories. Victorian readers enjoyed it as a good adventure story like many others, but it would not reach its iconic legendary status until later in the 20th century when film versions began to appear. 
Before writing "Dracula", Stoker spent seven years researching European folklore and stories of vampires, being most influenced by Emily Gerard's 1885 essay, "Transylvania Superstitions". Later he would also claim that he had a nightmare, caused by eating too much crab meat covered with mayonnaise sauce, about a "vampire king" rising from his grave.
Despite being the most widely known vampire novel, "Dracula" was not the first. It was preceded and partly inspired by Sheridan Le Fanu's 1871 "Carmilla", about a lesbian vampire who preys on a lonely young woman, and by "Varney the Vampire", a lengthy penny dreadful serial from the mid-Victorian period by James Malcolm Rymer. The image of a vampire portrayed as an aristocratic man, like the character of Dracula, was created by John Polidori in "The Vampyre" (1819), during the summer spent with "Frankenstein" creator Mary Shelley, her husband, the poet Percy Bysshe Shelley and Lord Byron in 1816. The Lyceum Theatre, where Stoker worked between 1878 and 1898, was headed by the actor-manager Henry Irving, who was Stoker's real-life inspiration for Dracula's mannerisms and who Stoker hoped would play Dracula in a stage version. Although Irving never did agree to do a stage version, Dracula's dramatic sweeping gestures and gentlemanly mannerisms drew their living embodiment from Irving.
"The Dead Un-Dead" was one of Stoker's original titles for "Dracula", and up until a few weeks before publication, the manuscript was titled simply "The Un-Dead". Stoker's notes for "Dracula" show that the name of the count was originally "Count Wampyr", but while doing research, Stoker became intrigued by the name "Dracula", after reading William Wilkinson's book "Account of the Principalities of Wallachia and Moldavia with Political Observations Relative to Them" (London 1820), which he found in the Whitby Library, and consulted a number of times during visits to Whitby in the 1890s. The name Dracula was the patronym ("Drăculea") of the descendants of Vlad II of Wallachia, who took the name "Dracul" after being invested in the Order of the Dragon in 1431. In the Romanian language, the word "dracul" (Romanian "drac" "dragon" + "-ul" "the") can mean either "the dragon" or, especially in the present day, "the devil".
"Dracula" was copyrighted in the United States in 1899 with the publication by Doubleday & McClure of New York. It was only when Universal Studio's purchased the rights that it came to light that Bram Stoker had not complied with a portion of US copyright law, placing the novel into the public domain. In the United Kingdom and other countries following the Berne Convention on copyrights, the novel was under copyright until April 1962, fifty years after Stoker's death. When F. W. Murnau's unauthorized film adaptation "Nosferatu" was released in 1922, the popularity of the novel increased considerably, owing to the controversy caused when Stoker's widow tried to have the film removed from public circulation. Florence Stoker sued the film company and won, however the company was bankrupt and Stoker only recovered her legal fees and an order by the court for all copies of the film to be destroyed. Some copies survived and found their way into theatres. Eventually Florence Stoker would simply give up the fight against public displays of the film.
Reaction and scholarly criticism.
When it was first published, in 1897, "Dracula" was not an immediate bestseller, although reviewers were unstinting in their praise. The contemporary "Daily Mail" ranked Stoker's powers above those of Mary Shelley and Edgar Allan Poe as well as Emily Brontë's "Wuthering Heights".
According to literary historians Nina Auerbach and David J. Skal in the Norton Critical Edition, the novel has become more significant for modern readers than it was for contemporary Victorian readers, most of whom enjoyed it just as a good adventure story; it reached its broad iconic legendary classic status only later in the 20th century when the movie versions appeared. A. Asbjørn Jøn has also noted that "Dracula" has had a significant impact on the image of the vampire in popular culture, folklore and legend.
It did not make much money for Stoker; the last year of his life he was so poor that he had to petition for a compassionate grant from the Royal Literary Fund, and in 1913 his widow was forced to sell his notes and outlines of the novel at a Sotheby's auction, where they were purchased for a little over 2 pounds. But when F. W. Murnau's unauthorized adaptation of the story in the form of "Nosferatu" was released in theatres in 1922, Stoker's widow took affront, and during the legal battle that followed, the novel's popularity started to grow. "Nosferatu" was followed by a highly successful stage adaptation, touring the UK for three years before arriving in the US where Stoker's creation caught Hollywood's attention, and after the American 1931 movie version was released, the book has never been out of print. However, some Victorian fans were ahead of the time, describing it as "the sensation of the season" and "the most blood-curdling novel of the paralysed century". Sherlock Holmes author Sir Arthur Conan Doyle wrote to Stoker in a letter, "I write to tell you how very much I have enjoyed reading "Dracula". I think it is the very best story of diablerie which I have read for many years." The "Daily Mail" review of 1 June 1897 proclaimed it a classic of Gothic horror, "In seeking a parallel to this weird, powerful, and horrorful story our mind reverts to such tales as "The Mysteries of Udolpho", "Frankenstein", "The Fall of the House of Usher" ... but Dracula is even more appalling in its gloomy fascination than any one of these."
Similarly good reviews appeared when the book was published in the U.S. in 1899. The first American edition was published by Doubleday & McClure in New York.
In the last several decades, literary and cultural scholars have offered diverse analyses of Stoker's novel and the character of Count Dracula. C.F. Bentley reads Dracula as an embodiment of the Freudian id. Carol A. Senf reads the novel as a response to the powerful New Woman, while Christopher Craft sees Dracula as embodying latent homosexuality. Stephen D. Arata interprets the events of the novel as anxiety over colonialism and racial mixing, and Talia Schaffer construes the novel as an indictment of Oscar Wilde. Franco Moretti reads Dracula as a figure of monopoly capitalism, though Hollis Robbins suggests that Dracula's inability to participate in social conventions and to forge business partnerships undermines his power. Richard Noll reads "Dracula" within the context of 19th century alienism (psychiatry) and asylum medicine. D. Bruno Starrs understands the novel to be a pro-Catholic pamphlet promoting proselytization.
Historical and geographical references.
Although "Dracula" is a work of fiction, it does contain some historical references. The historical connections with the novel and how much Stoker knew about the history are a matter of conjecture and debate.
Following the publication of "In Search of Dracula" by Radu Florescu and Raymond McNally in 1972, the supposed connections between the historical Transylvanian-born Vlad III Dracula of Wallachia and Bram Stoker's fictional Dracula attracted popular attention. During his main reign (1456–1462), "Vlad the Impaler" is said to have killed from 40,000 to 100,000 European civilians (political rivals, criminals, and anyone he considered "useless to humanity"), mainly by impaling. The sources depicting these events are records by Saxon settlers in neighbouring Transylvania, who had frequent clashes with Vlad III. Vlad III is revered as a folk hero by Romanians for driving off the invading Ottoman Turks, of which his impaled victims are said to have included as many as 100,000.
Historically, the name "Dracula" is derived from a Chivalric order called the Order of the Dragon, founded by Sigismund of Luxembourg (then king of Hungary) to uphold Christianity and defend the Empire against the Ottoman Turks. Vlad II Dracul, father of Vlad III, was admitted to the order around 1431, after which Vlad II wore the emblem of the order and later, as ruler of Wallachia, his coinage bore the dragon symbol, from which the name "Dracula" is derived. People of Wallachia only knew "voievod" (king) Vlad III as Vlad Țepeș (the Impaler). The name "Dracula" became popular in Romania after publication of Stoker's book. Contrary to popular belief, the name Dracula does not translate to "son of the devil" in Romanian, which would be "pui de drac".
Stoker came across the name Dracula in his reading on Romanian history, and chose this to replace the name ("Count Wampyr") originally intended for his villain. Some Dracula scholars, led by Elizabeth Miller, argue that Stoker knew little of the historic Vlad III except for the name "Dracula", whereas in the novel, Stoker mentions the Dracula who fought against the Turks, and was later betrayed by his brother, historical facts which unequivocally point to Vlad III:
The Count's identity is later speculated on by Professor Van Helsing:
Many of Stoker's biographers and literary critics have found strong similarities to the earlier Irish writer Sheridan Le Fanu's classic of the vampire genre, "Carmilla". In writing "Dracula", Stoker may also have drawn on stories about the sídhe, some of which feature blood-drinking women. The folkloric figure of Abhartach has also been suggested as a source.
In 1983, McNally additionally suggested that Stoker was influenced by the history of Hungarian Countess Elizabeth Bathory, who tortured and killed between 36 and 700 young women. It was later commonly believed that she committed these crimes to bathe in their blood, believing that this preserved her youth.
In her book "The Essential Dracula", Clare Haword-Maden opined the castle of Count Dracula was inspired by Slains Castle, at which Bram Stoker was a guest of the 19th Earl of Erroll. According to Miller, he first visited Cruden Bay in 1893, three years after work on "Dracula" had begun. Haining and Tremaine maintain that during this visit, Stoker was especially impressed by Slains Castle's interior and the surrounding landscape. Miller and Leatherdale question the stringency of this connection. Possibly, Stoker was not inspired by a real edifice at all, but by Jules Verne's novel "The Carpathian Castle" (1892) or Anne Radcliffe's "The Mysteries of Udolpho" (1794). A third possibility is that he copied information about a castle at Vécs from one of his sources on Transylvania, the book by Major E.C. Johnson. A further option is that Stoker saw an illustration of Castle Bran (Törzburg) in the book on Transylvania by Charles Boner, or read about it in the books by Mazuchelli or Crosse. 
Many of the scenes in Whitby and London are based on real places that Stoker frequently visited, although in some cases he distorts the geography for the sake of the story. One scholar has suggested that Stoker chose Whitby as the site of Dracula's first appearance in England because of the Synod of Whitby, given the novel's preoccupation with timekeeping and calendar disputes.
Daniel Farson, Leonard Wolf, and Peter Haining have suggested that Stoker received much historical information from Ármin Vámbéry, a Hungarian professor he met at least twice. Miller argues "there is nothing to indicate that the conversation included Vlad, vampires, or even Transylvania" and that, "furthermore, there is no record of any other correspondence between Stoker and Vámbéry, nor is Vámbéry mentioned in Stoker's notes for Dracula."
Adaptations.
The story of "Dracula" has been the basis for numerous films and plays. Stoker himself wrote the first theatrical adaptation, which was presented at the Lyceum Theatre under the title "Dracula, or The Undead" shortly before the novel's publication and performed only once. The first motion picture to feature Dracula, "Dracula's Death" was produced in Hungary in 1921. The film, now lost, however, was not an adaptation of Stoker's novel, and featured an original story. The following year, German director F. W. Murnau directed "Nosferatu, eine Symphonie des Grauens". Prana Film, the production company, had been unable to obtain permission to adapt the story from Florence Stoker, Bram's widow, so screenwriter Henrik Galeen was told to alter numerous details to avoid legal trouble. Galeen transplanted the action of the story from 1890s England to 1830s Germany and reworked several characters, dropping some (such as Lucy and all three of her suitors), and renaming others (Dracula became Orlok, Jonathan Harker became Thomas Hutter, Mina became Ellen, and so on). This attempt to avoid prosecution failed, however: Florence Stoker sued Prana Film, and all prints of the film were ordered destroyed. Although the film did survive the court-ordered purge, subsequent rereleases have typically undone some of the changes, most notably restoring the original character names (a practice also followed by Werner Herzog in his 1979 remake of Murnau's film, "Nosferatu the Vampyre").
Following "Nosferatu", Florence Stoker licensed the story to playwright Hamilton Deane, whose 1924 stage play adaptation toured England for several years before settling down in London. In 1927, American stage producer Horace Liveright hired John L. Balderston to revise Deane's script in advance of its American premiere. Balderston significantly compressed the story, most notably consolidation or removing several characters. The Deane play and its Balderston revisions introduced an expanded role and backstory for Renfield, who now replaced Jonathan Harker as Dracula's solicitor in the first part of the story, combined Mina Harker and Lucy Westenra into a single character (named Lucy), and omitted both Arthur Holmwood and Quincey Morris entirely. When the play premiered in New York, it was with Bela Lugosi in the title role, and with Edward van Sloan as Abraham Van Helsing, a role both actors (as well as Herbert Bunston as Dr. Seward) would reprise for the English-language version of the 1931 Universal Studios film production. One of the most commercially successful adaptations of the story to date, the 1931 film (and the Deane/Balderston play that preceded it) set the standard for film and television adaptations of the story, with the aforementioned alterations to the novel becoming standard for later adaptations for decades to come. Universal Studios continued to feature the character of Dracula in many of their horror films from the 1930s and 1940s.
In 1958, Hammer Film Productions followed the success of its "The Curse of Frankenstein" from the previous year with "Dracula", released in the United States as "The Horror of Dracula", directed by Terence Fisher. Fisher's production, which featured Christopher Lee as Dracula and Peter Cushing as Van Helsing, diverged considerably from both the original novel and from the Deane/Balderston adaptation. It was an international hit for Hammer Film, however, and both Lee and Cushing would reprise their roles multiple times over the next decade and half, concluding with "The Legend of the 7 Golden Vampires" (which Cushing but not Lee) in 1974. Christopher Lee would also take on the role of Dracula in "Count Dracula", a 1970 Spanish-Italian-German coproduction notable for its adherence to the plot of the original novel (for instance, it was the first film version of the story to include the character of Quincey Morris). Playing the part of Renfield in that version was Klaus Kinski, who would later play Dracula himself in 1979's "Nosferatu the Vampyre".
Later popular film adaptations include John Badham's 1979 "Dracula", starring Frank Langella and inspired by the 1977 Broadway revival of the Deane/Hamilton play, and Francis Ford Coppola's "Bram Stoker's Dracula", starring Gary Oldman. The character of Count Dracula has remained popular over the years, and many films have used the character as a villain, while others have named him in their titles, including "Dracula's Daughter" and "The Brides of Dracula". As of 2009, an estimated 217 films feature Dracula in a major role, a number second only to Sherlock Holmes (223 films). A large number of these appearances are not adaptations of Stoker's novel, but merely feature the character in an unrelated story.

</doc>
<doc id="7925" url="https://en.wikipedia.org/wiki?curid=7925" title="David Hume">
David Hume

David Hume (; 7 May 1711 NS – 25 August 1776) or David Home (birth name) was a Scottish philosopher, historian, economist, and essayist, who is best known today for his highly influential system of radical philosophical empiricism, skepticism, and naturalism.
Hume's empiricist approach to philosophy places him with John Locke, George Berkeley, Francis Bacon, and Thomas Hobbes as a British Empiricist. Beginning with his "A Treatise of Human Nature" (1739), Hume strove to create a total naturalistic science of man that examined the psychological basis of human nature. Against rationalists, Hume held that passion rather than reason governs human behaviour. He argued against the existence of innate ideas, postulating that humans can have knowledge only of the objects of experience, and the relations of ideas, calling the rest "nothing but sophistry and illusion", a dichotomy later given the name "Hume's fork". He also argued that inductive reasoning, and therefore causality, cannot, ultimately, be justified rationally: our belief in causality and induction instead results from custom, habit, and experience of "constant conjunction" rather than logic. He denied that humans have an actual conception of the self, positing that we experience only a bundle of sensations, and that the self is nothing more than this bundle of causally-connected perceptions. Hume's compatibilist theory of free will takes causal determinism as fully compatible with human freedom, and has proved extremely influential on subsequent moral philosophy.
Hume was also a sentimentalist who held that ethics are based on emotion or sentiment rather than abstract moral principle, famously proclaiming that "Reason Is and Ought Only to Be the Slave of the Passions". Contemporary scholars view Hume's moral theory as a unique attempt to synthesize the modern sentimentalist moral tradition to which Hume belonged, with the virtue ethics tradition of ancient philosophy, with which Hume concurred in regarding traits of character, rather than acts or their consequences, as ultimately the proper objects of moral evaluation. Hume's moral theory maintained an early commitment to naturalistic explanations of moral phenomena, and is usually taken to have first clearly expounded the is–ought problem, or the idea that a statement of fact alone can never give rise to a normative conclusion of what "ought" to be done.
While Hume was derailed in his attempts to start a university career by protests over his "atheism," and bemoaned that his literary debut, "A Treatise of Human Nature" 'fell dead-born from the press', Hume nevertheless found literary success in his lifetime as an essayist, and a career as a librarian at the University of Edinburgh. His tenure there, and the access to research materials it provided, ultimately resulted in Hume's writing the massive six-volume "The History of England", which became a bestseller and the standard history of England in its day. Hume described his lust for literary fame as his "ruling passion" and himself judged his two late works, the so-called "first" and "second" enquiries, "An Enquiry Concerning Human Understanding" and "An Enquiry Concerning the Principles of Morals", respectively, to be his greatest literary and philosophical achievements, asking his contemporaries to judge him on the merits of the later texts alone, rather than the more radical formulations of his early, youthful work, dismissing his philosophical debut as juvenilia: "A work which the Author had projected before he left College." Nevertheless, despite Hume's protestations, a general consensus exists today that Hume's strongest and most important arguments, and most philosophically distinctive doctrines, are found in the original form they take in the "Treatise", begun when Hume was just 23 years old, and now regarded as one of the most important works in the history of Western Philosophy. Hume has proved extremely influential on subsequent Western thought, especially on utilitarianism, logical positivism, William James, Immanuel Kant, the philosophy of science, early analytic philosophy, cognitive science, theology and other movements and thinkers.
Biography.
Early life and education.
David Hume was the second of two sons born to Joseph Home of Ninewells, an advocate, and his wife Katherine ("née" Falconer). He was born on 26 April 1711 (Old Style) in a tenement on the north side of the Lawnmarket in Edinburgh. Hume's father died when he was a child, just after the author's second birthday, and he was raised by his mother, who never re-married. He changed the spelling of his name in 1734, because of the fact that his surname "Home," pronounced "Hume," was not known in England. Throughout his life Hume, who never married, spent time occasionally at his family home at Ninewells in Berwickshire, which had belonged to his family since the sixteenth century. His finances as a young man were very "slender". His family was not rich and, as a younger son, he had little patrimony to live on. He was therefore forced to make a living somehow.
Hume attended the University of Edinburgh at the unusually early age of twelve (possibly as young as ten) at a time when fourteen was normal. At first, because of his family, he considered a career in law, but came to have, in his words, "an insurmountable aversion to everything but the pursuits of Philosophy and general Learning; and while y famil fanceyed I was poring over Voet and Vinnius, Cicero and Virgil were the Authors which I was secretly devouring". He had little respect for the professors of his time, telling a friend in 1735 that "there is nothing to be learnt from a Professor, which is not to be met with in Books". Hume did not graduate.
Aged around 18, he made a philosophical discovery that opened up to him "a new Scene of Thought", which inspired him "to throw up every other Pleasure or Business to apply entirely to it". He did not recount what this scene was, and commentators have offered a variety of speculations. One popular interpretation, prominent in contemporary Hume scholarship, is that the new "scene of thought" was Hume's realization that Francis Hutcheson's "moral sense" theory of morality could be applied to the understanding as well. Due to this inspiration, Hume set out to spend a minimum of ten years reading and writing. He soon came to the verge of a mental breakdown, suffering from what a doctor diagnosed as the "Disease of the Learned". Hume wrote that it started with a coldness, which he attributed to a "Laziness of Temper", that lasted about nine months. Later, some scurvy spots broke out on his fingers. This was what persuaded Hume's physician to make his diagnosis. Hume wrote that he "went under a Course of Bitters and Anti-Hysteric Pills", taken along with a pint of claret every day. Hume also decided to have a more active life to better continue his learning. His health improved somewhat, but, in 1731, he was afflicted with a ravenous appetite and palpitations of the heart. After eating well for a time, he went from being "tall, lean and raw-bon'd" to being "sturdy, robust n healthful-like". Indeed, Hume would become well known in his time for his "corpulence", and his fondness for good port and cheese.
Career.
At 25 years of age, Hume, although of noble ancestry, had no source of income and no learned profession. As was common at his time, he became a merchant's assistant, but he had to leave his native Scotland. He travelled via Bristol to La Flèche in Anjou, France. There he had frequent discourse with the Jesuits of the College of La Flèche.
He worked for four years on his first major work, "A Treatise of Human Nature", subtitled "Being an Attempt to Introduce the Experimental Method of Reasoning into Moral Subjects", completing it in 1738 at the age of 28. Although many scholars today consider the "Treatise" to be Hume's most important work and one of the most important books in Western philosophy, the critics in Great Britain at the time did not agree, describing it as "abstract and unintelligible". As Hume had spent most of his savings during those four years, he resolved "to make a very rigid frugality supply my deficiency of fortune, to maintain unimpaired my independency, and to regard every object as contemptible except the improvements of my talents in literature". Despite the disappointment, Hume later wrote, "Being naturally of a cheerful and sanguine temper, I soon recovered from the blow and prosecuted with great ardour my studies in the country." There, in an attempt to make his larger work better known and more intelligible, he published the "An Abstract of a Book lately Published" as a summary of the main doctrines of the "Treatise", without revealing its authorship. Although there has been some academic speculation as to who actually wrote this pamphlet it is generally regarded as Hume's creation.
After the publication of "Essays Moral and Political" in 1744, which was included in the later edition called "Essays, Moral, Political, and Literary", Hume applied for the Chair of Pneumatics and Moral Philosophy at the University of Edinburgh. However, the position was given to William Cleghorn after Edinburgh ministers petitioned the town council not to appoint Hume because he was seen as an atheist.
During the 1745 Jacobite rising, Hume tutored the Marquis of Annandale (1720–92), who was "judged to be a lunatic". This engagement ended in disarray after about a year. However, it was then that Hume started his great historical work "The History of England". This took him fifteen years and ran to over a million words. During this time he was also involved with the Canongate Theatre through his friend John Home, a preacher.
In this context, he associated with Lord Monboddo and other Scottish Enlightenment luminaries in Edinburgh. From 1746, Hume served for three years as secretary to General James St Clair, who was envoy to the courts of Turin and Vienna. At that time Hume also wrote "Philosophical Essays Concerning Human Understanding", later published as "An Enquiry Concerning Human Understanding". Often called the "First Enquiry", it proved little more successful than the "Treatise", perhaps because of the publishing of his short autobiography, "My Own Life", which "made friends difficult for the first Enquiry".
In 1749 he went to live with his brother in the countryside.
Hume's religious views were often suspect. It was necessary in the 1750s for his friends to avert a trial against him on the charge of heresy. However, he "would not have come and could not be forced to attend if he said he was not a member of the Established Church". Hume failed to gain the chair of philosophy at the University of Glasgow for his religious views, too. He had published the "Philosophical Essays" by this time which were decidedly anti-religious. Even Adam Smith, his personal friend who had vacated the Glasgow philosophy chair was against his appointment out of concern public opinion would be against it.
Hume returned to Edinburgh in 1751. In the following year "the Faculty of Advocates chose me their Librarian, an office from which I received little or no emolument, but which gave me the command of a large library". This resource enabled him to continue historical research for "The History of England". Hume's volume of "Political Discourses", written in 1749 and published by Kincaid & Donaldson in 1752, was the only work he considered successful on first publication.
Eventually, with the publication of his six volume "The History of England" between 1754 and 1762, Hume achieved the fame that he coveted. The volumes traced events from the Invasion of Julius Caesar to the Revolution of 1688, and was a bestseller in its day.
Later years.
From 1763 to 1765, Hume was invited to attend Lord Hertford in Paris, where he became secretary to the British embassy. While there he met with Isaac de Pinto and fell out with Jean-Jacques Rousseau. Hume was sufficiently worried about the damage to his reputation from the quarrel with Rousseau (who is generally believed to have suffered from Paranoia) to have authored an account of the dispute, which he titled, appropriately enough "A concise and genuine account of the dispute between Mr. Hume and Mr. Rousseau." In 1765, he served as British Chargé d'affaires, writing "despatches to the British Secretary of State". He wrote of his Paris life, "I really wish often for the plain roughness of The Poker Club of Edinburgh ... to correct and qualify so much lusciousness". In 1767, Hume was appointed Under Secretary of State for the Northern Department. Here he wrote that he was given "all the secrets of the Kingdom". In 1769 he returned to James' Court in Edinburgh, and then lived, from 1771 until his death in 1776, at the southwest corner of St. Andrew's Square in Edinburgh's New Town, at what is now 21 Saint David Street. A popular story, consistent with some historical evidence, suggests the street may have been named after Hume.
In the last year of his life, Hume wrote an extremely brief autobiographical essay titled "My Own Life" which summed up his entire life in "fewer than 5 pages", and notably contains many interesting judgments that have been of enduring interest to subsequent readers of Hume. The scholar of 18th century literature Donald Seibert judged it a "remarkable autobiography, even though it may lack the usual attractions of that genre. Anyone hankering for startling revelations or amusing anecdotes had better look elsewhere." Hume here confesses his belief that the"love of literary fame" had served as his "ruling passion" in life, and claims that this desire "never soured my temper, notwithstanding my frequent disappointments." One such disappointment Hume discusses in the mini-autobiography was his disappointment that with the initial literary reception of the "Treatise", which he claims to have overcome by means of the success of the "Essays": "the work was favourably received, and soon made me entirely forget my former disappointment". Perhaps most notable is Hume's revelation of his own retrospective judgment that his philosophical debut's apparent failure "had proceeded more from the manner than the matter." Hume thus suggests that "I had been guilty of a very usual indiscretion, in going to the press too early." Hume provides an unambiguous self-assessment of the relative value of his works: "my Enquiry concerning the Principles of Morals; which, in my own opinion (who ought not to judge on that subject) is of all my writings, historical, philosophical, or literary, incomparably the best." Hume also makes a number of self-assessments in the essay, writing of his social relations that "My company was not unacceptable to the young and careless, as well as to the studious and literary", noting of his complex relation to religion, as well as the state, that "though I wantonly exposed myself to the rage of both civil and religious factions, they seemed to be disarmed in my behalf of their wonted fury", and professing of his character that "My friends never had occasion to vindicate any one circumstance of my character and conduct." Hume concludes the essay with the frank admission: " I cannot say there is no vanity in making this funeral oration of myself, but I hope it is not a misplaced one; and this is a matter of fact which is easily cleared and ascertained."
Diarist and biographer James Boswell saw Hume a few weeks before his death, which was from some form of abdominal cancer. Hume told him he sincerely believed it a "most unreasonable fancy" that there might be life after death. This meeting was dramatised in semi-fictional form for the BBC by Michael Ignatieff as "Dialogue in the Dark". Hume asked that his body be interred in a "simple Roman tomb". In his will he requests that it be inscribed only with his name and the year of his birth and death, "leaving it to Posterity to add the Rest". It stands, as he wished it, on the southwestern slope of Calton Hill, in the Old Calton Cemetery. Adam Smith later recounted Hume's amusing speculation that he might ask Charon to allow him a few more years of life in order to see "the downfall of some of the prevailing systems of superstition." The ferryman replied, "You loitering rogue, that will not happen these many hundred years ... Get into the boat this instant".
Writings.
In the introduction to "A Treatise of Human Nature", Hume wrote, "'Tis evident, that all the sciences have a relation, more or less, to human nature ... Even Mathematics, Natural Philosophy, and Natural Religion, are in some measure dependent on the science of Man." He also wrote that the science of man is the "only solid foundation for the other sciences" and that the method for this science requires both experience and observation as the foundations of a logical argument. On this aspect of Hume's thought, philosophical historian Frederick Copleston wrote that it was Hume's aim to apply to the science of man the method of experimental philosophy (the term that was current at the time to imply Natural philosophy), and that "Hume's plan is to extend to philosophy in general the methodological limitations of Newtonian physics".
Until recently, Hume was seen as a forerunner of logical positivism; a form of anti-metaphysical empiricism. According to the logical positivists, unless a statement could be verified by experience, or else was true or false by definition (i.e. either tautological or contradictory), then it was meaningless (this is a summary statement of their verification principle). Hume, on this view, was a proto-positivist, who, in his philosophical writings, attempted to demonstrate how ordinary propositions about objects, causal relations, the self, and so on, are semantically equivalent to propositions about one's experiences.
Many commentators have since rejected this understanding of Humean empiricism, stressing an epistemological (rather than a semantic) reading of his project. According to this opposing view, Hume's empiricism consisted in the idea that it is our knowledge, and not our ability to conceive, that is restricted to what can be experienced. Hume thought that we can form beliefs about that which extends beyond any possible experience, through the operation of faculties such as custom and the imagination, but he was sceptical about claims to knowledge on this basis.
Impressions and ideas.
One of the most central doctrines of Hume's philosophy, stated in the very first lines of the "Treatise", is his notion that the mind consists of its mental perceptions, or the mental objects which are present to it, and which divide into two categories: "impressions" and "ideas". Hume's Treatise thus opens with the words: 'All the perceptions of the human mind resolve themselves into two distinct kinds, which I shall call IMPRESSIONS and IDEAS." Hume states that "I believe it will not be very necessary to employ many words in explaining this distinction" and commentators have generally taken Hume to mean the distinction between feeling and thinking. Controversially, Hume may regard the difference as in some sense a matter of degree, as he takes "impressions" to be distinguished from ideas, on the basis of their force, liveliness, and vivacity, or what Henry Allison calls the "FLV criterion" in his book on Hume. Ideas are therefore "faint" impressions. For Hume, impressions are meant to be the original form of all our ideas, and Don Garret has thus coined the term "the copy principle" to refer to Hume's doctrine that all ideas are ultimately all copied from some original impression, whether it be a passion or sensation, from which they derive.
Induction and causation.
The cornerstone of Hume's epistemology is the problem of induction. This may be the area of Hume's thought where his scepticism about human powers of reason is most pronounced. The problem revolves around the plausibility of inductive reasoning, that is, reasoning from the observed behaviour of objects to their behaviour when unobserved. As Hume wrote, induction concerns how things behave when they go "beyond the present testimony of the senses, or the records of our memory". Hume argues that we tend to believe that things behave in a regular manner, meaning that patterns in the behaviour of objects seem to persist into the future, and throughout the unobserved present. Hume's argument is that we cannot rationally justify the claim that nature will continue to be uniform, as justification comes in only two varieties—demonstrative reasoning and probable reasoning—and both of these are inadequate. With regard to demonstrative reasoning, Hume argues that the uniformity principle cannot be demonstrated, as it is "consistent and conceivable" that nature might stop being regular. Turning to probable reasoning, Hume argues that we cannot hold that nature will continue to be uniform because it has been in the past. As this is using the very sort of reasoning (induction) that is under question, it would be circular reasoning. Thus, no form of justification will rationally warrant our inductive inferences.
Hume's solution to this problem is to argue that, rather than reason, natural instinct explains the human practice of making inductive inferences. He asserts that "Nature, by an absolute and uncontroulable necessity has determin'd us to judge as well as to breathe and feel." Agreeing, philosopher John D. Kenyon writes: "Reason might manage to raise a doubt about the truth of a conclusion of natural inductive inference just for a moment ... but the sheer agreeableness of animal faith will protect us from excessive caution and sterile suspension of belief." Commentators such as Charles Sanders Peirce have demurred from Hume's solution, while, some, such as Kant and Karl Popper, saw that Hume's analysis "had posed a most fundamental challenge to all human knowledge claims."
The notion of causation is closely linked to the problem of induction. According to Hume, we reason inductively by associating constantly conjoined events. It is the mental act of association that is the basis of our concept of causation. There are at least three interpretations of Hume's theory of causation represented in the literature: (1) the logical positivist; (2) the sceptical realist; and (3) the quasi-realist.
The logical positivist interpretation is that Hume analyses causal propositions, such as "A caused B", in terms of regularities in perception: "A causes B" is equivalent to "Whenever A-type events happen, B-type ones follow", where "whenever" refers to all possible perceptions. In his "Treatise of Human Nature", Hume wrote:
power and necessity ... are ... qualities of perceptions, not of objects ... felt by the soul and not perceiv'd externally in bodies.
This view is rejected by sceptical realists, who argue that Hume thought that causation amounts to more than just the regular succession of events. Hume said that when two events are causally conjoined, a necessary connection underpins the conjunction:
Shall we rest contented with these two relations of contiguity and succession, as affording a complete idea of causation? By no means ... there is a "necessary connexion" to be taken into consideration.
Philosopher Angela Coventry writes that, for Hume, "there is nothing in any particular instance of cause and effect involving external objects which suggests the idea of power or necessary connection" and that "we are ignorant of the powers that operate between objects". However, while denying the possibility of knowing the powers between objects, Hume accepted the causal principle, writing, "I never asserted so absurd a proposition as that something could arise without a cause."
It has been argued that, while Hume did not think causation is reducible to pure regularity, he was not a fully fledged realist either. Philosopher Simon Blackburn calls this a quasi-realist reading. Blackburn writes that "Someone talking of cause is voicing a distinct mental set: he is by no means in the same state as someone merely describing regular sequences. In Hume's words, "nothing is more usual than to apply to external bodies every internal sensation, which they occasion".
The self.
Empiricist philosophers, such as Hume and Berkeley, favoured the bundle theory of personal identity. In this theory, "the mind itself, far from being an independent power, is simply 'a bundle of perceptions' without unity or cohesive quality." The self is nothing but a bundle of experiences linked by the relations of causation and resemblance; or, more accurately, that the empirically warranted idea of the self is just the idea of such a bundle. This view is forwarded by, for example, positivist interpreters, who saw Hume as suggesting that terms such as "self", "person", or "mind" referred to collections of "sense-contents". A modern-day version of the bundle theory of the mind has been advanced by Derek Parfit in his "Reasons and Persons".
However, some philosophers have criticised Hume's bundle-theory interpretation of personal identity. They argue that distinct selves can have perceptions that stand in relations of similarity and causality with one another. Thus, perceptions must already come parcelled into distinct "bundles" before they can be associated according to the relations of similarity and causality. In other words, the mind must already possess a unity that cannot be generated, or constituted, by these relations alone. Since the bundle-theory interpretation portrays Hume as answering an ontological question, philosophers, like Galen Strawson, who see Hume as not very concerned with such questions have queried whether the view is really Hume's. Instead, it is suggested by Strawson that Hume might have been answering an epistemological question about the causal origin of our concept of the self. In the Appendix to the "Treatise", Hume declares himself dissatisfied with his earlier account of personal identity in Book 1. Philosopher Corliss Swain notes that "Commentators agree that if Hume did find some new problem" when he reviewed the section on personal identity, "he wasn't forthcoming about its nature in the Appendix." One interpretation of Hume's view of the self has been argued for by philosopher and psychologist James Giles. According to his view, Hume is not arguing for a bundle theory, which is a form of reductionism, but rather for an eliminative view of the self. That is, rather than reducing the self to a bundle of perceptions, Hume is rejecting the idea of the self altogether. On this interpretation, Hume is proposing a "no-self theory" and thus has much in common with Buddhist thought. On this point, psychologist Alison Gopnik has argued that Hume was in a position to learn about Buddhist thought during his time in France in the 1730s.
Practical reason.
Hume's anti-rationalism informed much of his theory of belief and knowledge, as well as his treatment of the notions of induction, causation, and the external world. But it was not confined to this sphere, and also permeated his theories of motivation, action, and morality. In a famous sentence in the "Treatise", Hume circumscribes reason's role in the production of action:
Reason is, and ought only to be the slave of the passions, and can never pretend to any other office than to serve and obey them.
Hume's anti-rationalism is defended in contemporary philosophy of action by neo-Humeans such as Michael Smith and Simon Blackburn. It is opposed by cognitivists such as John McDowell, concerned with what it is to act for a reason, and Kantians, such as Christine Korsgaard.
Ethics.
Hume's writings on ethics began in the "Treatise" and were refined in his "An Enquiry Concerning the Principles of Morals" (1751). His views on ethics are that "oral decisions are grounded in moral sentiment." It is not knowing that governs ethical actions, but feelings. Arguing that reason cannot be behind morality, he wrote:
Morals excite passions, and produce or prevent actions. Reason itself is utterly impotent in this particular. The rules of morality, therefore, are not conclusions of our reason.
Hume's sentimentalism about morality was shared by his close friend Adam Smith, and Hume and Smith were mutually influenced by the moral reflections of their older contemporary Francis Hutcheson. Peter Singer claims that Hume's argument that morals cannot have a rational basis alone "would have been enough to earn him a place in the history of ethics".
Hume also put forward the is–ought problem, later called "Hume's Law", denying the possibility of logically deriving what "ought" to be from what "is". He wrote in the "Treatise" that in every system of morality he has read, the author begins with stating facts about the world, but then suddenly is always referring to what ought to be the case. Hume demands that a reason should be given for inferring what ought to be the case, from what is the case. This because it "seems altogether inconceivable, how this new relation can be a deduction from others".
Hume's theory of ethics has been influential in modern day meta-ethical theory, helping to inspire emotivism, and ethical expressivism and non-cognitivism, as well as Allan Gibbard's general theory of moral judgment and judgments of rationality.
Aesthetics.
Hume's ideas about aesthetics and the theory of art are spread throughout his works, but are particularly connected with his ethical writings, and also the essays "Of the Standard of Taste" and "Of Tragedy". His views are rooted in the work of Joseph Addison and Francis Hutcheson. In the "Treatise" he wrote of the connection between beauty and deformity and vice and virtue, and his later writings on this subject continue to draw parallels of beauty and deformity in art, with conduct and character.
In "Of the Standard of Taste", Hume argues that no rules can be drawn up about what is a tasteful object. However, a reliable critic of taste can be recognised as being objective, sensible and unprejudiced, and having extensive experience. "Of Tragedy" addresses the question of why humans enjoy tragic drama. Hume was concerned with the way spectators find pleasure in the sorrow and anxiety depicted in a tragedy. He argued that this was because the spectator is aware that he is witnessing a dramatic performance. There is pleasure in realising that the terrible events that are being shown are actually fiction. Furthermore, Hume laid down rules for educating people in taste and correct conduct, and his writings in this area have been very influential on English and Anglo-Saxon aesthetics.
Free will, determinism, and responsibility.
Hume, along with Thomas Hobbes, is cited as a classical compatibilist about the notions of freedom and determinism. The thesis of compatibilism seeks to reconcile human freedom with the mechanist belief that human beings are part of a deterministic universe, whose happenings are governed by physical laws. Hume, to this end, was influenced greatly by the scientific revolution and by in particular Sir Isaac Newton. Hume argued that the dispute about the compatibility of freedom and determinism has been continued over two thousand years by ambiguous terminology. He wrote: "From this circumstance alone, that a controversy has been long kept on foot ... we may presume that there is some ambiguity in the expression", and that different disputants use different meanings for the same terms.
Hume defines the concept of necessity as "the uniformity, observable in the operations of nature; where similar objects are constantly conjoined together", and liberty as "a power of acting or not acting, according to the determinations of the will". He then argues that, according to these definitions, not only are the two compatible, but liberty "requires" necessity. For if our actions were not necessitated in the above sense, they would "have so little in connexion with motives, inclinations and circumstances, that one does not follow with a certain degree of uniformity from the other". But if our actions are not thus connected to the will, then our actions can never be free: they would be matters of "chance; which is universally allowed to have no existence". Australian philosopher John Passmore writes that confusion has arisen because "necessity" has been taken to mean "necessary connexion". Once this has been abandoned, Hume argues that "liberty and necessity will be found not to be in conflict one with another".
Moreover, Hume goes on to argue that in order to be held morally responsible, it is required that our behaviour be caused or necessitated, for, as he wrote:
Actions are, by their very nature, temporary and perishing; and where they proceed not from some "cause" in the character and disposition of the person who performed them, they can neither redound to his honour, if good; nor infamy, if evil.
Hume describes the link between causality and our capacity to rationally make a decision from this an inference of the mind. Human beings assess a situation based upon certain predetermined events and from that form a choice. Hume believes that this choice is made spontaneously. Hume calls this form of decision making the liberty of spontaneity.
Education writer Richard Wright considers that Hume's position rejects a famous moral puzzle attributed to French philosopher Jean Buridan. The Buridan's ass puzzle describes a donkey that is hungry. This donkey has on both sides of him separate bales of hay, which are of equal distances from him. The problem concerns which bale the donkey chooses. Buridan was said to believe that the donkey would die, because he has no autonomy. The donkey is incapable of forming a rational decision as there is no motive to choose one bale of hay over the other. However, human beings are different, because a human who is placed in a position where he is forced to choose one loaf of bread over another will make a decision to take one in lieu of the other. For Buridan, humans have the capacity of autonomy, and he recognises the choice that is ultimately made will be based on chance, as both loaves of bread are exactly the same. However, Wright says that Hume completely rejects this notion, arguing that a human will spontaneously act in such a situation because he is faced with impending death if he fails to do so. Such a decision is not made on the basis of chance, but rather on necessity and spontaneity, given the prior predetermined events leading up to the predicament.
Hume's argument is supported by modern day compatibilists such as R. E. Hobart, a pseudonym of philosopher Dickinson S. Miller. However, P. F. Strawson argued that the issue of whether we hold one another morally responsible does not ultimately depend on the truth or falsity of a metaphysical thesis such as determinism. This is because our so holding one another is a non-rational human sentiment that is not predicated on such theses.
Writings on religion.
The "Stanford Encyclopedia of Philosophy" states that Hume "wrote forcefully and incisively on almost every central question in the philosophy of religion." His "various writings concerning problems of religion are among the most important and influential contributions on this topic." His writings in this field cover the philosophy, psychology, history, and anthropology of religious thought. All of these aspects were discussed in Hume's 1757 dissertation, "The Natural History of Religion". Here he argued that the monotheistic religions of Judaism, Christianity and Islam all derive from earlier polytheistic religions. He also suggested that all religious belief "traces, in the end, to dread of the unknown." Hume had also written on religious subjects in the first "Enquiry", as well as later in the "Dialogues Concerning Natural Religion".
Religious views.
Although he wrote a great deal about religion, Hume's personal views are unclear, and there has been much discussion concerning his religious position. Contemporaries considered him to be an atheist, or at least un-Christian, and the Church of Scotland seriously considered bringing charges of infidelity against him. The fact that contemporaries thought that he may have been an atheist is exemplified by a story Hume liked to tell:
The best theologian he ever met, he used to say, was the old Edinburgh fishwife who, having recognized him as Hume the atheist, refused to pull him out of the bog into which he had fallen until he declared he was a Christian and repeated the Lord's prayer.
However, in works such as "Of Superstition and Enthusiasm", Hume specifically seems to support the standard religious views of his time and place. This still meant that he could be very critical of the Catholic Church, dismissing it with the standard Protestant accusations of superstition and idolatry, as well as dismissing as idolatry what his compatriots saw as uncivilised beliefs. He also considered extreme Protestant sects, the members of which he called "enthusiasts", to be corrupters of religion. By contrast, in his "The Natural History of Religion", Hume presented arguments suggesting that polytheism had much to commend it over monotheism.
Philosopher Paul Russell writes that it is likely that Hume was sceptical about religious belief, but not to the extent of complete atheism. He suggests that perhaps Hume's position is best characterised by the term "irreligion", while philosopher David O'Connor argues that Hume's final position was "weakly deistic". For O'Connor, Hume's "position is deeply ironic. This is because, while inclining towards a weak form of deism, he seriously doubts that we can ever find a sufficiently favourable balance of evidence to justify accepting any religious position." He adds that Hume "did not believe in the God of standard theism ... but he did not rule out all concepts of deity", and that "ambiguity suited his purposes, and this creates difficulty in definitively pinning down his final position on religion".
Design argument.
One of the traditional topics of natural theology is that of the existence of God, and one of the a posteriori arguments for this is the "argument from design" or the teleological argument. The argument is that the existence of God can be proved by the design that is obvious in the complexity of the world. "Encyclopaedia Britannica" states that this is "the most popular, because t i the most accessible of the theistic arguments ... which identifies evidences of design in nature, inferring from them a divine designer ... The fact that the universe as a whole is a coherent and efficiently functioning system likewise, in this view, indicates a divine intelligence behind it."
In "An Enquiry Concerning Human Understanding", Hume wrote that the design argument seems to depend upon our experience, and its proponents "always suppose the universe, an effect quite singular and unparalleled, to be the proof of a Deity, a cause no less singular and unparalleled". Philosopher Louise E. Loeb notes that Hume is saying that only experience and observation can be our guide to making inferences about the conjunction between events. However, according to Hume, "we observe neither God nor other universes, and hence no conjunction involving them. There is no observed conjunction to ground an inference either to extended objects or to God, as unobserved causes."
Hume also criticised the argument in his "Dialogues Concerning Natural Religion" (1779). In this, he suggested that, even if the world is a more or less smoothly functioning system, this may only be a result of the "chance permutations of particles falling into a temporary or permanent self-sustaining order, which thus has the appearance of design."
A century later, the idea of order without design was rendered more plausible by Charles Darwin's discovery that the adaptations of the forms of life are a result of the natural selection of inherited characteristics. For philosopher James D. Madden, it is "Hume, rivaled only by Darwin, h has done the most to undermine in principle our confidence in arguments from design among all figures in the Western intellectual tradition."
Finally, Hume discussed a version of the anthropic principle. This is the idea that theories of the universe are constrained by the need to allow for man's existence in it as an observer. Hume has his sceptical mouthpiece Philo suggest that there may have been many worlds, produced by an incompetent designer, who he called a "stupid mechanic". In his "Dialogues Concerning Natural Religion", Hume wrote:
Many worlds might have been botched and bungled throughout an eternity, ere this system was struck out: much labour lost: many fruitless trials made: and a slow, but continued improvement carried on during infinite ages in the art of world-making.
American philosopher Daniel Dennett has suggested that this mechanical explanation of teleology, although "obviously ... an amusing philosophical fantasy", anticipated the notion of natural selection, the 'continued improvement' being like "any Darwinian selection algorithm."
Problem of miracles.
In his discussion of miracles, Hume argues that we should not believe that miracles have occurred and that they do not therefore provide us with any reason to think that God exists. In "An Enquiry Concerning Human Understanding" (Section 10), Hume defines a miracle as "a transgression of a law of nature by a particular volition of the Deity, or by the interposition of some invisible agent". Hume says that we believe an event that has frequently occurred is likely to occur again, but we also take into account those instances where the event did not occur. Hume wrote:
A wise man . considers which side is supported by the greater number of experiments . A hundred instances or experiments on one side, and fifty on another, afford a doubtful expectation of any event; though a hundred uniform experiments, with only one that is contradictory, reasonably beget a pretty strong degree of assurance. In all cases, we must balance the opposite experiments . and deduct the smaller number from the greater, in order to know the exact force of the superior evidence.
Hume discusses the testimony of those who report miracles. He wrote that testimony might be doubted even from some great authority in case the facts themselves are not credible. "he evidence, resulting from the testimony, admits of a diminution, greater or less, in proportion as the fact is more or less unusual."
Although Hume leaves open the possibility for miracles to occur and be reported, he offers various arguments against this ever having happened in history: He points out that people often lie, and they have good reasons to lie about miracles occurring either because they believe they are doing so for the benefit of their religion or because of the fame that results. Furthermore, people by nature enjoy relating miracles they have heard without caring for their veracity and thus miracles are easily transmitted even where false. Also, Hume notes that miracles seem to occur mostly in "ignorant and barbarous nations" and times, and the reason they do not occur in the civilised societies is such societies are not awed by what they know to be natural events. Finally, the miracles of each religion argue against all other religions and their miracles, and so even if a proportion of all reported miracles across the world fit Hume's requirement for belief, the miracles of each religion make the other less likely.
Hume was extremely pleased with his argument against miracles in his "Enquiry". He states "I flatter myself, that I have discovered an argument of a like nature, which, if just, will, with the wise and learned, be an everlasting check to all kinds of superstitious delusion, and consequently, will be useful as long as the world endures." Thus, Hume's argument against miracles had a more abstract basis founded upon the scrutiny, not just primarily of miracles, but of all forms of belief systems. It is a common sense notion of veracity based upon epistemological evidence, and founded on a principle of rationality, proportionality and reasonability.
The criterion for assessing a belief system for Hume is based on the balance of probability whether something is more likely than not to have occurred. Since the weight of empirical experience contradicts the notion for the existence of miracles, such accounts should be treated with scepticism. Further, the myriad of accounts of miracles contradict one another, as some people who receive miracles will aim to prove the authority of Jesus, whereas others will aim to prove the authority of Muhammad or some other religious prophet or deity. These various differing accounts weaken the overall evidential power of miracles.
Despite all this, Hume observes that belief in miracles is popular, and that "The gazing populace . receive greedily, without examination, whatever soothes superstition, and promotes wonder."
Critics have argued that Hume's position assumes the character of miracles and natural laws prior to any specific examination of miracle claims, thus it amounts to a subtle form of begging the question. To assume that testimony is a homogeneous reference group seems unwise- to compare private miracles with public miracles, unintellectual observers with intellectual observers and those who have little to gain and much to lose with those with much to gain and little to lose is not convincing to many. Indeed, many have argued that miracles not only do not contradict the laws of nature, but require the laws of nature to be intelligible as miraculous, and thus subverting the law of nature. For example, William Adams remarks that "there must be an ordinary course of nature before anything can be extraordinary. There must be a stream before anything can be interrupted". They have also noted that it requires an appeal to inductive inference, as none have observed every part of nature nor examined every possible miracle claim, for instance those in the future. This, in Hume's philosophy, was especially problematic.
Little appreciated is the voluminous literature either foreshadowing Hume, in the likes of Thomas Sherlock or directly responding to and engaging with Hume- from William Paley, William Adams, John Douglas, John Leland and George Campbell, among others. Of Campbell, it is rumoured that, having read Campbell's Dissertation, Hume remarked that "the Scotch theologue had beaten him".
Hume's main argument concerning miracles is that miracles by definition are singular events that differ from the established laws of nature. Such natural laws are codified as a result of past experiences. Therefore, a miracle is a violation of all prior experience and thus incapable on this basis of reasonable belief. However, the probability that something has occurred in contradiction of all past experience should always be judged to be less than the probability that either ones senses have deceived one, or the person recounting the miraculous occurrence is lying or mistaken. Hume would say, all of which he had past experience of. For Hume, this refusal to grant credence does not guarantee correctness. He offers the example of an Indian Prince, who, having grown up in a hot country, refuses to believe that water has frozen. By Hume's lights, this refusal is not wrong and the Prince "reasoned justly"; it is presumably only when he has had extensive experience of the freezing of water that he has warrant to believe that the event could occur.
So for Hume, either the miraculous event will become a recurrent event or else it will never be rational to believe it occurred. The connection to religious belief is left unexplained throughout, except for the close of his discussion where Hume notes the reliance of Christianity upon testimony of miraculous occurrences. He makes an ironic remark that anyone who "is moved by faith to assent" to revealed testimony "is conscious of a continued miracle in his own person, which subverts all principles of his understanding, and gives him a determination to believe what is most contrary to custom and experience." Hume writes that "All the testimony which ever was really given for any miracle, or ever will be given, is a subject of derision."
As historian of England.
From 1754 to 1762 Hume published "The History of England", a 6-volume work, which extends, says its subtitle, "From the Invasion of Julius Caesar to the Revolution in 1688". Inspired by Voltaire's sense of the breadth of history, Hume widened the focus of the field away from merely kings, parliaments, and armies, to literature and science as well. He argued that the quest for liberty was the highest standard for judging the past, and concluded that after considerable fluctuation, England at the time of his writing had achieved "the most entire system of liberty that was ever known amongst mankind". It "must be regarded as an event of cultural importance. In its own day, moreover, it was an innovation, soaring high above its very few predecessors."
Hume's coverage of the political upheavals of the 17th century relied in large part on the Earl of Clarendon's "History of the Rebellion and Civil Wars in England" (1646–69). Generally, Hume took a moderate royalist position and considered revolution unnecessary to achieve necessary reform. Hume was considered a Tory history, and emphasised religious differences more than constitutional issues. Laird Okie explains that "Hume preached the virtues of political moderation, but ... it was moderation with an anti-Whig, pro-royalist coloring." For "Hume shared the ... Tory belief that the Stuarts were no more high-handed than their Tudor predecessors". "Even though Hume wrote with an anti-Whig animus, it is, paradoxically, correct to regard the "History" as an establishment work, one which implicitly endorsed the ruling oligarchy".
Historians have debated whether Hume posited a universal unchanging human nature, or allowed for evolution and development.
Robert Roth argues that Hume's histories display his biases against Presbyterians and Puritans. Roth says his anti-Whig pro-monarchy position diminished the influence of his work, and that his emphasis on politics and religion led to a neglect of social and economic history.
Hume was an early cultural historian of science. His short biographies of leading scientists explored the process of scientific change. He developed new ways of seeing scientists in the context of their times by looking at how they interacted with society and each other. He covers over forty scientists, with special attention paid to Francis Bacon, Robert Boyle, and Isaac Newton. Hume particularly praised William Harvey, writing about his treatise of the circulation of the blood: "Harvey is entitled to the glory of having made, by reasoning alone, without any mixture of accident, a capital discovery in one of the most important branches of science".
The "History" became a best-seller and made Hume a wealthy man who no longer had to take up salaried work for others. It was influential for nearly a century, despite competition from imitations by Smollett (1757), Goldsmith (1771) and others. By 1894, there were at least 50 editions as well as abridgements for students, and illustrated pocket editions, probably produced specifically for women.
Political theory.
It is difficult to categorise Hume's political affiliations. His writings contain elements that are, in modern terms, both conservative and liberal, although these terms are anachronistic. Thomas Jefferson banned the "History" from University of Virginia, feeling that it had "spread universal toryism over the land". By comparison, Samuel Johnson thought Hume "a Tory by chance ... for he has no principle. If he is anything, he is a Hobbist", a follower of Thomas Hobbes. A major concern of Hume's political philosophy is the importance of the rule of law. He also stresses throughout his political essays the importance of moderation in politics: public spirit and regard to the community.
This outlook needs to be seen within the historical context of eighteenth century Scotland. Here, the legacy of religious civil war, combined with the relatively recent memory of the 1715 and 1745 Jacobite risings, fostered in a historian such as Hume a distaste for enthusiasm and factionalism. These appeared to threaten the fragile and nascent political and social stability of a country that was deeply politically and religiously divided. Hume thought that society is best governed by a general and impartial system of laws; he is less concerned about the form of government that administers these laws, so long as it does so fairly. However, he does write that a republic must produce laws, while "monarchy, when absolute, contains even something repugnant to law."
Hume expressed suspicion of attempts to reform society in ways that departed from long-established custom, and he counselled peoples not to resist their governments except in cases of the most egregious tyranny. However, he resisted aligning himself with either of Britain's two political parties, the Whigs and the Tories. Hume wrote:
My views of "things" are more conformable to Whig principles; my representations of "persons" to Tory prejudices.
Canadian philosopher Neil McArthur writes that Hume believed that we should try to balance our demands for liberty with the need for strong authority, without sacrificing either. McArthur characterises Hume as a "precautionary conservative", whose actions would have been "determined by prudential concerns about the consequences of change, which often demand we ignore our own principles about what is ideal or even legitimate." Hume supported the liberty of the press, and was sympathetic to democracy, when suitably constrained. American historian Douglass Adair has argued that Hume was a major inspiration for James Madison's writings, and the essay "Federalist No. 10" in particular.
Hume offered his view on the best type of society in an essay titled "Idea of a Perfect Commonwealth", which lays out what he thought was the best form of government. He hoped that, "in some future age, an opportunity might be afforded of reducing the theory to practice, either by a dissolution of some old government, or by the combination of men to form a new one, in some distant part of the world". He defended a strict separation of powers, decentralisation, extending the franchise to anyone who held property of value and limiting the power of the clergy. The system of the Swiss militia was proposed as the best form of protection. Elections were to take place on an annual basis and representatives were to be unpaid. Political philosophers Leo Strauss and Joseph Cropsey, writing of Hume's thoughts about "the wise statesman", note that he "will bear a reverence to what carries the marks of age". Also, if he wishes to improve a constitution, his innovations will take account of the "ancient fabric", in order not to disturb society.
In the political analysis of philosopher George Sabine, the scepticism of Hume extended to the doctrine of government by consent. He notes that "allegiance is a habit enforced by education and consequently as much a part of human nature as any other motive."
Contributions to economic thought.
Through his discussions on politics, Hume developed many ideas that are prevalent in the field of economics. This includes ideas on private property, inflation, and foreign trade. Referring to his essay "Of the Balance of Trade", economist Paul Krugman has remarked that "David Hume created what I consider the first true economic model."
In contrast to Locke, Hume believes that private property is not a natural right. Hume argues it is justified, because resources are limited. Private property would be an unjustified, "idle ceremonial", if all goods were unlimited and available freely. Hume also believed in an unequal distribution of property, because perfect equality would destroy the ideas of thrift and industry. Perfect equality would thus lead to impoverishment.
Influence.
Due to Hume's vast influence on contemporary philosophy, a large number of approaches in contemporary philosophy and cognitive science are today called "Humean."
Attention to Hume's philosophical works grew after the German philosopher Immanuel Kant, in his "Prolegomena to Any Future Metaphysics" (1783), credited Hume with awakening him from "dogmatic slumbers".
According to Schopenhauer, "there is more to be learned from each page of David Hume than from the collected philosophical works of Hegel, Herbart and Schleiermacher taken together."
A. J. Ayer, while introducing his classic exposition of logical positivism in 1936, claimed: "The views which are put forward in this treatise derive from ... doctrines ... which are themselves the logical outcome of the empiricism of Berkeley and David Hume." Albert Einstein, in 1915, wrote that he was inspired by Hume's positivism when formulating his theory of special relativity.
Hume's problem of induction was also of fundamental importance to the philosophy of Karl Popper. In his autobiography, "Unended Quest", he wrote: "Knowledge ... is "objective"; and it is hypothetical or conjectural. This way of looking at the problem made it possible for me to reformulate Hume's "problem of induction"". This insight resulted in Popper's major work "The Logic of Scientific Discovery". Also, in his "Conjectures and Refutations", he wrote:
I approached the problem of induction through Hume. Hume, I felt, was perfectly right in pointing out that induction cannot be logically justified.
The writings of Scottish philosopher and contemporary of Hume, Thomas Reid, were often criticisms of Hume's scepticism. Reid formulated his common sense philosophy in part as a reaction against Hume's views.
Hume influenced and was influenced by the Christian philosopher Joseph Butler. Hume was impressed by Butler's way of thinking about religion, and Butler may well have been influenced by Hume's writings.
Hume's rationalism in religious subjects influenced, via German-Scottish theologian Johann Joachim Spalding, the German neology school and rational theology, and contributed to the transformation of German theology in the age of enlightenment. Hume pioneered a comparative history of religion, tried to explain various rites and traditions as being based on deception and challenged various aspects of rational and natural theology, such as the argument from design.
Danish theologian and philosopher Søren Kierkegaard adopted "Hume's suggestion that the role of reason is not to make us wise but to reveal our ignorance." However, Kierkegaard took this as a reason for the necessity of religious faith, or fideism. The "fact that Christianity is contrary to reason ... is the necessary precondition for true faith." Political theorist Isaiah Berlin, for example, has pointed out the similarities between the arguments of Hume and Kierkegaard against rational theology. Berlin also writes about Hume's influence on what Berlin calls the counter-enlightenment, and German anti-rationalism.
According to philosopher Jerry Fodor, Hume's "Treatise" is "the founding document of cognitive science".
Hume engaged with contemporary intellectual luminaries such as Jean-Jacques Rousseau, James Boswell, and Adam Smith (who acknowledged Hume's influence on his economics and political philosophy).
Isaiah Berlin once said of Hume that "No man has influenced the history of philosophy to a deeper or more disturbing degree."
The "Stanford Encyclopedia of Philosophy" writes that Hume is "enerally regarded as one of the most important philosophers to write in English."

</doc>
<doc id="7928" url="https://en.wikipedia.org/wiki?curid=7928" title="Dalton Trumbo">
Dalton Trumbo

James Dalton Trumbo (December 9, 1905 – September 10, 1976) was an American screenwriter and novelist, who scripted films including "Roman Holiday", "Exodus", "Spartacus", and "Thirty Seconds Over Tokyo". One of the Hollywood Ten, he refused to testify before the House Un-American Activities Committee (HUAC) in 1947 during the committee's investigation of Communist influences in the motion picture industry, and was subsequently blacklisted by that industry. He continued working clandestinely, and his uncredited work won two Academy Awards; the one for "Roman Holiday" (1953) was given to a front writer, and the one for "The Brave One" (1956) was awarded to a pseudonym. The public crediting of him as the writer of both "Exodus" and "Spartacus" in 1960 marked the end of the Hollywood Blacklist, and his earlier achievements were eventually credited to him.
Early life.
Trumbo was born in Montrose, Colorado, the son of Maud (née Tillery) and Orus Bonham Trumbo. His family moved to Grand Junction in 1908. He was proud of his paternal ancestor, a Swiss immigrant named Jacob Trumbo, who settled in the colony of Virginia in 1736. Trumbo graduated from Grand Junction High School. While still in high school, he worked as a cub reporter for the "Grand Junction Daily Sentinel", covering courts, the high school, the mortuary and civic organizations. He attended the University of Colorado at Boulder for two years, working as a reporter for the "Boulder Daily Camera" and contributing to the campus humor magazine, the yearbook and the campus newspaper. He was also a member of Delta Tau Delta International Fraternity.
For nine years after his father died, he worked the night shift wrapping bread at a Los Angeles bakery, attended University of Southern California, reviewed some movies, and wrote 88 short stories and six novels that were rejected for publication.
Career.
Early career.
Trumbo began his writing career in the early 1930s when several of his articles and stories appeared in magazines including the "Saturday Evening Post", "McCall's Magazine", "Vanity Fair", and the "Hollywood Spectator". In 1934 he became managing editor of the "Hollywood Spectator" and subsequently left to become a reader in the story department at Warner Bros. studio.
He wrote his first published novel, "Eclipse" (1935), about a town and its people, in the social realist style, drawing on his years in Grand Junction. The book was controversial in Grand Junction and many people were unhappy with his portrayal. Years after his death, he would be honored with a statue in front of the Avalon Theater on Main Street, where he was depicted writing a screenplay in a bathtub.
He started working in movies in 1937 but continued writing prose. His anti-war novel "Johnny Got His Gun" won one of the early National Book Awards: the Most Original Book of 1939. It was inspired by an article Trumbo read several years earlier, concerning the Prince of Wales' hospital visit to a Canadian soldier who had lost all his limbs in World War I.
Over the late 1930s and early 1940s, Trumbo became one of Hollywood's highest paid writers, at about $4000 per week while on assignment, as much as $80,000 in one year. He worked on such films as "Thirty Seconds Over Tokyo" (1944), "Our Vines Have Tender Grapes" (1945), and "Kitty Foyle" (1940), for which he was nominated for an Academy Award for Writing Adapted Screenplay.
Political advocacy and blacklisting.
Trumbo aligned himself with the Communist Party in the United States before the 1940s, although he did not join the party until 1943. He was an isolationist, and wrote a novel "The Remarkable Andrew", in which the ghost of Andrew Jackson appears, to caution the United States not to get involved in World War II. In a review of the book, "Time Magazine" wise-cracked "General Jackson's opinions need surprise no one who has observed George Washington and Abraham Lincoln zealously following the Communist Party Line in recent years." Shortly after the 1941 German invasion of the Soviet Union, Trumbo and his publisher decided to suspend reprinting "Johnny Got His Gun" until the end of the war. During the war, Trumbo received letters from individuals "denouncing Jews" and using "Johnny" to support their arguments for "an immediate negotiated peace" with Nazi Germany; Trumbo reported these correspondents to the FBI. Trumbo regretted this decision, which he called "foolish". After two FBI agents showed up at his home, he understood that "their interest lay not in the letters but in me."
In a 1946 article titled "The Russian Menace" published in Rob Wagner's "Script Magazine", Trumbo wrote from the perspective of a post-World War II Russian citizen. He argued that Russians were likely fearful of the mass of US military power that surrounded them on all sides at a time when any sympathetic view towards communist countries was viewed with suspicion. He ended the articles by stating, "If I were a Russian...I would be alarmed, and I would petition my government to take measures at once against what would seem an almost certain blow aimed at my existence. This is how it must appear in Russia today." He argued that the US was a "menace" to Russia, rather than the more popular American view of Russia as the "red menace". According to anti-communist author Kenneth Billingsley, Trumbo had written in "The Daily Worker" that communist influence in Hollywood had prevented films from being made from anti-communist books, such as Arthur Koestler's "Darkness at Noon" and "The Yogi and the Commissar."
On July 29, 1946, William R. Wilkerson, publisher and founder of "The Hollywood Reporter", published a "TradeView" column entitled "A Vote For Joe Stalin". It named Trumbo and several others as Communist sympathizers, the initial individuals on what became known as "Billy's Blacklist." In October 1947, drawing upon these names, the House Un-American Activities Committee (HUAC) summoned Trumbo and nine others to testify for their investigation whether Communist agents and sympathizers had been surreptitiously planting propaganda in U.S. films. They refused to give information, and were convicted for contempt of Congress. They appealed the conviction to the Supreme Court on First Amendment grounds, but it allowed their convictions to stand. In 1950, Trumbo served eleven months in the federal penitentiary in Ashland, Kentucky. In the 1976 documentary "Hollywood On Trial" Trumbo said of his trial:
Meanwhile, the MPAA had issued a statement that Trumbo and his compatriots would not be permitted to work in the industry unless they disavowed Communism under oath. After completing his sentence, he sold his ranch and his family moved to Mexico City with Hugo Butler and his wife Jean Rouverol, who had also been blacklisted. In Mexico he wrote thirty scripts under pseudonyms, for B-movie studios such as King Brothers Productions. In the case of "Gun Crazy" (1950), MacKinlay Kantor – author of the short story it was based on – was the front for Trumbo's screenplay; it was not until 1992 that Trumbo's role was revealed. During this time he wrote "The Brave One" (1956) for King Brothers, which received an Academy Award for Best Story credited to "Robert Rich", a name borrowed from a nephew of the producers. Trumbo recalled earning an average fee of $1750 for eighteen screenplays written in two years and said, "None was very good."
In 1956 he published "The Devil in the Book", an analysis of the conviction of fourteen California Smith Act defendants. The statute set criminal penalties for advocating the overthrow of the U.S. government, and required all non-citizen adult residents to register with the government.
Later career.
Gradually the blacklist weakened. With the support of director Otto Preminger, Trumbo was credited for his screenplay for the 1960 film "Exodus", which he adapted from the novel by Leon Uris. Shortly thereafter, Kirk Douglas made public Trumbo's writing of the screenplay for Stanley Kubrick's film "Spartacus" (1960) starring Douglas, an event which has been cited as the beginning of the end of the blacklist. Trumbo was reinstated into the Writers Guild of America, West, and was credited on all subsequent scripts. Eventually in 2011 he was given full credit for the script of "Roman Holiday".
In 1971, Trumbo directed the film adaptation of his novel "Johnny Got His Gun", which starred Timothy Bottoms, Diane Varsi, Jason Robards, and Donald Sutherland. One of the last films Trumbo wrote, "Executive Action" (1973), was based on the Kennedy assassination.
In 1975, the Academy officially recognized Trumbo as the winner of the Oscar for "The Brave One", and presented him with a statuette.
Personal life.
In 1938, Trumbo married Cleo Fincher. She was born in Fresno on July 17, 1916, and later moved in with her divorced mother and her brother and sister to Los Angeles. Cleo Trumbo died of natural causes at the age of 93 on October 9, 2009, in the Bay Area city of Los Altos. At the time she was living with her younger daughter Mitzi.
They had three children: the filmmaker and screenwriter Christopher Trumbo, who became an expert on the Hollywood blacklist; Melissa, known as Mitzi, a photographer; and Nikola Trumbo, a psychotherapist.
His daughter Mitzi dated comedian Steve Martin when they were both in their early 20s, which is recounted in Steve Martin's 2007 book "Born Standing Up". Many of Martin's early promotional photographs were taken by Trumbo.
Death and legacy.
Trumbo died in Los Angeles of a heart attack at the age of 70 on September 10, 1976. He donated his body to scientific research.
In 1993, Trumbo was posthumously awarded the Academy Award for writing "Roman Holiday" (1953). The screen credit and award were previously given to Ian McLellan Hunter, who had been a front for Trumbo. A new statue was made for this award, because Hunter's son refused to hand over the one his father had received for it.
In 2003, Christopher Trumbo mounted an Off-Broadway play based on his father's letters called "Trumbo: Red, White and Blacklisted", in which a wide variety of actors played his father during the run, including Nathan Lane, Tim Robbins, Brian Dennehy, Ed Harris, Chris Cooper and Gore Vidal. He adapted it as the film "Trumbo" (2007), which added documentary footage and new interviews.
A dramatization of Trumbo's life, also called "Trumbo", was released in November 2015. It starred Bryan Cranston as the screenwriter, and was directed by Jay Roach. For his portrayal of Trumbo Cranston was nominated to receive the Academy Award for Best Actor at the 88th Academy Awards, but did not win.

</doc>
<doc id="7930" url="https://en.wikipedia.org/wiki?curid=7930" title="Delaware">
Delaware

Delaware () is one of the Mid-Atlantic states located in the Northeast megalopolis region of the United States. It is bordered to the south and west by Maryland, to the northeast by New Jersey, and to the north by Pennsylvania. The state takes its name from Thomas West, 3rd Baron De La Warr, an English nobleman and Virginia's first colonial governor, after whom what is now called Cape Henlopen was originally named.
Delaware is in the northeastern portion of the Delmarva Peninsula and is the second smallest, the sixth least populous, but the sixth most densely populated of the 50 United States. Delaware is divided into three counties, the lowest number of counties of any state. From north to south, the three counties are New Castle, Kent, and Sussex. While the southern two counties have historically been predominantly agricultural, New Castle County has been more industrialized.
Before its coastline was explored by Europeans in the 16th century, Delaware was inhabited by several groups of Native Americans, including the Lenape in the north and Nanticoke in the south. It was initially colonized by Dutch traders at Zwaanendael, near the present town of Lewes, in 1631. Delaware was one of the 13 colonies participating in the American Revolution and on December 7, 1787, became the first state to ratify the Constitution of the United States, thereby becoming known as "The First State".
Etymology.
The state was named after the Delaware River, which in turn derived its name from Thomas West, 3rd Baron De La Warr (1577–1618) who was the ruling governor of the Colony of Virginia at the time Europeans first explored the river. The Delaware Indians, a name used by Europeans for Lenape people indigenous to the Delaware Valley, also derive their name from the same source.
The surname "de La Warr" comes from Sussex and is of Anglo-Norman origin. It came probably from a Norman lieu-dit "La Guerre". This toponymic could derive from the Latin word "ager", from the Breton "gwern" or from the Late Latin "varectum" (fallow). The toponyms Gara, Gare, Gaire (the sound [ä] often mutated in [æ]) also appear in old texts cited by Lucien Musset, where the word "ga(i)ra" means gore. It could also be linked with a patronymic from the Old Norse "verr".
Geography.
Delaware is long and ranges from to across, totaling , making it the second-smallest state in the United States after Rhode Island. Delaware is bounded to the north by Pennsylvania; to the east by the Delaware River, Delaware Bay, New Jersey and the Atlantic Ocean; and to the west and south by Maryland. Small portions of Delaware are also situated on the eastern side of the Delaware River sharing land boundaries with New Jersey. The state of Delaware, together with the Eastern Shore counties of Maryland and two counties of Virginia, form the Delmarva Peninsula, which stretches down the Mid-Atlantic Coast.
The definition of the northern boundary of the state is unusual. Most of the boundary between Delaware and Pennsylvania was originally defined by an arc extending from the cupola of the courthouse in the city of New Castle. This boundary is often referred to as the Twelve-Mile Circle. This is the only nominally circular state boundary in the United States.
This border extends all the way east to the low-tide mark on the New Jersey shore, then continues south along the shoreline until it again reaches the 12-mile (19 km) arc in the south; then the boundary continues in a more conventional way in the middle of the main channel (thalweg) of the Delaware River. To the west, a portion of the arc extends past the easternmost edge of Maryland. The remaining western border runs slightly east of due south from its intersection with the arc. The Wedge of land between the northwest part of the arc and the Maryland border was claimed by both Delaware and Pennsylvania until 1921, when Delaware's claim was confirmed.
Topography.
Delaware is on a level plain, with the lowest mean elevation of any state in the nation. Its highest elevation, located at Ebright Azimuth, near Concord High School, does not rise fully above sea level. The northernmost part of the state is part of the Piedmont Plateau with hills and rolling surfaces. The Atlantic Seaboard fall line approximately follows the Robert Kirkwood Highway between Newark and Wilmington; south of this road is the Atlantic Coastal Plain with flat, sandy, and, in some parts, swampy ground. A ridge about in elevation extends along the western boundary of the state and separates the watersheds that feed Delaware River and Bay to the east and the Chesapeake Bay to the west.
Climate.
Since almost all of Delaware is a part of the Atlantic Coastal Plain, the effects of the ocean moderate its climate. The state is in a transitional zone between a humid subtropical climate and a continental climate. Despite its small size (roughly from its northernmost to southernmost points), there is significant variation in mean temperature and amount of snowfall between Sussex County and New Castle County. Moderated by the Atlantic Ocean and Delaware Bay, the southern portion of the state has a milder climate and a longer growing season than the northern portion of the state. Delaware's all-time record high of was recorded at Millsboro on July 21, 1930; the all-time record low of was also recorded at Millsboro on January 17, 1893.
Environment.
The transitional climate of Delaware supports a wide variety of vegetation. In the northern third of the state are found Northeastern coastal forests and mixed oak forests typical of the northeastern United States. In the southern two-thirds of the state are found Middle Atlantic coastal forests. Trap Pond State Park in Sussex County, for example, supports what may be one of the northernmost stands of bald cypress.
Environmental management.
Delaware provides government subsidy support for the clean-up of property "lightly contaminated" by hazardous waste, the proceeds for which come from a tax on wholesale petroleum sales.
History.
Native Americans.
Before Delaware was settled by European colonists, the area was home to the Eastern Algonquian tribes known as the Unami Lenape or Delaware throughout the Delaware valley, and the Nanticoke along the rivers leading into the Chesapeake Bay. The Unami Lenape in the Delaware Valley were closely related to Munsee Lenape tribes along the Hudson River. They had a settled hunting and agricultural society, and they rapidly became middlemen in an increasingly frantic fur trade with their ancient enemy, the Minqua or Susquehannock. With the loss of their lands on the Delaware River and the destruction of the Minqua by the Iroquois of the Five Nations in the 1670s, the remnants of the Lenape who wished to remain identified as such left the region and moved over the Alleghany Mountains by the mid-18th century. Generally, those who did not relocate out of the state of Delaware were baptized, became Christian and were grouped together with other persons of color in official records and in the minds of their non-Native American neighbors.
Colonial Delaware.
The Dutch were the first Europeans to settle in present-day Delaware in the Middle region by establishing a trading post at Zwaanendael, near the site of Lewes in 1631. Within a year all the settlers were killed in a dispute with area Native American Tribes. In 1638 New Sweden, a Swedish trading post and colony, was established at Fort Christina (now in Wilmington) by Peter Minuit at the head of a group of Swedes, Finns and Dutch. The colony of New Sweden lasted for 17 years. In 1651, the Dutch, reinvigorated by the leadership of Peter Stuyvesant, established a fort at present-day New Castle, and in 1655 they conquered the New Sweden colony, annexing it into the Dutch New Netherland. Only nine years later, in 1664, the Dutch were conquered by a fleet of English ships by Sir Robert Carr under the direction of James, the Duke of York. Fighting off a prior claim by Cecil Calvert, 2nd Baron Baltimore, Proprietor of Maryland, the Duke passed his somewhat dubious ownership on to William Penn in 1682. Penn strongly desired access to the sea for his Pennsylvania province and leased what then came to be known as the "Lower Counties on the Delaware" from the Duke.
Penn established representative government and briefly combined his two possessions under one General Assembly in 1682. However, by 1704 the Province of Pennsylvania had grown so large that their representatives wanted to make decisions without the assent of the Lower Counties and the two groups of representatives began meeting on their own, one at Philadelphia, and the other at New Castle. Penn and his heirs remained proprietors of both and always appointed the same person Governor for their Province of Pennsylvania and their territory of the Lower Counties. The fact that Delaware and Pennsylvania shared the same governor was not unique. From 1703 to 1738, New York and New Jersey shared a governor. Massachusetts and New Hampshire also shared a governor for some time.
Dependent in early years on indentured labor, Delaware imported more slaves as the number of English immigrants decreased with better economic conditions in England. The colony became a slave society and cultivated tobacco as a cash crop, although English immigrants continued to arrive.
American Revolution.
Like the other middle colonies, the Lower Counties on the Delaware initially showed little enthusiasm for a break with Britain. The citizenry had a good relationship with the Proprietary government, and generally were allowed more independence of action in their Colonial Assembly than in other colonies. Merchants at the port of Wilmington had trading ties with the British.
So it was that New Castle lawyer Thomas McKean denounced the Stamp Act in the strongest terms, and Kent County native John Dickinson became the "Penman of the Revolution." Anticipating the Declaration of Independence, Patriot leaders Thomas McKean and Caesar Rodney convinced the Colonial Assembly to declare itself separated from British and Pennsylvania rule on June 15, 1776. The person best representing Delaware's majority, George Read, could not bring himself to vote for a Declaration of Independence. Only the dramatic overnight ride of Caesar Rodney gave the delegation the votes needed to cast Delaware's vote for independence.
Initially led by John Haslet, Delaware provided one of the premier regiments in the Continental Army, known as the "Delaware Blues" and nicknamed the "Blue Hen's Chicks." In August 1777, General Sir William Howe led a British army through Delaware on his way to a victory at the Battle of Brandywine and capture of the city of Philadelphia. The only real engagement on Delaware soil was the Battle of Cooch's Bridge, fought on September 3, 1777, at Cooch's Bridge in New Castle County.
Following the Battle of Brandywine, Wilmington was occupied by the British, and State President John McKinly was taken prisoner. The British remained in control of the Delaware River for much of the rest of the war, disrupting commerce and providing encouragement to an active Loyalist portion of the population, particularly in Sussex County. Because the British promised slaves of rebels freedom for fighting with them, escaped slaves flocked north to join their lines.
Following the American Revolution, statesmen from Delaware were among the leading proponents of a strong central United States with equal representation for each state.
Slavery and race.
Many colonial settlers came to Delaware from Maryland and Virginia, which had been experiencing a population boom. The economies of these colonies were chiefly based on tobacco culture and were increasingly dependent on slave labor for its intensive cultivation. Most of the English colonists arrived as indentured servants, hiring themselves out as laborers for a fixed period to pay for their passage. In the early years the line between indentured servants and African slaves or laborers was fluid. Most of the free African-American families in Delaware before the Revolution had migrated from Maryland to find more affordable land. They were descendants chiefly of relationships or marriages between servant women and enslaved, servant or free African or African-American men. As the flow of indentured laborers to the colony decreased with improving economic conditions in England, more slaves were imported for labor.
At the end of the colonial period, the number of enslaved people in Delaware began to decline. Shifts in the agriculture economy from tobacco to mixed farming created less need for slaves' labor. Local Methodists and Quakers encouraged slaveholders to free their slaves following the American Revolution, and many did so in a surge of individual manumissions for idealistic reasons. By 1810 three-quarters of all blacks in Delaware were free. When John Dickinson freed his slaves in 1777, he was Delaware's largest slave owner with 37 slaves. By 1860, the largest slaveholder owned only 16 slaves.
Although attempts to abolish slavery failed by narrow margins in the legislature, in practical terms, the state had mostly ended the practice. By the 1860 census on the verge of the Civil War, 91.7% of the black population were free; 1,798 were slaves, as compared to 19,829 "free colored persons".
The independent black denomination was chartered by freed slave Peter Spencer in 1813 as the "Union Church of Africans". This followed the 1793 establishment of the African Methodist Episcopal Church in Philadelphia, which had ties to the Methodist Episcopal Church until 1816. Spencer built a church in Wilmington for the new denomination.
This was renamed the African Union First Colored Methodist Protestant Church and Connection, more commonly known as the A.U.M.P. Church. Begun by Spencer in 1814, the annual gathering of the Big August Quarterly still draws people together in a religious and cultural festival, the oldest such cultural festival in the nation.
Delaware voted against secession on January 3, 1861 and so remained in the Union. While most Delaware citizens who fought in the war served in the regiments of the state, some served in companies on the Confederate side in Maryland and Virginia Regiments. Delaware is notable for being the only slave state from which no Confederate regiments or militia groups were assembled. Delaware essentially freed the few slaves that were still in bondage shortly after the Civil War, but rejected the 13th, 14th, and 15th Amendments to the Constitution; the 13th Amendment was rejected on February 8, 1865, the 14th Amendment was rejected on February 8, 1867, and the 15th Amendment was rejected on March 18, 1869. Delaware officially ratified the 13th, 14th, and 15th amendments on February 12, 1901.
Demographics.
The United States Census Bureau estimates that the population of Delaware was 945,934 people on July 1, 2015, a 5.35% increase since the 2010 United States Census.
Race and ancestry.
According to the 2010 United States Census, Delaware had a population of 897,934 people. The racial composition of the state was:
Ethnically, Hispanics and Latinos of any race made up 8.2% of the population.
Delaware is the sixth most densely populated state, with a population density of 442.6 people per square mile, 356.4 per square mile more than the national average, and ranking 45th in population. Delaware is one of five states that do not have a single city with a population over 100,000 as of the 2010 census, the other four being West Virginia, Vermont, Maine and Wyoming. The center of population of Delaware is located in New Castle County, in the town of Townsend.
As of 2011, 49.7% of Delaware's population younger than one year of age belonged to minority groups (i.e., did not have two parents of non-Hispanic white ancestry). In 2000, approximately 19% of the population were African-American and 5% of the population is Hispanic (mostly of Puerto Rican or Mexican ancestry).
The largest ancestry groups in Delaware are, according to 2012 Census Bureau estimates:
Languages.
As of 2000, 91% of Delaware residents age 5 and older speak only English at home; 5% speak Spanish. French is the third most spoken language at 0.7%, followed by Chinese at 0.5% and German at 0.5%.
Legislation had been proposed in both the House and the Senate in Delaware to designate English as the official language. Neither bill was passed in the legislature.
Religion.
The religious affiliations of the people of Delaware are:
As of the year 2010, The Association of Religion Data Archives reported that the three largest denominational groups in Delaware by number of adherents are the Catholic Church at 182,532 adherents, the United Methodist Church with 53,656 members reported, and non-denominational Evangelical Protestant with 22,973 adherents reported. The religious body with the largest number of congregations is the United Methodist Church (with 158 congregations) followed by non-denominational Evangelical Protestant (with 106 congregations), then the Catholic Church (with 45 congregations).
The Roman Catholic Diocese of Wilmington and the Episcopal Diocese of Delaware oversee the parishes within their denominations. The A.U.M.P. Church, the oldest African-American denomination in the nation, was founded in Wilmington. It still has a substantial presence in the state. Reflecting new immigrant populations, an Islamic mosque has been built in the Ogletown area, and a Hindu temple in Hockessin.
A 2012 survey of religious attitudes in the United States found that 34% of Delaware residents considered themselves "moderately religious," 33% "very religious," and 33% as "non-religious."
Sexual orientation.
A 2012 poll by Gallup found that Delaware's proportion of lesbian, gay, bisexual, and transgender adults stood at 3.4 per cent of the population. This constitutes a total LGBT adult population estimate of 23,698 people. The number of same-sex couple households in 2010 stood at 2,646. This grew by 41.65% from a decade earlier. On July 1, 2013, same-sex marriage was legalized, and all civil unions would be converted into marriages.
Economy.
Affluence.
According to a 2013 study by Phoenix Marketing International, Delaware had the ninth-largest number of millionaires per capita in the United States, with a ratio of 6.20 percent.
Agriculture.
Delaware's agricultural output consists of poultry, nursery stock, soybeans, dairy products and corn.
Industries.
As of October 2015, the state's unemployment rate was 5.1%.
The state's largest employers are:
Dover Air Force Base, located next to the state capital of Dover, is one of the largest Air Force bases in the country and is a major employer in Delaware. In addition to its other responsibilities in the United States Air Force Air Mobility Command, this air base serves as the entry point and mortuary for American military personnel and some U.S. government civilians who die overseas.
Recent downfalls and economic outlook.
The recent merger of E.I. du Pont de Nemours & Co. and Dow Chemical Company (pending likely federal regulatory approval) has caused many to question the volatility of DuPont in Delaware, which employs over 8,000 as the state's second largest private employer, as well as the stability of the state's economic future. In late 2015, DuPont announced that 1,700 employees, nearly a third of its footprint in Delaware, would be laid off in early 2016.
Since the mid-2000s, Delaware has suffered an onslaught of economic downfalls affecting stable middle class jobs including: the departure of the state's automotive manufacturing industry (General Motors Wilmington Assembly and Chrysler Newark Assembly), the corporate buyout of a major bank holding company (MBNA), the departure of the state's steel industry (Evraz Claymont Steel), the bankruptcy of a fiber mill (National Vulcanized Fibre), and the diminishing presence of Astra Zeneca in Wilmington.
Incorporation in Delaware.
More than 50% of all U.S. publicly traded companies and 63% of the Fortune 500 are incorporated in Delaware. The state's attractiveness as a corporate haven is largely because of its business-friendly corporation law. Franchise taxes on Delaware corporations supply about one-fifth of its state revenue. Although "USA (Delaware)" ranked as the world's most opaque jurisdiction on the Tax Justice Network's 2009 Financial Secrecy Index, the same group's 2011 Index ranks the USA fifth and does not specify Delaware.
Food and drink.
 stipulates that alcoholic liquor only be sold in specifically licensed establishments, and only between 9:00 am and 1:00 am. Until 2003, Delaware was among the several states enforcing blue laws and banned sale of liquor on Sunday.
Transportation.
The transportation system in Delaware is under the governance and supervision of the Delaware Department of Transportation, also known as "DelDOT". Funding for DelDOT projects is drawn, in part, from the Delaware Transportation Trust Fund, established in 1987 to help stabilize transportation funding; the availability of the Trust led to a gradual separation of DelDOT operations from other Delaware state operations. DelDOT manages programs such as a Delaware Adopt-a-Highway program, major road route snow removal, traffic control infrastructure (signs and signals), toll road management, Delaware Division of Motor Vehicles, the Delaware Transit Corporation (branded as "DART First State", the state government public transportation organization), among others. In 2009, DelDOT maintained 13,507 lane miles of roads, totaling 89 percent of the state's public roadway system; the remaining public road miles are under the supervision of individual municipalities. This far exceeds the United States national average of 20 percent for state department of transportation maintenance responsibility.
The "DART First State" public transportation system was named "Most Outstanding Public Transportation System" in 2003 by the American Public Transportation Association. Coverage of the system is broad within northern New Castle County with close association to major highways in Kent and Sussex counties. The system includes bus, subsidized passenger rail operated by Philadelphia transit agency SEPTA, and subsidized taxi and paratransit modes. The paratransit system, consisting of a statewide door-to-door bus service for the elderly and disabled, has been described by a Delaware state report as "the most generous paratransit system in the United States." , fees for the paratransit service have not changed since 1988.
Roads.
One major branch of the U.S. Interstate Highway System, Interstate 95 (I-95), crosses Delaware southwest-to-northeast across New Castle County. In addition to I-95, there are six U.S. highways that serve Delaware: U.S. Route 9 (US 9), US 13, US 40, US 113, US 202, and US 301. There are also several state highways that cross the state of Delaware; a few of them include Delaware Route 1 (DE 1), DE 9, and DE 404. US 13 and DE 1 are primary north-south highways connecting Wilmington and Pennsylvania with Maryland, with DE 1 serving as the main route between Wilmington and the Delaware beaches. DE 9 is a north-south highway connecting Dover and Wilmington via a scenic route along the Delaware Bay. US 40, is a primary east-west route, connecting Maryland with New Jersey. DE 404 is another primary east-west highway connecting the Chesapeake Bay Bridge in Maryland with the Delaware beaches. The state also operates two toll highways, the Delaware Turnpike, which is I-95, between Maryland and New Castle and the Korean War Veterans Memorial Highway, which is DE 1, between Wilmington and Dover.
A bicycle route, Delaware Bicycle Route 1, spans the north-south length of the state from the Maryland border in Fenwick Island to the Pennsylvania border north of Montchanin. It is the first of several signed bike routes planned in Delaware.
Delaware has around 1,450 bridges, 95 percent of which are under the supervision of DelDOT. About 30 percent of all Delaware bridges were built prior to 1950 and about 60 percent of the number are included in the National Bridge Inventory. Some bridges not under DelDOT supervision includes the four bridges on the Chesapeake and Delaware Canal, which are under the jurisdiction of the U.S. Army Corps of Engineers, and the Delaware Memorial Bridge, which is under the bi-state Delaware River and Bay Authority.
It has been noted that the tar and chip composition of secondary roads in Sussex County make them more prone to deterioration than asphalt roadways found in almost the rest of the state. Among these roads, Sussex (county road) 236 is among the most problematic.
Ferries.
There are three ferries that operate in the state of Delaware:
Rail and bus.
Amtrak has two stations in Delaware along the Northeast Corridor; the relatively quiet Newark Rail Station in Newark, and the busier Wilmington Rail Station in Wilmington. The Northeast Corridor is also served by SEPTA's Wilmington/Newark Line of Regional Rail, which serves Claymont, Wilmington, Churchmans Crossing, and Newark. The major freight railroad in Delaware is the Class I railroad Norfolk Southern, which provides service to most of Delaware. It connects with two shortline railroads, the Delaware Coast Line Railroad and the Maryland and Delaware Railroad, which serve local customers in Sussex County. Another Class I railroad, CSX, passes through northern New Castle County parallel to the Amtrak Northeast Corridor. CSX connects with the freight/heritage operation, the Wilmington and Western Railroad, based in Wilmington and the East Penn Railroad, which operates a line from Wilmington to Coatesville, Pennsylvania.
The last north-south passenger train through the main part of Delaware was the Pennsylvania Railroad's "The Cavalier," which ended service from Philadelphia through the state's interior in 1951.
Air.
Wilmington Airport near Wilmington was served by commercial airline Frontier Airlines. Currently there is no scheduled air-service from Wilmington Airport. In the past, Skybus Airlines also serviced in Wilmington, which provided service to Columbus, Ohio and Greensboro, North Carolina from March 7, 2008 until its bankruptcy on April 5, 2008.
Delaware is centrally situated in the Northeast megalopolis region of cities along I-95. Therefore, Delaware commercial airline passengers most frequently use Philadelphia International Airport (PHL), Baltimore-Washington International Thurgood Marshall Airport (BWI) and Washington Dulles International Airport (IAD) for domestic and international transit. Residents of Sussex County will also use Wicomico Regional Airport (SBY), as it is located less than from the Delaware border. Newark Liberty International Airport (EWR) and Ronald Reagan Washington National Airport (DCA) are also within a radius of New Castle County.
The Dover Air Force Base of the Air Mobility Command is located in the central part of the state, and it is the home of the 436th Airlift Wing and the 512th Airlift Wing.
Other general aviation airports in Delaware include Summit Airport near Middletown, Delaware Airpark near Cheswold, and Delaware Coastal Airport near Georgetown.
Law and government.
Delaware's fourth and current constitution, adopted in 1897, provides for executive, judicial and legislative branches.
Legislative branch.
The Delaware General Assembly consists of a House of Representatives with 41 members and a Senate with 21 members. It sits in Dover, the state capital. Representatives are elected to two-year terms, while senators are elected to four-year terms. The Senate confirms judicial and other nominees appointed by the governor.
Delaware's U.S. Senators are Tom Carper (Democrat) and Chris Coons (Democrat). Delaware's single U.S. Representative is John Carney (Democrat).
Judicial branch.
The Delaware Constitution establishes a number of courts:
Minor non-constitutional courts include the Justice of the Peace Courts and Aldermen's Courts.
Significantly, Delaware has one of the few remaining Courts of Chancery in the nation, which has jurisdiction over equity cases, the vast majority of which are corporate disputes, many relating to mergers and acquisitions. The Court of Chancery and the Delaware Supreme Court have developed a worldwide reputation for rendering concise opinions concerning corporate law which generally (but not always) grant broad discretion to corporate boards of directors and officers. In addition, the Delaware General Corporation Law, which forms the basis of the Courts' opinions, is widely regarded as giving great flexibility to corporations to manage their affairs. For these reasons, Delaware is considered to have the most business-friendly legal system in the United States; therefore a great number of companies are incorporated in Delaware, including 60% of the companies listed on the New York Stock Exchange. Delaware was the last US state to use judicial corporal punishment, in 1952.
Executive branch.
The executive branch is headed by the Governor of Delaware. The present governor is Jack Markell (Democrat), who took office January 20, 2009. The lieutenant governor is Matthew P. Denn. The governor presents a "State of the State" speech to a joint session of the Delaware legislature annually.
Counties.
Delaware is subdivided into three counties; from north to south they are New Castle, Kent County and Sussex. This is the fewest among all states. Each county elects its own legislative body (known in New Castle and Sussex counties as County Council, and in Kent County as Levy Court), which deal primarily in zoning and development issues. Most functions which are handled on a county-by-county basis in other states – such as court and law enforcement – have been centralized in Delaware, leading to a significant concentration of power in the Delaware state government. The counties were historically divided into hundreds, which were used as tax reporting and voting districts until the 1960s, but now serve no administrative role, their only current official legal use being in real-estate title descriptions.
Politics.
The Democratic Party holds a plurality of registrations in Delaware. Until the 2000 presidential election, the state tended to be a Presidential bellwether, sending its three electoral votes to the winning candidate since 1952. This trend ended in 2000, when Delaware's electoral votes went to Al Gore; in 2004, John Kerry won Delaware by eight percentage points. In 2008, Democrat Barack Obama defeated Republican John McCain in Delaware 62.63% to 37.37%. Obama's running mate was Joe Biden, who had represented Delaware in the United States Senate since 1973.
Delaware's swing to the Democrats is in part due to a strong Democratic trend in New Castle County, home to 55 percent of Delaware's population—more than the populations of Kent and Sussex counties combined (the two smaller counties have only 359,000 people between them to New Castle's 535,000). New Castle has not gone Republican in a presidential election since 1988. In 1992, 2000 and 2004, the Republican presidential candidate carried both Kent and Sussex but lost by double-digits each time in New Castle, which was a large enough margin to swing the state to the Democrats. New Castle also elects a substantial majority of the legislature; 27 of the 41 state house districts and 14 of the 21 state senate districts are based in New Castle.
The Democrats have held the governorship since 1993, having won the last six gubernatorial elections in a row. Democrats presently hold seven of the nine statewide elected offices, while the Republicans hold only two statewide offices, State Auditor and State Treasurer.
Freedom of information.
Each of the 50 states of the United States has passed some form of freedom of information legislation, which provides a mechanism for the general public to request information of the government. In 2011, Delaware passed legislation placing a 15 business day time limit on addressing freedom-of-information requests, to either produce information or an explanation of why such information would take longer than this time to produce.
Government revenue.
Delaware has six different income tax brackets, ranging from 2.2% to 5.95%. The state does not assess sales tax on consumers. The state does, however, impose a tax on the gross receipts of most businesses. Business and occupational license tax rates range from 0.096% to 1.92%, depending on the category of business activity.
Delaware does not assess a state-level tax on real or personal property. Real estate is subject to county property taxes, school district property taxes, vocational school district taxes, and, if located within an incorporated area, municipal property taxes.
Gambling provides significant revenue to the state. For instance, the casino at Delaware Park Racetrack provided more than $100 million USD to the state in 2010.
Municipalities.
Wilmington is the state's largest city and its economic hub. It is located within commuting distance of both Philadelphia and Baltimore. All regions of Delaware are enjoying phenomenal growth, with Dover and the beach resorts expanding at a rapid rate.
Ten wealthiest places in Delaware.
Ranked by per capita income
Education.
Delaware was the origin of "Belton v. Gebhart", one of the four cases which was combined into "Brown v. Board of Education", the Supreme Court of the United States decision that led to the end of segregated public schools. Significantly, "Belton" was the only case in which the state court found for the plaintiffs, thereby ruling that segregation was unconstitutional.
Unlike many states, Delaware's educational system is centralized in a state Superintendent of Education, with local school boards retaining control over taxation and some curriculum decisions.
, the Delaware Department of Education had authorized the founding of 25 charter schools in the state, among them one all-girls facility.
All teachers in the State's public school districts are unionized. , none of the State's charter schools are members of a teachers union. One of the State's teachers' unions is Delaware State Education Association (DSEA), which President as of January 2012 is Frederika Jenner.
Sister cities and states.
Delaware's sister state in Japan is Miyagi Prefecture.
Media.
Television.
The northern part of the state is served by network stations in Philadelphia and the southern part by network stations in Baltimore and Salisbury, Maryland. Philadelphia's ABC affiliate, WPVI-TV, maintains a news bureau in downtown Wilmington. Salisbury's ABC affiliate, WMDT covers Sussex and lower Kent County; while CBS affiliate, WBOC-TV, maintains bureaus in Dover and Milton.
Few television stations are based solely in Delaware; the local PBS station from Philadelphia (but licensed to Wilmington), WHYY-TV, maintains a studio and broadcasting facility in Wilmington and Dover, while Ion Television affiliate WPPX is licensed to Wilmington but maintains their offices in Philadelphia and their digital transmitter outside of that city and an analog tower in New Jersey.
In April 2014, it was revealed that Rehoboth Beach's WRDE-LD would affiliate with NBC, becoming the first major network-affiliated station in Delaware.
Tourism.
In addition to First State National Historical Park, Delaware has several museums, , , , , and other .
Rehoboth Beach, together with the towns of Lewes, Dewey Beach, Bethany Beach, South Bethany, and Fenwick Island, comprise Delaware's beach resorts. Rehoboth Beach often bills itself as "The Nation's Summer Capital" because it is a frequent summer vacation destination for Washington, D.C. residents as well as visitors from Maryland, Virginia, and in lesser numbers, Pennsylvania. Vacationers are drawn for many reasons, including the town's charm, artistic appeal, nightlife, and tax free shopping.
Delaware is home to several festivals, fairs, and events. Some of the more notable festivals are the Riverfest held in Seaford, the World Championship Punkin Chunkin held at various locations throughout the state since 1986, the Rehoboth Beach Chocolate Festival, the Bethany Beach Jazz Funeral to mark the end of summer, the Apple Scrapple Festival held in Bridgeville, the Clifford Brown Jazz Festival in Wilmington, the Rehoboth Beach Jazz Festival, the Sea Witch Halloween Festival and Parade in Rehoboth Beach, the Rehoboth Beach Independent Film Festival, the Nanticoke Indian Pow Wow in Oak Orchard, Firefly Music Festival, and the Return Day Parade held after every election in Georgetown.
Culture and entertainment.
Sports.
As Delaware has no franchises in the major American professional sports leagues, many Delawareans follow either Philadelphia or Baltimore teams. The University of Delaware's football team has a large following throughout the state with the Delaware State University and Wesley College teams also enjoying a smaller degree of support.
Delaware is home to Dover International Speedway and Dover Downs. DIS, also known as the "Monster Mile", hosts two NASCAR races each year. Dover Downs is a popular harness racing facility. It is the only co-located horse and car-racing facility in the nation, with the Dover Downs track located inside the DIS track.
Delaware is represented in USA Rugby League by 2015 expansion club, the Delaware Black Foxes.
Delaware has been home to professional wrestling outfit Combat Zone Wrestling (CZW). CZW has been affiliated with the annual Tournament of Death and ECWA with its annual Super 8 Tournament.
Delaware's official state sport is bicycling.
Delaware Native Americans.
Delaware is also the name of a Native American group (called in their own language Lenni Lenape) that was influential in the colonial period of the United States and is today headquartered in Cheswold, Kent County, Delaware. A band of the Nanticoke tribe of American Indians today resides in Sussex County and is headquartered in Millsboro, Sussex County, Delaware.

</doc>
<doc id="7931" url="https://en.wikipedia.org/wiki?curid=7931" title="Dictionary">
Dictionary

A dictionary is a collection of words in one or more specific languages, often alphabetically (or by radical and stroke for ideographic languages), with usage of information, definitions, etymologies, phonetics, pronunciations, translation, and other information; or a book of words in one language with their equivalents in another, also known as a lexicon. It is a lexicographical product designed for utility and function, curated with selected data, presented in a way that shows inter-relationship among the data.
A broad distinction is made between general and specialized dictionaries. Specialized dictionaries do not contain information about words that are used in language for general purposes—words used by ordinary people in everyday situations. Lexical items that describe concepts in specific fields are usually called terms instead of words, although there is no consensus whether lexicology and terminology are two different fields of study. In theory, general dictionaries are supposed to be semasiological, mapping word to definition, while specialized dictionaries are supposed to be onomasiological, first identifying concepts and then establishing the terms used to designate them. In practice, the two approaches are used for both types. There are other types of dictionaries that don't fit neatly in the above distinction, for instance bilingual (translation) dictionaries, dictionaries of synonyms (thesauri), or rhyming dictionaries. The word dictionary (unqualified) is usually understood to refer to a monolingual general-purpose dictionary.
A different dimension on which dictionaries (usually just general-purpose ones) are sometimes distinguished is whether they are "prescriptive" or "descriptive", the latter being in theory largely based on linguistic corpus studies—this is the case of most modern dictionaries. However, this distinction cannot be upheld in the strictest sense. The choice of headwords is considered itself of prescriptive nature; for instance, dictionaries avoid having too many taboo words in that position. Stylistic indications (e.g. ‘informal’ or ‘vulgar’) present in many modern dictionaries is considered less than objectively descriptive as well.
Although the first recorded dictionaries date back to Sumerian times (these were bilingual dictionaries), the systematic study of dictionaries as objects of scientific interest themselves is a 20th-century enterprise, called lexicography, and largely initiated by Ladislav Zgusta. The birth of the new discipline was not without controversy, the practical dictionary-makers being sometimes accused by others of "astonishing" lack of method and critical-self reflection.
History.
The oldest known dictionaries were Akkadian Empire cuneiform tablets with bilingual Sumerian–Akkadian wordlists, discovered in Ebla (modern Syria) and dated roughly 2300 BCE. The early 2nd millennium BCE "Urra=hubullu" glossary is the canonical Babylonian version of such bilingual Sumerian wordlists. A Chinese dictionary, the c. 3rd century BCE "Erya", was the earliest surviving monolingual dictionary; although some sources cite the c. 800 BCE Shizhoupian as a "dictionary", modern scholarship considers it a calligraphic compendium of Chinese characters from Zhou dynasty bronzes. Philitas of Cos (fl. 4th century BCE) wrote a pioneering vocabulary "Disorderly Words" (Ἄτακτοι γλῶσσαι, "") which explained the meanings of rare Homeric and other literary words, words from local dialects, and technical terms. Apollonius the Sophist (fl. 1st century CE) wrote the oldest surviving Homeric lexicon. The first Sanskrit dictionary, the Amarakośa, was written by Amara Sinha c. 4th century CE. Written in verse, it listed around 10,000 words. According to the "Nihon Shoki", the first Japanese dictionary was the long-lost 682 CE "Niina" glossary of Chinese characters. The oldest existing Japanese dictionary, the c. 835 CE "Tenrei Banshō Meigi", was also a glossary of written Chinese. A 9th-century CE Irish dictionary, Sanas Cormaic, contained etymologies and explanations of over 1,400 Irish words. In India around 1320, Amir Khusro compliled the Khaliq-e-bari which mainly dealt with Hindvi and Persian words.
Arabic dictionaries were compiled between the 8th and 14th centuries CE, organizing words in rhyme order (by the last syllable), by alphabetical order of the radicals, or according to the alphabetical order of the first letter (the system used in modern European language dictionaries). The modern system was mainly used in specialist dictionaries, such as those of terms from the Qur'an and hadith, while most general use dictionaries, such as the "Lisan al-`Arab" (13th century, still the best-known large-scale dictionary of Arabic) and "al-Qamus al-Muhit" (14th century) listed words in the alphabetical order of the radicals. The "Qamus al-Muhit" is the first handy dictionary in Arabic, which includes only words and their definitions, eliminating the supporting examples used in such dictionaries as the "Lisan" and the "Oxford English Dictionary".
In medieval Europe, glossaries with equivalents for Latin words in vernacular or simpler Latin were in use (e.g. the Leiden Glossary). The "Catholicon" (1287) by Johannes Balbus, a large grammatical work with an alphabetical lexicon, was widely adopted. It served as the basis for several bilingual dictionaries and was one of the earliest books (in 1460) to be printed. In 1502 Ambrogio Calepino's "Dictionarium" was published, originally a monolingual Latin dictionary, which over the course of the 16th century was enlarged to become a multilingual glossary. In 1532 Robert Estienne published the "Thesaurus linguae latinae" and in 1572 his son Henri Estienne published the "Thesaurus linguae graecae", which served up to the 19th century as the basis of Greek lexicography. The first monolingual dictionary written in Europe was the Spanish, written by Sebastián Covarrubias' "Tesoro de la lengua castellana o española", published in 1611 in Madrid, Spain. In 1612 the first edition of the "Vocabolario dell'Accademia della Crusca", for Italian, was published. It served as the model for similar works in French and English. In 1690 in Rotterdam was published, posthumously, the "Dictionnaire Universel" by Antoine Furetière for French. In 1694 appeared the first edition of the "Dictionnaire de l'Académie française". Between 1712 and 1721 was published the "Vocabulario portughez e latino" written by Raphael Bluteau. The Real Academia Española published the first edition of the "Diccionario de la lengua española" in 1780, but their "Diccionario de Autoridades", which included quotes taken from literary works, was published in 1726. The "Totius Latinitatis lexicon" by Egidio Forcellini was firstly published in 1777; it has formed the basis of all similar works that have since been published.
The first edition of "A Greek-English Lexicon" by Henry George Liddell and Robert Scott appeared in 1843; this work remained the basic dictionary of Greek until the end of the 20th century. And in 1858 was published the first volume of the Deutsches Wörterbuch by the Brothers Grimm; the work was completed in 1961. Between 1861 and 1874 was published the "Dizionario della lingua italiana" by Niccolò Tommaseo. Émile Littré published the Dictionnaire de la langue française between 1863 and 1872. In the same year 1863 appeared the first volume of the "Woordenboek der Nederlandsche Taal" which was completed in 1998. Also in 1863 Vladimir Ivanovich Dahl published the "Explanatory Dictionary of the Living Great Russian Language". The Duden dictionary dates back to 1880, and is currently the prescriptive source for the spelling of German. The decision to start work on the "Svenska Akademiens ordbok" was taken in 1787.
English Dictionaries.
The earliest dictionaries in the English language were glossaries of French, Spanish or Latin words along with definitions of the foreign words in English. Of note, the word "dictionary" was invented by an Englishman called John of Garland in 1220 - he had written a book "Dictionarius" to help with Latin "diction". An early non-alphabetical list of 8000 English words was the "Elementarie" created by Richard Mulcaster in 1582.
The first purely English alphabetical dictionary was "A Table Alphabeticall", written by English schoolteacher Robert Cawdrey in 1604. The only surviving copy is found at the Bodleian Library in Oxford. Yet this early effort, as well as the many imitators which followed it, was seen as unreliable and nowhere near definitive. Philip Stanhope, 4th Earl of Chesterfield was still lamenting in 1754, 150 years after Cawdrey's publication, that it is "a sort of disgrace to our nation, that hitherto we have had no… standard of our language; our dictionaries at present being more properly what our neighbors the Dutch and the Germans call theirs, word-books, than dictionaries in the superior sense of that title." 
On 1616, John Bullokar contributed on the history of dictionary with his "English Expositor". Elisha Coles published his "English Dictionary" in 1676. "Glossographia" by Thomas Blount on 1656, contains more than 10,000 words along with their etymologies or histories. Edward Phillips wrote another dictionary entitled "The New World of English Words: Or a General Dictionary." Phillips boldly plagiarized Blount's work, and the two renounced each other. This created more interest in the dictionaries.
It was not until Samuel Johnson's "A Dictionary of the English Language" (1755) that a truly noteworthy, reliable English Dictionary was deemed to have been produced, and the fact that today many people still mistakenly believe Johnson to have written the first English dictionary is a testimony to this legacy. By this stage, dictionaries had evolved to contain textual references for most words, and were arranged alphabetically, rather than by topic (a previously popular form of arrangement, which meant all animals would be grouped together, etc.). Johnson's masterwork could be judged as the first to bring all these elements together, creating the first 'modern' dictionary.
Johnson's "Dictionary" remained the English-language standard for over 150 years, until the Oxford University Press began writing and releasing the "Oxford English Dictionary" in short fascicles from 1884 onwards. It took nearly 50 years to finally complete the huge work, and they finally released the complete "OED" in twelve volumes in 1928. It remains the most comprehensive and trusted English language dictionary to this day, with revisions and updates added by a dedicated team every three months. One of the main contributors to this modern day dictionary was an ex-army surgeon, William Chester Minor, a convicted murderer who was confined to an asylum for the criminally insane.
American English Dictionaries.
In 1806, American Noah Webster published his first dictionary, "". In 1807 Webster began compiling an expanded and fully comprehensive dictionary, "An American Dictionary of the English Language;" it took twenty-seven years to complete. To evaluate the etymology of words, Webster learned twenty-six languages, including Old English (Anglo-Saxon), German, Greek, Latin, Italian, Spanish, French, Hebrew, Arabic, and Sanskrit.
Webster completed his dictionary during his year abroad in 1825 in Paris, France, and at the University of Cambridge. His book contained seventy thousand words, of which twelve thousand had never appeared in a published dictionary before. As a spelling reformer, Webster believed that English spelling rules were unnecessarily complex, so his dictionary introduced American English spellings, replacing "colour" with "color", substituting "wagon" for "waggon", and printing "center" instead of "centre". He also added American words, like "skunk" and "squash", that did not appear in British dictionaries. At the age of seventy, Webster published his dictionary in 1828; it sold 2500 copies. In 1840, the second edition was published in two volumes.
Austin (2005) explores the intersection of lexicographical and poetic practices in American literature, and attempts to map out a "lexical poetics" using Webster's definitions as his base. He explores how American poets used Webster's dictionaries, often drawing upon his lexicography in order to express their word play. Austin explicates key definitions from both the "Compendious" (1806) and "American" (1828) dictionaries, and brings into its discourse a range of concerns, including the politics of American English, the question of national identity and culture in the early moments of American independence, and the poetics of citation and of definition. Austin concludes that Webster's dictionaries helped redefine Americanism in an era of an emergent and unstable American political and cultural identity. Webster himself saw the dictionaries as a nationalizing device to separate America from Britain, calling his project a "federal language", with competing forces towards regularity on the one hand and innovation on the other. Austin suggests that the contradictions of Webster's lexicography were part of a larger play between liberty and order within American intellectual discourse, with some pulled toward Europe and the past, and others pulled toward America and the new future.
Types.
In a general dictionary, each word may have multiple meanings. Some dictionaries include each separate meaning in the order of most common usage while others list definitions in historical order, with the oldest usage first.
In many languages, words can appear in many different forms, but only the undeclined or unconjugated form appears as the headword in most dictionaries. Dictionaries are most commonly found in the form of a book, but some newer dictionaries, like StarDict and the "New Oxford American Dictionary" are dictionary software running on PDAs or computers. There are also many online dictionaries accessible via the Internet.
Specialized dictionaries.
According to the "Manual of Specialized Lexicographies" a specialized dictionary (also referred to as a technical dictionary) is a lexicon that focuses upon a specific subject field. Following the description in "The Bilingual LSP Dictionary" lexicographers categorize specialized dictionaries into three types. A multi-field dictionary broadly covers several subject fields (e.g., a business dictionary), a single-field dictionary narrowly covers one particular subject field (e.g., law), and a sub-field dictionary covers a singular field (e.g., constitutional law). For example, the 23-language Inter-Active Terminology for Europe is a multi-field dictionary, the American National Biography is a single-field, and the African American National Biography Project is a sub-field dictionary. In terms of the above coverage distinction between "minimizing dictionaries" and "maximizing dictionaries", multi-field dictionaries tend to minimize coverage across subject fields (for instance, "Oxford Dictionary of World Religions" and "Yadgar Dictionary of Computer and Internet Terms") whereas single-field and sub-field dictionaries tend to maximize coverage within a limited subject field ("The Oxford Dictionary of English Etymology").
Another variant is the glossary, an alphabetical list of defined terms in a specialised field, such as medicine (medical dictionary).
Defining dictionaries.
The simplest dictionary, a defining dictionary, provides a core glossary of the simplest meanings of the simplest concepts. From these, other concepts can be explained and defined, in particular for those who are first learning a language. In English, the commercial defining dictionaries typically include only one or two meanings of under 2000 words. With these, the rest of English, and even the 4000 most common English idioms and metaphors, can be defined.
Prescriptive vs. descriptive.
Lexicographers apply two basic philosophies to the defining of words: "prescriptive" or "descriptive". Noah Webster, intent on forging a distinct identity for the American language, altered spellings and accentuated differences in meaning and pronunciation of some words. This is why American English now uses the spelling "color" while the rest of the English-speaking world prefers "colour". (Similarly, British English subsequently underwent a few spelling changes that did not affect American English; see further at American and British English spelling differences.)
Large 20th-century dictionaries such as the "Oxford English Dictionary" (OED) and "Webster's Third" are descriptive, and attempt to describe the actual use of words. Most dictionaries of English now apply the descriptive method to a word's definition, and then, outside of the definition itself, add information alerting readers to attitudes which may influence their choices on words often considered vulgar, offensive, erroneous, or easily confused. "Merriam-Webster" is subtle, only adding italicized notations such as, "sometimes offensive" or "nonstand" (nonstandard). "American Heritage" goes further, discussing issues separately in numerous "usage notes." "Encarta" provides similar notes, but is more prescriptive, offering warnings and admonitions against the use of certain words considered by many to be offensive or illiterate, such as, "an offensive term for..." or "a taboo term meaning...".
Because of the widespread use of dictionaries in schools, and their acceptance by many as language authorities, their treatment of the language does affect usage to some degree, with even the most descriptive dictionaries providing conservative continuity. In the long run, however, the meanings of words in English are primarily determined by usage, and the language is being changed and created every day. As Jorge Luis Borges says in the prologue to "El otro, el mismo": "It is often forgotten that (dictionaries) are artificial repositories, put together well after the languages they define. The roots of language are irrational and of a magical nature."
Dictionaries for natural language processing.
In contrast to traditional dictionaries, which are designed to be used by human beings, dictionaries for natural language processing (NLP) are built to be used by computer programs. The final user is a human being but the direct user is a program. Such a dictionary does not need to be able to be printed on paper. The structure of the content is not linear, ordered entry by entry but has the form of a complex network (see Diathesis alternation). Because most of these dictionaries are used to control machine translations or cross-lingual information retrieval (CLIR) the content is usually multilingual and usually of huge size. In order to allow formalized exchange and merging of dictionaries, an ISO standard called Lexical Markup Framework (LMF) has been defined and used among the industrial and academic community.
Pronunciation.
In some languages, such as the English language, a dictionary has more than 5,000 words. The pronunciation of words is not apparent from their spelling. In these languages, dictionaries usually provide the pronunciation. For example, the definition for the word "dictionary" might be followed by the International Phonetic Alphabet spelling . American English dictionaries often use their own pronunciation spelling systems, for example "dictionary" . The IPA is more commonly used within the British Commonwealth countries. Yet others use a pronunciation respelling system; for example, "dictionary" may respelled . Some online or electronic dictionaries provide audio recordings of words being spoken.
Examples.
Dictionaries of other languages.
Histories and descriptions of the dictionaries of other languages include:
Online dictionaries.
The age of the Internet brought online dictionaries to the desktop and, more recently, to the smart phone. David Skinner in 2013 noted that, "Among the top ten lookups on Merriam-Webster Online at this moment are 'holistic, pragmatic, caveat, esoteric' and 'bourgeois.' Teaching users about words they don’t already know has been, historically, an aim of lexicography, and modern dictionaries do this well."
There exist a number of websites which operate as online dictionaries, usually with a specialized focus. Some of them have exclusively user driven content, often consisting of neologisms. Some of the more notable examples include:

</doc>
<doc id="7935" url="https://en.wikipedia.org/wiki?curid=7935" title="David D. Friedman">
David D. Friedman

David Director Friedman (born February 12, 1945) is an American economist, physicist, legal scholar, and libertarian theorist. He is known for his writings in market anarchist theory, which is the subject of his most popular book, "The Machinery of Freedom" (1973, revised 1989 and 2014). He has authored several other books and articles, including "Price Theory: An Intermediate Text" (1986), "Law's Order: What Economics Has to Do with Law and Why It Matters" (2000), "Hidden Order: The Economics of Everyday Life" (1996), and "Future Imperfect" (2008).
Life and work.
David Friedman is the son of economists Rose and Milton Friedman. He graduated "magna cum laude" from Harvard University in 1965, with a bachelor's degree in chemistry and physics. He later earned a master's (1967) and a Ph.D. (1971) in theoretical physics from the University of Chicago. He was previously a Markle Senior Fellow at New America Foundation from 2000 to 2002. He is currently a professor of law at Santa Clara University, and a contributing editor for "Liberty" magazine. He is an atheist. His son, Patri Friedman, has also written about libertarian theory and market anarchism, particularly seasteading.
The Machinery of Freedom.
In his book "The Machinery of Freedom" (1973), Friedman sketched a form of anarcho-capitalism where all goods and services including law itself can be produced by the free market. This differs from the version proposed by Murray Rothbard, where a legal code would first be consented to by the parties involved in setting up the anarcho-capitalist society. Friedman advocates an incrementalist approach to achieve anarcho-capitalism by gradual privatization of areas that government is involved in, ultimately privatizing law and order itself. In the book, he states his opposition to violent anarcho-capitalist revolution.
He advocates a consequentialist version of anarcho-capitalism, arguing for anarchism on a cost-benefit analysis of state versus no state. It is contrasted with the natural-rights approach as propounded most notably by economist and libertarian theorist Murray Rothbard.
Non-academic interests.
Friedman is a longtime member of the Society for Creative Anachronism, where he is known as "Duke Cariadoc of the Bow". He is known throughout the worldwide society for his articles on the philosophy of recreationism and practical historical recreations, especially those relating to the medieval Middle East. His work is compiled in the popular "Cariadoc's Miscellany". He is sometimes credited with founding the largest and longest-running SCA event, the Pennsic War; as king of the Middle Kingdom he challenged the East Kingdom, and later as king of the East accepted the challenge...and lost.
He is a long-time science fiction fan, and has written two fantasy novels, "Harald" (Baen Books, 2006) and "Salamander" (2011).

</doc>
<doc id="7938" url="https://en.wikipedia.org/wiki?curid=7938" title="Diatomic molecule">
Diatomic molecule

Diatomic molecules are molecules composed of only two atoms, of either the same or different chemical elements. The prefix "di-" is of Greek origin, meaning "two". If a diatomic molecule consists of two atoms of the same element, such as hydrogen (H) or oxygen (O), then it is said to be homonuclear. Otherwise, if a diatomic molecule consists of two different atoms, such as carbon monoxide (CO) or nitric oxide (NO), the molecule is said to be heteronuclear.
The only chemical elements that form stable homonuclear diatomic molecules at standard temperature and pressure (STP) (or typical laboratory conditions of 1 bar and 25 °C) are the gases hydrogen (H), nitrogen (N), oxygen (O), fluorine (F), and chlorine (Cl).
The noble gases (helium, neon, argon, krypton, xenon, and radon) are also gases at STP, but they are monatomic. The homonuclear diatomic gases and noble gases together are called "elemental gases" or "molecular gases", to distinguish them from other gases that are chemical compounds.
At slightly elevated temperatures, the halogens bromine (Br) and iodine (I) also form diatomic gases. All halogens have been observed as diatomic molecules, except for astatine, which is uncertain.
Other elements form diatomic molecules when evaporated, but these diatomic species repolymerize when cooled. Heating ("cracking") elemental phosphorus gives diphosphorus, P. Sulfur vapor is mostly disulfur (S). Dilithium (Li) is known in the gas phase. Ditungsten (W) and dimolybdenum (Mo) form with sextuple bonds in the gas phase. The bond in a homonuclear diatomic molecule is non-polar.
Heteronuclear molecules.
All other diatomic molecules are chemical compounds of two different elements. Many elements can combine to form heteronuclear diatomic molecules, depending on temperature and pressure.
Common examples include the gases carbon monoxide (CO), nitric oxide (NO), and hydrogen chloride (HCl).
Many 1:1 binary compounds are not normally considered diatomic because they are polymeric at room temperature, but they form diatomic molecules when evaporated, for example gaseous MgO, SiO, and many others.
Occurrence.
Hundreds of diatomic molecules have been identified in the environment of the Earth, in the laboratory, and in interstellar space. About 99% of the Earth's atmosphere is composed of two species of diatomic molecules: nitrogen (78%) and oxygen (21%). The natural abundance of hydrogen (H) in the Earth's atmosphere is only of the order of parts per million, but H is the most abundant diatomic molecule in the universe. The interstellar medium is, indeed, dominated by hydrogen atoms.
Molecular geometry.
Diatomic molecules cannot have any geometry but linear, as any two points always lie in a straight line. This is the simplest spatial arrangement of atoms.
Historical significance.
Diatomic elements played an important role in the elucidation of the concepts of element, atom, and molecule in the 19th century, because some of the most common elements, such as hydrogen, oxygen, and nitrogen, occur as diatomic molecules. John Dalton's original atomic hypothesis assumed that all elements were monatomic and that the atoms in compounds would normally have the simplest atomic ratios with respect to one another. For example, Dalton assumed water's formula to be HO, giving the atomic weight of oxygen as eight times that of hydrogen, instead of the modern value of about 16. As a consequence, confusion existed regarding atomic weights and molecular formulas for about half a century.
As early as 1805, Gay-Lussac and von Humboldt showed that water is formed of two volumes of hydrogen and one volume of oxygen, and by 1811 Amedeo Avogadro had arrived at the correct interpretation of water's composition, based on what is now called Avogadro's law and the assumption of diatomic elemental molecules. However, these results were mostly ignored until 1860, partly due to the belief that atoms of one element would have no chemical affinity toward atoms of the same element, and partly due to apparent exceptions to Avogadro's law that were not explained until later in terms of dissociating molecules.
At the 1860 Karlsruhe Congress on atomic weights, Cannizzaro resurrected Avogadro's ideas and used them to produce a consistent table of atomic weights, which mostly agree with modern values. These weights were an important prerequisite for the discovery of the periodic law by Dmitri Mendeleev and Lothar Meyer.
Excited electronic states.
Diatomic molecules are normally in their lowest or ground state, which conventionally is also known as the formula_1 state. When a gas of diatomic molecules is bombarded by energetic electrons, some of the molecules may be excited to higher electronic states, as occurs, for example, in the natural aurora; high-altitude nuclear explosions; and rocket-borne electron gun experiments. Such excitation can also occur when the gas absorbs light or other electromagnetic radiation. The excited states are unstable and naturally relax back to the ground state. Over various short time scales after the excitation (typically a fraction of a second, or sometimes longer than a second if the excited state is metastable), transitions occur from higher to lower electronic states and ultimately to the ground state, and in each transition results a photon is emitted. This emission is known as fluorescence. Successively higher electronic states are conventionally named formula_2, formula_3, formula_4, etc. (but this convention is not always followed, and sometimes lower case letters and alphabetically out-of-sequence letters are used, as in the example given below). The excitation energy must be greater than or equal to the energy of the electronic state in order for the excitation to occur.
In quantum theory, an electronic state of a diatomic molecule is represented by
where formula_6 is the total electronic spin quantum number, formula_7 is the total electronic angular momentum quantum number along the internuclear axis, and formula_8 is the vibrational quantum number. formula_7 takes on values 0, 1, 2, …, which are represented by the electronic state symbols formula_10, formula_11, formula_12,….
For example, the following table lists the common electronic states (without vibrational quantum numbers) along with the energy of the lowest vibrational level (formula_13) of diatomic nitrogen (N), the most abundant gas in the Earth's atmosphere. In the table, the subscripts and superscripts after formula_7 give additional quantum mechanical details about the electronic state.
Note: The "energy" units in the above table are actually the reciprocal of the wavelength of a photon emitted in a transition to the lowest energy state. The actual energy can be found by multiplying the given statistic by the product of "c" (the speed of light) and "h" (Planck's constant), i.e., about 1.99 × 10 Joule metres, and then multiplying by a further factor of 100 to convert from cm to m.
The aforementioned fluorescence occurs in distinct regions of the electromagnetic spectrum, called "emission bands": each band corresponds to a particular transition from a higher electronic state and vibrational level to a lower electronic state and vibrational level (typically, many vibrational levels are involved in an excited gas of diatomic molecules). For example, N formula_2-formula_1 emission bands (a.k.a. Vegard-Kaplan bands) are present in the spectral range from 0.14 to 1.45 μm (micrometres). A given band can be spread out over several nanometers in electromagnetic wavelength space, owing to the various transitions that occur in the molecule's rotational quantum number, formula_17. These are classified into distinct sub-band branches, depending on the change in formula_17. The formula_19 branch corresponds to formula_20, the formula_21 branch to formula_22, and the formula_23 branch to formula_24. Bands are spread out even further by the limited spectral resolution of the spectrometer that is used to measure the spectrum. The spectral resolution depends on the instrument's point spread function.
Energy levels.
The molecular term symbol is a shorthand expression of the angular momenta that characterize the electronic quantum states of a diatomic molecule, which are eigenstates of the electronic molecular Hamiltonian. It is also convenient, and common, to represent a diatomic molecule as two point masses connected by a massless spring. The energies involved in the various motions of the molecule can then be broken down into three categories: the translational, rotational, and vibrational energies.
Translational energies.
The translational energy of the molecule is given by the kinetic energy expression:
where formula_26 is the mass of the molecule and formula_8 is its velocity.
Rotational energies.
Classically, the kinetic energy of rotation is
For microscopic, atomic-level systems like a molecule, angular momentum can only have specific discrete values given by
Also, for a diatomic molecule the moment of inertia is
So, substituting the angular momentum and moment of inertia into E, the rotational energy levels of a diatomic molecule are given by:
Vibrational energies.
Another type of motion of a diatomic molecule is for each atom to oscillate—or vibrate—along the line connecting the two atoms. The vibrational energy is approximately that of a quantum harmonic oscillator:
Comparison between rotational and vibrational energy spacings.
The spacing, and the energy of a typical spectroscopic transition, between vibrational energy levels is about 100 times greater than that of a typical transition between rotational energy levels.
Hund's cases.
The good quantum numbers for a diatomic molecule, as well as good approximations of rotational energy levels, can be obtained by modeling the molecule using Hund's cases.

</doc>
<doc id="7939" url="https://en.wikipedia.org/wiki?curid=7939" title="Duopoly">
Duopoly

A true duopoly (from Greek "duo" δύο (two) + "polein" πωλεῖν (to sell)) is a specific type of oligopoly where only two producers exist in one market. In reality, this definition is generally used where only two firms have dominant control over a market. In the field of industrial organization, it is the most commonly studied form of oligopoly due to its simplicity.
Duopoly models in economics.
There are two principal duopoly models, Cournot duopoly and Bertrand duopoly:
Politics.
Modern American politics, in particular the electoral college system has been described as duopolistic since the Republican and Democratic parties have dominated and framed policy debate as well as the public discourse on matters of national concern for about a century and a half. Third Parties have encountered various blocks in getting onto ballots at different levels of government as well as other electoral obstacles, more so in recent decades.
Examples in business.
The most commonly cited duopoly is that between Visa and Mastercard, who between them control a large proportion of the electronic payment processing market. In 2000 they were the defendants in a U.S. Department of Justice antitrust lawsuit. An appeal was upheld in 2004.
Examples where two companies control a large proportion of a market are:
Media.
In Finland, the state-owned broadcasting company Yleisradio and the private broadcaster Mainos-TV had a legal duopoly (in the economists' sense of the word) from the 1950s to 1993. No other broadcasters were allowed. Mainos-TV operated by leasing air time from Yleisradio, broadcasting in reserved blocks between Yleisradio's own programming on its two channels. This was a unique phenomenon in the world. Between 1986 and 1992 there was an independent third channel but it was jointly owned by Yle and MTV; only in 1993 did MTV get its own channel.
Safaricom mobile service provider and Airtel in Kenya are perfect examples of Duopoly market in the African telecommunication industry.
Broadcasting.
Duopoly is also used in the United States broadcast television and radio industry to refer to a single company owning two outlets in the same city.
This usage is technically incompatible with the normal definition of the word and leads to confusion, inasmuch as there are generally more than two owners of broadcast television stations in markets with broadcast duopolies. In Canada, this definition is therefore more commonly called a "twinstick".

</doc>
<doc id="7940" url="https://en.wikipedia.org/wiki?curid=7940" title="Dungeons &amp; Dragons">
Dungeons &amp; Dragons

Dungeons & Dragons (abbreviated as D&D or DnD) is a fantasy tabletop role-playing game (RPG) originally designed by Gary Gygax and Dave Arneson, and first published in 1974 by Tactical Studies Rules, Inc. (TSR). The game has been published by Wizards of the Coast (now a subsidiary of Hasbro) since 1997. It was derived from miniature wargames with a variation of the "Chainmail" game serving as the initial rule system. "D&D" publication is commonly recognized as the beginning of modern role-playing games and the role-playing game industry.
"D&D" departs from traditional wargaming and assigns each player a specific character to play instead of a military formation. These characters embark upon imaginary adventures within a fantasy setting. A Dungeon Master serves as the game's referee and storyteller, while also maintaining the setting in which the adventures occur and playing the role of the inhabitants. The characters form a party that interacts with the setting's inhabitants (and each other). Together they solve dilemmas, engage in battles and gather treasure and knowledge. In the process the characters earn experience points to become increasingly powerful over a series of sessions.
The early success of "Dungeons & Dragons" led to a proliferation of similar game systems. Despite this competition, "D&D" remains the market leader in the role-playing game industry. In 1977, the game was split into two branches: the relatively rules-light game system of "Dungeons & Dragons" and the more structured, rules-heavy game system of "Advanced Dungeons & Dragons" (abbreviated as "AD&D" or "ADnD"). "AD&D" 2nd Edition was published in 1989. In 2000, the original line of the game was discontinued and the "AD&D" version was renamed "Dungeons & Dragons" with the release of its 3rd edition with a new system. These rules formed the basis of the d20 System which is available under the Open Game License (OGL) for use by other publishers. "Dungeons & Dragons" version 3.5 was released in June 2003, with a (non-OGL) 4th edition in June 2008. A 5th edition was released during the second half of 2014.
, "Dungeons & Dragons" remained the best-known and best-selling role-playing game, with an estimated 20 million people having played the game and more than US$1 billion in book and equipment sales. The game has been supplemented by many pre-made adventures as well as commercial campaign settings suitable for use by regular gaming groups. "Dungeons & Dragons" is known beyond the game for other "D&D"-branded products, references in popular culture and some of the controversies that have surrounded it, particularly a moral panic in the 1980s falsely linking it to Satanism and suicide. The game has won multiple awards and has been translated into many languages beyond the original English.
Play overview.
"Dungeons & Dragons" is a structured yet open-ended role-playing game. It is normally played indoors with the participants seated around a tabletop. Typically, each player controls only a single character, which represents an individual in a fictional setting. When working together as a group, these player characters (PCs) are often described as a "party" of adventurers, with each member often having their own area of specialty which contributes to the success of the whole. During the course of play, each player directs the actions of their character and their interactions with other characters in the game. This activity is performed through the verbal impersonation of the characters by the players, while also employing a variety of social and other useful cognitive skills, such as logic, basic mathematics and imagination. A game often continues over a series of meetings to complete a single adventure, and longer into a series of related gaming adventures, called a "campaign".
The results of the party's choices and the overall storyline for the game are determined by the Dungeon Master (DM) according to the rules of the game and the DM's interpretation of those rules. The DM selects and describes the various non-player characters (NPCs) that the party encounters, the settings in which these interactions occur, and the outcomes of those encounters based on the players' choices and actions. Encounters often take the form of battles with "monsters" – a generic term used in "D&D" to describe potentially hostile beings such as animals, aberrant beings, or mythical creatures. The game's extensive rules – which cover diverse subjects such as social interactions, magic use, combat, and the effect of the environment on PCs – help the DM to make these decisions. The DM may choose to deviate from the published rules or make up new ones if they feel it is necessary.
The most recent versions of the game's rules are detailed in three core rulebooks: The "Player's Handbook", the "Dungeon Master's Guide" and the "Monster Manual".
The only items required to play the game are the rulebooks, a character sheet for each player, and a number of polyhedral dice. Many players also use miniature figures on a grid map as a visual aid, particularly during combat. Some editions of the game presume such usage. Many optional accessories are available to enhance the game, such as expansion rulebooks, pre-designed adventures and various campaign settings.
Game mechanics.
Before the game begins, each player creates their player character and records the details (described below) on a character sheet. First, a player determines their character's ability scores, which consist of Strength, Constitution, Dexterity, Intelligence, Wisdom, and Charisma. Each edition of the game has offered differing methods of determining these statistics. The player then chooses a race (species) such as human or elf, a character class (occupation) such as fighter or wizard, an alignment (a moral and ethical outlook), and additional features to round out the character's abilities and backstory, which have varied in nature through differing editions.
During the game, players describe their PC's intended actions, such as punching an opponent or picking a lock, and converse with the DM, who then describes the result or response. Trivial actions, such as picking up a letter or opening an unlocked door, are usually automatically successful. The outcomes of more complex or risky actions are determined by rolling dice. Factors contributing to the outcome include the character's ability scores, skills and the difficulty of the task. In circumstances where a character does not have control of an event, such as when a trap or magical effect is triggered or a spell is cast, a saving throw can be used to determine whether the resulting damage is reduced or avoided. In this case the odds of success are influenced by the character's class, levels and ability scores.
As the game is played, each PC changes over time and generally increases in capability. Characters gain (or sometimes lose) experience, skills and wealth, and may even alter their alignment or gain additional character classes. The key way characters progress is by earning experience points (XP), which happens when they defeat an enemy or accomplish a difficult task. Acquiring enough XP allows a PC to advance a level, which grants the character improved class features, abilities and skills. XP can also be lost in some circumstances, such as encounters with creatures that drain life energy, or by use of certain magical powers that come with an XP cost.
Hit points (HP) are a measure of a character's vitality and health and are determined by the class, level and constitution of each character. They can be temporarily lost when a character sustains wounds in combat or otherwise comes to harm, and loss of HP is the most common way for a character to die in the game. Death can also result from the loss of key ability scores or character levels. When a PC dies, it is often possible for the dead character to be resurrected through magic, although some penalties may be imposed as a result. If resurrection is not possible or not desired, the player may instead create a new PC to resume playing the game.
Adventures, campaigns, and modules.
A typical "Dungeons & Dragons" game consists of an "adventure", which is roughly equivalent to a single story. The DM can either design an adventure on their own, or follow one of the many additional pre-made adventures (also known as "modules") that have been published throughout the history of "Dungeons & Dragons". Published adventures typically include a background story, illustrations, maps and goals for PCs to achieve. Some also include location descriptions and handouts. Although a small adventure entitled 'Temple of the Frog' was included in the "Blackmoor" rules supplement in 1975, the first stand-alone "D&D" module published by TSR was 1978's "Steading of the Hill Giant Chief", written by Gygax.
A linked series of adventures is commonly referred to as a "campaign". The locations where these adventures occur, such as a city, country, planet or an entire fictional universe, are referred to as "campaign settings" or "world". "D&D" settings are based in various fantasy subgenres and feature varying levels of magic and technology. Popular commercially published campaign settings for "Dungeons & Dragons" include Greyhawk, Dragonlance, Forgotten Realms, Mystara, Spelljammer, Ravenloft, Dark Sun, Planescape, Birthright, and Eberron. Alternatively, DMs may develop their own fictional worlds to use as campaign settings.
Miniature figures.
The wargames from which "Dungeons & Dragons" evolved used miniature figures to represent combatants. "D&D" initially continued the use of miniatures in a fashion similar to its direct precursors. The original "D&D" set of 1974 required the use of the "Chainmail" miniatures game for combat resolution. By the publication of the 1977 game editions, combat was mostly resolved verbally. Thus miniatures were no longer required for game play, although some players continued to use them as a visual reference.
In the 1970s, numerous companies began to sell miniature figures specifically for "Dungeons & Dragons" and similar games. Licensed miniature manufacturers who produced official figures include Grenadier Miniatures (1980–1983), Citadel Miniatures (1984–1986), Ral Partha, and TSR itself. Most of these miniatures used the 25 mm scale.
Periodically, "Dungeons & Dragons" has returned to its wargaming roots with supplementary rules systems for miniatures-based wargaming. Supplements such as "Battlesystem" (1985 & 1989) and a new edition of "Chainmail" (2001) provided rule systems to handle battles between armies by using miniatures.
Game history.
Sources and influences.
An immediate predecessor of "Dungeons & Dragons" was a set of medieval miniature rules written by Jeff Perren. These were expanded by Gary Gygax, whose additions included a fantasy supplement, before the game was published as "Chainmail". When Dave Wesely entered the Army in 1970, his friend and fellow Napoleonics wargamer Dave Arneson began a medieval variation of Wesely's Braunstein games, where players control individuals instead of armies. Arneson used "Chainmail" to resolve combats. As play progressed, Arneson added such innovations as character classes, experience points, level advancement, armor class, and others. Having partnered previously with Gygax on "Don't Give Up the Ship!", Arneson introduced Gygax to his Blackmoor game and the two then collaborated on developing "The Fantasy Game", the role-playing game (RPG) that became "Dungeons & Dragons", with the final writing and preparation of the text being done by Gygax.
Many "Dungeons & Dragons" elements appear in hobbies of the mid-to-late 20th century. For example, character-based role playing can be seen in improvisational theatre. Game-world simulations were well developed in wargaming. Fantasy milieux specifically designed for gaming could be seen in Glorantha's board games among others. Ultimately, however, "Dungeons & Dragons" represents a unique blending of these elements.
The world of "D&D" was influenced by world mythology, history, pulp fiction, and contemporary fantasy novels. The importance of J.R.R. Tolkien's "The Lord of the Rings" and "The Hobbit" as an influence on "D&D" is controversial. The presence in the game of halflings, elves, half-elves, dwarves, orcs, rangers, and the like, draw comparisons to these works. The resemblance was even closer before the threat of copyright action from Tolkien Enterprises prompted the name changes of hobbit to 'halfling', ent to 'treant', and balrog to 'balor'. For many years Gygax played down the influence of Tolkien on the development of the game. However, in an interview in 2000, he acknowledged that Tolkien's work had a "strong impact".
The D&D magic system, in which wizards memorize spells that are used up once cast and must be re-memorized the next day, was heavily influenced by the "Dying Earth" stories and novels of Jack Vance. The original alignment system (which grouped all characters and creatures into 'Law', 'Neutrality' and 'Chaos') was derived from the novel "Three Hearts and Three Lions" by Poul Anderson. A troll described in this work also influenced the "D&D" definition of that monster.
Other influences include the works of Robert E. Howard, Edgar Rice Burroughs, A. Merritt, H. P. Lovecraft, Fritz Leiber, L. Sprague de Camp, Fletcher Pratt, Roger Zelazny, and Michael Moorcock. Monsters, spells, and magic items used in the game have been inspired by hundreds of individual works such as A. E. van Vogt's "Black Destroyer", Coeurl (the Displacer Beast), Lewis Carroll's "Jabberwocky" (vorpal sword) and the Book of Genesis (the clerical spell 'Blade Barrier' was inspired by the "flaming sword which turned every way" at the gates of Eden).
Edition history.
"Dungeons & Dragons" has gone through several revisions. Parallel versions and inconsistent naming practices can make it difficult to distinguish between the different editions.
Original game.
The original "Dungeons & Dragons", now referred to as OD&D, was a small box set of three booklets published in 1974. It was amateurish in production and assumed the player was familiar with wargaming. Nevertheless, it grew rapidly in popularity, first among wargamers and then expanding to a more general audience of college and high school students. Roughly 1,000 copies of the game were sold in the first year followed by 3,000 in 1975, and much more in the following years. This first set went through many printings and was supplemented with several official additions, such as the original Greyhawk and Blackmoor supplements (both 1975), as well as magazine articles in TSR's official publications and countless fanzines.
Two-pronged strategy.
In 1977, TSR created the first element of a two-pronged strategy that would divide the "D&D" game for nearly two decades. A "Dungeons & Dragons Basic Set" boxed edition was introduced that cleaned up the presentation of the essential rules, made the system understandable to the general public, and was sold in a package that could be stocked in toy stores. Also in 1977, the first part of "Advanced Dungeons & Dragons" ("AD&D") was published, which brought together the various published rules, options and corrections, then expanded them into a definitive, unified game for hobbyist gamers. TSR marketed them as an introductory game for new players and a more complex game for experienced ones; the "Basic Set" directed players who exhausted the possibilities of that game to switch to the advanced rules.
As a result of this parallel development, the basic game included many rules and concepts which contradicted comparable ones in "AD&D". John Eric Holmes, the editor of the basic game, preferred a lighter tone with more room for personal improvisation. "AD&D", on the other hand, was designed to create a tighter, more structured game system than the loose framework of the original game. Between 1977 and 1979, three hardcover rulebooks, commonly referred to as the "core rulebooks", were released: the "Player's Handbook" (PHB), the "Dungeon Master's Guide" (DMG), and the "Monster Manual" (MM). Several supplementary books were published throughout the 1980s, notably "Unearthed Arcana" (1985) that included a large number of new rules. Confusing matters further, the original "D&D" boxed set remained in publication until 1979, since it remained a healthy seller for TSR.
Revised editions.
In the 1980s, the rules for "Advanced Dungeons & Dragons" and "basic" "Dungeons & Dragons" remained separate, each developing along different paths.
In 1981, the basic version of "Dungeons & Dragons" was revised by Tom Moldvay to make it even more novice-friendly. It was promoted as a continuation of the original "D&D" tone, whereas "AD&D" was promoted as advancement of the mechanics. An accompanying "Expert Set", originally written by David "Zeb" Cook, allowed players to continue using the simpler ruleset beyond the early levels of play. In 1983, revisions of those sets by Frank Mentzer were released, revising the presentation of the rules to a more tutorial format. These were followed by "Companion" (1983), "Master" (1985), and "Immortals" (1986) sets. Each set covered game play for more powerful characters than the previous. The first four sets were later compiled as a single hardcover book, the "Dungeons & Dragons Rules Cyclopedia" (1991), along with a new introductory boxed set.
"Advanced Dungeons & Dragons 2nd Edition" was published in 1989, again as three core rulebooks; the primary designer was David "Zeb" Cook. The "Monster Manual" was replaced by the "Monstrous Compendium", a loose-leaf binder that was subsequently replaced by the hardcover "Monstrous Manual" in 1993. In 1995, the core rulebooks were slightly revised, although still referred to by TSR as the 2nd Edition, and a series of "Player's Option" manuals were released as optional rulebooks.
The release of "AD&D2" deliberately excluded some aspects of the game that had attracted negative publicity. References to demons and devils, sexually suggestive artwork, and playable, evil-aligned character types – such as assassins and half-orcs – were removed. The edition moved away from a theme of 1960s and 1970s "sword and sorcery" fantasy fiction to a mixture of medieval history and mythology. The rules underwent minor changes, including the addition of non-weapon proficiencies – skill-like abilities that originally appeared in 1st Edition supplements. The game's magic spells were divided into schools and spheres. A major difference was the promotion of various game settings beyond that of traditional fantasy. This included blending fantasy with other genres, such as horror (Ravenloft), science fiction (Spelljammer), and apocalyptic (Dark Sun), as well as alternative historical and non-European mythological settings.
Wizards of the Coast.
In 1997, a near-bankrupt TSR was purchased by Wizards of the Coast. Following three years of development, "Dungeons & Dragons" 3rd edition was released in 2000. The new release folded the Basic and Advanced lines back into a single unified game. It was the largest revision of the "D&D" rules to date, and also served as the basis for a multi-genre role-playing system designed around 20-sided dice, called the d20 System. The 3rd Edition rules were designed to be internally consistent and less restrictive than previous editions of the game, allowing players more flexibility to create the characters they wanted to play. Skills and feats were introduced into the core rules to encourage further customization of characters. The new rules also standardized the mechanics of action resolution and combat. In 2003, "Dungeons & Dragons v.3.5" was released as a revision of the 3rd Edition rules. This release incorporated hundreds of rule changes, mostly minor, and expanded the core rulebooks.
In early 2005, Wizards of the Coast's R&D team started to develop "Dungeons & Dragons 4th Edition", prompted mainly by the feedback obtained from the "D&D" playing community and a desire to make the game faster, more intuitive, and with a better play experience than under the 3rd Edition. The new game was developed through a number of design phases spanning from May 2005 until its release. "Dungeons & Dragons 4th Edition" was announced at Gen Con in August 2007, and the initial three core books were released June 6, 2008. 4th Edition streamlined the game into a simplified form and introduced numerous rules changes. Many character abilities were restructured into "Powers". These altered the spell-using classes by adding abilities that could be used at will, per encounter, or per day. Likewise, non-magic-using classes were provided with parallel sets of options. Software tools, including player character and monster building programs, became a major part of the game.
On January 9, 2012, Wizards of the Coast announced that it was working on a 5th edition of the game. The company planned to take suggestions from players and let them playtest the rules. Public playtesting began on May 24, 2012. At Gen Con 2012 in August, Mike Mearls, lead developer for 5th Edition, said that Wizards of the Coast had received feedback from more than 75,000 playtesters, but that the entire development process would take two years, adding, "I can't emphasize this enough ... we're very serious about taking the time we need to get this right." The release of the 5th Edition, coinciding with "D&D"s 40th anniversary, occurred in the second half of 2014.
Acclaim and influence.
The game had more than 3 million players around the world by 1981, and copies of the rules were selling at a rate of about 750,000 per year by 1984. Beginning with a French language edition in 1982, "Dungeons & Dragons" has been translated into many languages beyond the original English. By 2004, consumers had spent more than US$1 billion on "Dungeons & Dragons" products and the game had been played by more than 20 million people. As many as 6 million people played the game in 2007.
The various editions of "Dungeons & Dragons" have won many Origins Awards, including "All Time Best Roleplaying Rules of 1977", "Best Roleplaying Rules of 1989", and "Best Roleplaying Game of 2000" for the three flagship editions of the game. Both "Dungeons & Dragons" and "Advanced Dungeons & Dragons" are Origins Hall of Fame Games inductees as they were deemed sufficiently distinct to merit separate inclusion on different occasions. The independent "Games" magazine placed "Dungeons & Dragons" on their "Games 100" list from 1980 through 1983, then entered the game into the magazine's Hall of Fame in 1984.
"Dungeons & Dragons" was the first modern role-playing game and it established many of the conventions that have dominated the genre. Particularly notable are the use of dice as a game mechanic, character record sheets, use of numerical attributes and gamemaster-centered group dynamics. Within months of "Dungeons & Dragons"'s release, new role-playing game writers and publishers began releasing their own role-playing games, with most of these being in the fantasy genre. Some of the earliest other role-playing games inspired by "D&D" include "Tunnels & Trolls" (1975), "Empire of the Petal Throne" (1975), and "Chivalry & Sorcery" (1976).
The role-playing movement initiated by "D&D" would lead to release of the science fiction game "Traveller" (1977), the fantasy game "RuneQuest" (1978), and subsequent game systems such as Chaosium's "Call of Cthulhu" (1981), "Champions" (1982), "GURPS" (1986), and "" (1991). "Dungeons & Dragons" and the games it influenced fed back into the genre's origin – miniatures wargames – with combat strategy games like "Warhammer Fantasy Battles". "D&D" also had a large impact on modern video games.
Director Jon Favreau credits "Dungeons & Dragons" with giving him "... a really strong background in imagination, storytelling, understanding how to create tone and a sense of balance."
Licensing.
Early in the game's history, TSR took no action against small publishers' production of "D&D" compatible material, and even licensed Judges Guild to produce "D&D" materials for several years, such as "City State of the Invincible Overlord." This attitude changed in the mid-1980s when TSR took legal action to try to prevent others from publishing compatible material. This angered many fans and led to resentment by the other gaming companies. Although TSR took legal action against several publishers in an attempt to restrict third-party usage, it never brought any court cases to completion, instead settling out of court in every instance. TSR itself ran afoul of intellectual property law in several cases.
With the launch of "Dungeons & Dragons"'s 3rd Edition, Wizards of the Coast made the d20 System available under the Open Game License (OGL) and d20 trademark license. Under these licenses, authors are free to use the d20 System when writing games and game supplements. The OGL and d20 Trademark License made possible new games, some based on licensed products like "Star Wars", and new versions of older games, such as "Call of Cthulhu".
During the 2000s, there has been a trend towards recreating older editions of "D&D". Necromancer Games, with its slogan "Third Edition Rules, First Edition Feel" and Goodman Games' "Dungeon Crawl Classics" range are both examples of this in material for d20 System. Other companies have created complete game systems based on earlier editions of "D&D". An example is "HackMaster" (2001) by Kenzer and Company, a licensed, non-OGL, semi-satirical follow-on to 1st and 2nd Edition. "Castles & Crusades" (2005), by Troll Lord Games, is a reimagining of early editions by streamlining rules from OGL that was supported by Gary Gygax prior to his death.
With the release of the fourth edition, Wizards of the Coast has introduced its Game System License, which represents a significant restriction compared to the very open policies embodied by the OGL. In part as a response to this, some publishers (such as Paizo Publishing with its "Pathfinder Roleplaying Game") who previously produced materials in support of the "D&D" product line, have decided to continue supporting the 3rd Edition rules, thereby competing directly with Wizards of the Coast. Others, such as Kenzer & Company, are returning to the practice of publishing unlicensed supplements and arguing that copyright law does not allow Wizards of the Coast to restrict third-party usage.
Controversy and notoriety.
At various times in its history, "Dungeons & Dragons" has received negative publicity, in particular from some Christian groups, for alleged promotion of such practices as devil worship, witchcraft, suicide, and murder, and for the presence of naked breasts in drawings of female humanoids in the original "AD&D" manuals (mainly monsters such as harpies, succubi, etc.). These controversies led TSR to remove many potentially controversial references and artwork when releasing the 2nd Edition of "AD&D". Many of these references, including the use of the names "devils" and "demons", were reintroduced in the 3rd edition. The moral panic over the game also led to problems for fans of "D&D" who faced social ostracism, unfair treatment, and false association with the occult and Satanism, regardless of an individual fan's actual religious affiliation and beliefs.
"Dungeons & Dragons" has also been the subject of rumors regarding players having difficulty separating fantasy from reality, even leading to psychotic episodes. The most notable of these was the saga of James Dallas Egbert III, the facts of which were fictionalized in the novel "Mazes and Monsters" and later made into a TV movie. The game was also blamed for some of the actions of Chris Pritchard, who was convicted in 1990 of murdering his stepfather. Research by various psychologists, the first being that of Armando Simon, has concluded that no harmful effects are related to the playing of "D&D".
The game's commercial success was a factor that led to lawsuits regarding distribution of royalties between original creators Gygax and Arneson. Gygax later became embroiled in a political struggle for control of TSR which culminated in a court battle and Gygax's decision to sell his ownership interest in the company in 1985.
Related products.
"D&D"'s commercial success has led to many other related products, including "Dragon" Magazine, "Dungeon" Magazine, an animated television series, a film series, an official role-playing soundtrack, novels, and computer games such as the MMORPG "Dungeons & Dragons Online". Hobby and toy stores sell dice, miniatures, adventures, and other game aids related to "D&D" and its game offspring.
In popular culture.
"D&D" grew in popularity through the late 1970s and 1980s. Numerous games, films, and cultural references based on "D&D" or "D&D"-like fantasies, characters or adventures have been ubiquitous since the end of the 1970s. "D&D" players are (sometimes pejoratively) portrayed as the epitome of geekdom, and have become the basis of much geek and gamer humor and satire. Famous D&D players include professional basketball player Tim Duncan, comedian Stephen Colbert, and actors Vin Diesel and Robin Williams. "D&D" and its fans have been the subject of spoof films, including "Fear of Girls" and "".

</doc>
<doc id="7941" url="https://en.wikipedia.org/wiki?curid=7941" title="Double jeopardy">
Double jeopardy

Double jeopardy is a procedural defence that forbids a defendant from being tried again on the same (or similar) charges in the same case following a legitimate acquittal or conviction. In common law countries, a defendant may enter a peremptory plea of autrefois acquité or autrefois convict ("autrefois" means "in the past" in French), meaning the defendant has been acquitted or convicted of the same offence and hence that they cannot be retried under the principle of double jeopardy.
If this issue is raised, evidence will be placed before the court, which will normally rule as a preliminary matter whether the plea is substantiated; if it is, the projected trial will be prevented from proceeding. In some countries, including Canada, Mexico and the United States, the guarantee against being "twice put in jeopardy" is a constitutional right. In other countries, the protection is afforded by statute.
International Covenant on Civil and Political Rights.
The 72 signatories and 166 parties to the International Covenant on Civil and Political Rights recognise, under Article 14 (7): No one shall be liable to be tried or punished again for an offence for which he has already been finally convicted or acquitted in accordance with the law and penal procedure of each country.
European Convention on Human Rights.
All members of the Council of Europe (which includes nearly all European countries, and every member of the European Union) have signed the European Convention on Human Rights. The optional Seventh Protocol to the Convention, Article Four, protects against double jeopardy and says:
No one shall be liable to be tried or punished again in criminal proceedings under the jurisdiction of the same State for an offence for which he or she has already been finally acquitted or convicted in accordance with the law and penal procedure of that State.
Member states may, however, implement legislation which allows reopening of a case in the event that new evidence is found or if there was a fundamental defect in the previous proceedings:
The provisions of the preceding paragraph shall not prevent the reopening of the case in accordance with the law and penal procedure of the State concerned, if there is evidence of new or newly discovered facts, or if there has been a fundamental defect in the previous proceedings, which could affect the outcome of the case.
This optional protocol has been ratified by all EU states except five: Belgium, Germany, Spain, the Netherlands, and the United Kingdom. In those member states, national rules governing double jeopardy may or may not comply with the provision cited above.
In many European countries, the prosecution may appeal an acquittal to a higher court (similar to the provisions of Canadian law); this is not counted as double jeopardy, but as a continuation of the same trial. This is allowed by the European Convention on Human Rights (note the word "finally" in the above quotation).
By country.
Australia.
In contrast to other common law nations, Australian double jeopardy law has been held to further prevent the prosecution for perjury following a previous acquittal where a finding of perjury would controvert the acquittal. This was confirmed in the case of "R v Carroll", where the police found new evidence convincingly disproving Carroll's sworn alibi two decades after he had been acquitted of murder charges in the death of Ipswich child Deidre Kennedy, and successfully prosecuted him for perjury. Public outcry following the overturn of his conviction (for perjury) by the High Court has led to widespread calls for reform of the law along the lines of the England and Wales legislation.
During a Council of Australian Governments (COAG) meeting of 2007, model legislation to rework double jeopardy laws was drafted, but there was no formal agreement for each state to introduce it. All states have now chosen to introduce legislation that mirrors COAG's recommendations on "fresh and compelling" evidence.
In New South Wales, retrials of serious cases with a minimum sentence of 20 years or more are now possible, whether or not the original trial preceded the 2006 reform. On 17 October 2006, the New South Wales Parliament passed legislation abolishing the rule against double jeopardy in cases where:
On 30 July 2008, South Australia also introduced legislation to scrap parts of its double jeopardy law, legalising retrials for serious offences with "fresh and compelling" evidence, or if the acquittal was tainted.
In Western Australia, on 8 September 2011 amendments were introduced that would allow also retrial if "new and compelling" evidence was found. It would apply to serious offences where the penalty was life imprisonment or imprisonment for 14 years or more. Acquittal because of tainting (witness intimidation, jury tampering, or perjury) would also allow retrial.
In Tasmania, on 19 August 2008, amendments were introduced to allow retrial in serious cases, if there is "fresh and compelling" evidence.
In Victoria on 21 December 2011, legislation was passed allowing new trials where there is "fresh and compelling DNA evidence, where the person acquitted subsequently admits to the crime, or where it becomes clear that key witnesses have given false evidence". Retrial applications however could only be made for serious offences such as murder, manslaughter, arson causing death, serious drug offences and aggravated forms of rape and armed robbery.
In Queensland on 18 October 2007, the double jeopardy laws were modified to allow a retrial where fresh and compelling evidence becomes available after an acquittal for murder or a "tainted acquittal" for a crime carrying a 25-year or more sentence. A "tainted acquittal" requires a conviction for an administration of justice offence, such as perjury, that led to the original acquittal. Unlike reforms in the United Kingdom, New South Wales, Tasmania, Victoria, South Australia, Western Australia, this law does not have a retrospective effect, which is unpopular with some advocates of the reform.
Canada.
The Canadian Charter of Rights and Freedoms includes provisions such as section 11(h) prohibiting double jeopardy. However, this prohibition applies only after an accused person has been "finally" convicted or acquitted. Canadian law allows the prosecution to appeal an acquittal: if the acquittal is thrown out, the new trial is not considered to be double jeopardy, as the verdict of the first trial would have been annulled. In rare circumstances, a court of appeal might also substitute a conviction for an acquittal. This is not considered to be double jeopardy, either – in this case, the appeal and subsequent conviction are deemed to be a continuation of the original trial.
For an appeal from an acquittal to be successful, the Supreme Court of Canada requires that the Crown show that an error in law was made during the trial and that the error contributed to the verdict. It has been suggested that this test is unfairly beneficial to the prosecution. For instance, lawyer Martin Friedland, in his book "My Life in Crime and Other Academic Adventures", contends that the rule should be changed so that a retrial is granted only when the error is shown to be "responsible" for the verdict, not just a factor.
A notable example of this is Guy Paul Morin, who was wrongfully convicted in his second trial after the acquittal in his first trial was vacated by the Supreme Court of Canada.
In the Guy Turcotte case, for instance, the Quebec Court of Appeal overturned Turcotte's not guilty verdict and ordered a second trial after it found that the judge committed an error in the first trial while giving instructions to the jury. Turcotte was later convicted of second-degree murder in the second trial.
France.
Once all appeals have been exhausted on a case, the judgement is final and the action of the prosecution is closed (code of penal procedure, art. 6), except if the final ruling was forged. Prosecution for a crime already judged is impossible even if incriminating evidence has been found. However, a person who has been convicted may request another trial on grounds of new exculpating evidence through a procedure known as "révision".
Germany.
The Basic Law ("Grundgesetz") for the Federal Republic of Germany does provide protection against double jeopardy if a final verdict is pronounced. A verdict is final if nobody appeals against it.
However, each trial party can appeal against a verdict in the first instance. This means the prosecution and/or the defendants can appeal against a judgement if they don´t agree with it. In this case the trial starts again in the second instance, the court of appeal ("Berufungsgericht"), which considers the facts and reasons again and delivers the final judgement then.
If one of the parties disagrees with the judgement of the second instance he or she can appeal it, but only on formal judicial reasons. The case will checked in the third instance ("Revisionsgericht"), whether all laws are applied correctly.
The rule applies to the whole "historical event, which is usually considered a single historical course of actions the separation of which would seem unnatural". This is true even if new facts occur that indicate other and/or much serious crimes.
The Penal Procedural Code ("Strafprozessordnung") permits a retrial ("Wiederaufnahmeverfahren"), if it is in favor of the defendant or if following events had happened:
In the case of an order of summary punishment, which can be issued by the court without a trial for lesser misdemeanours, there is a further exception:
In Germany, a felony is defined as a crime which (usually) has a minimum of one year of imprisonment.
India.
A partial protection against double jeopardy is a Fundamental Right guaranteed under Article 20 (2) of the Constitution of India, which states "No person shall be prosecuted and punished for the same offence more than once". Like if a Death Execution is failed, that means the culprit cannot be executed twice for same offence. This provision enshrines the concept of "autrefois convict", that no one convicted of an offence can be tried or punished a second time. However, it does not extend to "autrefois acquit", and so if a person is acquitted of a crime he can be retried. In India, protection against "autrefois acquit" is a statutory right, not a fundamental one. Such protection is provided by provisions of the Code of Criminal Procedure rather than by the Constitution.
Japan.
The Constitution of Japan states in Article 39 that
In practice, however, if someone is acquitted in a lower District Court, then the prosecutor can appeal to the High Court, and then to the Supreme Court. Only the acquittal in the Supreme Court is the final acquittal which prevents any further retrial. This process sometimes takes decades.
The above is not considered a violation of the constitution. Because of Supreme Court precedent, this process is all considered part of a single proceeding.
The Netherlands.
In the Netherlands, the state prosecution can appeal a not-guilty verdict at the bench. New evidence can be brought to bear during a retrial at a district court. Thus one can be tried twice for the same alleged crime. If one is convicted at the district court, the defence can make an appeal on procedural grounds to the supreme court. The supreme court might admit this complaint, and the case will be reopened yet again, at another district court. Again, new evidence might be introduced by the prosecution.
According to Dutch legal experts Crombag, Wagenaar, van Koppen, the Dutch system contravenes the provisions of the European Human Rights convention, in the imbalance between the power of the prosecution service and the defence.
On 9 April 2013 the Dutch senate voted 36 "yes" versus 35 "no" in favor of a new law that allows the prosecutor to re-try a person who was found not guilty in court. This new law is limited to crimes where someone died and new evidence must have been gathered. The new law also works retroactively.
Pakistan.
Article 13 of the Constitution of Pakistan protects a person from being punished or prosecuted more than once for the same offence.
Serbia.
This principle is incorporated in to the Constitution of the Republic of Serbia and further elaborated in its Criminal Procedure Act.
South Africa.
The Bill of Rights in the Constitution of South Africa forbids a retrial when there has already been an acquittal or a conviction.
South Korea.
Article 13 of the South Korean constitution provides that no citizen shall be placed in double jeopardy.
United Kingdom.
England and Wales.
Double jeopardy has been permitted in England and Wales in certain (exceptional) circumstances since the Criminal Justice Act 2003.
Pre-2003.
The doctrines of "autrefois acquit" and "autrefois convict" persisted as part of the common law from the time of the Norman conquest of England; they were regarded as essential elements of protection of the liberty of the subject and respect for due process of law in that there should be finality of proceedings. There were only three exceptions, all relatively recent, to the rules:
In Connelly v DPP (96 AC 1254), the Law Lords ruled that a defendant could not be tried for any offence arising out of substantially the same set of facts relied upon in a previous charge of which he had been acquitted, unless there are "special circumstances" proven by the prosecution. There is little case law on the meaning of "special circumstances", but it has been suggested that the emergence of new evidence would suffice.
A defendant who had been convicted of an offence could be given a second trial for an aggravated form of that offence if the facts constituting the aggravation were discovered after the first conviction. By contrast, a person who had been acquitted of a lesser offence could not be tried for an aggravated form even if new evidence became available.
Post-2003.
Following the murder of Stephen Lawrence, the Macpherson Report recommended that the double jeopardy rule should be abrogated in murder cases, and that it should be possible to subject an acquitted murder suspect to a second trial if "fresh and viable" new evidence later came to light. The Law Commission later added its support to this in its report "Double Jeopardy and Prosecution Appeals" (2001). A parallel report into the criminal justice system by Lord Justice Auld, a past Senior Presiding Judge for England and Wales, had also commenced in 1999 and was published as the Auld Report six months after the Law Commission report. It opined that the Law Commission had been unduly cautious by limiting the scope to murder and that "the exceptions should . extend to other grave offences punishable with life and/or long terms of imprisonment as Parliament might specify."
Both Jack Straw (then Home Secretary) and William Hague (then Leader of the Opposition) favoured this measure. These recommendations were implemented—not uncontroversially at the time—within the Criminal Justice Act 2003, and this provision came into force in April 2005. It opened certain serious crimes (including murder, manslaughter, kidnapping, rape, armed robbery, and serious drug crimes) to a retrial, regardless of when committed, with two conditions: the retrial must be approved by the Director of Public Prosecutions, and the Court of Appeal must agree to quash the original acquittal due to "new and compelling evidence". Pressure by Ann Ming, the mother of 1989 murder victim Julie Hogg—whose killer, William Dunlop, was initially acquitted in 1991 and subsequently confessed—also contributed to the demand for legal change.
On 11 September 2006, Dunlop became the first person to be convicted of murder following a prior acquittal for the same crime, in his case his 1991 acquittal of Julie Hogg's murder. Some years later he had confessed to the crime, and was convicted of perjury, but was unable to be retried for the killing itself. The case was re-investigated in early 2005, when the new law came into effect, and his case was referred to the Court of Appeal in November 2005 for permission for a new trial, which was granted. Dunlop pleaded guilty to murdering Julie Hogg and was sentenced to life imprisonment, with a recommendation he serve no less than 17 years.
On 13 December 2010, Mark Weston became the first person to be retried and found guilty of murder by a jury (Dunlop having confessed). In 1996 Weston had been acquitted of the murder of Vikki Thompson at Ascott-under-Wychwood on 12 August 1995, but following the discovery in 2009 of compelling new evidence (Thompson's blood on Weston's boots) he was arrested and tried for a second time. He was sentenced to life imprisonment, to serve a minimum of 13 years.
Scotland.
The double jeopardy rule no longer applies absolutely in Scotland since the Double Jeopardy (Scotland) Act 2011 came into force on 28 November 2011. The Act introduced three broad exceptions to the rule: where the acquittal had been tainted by an attempt to pervert the course of justice; where the accused admitted his guilt after acquittal; and where there was new evidence.
Northern Ireland.
In Northern Ireland the Criminal Justice Act 2003, effective 18 April 2005, makes certain "qualifying offence" (including murder, rape, kidnapping, specified sexual acts with young children, specified drug offences, defined acts of terrorism, as well as in certain cases attempts or conspiracies to commit the foregoing) subject to retrial after acquittal (including acquittals obtained before passage of the Act) if there is a finding by the Court of Appeal that there is "new and compelling evidence."
United States.
The ancient protection of the Common Law against double jeopardy is maintained in its full rigor in the United States, beyond reach of any change save that of a Constitutional Amendment. The Fifth Amendment to the United States Constitution provides:
Conversely, double jeopardy comes with a key exception. Dual sovereignty doctrine, altered sovereign situations can indict a defendant various times for the same crime. Dual Sovereignty doctrine allows federal and state government can have overlapping criminal laws, so criminal offender may be convicted in individual states and federal courts for exactly the same crime. The dual sovereignty doctrine has been the subject of substantial scholarly criticism. However, Double Jeopardy Clause does not apply in a multi-sovereign situation.
The Double Jeopardy Clause encompasses four distinct prohibitions: subsequent prosecution after acquittal, subsequent prosecution after conviction, subsequent prosecution after certain mistrials, and multiple punishment in the same indictment. Jeopardy "attaches" when the jury is empanelled, the first witness is sworn, or a plea is accepted.
The government is not permitted to appeal or retry the defendant once jeopardy attaches to a trial unless the case does not conclude. Conditions which constitute "conclusion" of a case include
In these cases the trial is concluded and the prosecution is precluded from appealing or retrying the defendant over the offense to which they were acquitted.
This principle does not prevent the government from appealing a pre-trial motion to dismiss or other non-merits dismissal, or a directed verdict after a jury conviction, nor does it prevent the trial judge from entertaining a motion for reconsideration of a directed verdict, if the jurisdiction has so provided by rule or statute. Nor does it prevent the government from retrying the defendant after an appellate reversal other than for sufficiency, including "habeas corpus", or "thirteenth juror" appellate reversals notwithstanding sufficiency on the principle that jeopardy has not "terminated." There may also be an exception for judicial bribery, but not jury bribery.
The "dual sovereignty" doctrine allows a federal prosecution of an offense to proceed regardless of a previous state prosecution for that same offense and vice versa because "an act denounced as a crime by both national and state sovereignties is an offense against the peace and dignity of both and may be punished by each." The doctrine is solidly entrenched in the law, but there has been a traditional reluctance in the federal executive branch to gratuitously wield the power it grants, due to public opinion being generally hostile to such action.
Another exception is that the perpetrator can be retried by court martial in a military court, if they have been previously acquitted by a civilian court, and are members of the military.
In "Blockburger v. United States" (1932), the Supreme Court announced the following test: the government may separately try and punish the defendant for two crimes if each crime contains an element that the other does not. "Blockburger" is the default rule, unless the governing statute legislatively intends to depart; for example, Continuing Criminal Enterprise (CCE) may be punished separately from its predicates, as can conspiracy.
The "Blockburger" test, originally developed in the multiple punishments context, is also the test for prosecution after conviction. In "Grady v. Corbin" (1990), the Court held that a double jeopardy violation could lie even where the "Blockburger" test was not satisfied, but "Grady" was overruled in "United States v. Dixon" (1993).
The rule for mistrials depends upon who sought the mistrial. If the defendant moves for a mistrial, there is no bar to retrial, unless the prosecutor acted in "bad faith," i.e. goaded the defendant into moving for a mistrial because the government specifically wanted a mistrial. If the prosecutor moves for a mistrial, there is no bar to retrial if the trial judge finds "manifest necessity" for granting the mistrial. The same standard governs mistrials granted sua sponte.
Retrials are not common, due to the legal expenses to the government. However, in the mid-1980s Georgia antiques dealer James Arthur Williams was tried a record "four" times for the murder of Danny Hansford and (after three mistrials) was finally acquitted on the grounds of self-defense. The case is recounted in the book "Midnight in the Garden of Good and Evil" which was adapted into a film directed by Clint Eastwood (the movie omits the first three murder trials).
External links.
United Kingdom.
Research and Notes produced for the UK Parliament, summarising the history of legal change, views and responses, and analyses:

</doc>
<doc id="7942" url="https://en.wikipedia.org/wiki?curid=7942" title="Disbarment">
Disbarment

Disbarment is the removal of a lawyer from a bar association or the practice of law, thus revoking his or her law license or admission to practice law. Disbarment is usually a punishment for unethical or criminal conduct. Procedures vary depending on the law society.
In the U.S..
Overview.
Generally disbarment is imposed as a sanction for conduct indicating that an attorney is not fit to practice law, willfully disregarding the interests of a client, or engaging in fraud which impedes the administration of justice. In addition, any lawyer who is convicted of a felony is automatically disbarred in most jurisdictions, a policy that, although opposed by the American Bar Association, has been described as a convicted felon's just deserts.
In the United States legal system, disbarment is specific to regions; one can be disbarred from some courts, while still being a member of the bar in another jurisdiction. However, under the American Bar Association's Model Rules of Professional Conduct, which have been adopted in most states, disbarment in one state or court is grounds for disbarment in a jurisdiction which has adopted the Model Rules.
Disbarment is quite rare. Instead, lawyers are usually sanctioned by their own clients through civil malpractice proceedings, or via fine, censure, suspension, or other punishments from the disciplinary boards. To be disbarred is considered a great embarrassment and shame, even if one no longer wishes to pursue a career in the law.
Because disbarment rules vary by area, different rules can apply depending on where a lawyer is disbarred. Notably, the majority of US states have no procedure for permanently disbarring a person. Depending on the jurisdiction, a lawyer may reapply to the bar immediately, after five to seven years, or be banned for life.
Notable U.S. disbarments.
The 20th and the 21st centuries have seen one former U.S. president and one former U.S. vice president disbarred, and another president suspended from one bar and caused to resign from another bar rather than face disbarment.
Former Vice President Spiro Agnew, having pleaded no contest (which subjects a person to the same criminal penalties as a guilty plea, but is not an admission of guilt for a civil suit) to charges of bribery and tax evasion, was disbarred from Maryland, the state of which he had previously been governor.
Former President Richard Nixon was disbarred from New York in 1976 for obstruction of justice related to the Watergate scandal.
In October 2001 the United States Supreme Court barred President Bill Clinton from practicing before it. He then resigned from the Supreme Court bar in November.
Alger Hiss was disbarred for a felony conviction, but later became the first person reinstated to the bar in Massachusetts after disbarment.
In 2007, Mike Nifong, the District Attorney of Durham County, North Carolina who presided over the 2006 Duke University lacrosse case, was disbarred for prosecutorial misconduct related to his handling of the case.
In April 2012, a three member panel appointed by the Arizona Supreme Court voted unanimously to disbar Andrew Thomas, former County Attorney of Maricopa County, Arizona, and a former close confederate of Maricopa County Sheriff Joe Arpaio. According to the panel, Thomas "outrageously exploited power, flagrantly fostered fear, and disgracefully misused the law" while serving as Maricopa County Attorney. The panel found "clear and convincing evidence" that Thomas brought unfounded and malicious criminal and civil charges against political opponents, including four state judges and the state attorney general. "Were this a criminal case," the panel concluded, "we are confident that the evidence would establish this conspiracy beyond a reasonable doubt."
Jack Thompson, the Florida lawyer noted for his activism against video games and rap music, was permanently disbarred for various charges of misconduct. The action was the result of several grievances claiming that Thompson had made defamatory, false statements and attempted to humiliate, embarrass, harass or intimidate his opponents. The order was made on September 25, 2008, effective October 25. However, Thompson attempted to appeal to the higher courts in order to avoid the penalty actually taking effect. Neither the US District court, nor the US Supreme Court would hear his appeal, rendering the judgement of the Florida Supreme Court final.
Ed Fagan, a New York lawyer who prominently represented Holocaust victims against Swiss banks, was disbarred in New York (in 2008) and New Jersey (in 2009) for failing to pay court fines and fees; and for misappropriating client and escrow trust funds.
F. Lee Bailey, noted criminal defense attorney, was disbarred by the state of Florida in 2001, with reciprocal disbarment in Massachusetts in 2002. The Florida disbarment was the result of his handling of stock in the DuBoc marijuana case. Bailey was found guilty of 7 counts of attorney misconduct by the Florida Supreme Court. Bailey had transferred a large portion of DuBoc's assets into his own accounts, using the interest gained on those assets to pay for personal expenses. In March 2005, Bailey filed to regain his law license in Massachusetts. The book "Florida Pulp Nonfiction" details the peculiar facts of the DuBoc case along with extended interviews with Bailey that include his own defense.

</doc>
<doc id="7946" url="https://en.wikipedia.org/wiki?curid=7946" title="Dog tag">
Dog tag

"Dog tags" is an informal term for the identification tags worn by military personnel, because of their resemblance to animal registration tags. The tags are primarily used for the identification of dead and wounded soldiers; they have personal info about the soldiers and convey essential basic medical information, such as blood type and history of inoculations. The tags often indicate religious preference as well. Dog tags are usually fabricated from a corrosion-resistant metal. They commonly contain two copies of the information, either in the form of a single tag that can be broken in half or two identical tags on the same chain. This duplication allows one tag (or half-tag) to be collected from a soldier's body for notification and the second to remain with the corpse when battle conditions prevent it from being immediately recovered.
History.
Dog tags were provided to Chinese soldiers as early as the mid-19th century. During the Taiping revolt (1851–66), both the Imperialists (i.e., the Chinese Imperial Army regular servicemen) and those Taiping rebels wearing a uniform wore a wooden dog tag at the belt, bearing the soldier's name, age, birthplace, unit, and date of enlistment.
During the American Civil War from 1861–1865, some soldiers pinned paper notes with their name and home address to the backs of their coats. Other soldiers stenciled identification on their knapsacks or scratched it in the soft lead backing of their army belt buckle.
Manufacturers of identification badges recognized a market and began advertising in periodicals. Their pins were usually shaped to suggest a branch of service, and engraved with the soldier's name and unit. Machine-stamped tags were also made of brass or lead with a hole and usually had (on one side) an eagle or shield, and such phrases as "War for the Union" or "Liberty, Union, and Equality". The other side had the soldier's name and unit, and sometimes a list of battles in which he had participated.
Franco-Prussian War.
The Prussian Army issued identification tags for its troops at the beginning of the Franco-Prussian War in 1870. They were nicknamed "Hundemarken" (the German equivalent of "dog tags") and compared to a similar identification system instituted for dogs in the Prussian capital city of Berlin at about the same time.
First World War.
The British Army introduced identity discs in place of identity cards in 1907, in the form of aluminium discs, typically made at Regimental depots using machines similar to those common at fun fairs, the details being pressed into the thin metal one letter at a time. British Regimental Identity Disk circa 1907
By 1915 the British Army required all soldiers to wear two official tags, both made of compressed fibre (which were more comfortable to wear in hot climates) carrying identical details, again impressed one character at a time.
The tags, one an octagonal green tag punched with two holes for cords, the other a red circular tag punched once, were suspended from the serviceman's neck using cord resembling butcher's twine, being passed through one hole in the octagonal green tag, a much shorter cord used to link the two tags, which could be easily separated by cutting.
British Identity Discs 1914-18
On discovering a body with both tags the second could be taken to record the death, others subsequently finding the same body knowing the death either had been (or would soon be) reported, the details on the green tag remaining with the body for the benefit of burial parties who whenever possible would prepare markers.
British and Empire/Commonwealth forces (Australia, Canada, and New Zealand) were issued identification discs of essentially identical basic pattern during the Great War, Second World War and Korea, though official identity discs were frequently supplemented by private-purchase items such as identity bracelets, particularly favoured by sailors who rightly believed the official discs were unlikely to survive long immersion in water.
The U.S. Army first authorized identification tags in War Department General Order No. 204, dated December 20, 1906, which essentially prescribes the Kennedy identification tag:
An aluminum identification tag, the size of a silver half dollar and of suitable thickness, stamped with the name, rank, company, regiment, or corps of the wearer, will be worn by each officer and enlisted man of the Army whenever the field kit is worn, the tag to be suspended from the neck, underneath the clothing, by a cord or thong passed through a small hole in the tab. It is prescribed as a part of the uniform and when not worn as directed herein will be habitually kept in the possession of the owner. The tag will be issued by the Quartermaster's Department gratuitously to enlisted men and at cost price to officers.
The army changed regulations on July 6, 1916, so that all soldiers were issued two tags: one to stay with the body and the other to go to the person in charge of the burial for record-keeping purposes. In 1918, the army adopted and allotted the serial number system, and name and serial numbers were ordered stamped on the identification tags. (Serial number 1 was assigned to enlisted man Arthur B. Crane of Chicago in the course of his fifth enlistment period.)
World War II "Notched" tags.
There is a recurring myth about the notch situated in one end of the dog tags issued to United States Army personnel during World War II, and up until the Vietnam War era. It was rumored that the notch's purpose was that, if a soldier found one of his comrades on the battlefield, he could take one tag to the commanding officer and stick the other between the teeth of the soldier to ensure that the tag would remain with the body and be identified.
In reality, the notch was used with the Model 70 Addressograph Hand Identification Imprinting Machine (a pistol-type imprinter used primarily by the Medical Department during World War II). American dogtags of the 1930s through 1970s were produced using the Graphotype system, in which characters are debossed into metal plates. Some tags are still debossed, using earlier equipment, and some are embossed (with raised letters) on computer-controlled equipment.
In the Graphotype process, commonly used commercially from the early 1900s through the 1980s, a debossing machine was used to stamp characters into metal plates; the plates could then be used to repetitively stamp such things as addresses onto paper in the same way that a typewriter functions, except that a single stroke of the printer could produce a block of text rather than requiring each character to be printed individually. The debossing process creates durable, easily legible metal plates, well-suited for military identification tags, leading to adoption of the system by the American military. It was also realized that debossed tags can function the same way the original Graphotype plates do.
The Model 70 took advantage of this fact, and was intended to rapidly print all of the information from a soldier's dogtag directly onto medical and personnel forms, with a single squeeze of the trigger. However, this requires that the tag being inserted with the proper orientation (stamped characters facing down), and it was believed that battlefield stress could lead to errors. To force proper orientation of the tags, the tags are produced with a notch, and there is a locator tab inside the Model 70 which prevents the printer from operating if the tag is inserted with the notch in the wrong place (as it is if the tag is upside down).
This feature was not as useful in the field as had been hoped, however, due to adverse conditions such as weather, dirt and dust, water, etc. In addition, the Model 70 resembled a pistol, thus attracting the attention of snipers (who might assume that a man carrying a pistol was an officer). As a result, use of the Model 70 hand imprinter by field medics was rapidly abandoned (as were most of the Model 70s themselves), and eventually the specification that tags include the locator notch was removed from production orders. Existing stocks of tags were used until depleted, and in the 1960s it was not uncommon for a soldier to be issued one tag with the notch and one tag without. Notched tags are still in production, to satisfy the needs of hobbyists, film production, etc., while the Model 70 imprinter has become a rare collector's item.
It appears instructions that would confirm the notch's mythical use were issued at least unofficially by the Graves Registration Service during the Vietnam War to Army troops headed overseas.
Dog tags are traditionally part of the makeshift battlefield memorials soldiers created for their fallen comrades. The casualty's rifle with bayonet affixed is stood vertically atop the empty boots, with the helmet over the rifle's stock. The dog tags hang from the rifle's handle or trigger guard.
Non-military usage.
Medical condition identification.
Some tags (along with similar items such as MedicAlert bracelets) are used also by civilians to identify their wearers and specify them as having health problems that may <br>
"(a)" suddenly incapacitate their wearers and render them incapable of providing treatment guidance (as in the cases of heart problems, epilepsy, diabetic coma, accident or major trauma) and/or <br> 
"(b)" interact adversely with medical treatments, especially standard or "first-line" ones (as in the case of an allergy to common medications) and/or <br>
"(c)" provide in case of emergency ("ICE") contact information and/or <br> 
"(d)" state a religious, moral, or other objection to artificial resuscitation, if a first responder attempts to administer such treatment when the wearer is non-responsive and thus unable to warn against doing so.
Military personnel in some jurisdictions may wear a supplementary medical information tag.
Fashion.
Dog tags have recently found their way into youth fashion by way of military chic. Originally worn as a part of a military uniform by youth wishing to present a tough or militaristic image, dog tags have since seeped out into wider fashion circles. They may be inscribed with a person's details, their beliefs or tastes, a favorite quote, or may bear the name or logo of a band or performer.
Since the late 1990s, custom dog tags have been fashionable amongst musicians (particularly rappers), and as a marketing give-away item. Numerous companies offer customers the opportunity to create their own personalized dog tags with their own photos, logos, and text. Even high-end jewelers have featured gold and silver dog tags encrusted with diamonds and other jewels.
Variations by country.
Austria.
The Austrian Bundesheer utilized a single long, rectangular tag, with oval ends, stamped with blood group & Rh factor at the end, with ID number underneath. Two slots and a hole stamped beneath the nunicew the tag to be broken in half, and the long bottom portion has both the ID number and a series of holes which allows the tag to be inserted into a dosimeter. This has been replaced with a more conventional, wider and rounded rectangle which can still be halved, but lacks the dosimeter reading holes.
Australia.
The Australian Defence Force issues soldiers two tags of different shapes: Number 1 Tag (the octagonal shaped disc) and Number 2 Tag (the circular disc). The format is as follows:
Example:
The information is printed exactly the same on both discs. In the event of a casualty, the circular tag is removed from the body.
Belgium.
Belgian Forces identity tags are, like the Canadian and Norwegian, designed to be broken in two in case of fatality; the lower half is returned to Belgian Defence tail, while the upper half remains on the body. The tags contain the following information:
Canada.
Canadian Forces identity discs (abbreviated "I discs") are designed to be broken in two in the case of fatality; the lower half is returned to National Defence Headquarters with the member's personal documents, while the upper half remains on the body. The tags contain the following information:
Before the Service Number was introduced in the 1990s, military personnel were identified on the I discs (as well as other documents) by their Social Insurance Number.
China.
The People's Liberation Army issues two long, rectangular tags. All information is stamped in Simplified Chinese:
Colombia.
The Ejército Nacional de Colombia uses long, rectangular metal tags with oval ends tags stamped with the following information:
Duplicate tags are issued. Often, tags are issued with a prayer inscribed on the reverse.
Cyprus.
In Cyprus, identification tags include the following information:
Denmark.
The military of Denmark use dog tags made from small, rectangular metal plates. The tag is designed to be broken into two pieces each with the following information stamped onto it:
Additionally, the right hand side of each half-tag is engraved 'DANMARK', .
Starting in 1985, the individual's service number (which is the same as the social security number) is included on the tag. In case the individual dies, the lower half-tag is supposed to be collected, while the other will remain with the corpse. In the army, navy, and air force but not in the national guard, the individual's blood type is indicated on the lower half-tag only, since this information becomes irrelevant if the individual dies. In 2009, Danish dog tag were discontinued for conscripts.
East Germany.
The Nationale Volksarmee used a tag nearly identical with that used by both the Wehrmacht and the West German Bundeswehr. The oval aluminum tag was stamped "DDR" (Deutsche Demokratische Republik) above the personal ID number; this information was repeated on the bottom half, which was intended to be broken off in case of death. Oddly, the tag was not worn, but required to be kept in a plastic sleeve in the back of the WDA identity booklet.
Ecuador.
The "Placas de identificación de campaña" consists of two long, rectangular steel or aluminum tags with rounded corners and a single hole punched in one end. It is suspended by a US-type ball chain, with a shorter chain for the second tag. The information on the tag is:
Estonia.
Estonian dog tags are designed to be broken in two. The dog tag is a metallic rounded rectangle suspended by a ball chain. Information consists of four fields: 
Example: 
Finland.
In the Finnish Defence Forces, dog tags (sometimes called "raatolaatta"; "corpse plaque"/"corpse plate"-) are also designed to be broken in two; however, the only text on it is the personal identification number and the letters "SF" (rarely FI), which stands for Suomi Finland, within a tower stamped atop of the upper half.
France.
France issues either a metallic rounded rectangle (army) or disk (navy), designed to be broken in half, bearing family name & first name above the ID number.
Germany.
German Bundeswehr ID tags are an oval-shaped disc designed to be broken in half. The two sides contain different information which are mirrored upside-down on the lower half of the ID tag. They feature the following information on segmented and numbered fields:
On the front
On the back
Greece.
In Greece, identification tags include the following information:
Hungary.
The Hungarian army dog tag is made out of steel, forming a 25×35 mm tag designed to split diagonally. Both sides contain the same information: the soldier's personal identity code, blood group and the word HUNGARIA. Some may not have the blood group on them. These are only issued to soldiers who are serving outside of the country. If the soldier should die, one side is removed and kept for the army's official records, while the other side is left attached to the body.
Iraq.
The Saddam-era Iraqi Army utilized a single, long, rectangular metal tag with oval ends, inscribed (usually by hand) with Name and Number or Unit, and occasionally Blood Type.
Israel.
Dog tags of the Israel Defense Forces are designed to be broken in two. The information appears in three lines (twice):
Another dog tag is kept inside the military boot in order to identify dead soldiers.
Originally the IDF issued two circular aluminum tags (1948 – late 1950s) stamped in three lines with serial number, family name, and first name. The tags were threaded together through a single hole onto a cord worn around the neck.
Japan.
Japan follows a similar system to the US Army for its Self Defence Force personnel, and the appearance of the tags is similar, although laser etched. The exact information order is as follows.
Malaysia.
Malaysian Armed Forces have two identical oval tags with this information:
If more information needed, another two oval wrist tags are provided. The term "wrist tags" can be used to refer to the bracelet-like wristwatch. The additional tags only need to be worn on the wrist, with the main tags still on the neck. All personnel are allowed to attach a small religious pendant or locket; this makes a quick identifiable reference for their funeral services.
Mexico.
The Ejército de Mexico uses a single long, rectangular metal tag with oval ends, embossed with Name, serial number, and blood type plus Rh factor.
Netherlands.
Military of the Netherlands identity tags, like the Canadian and Norwegian ones, are designed to be broken in two in case of a fatality; the lower end is returned to Dutch Defence Headquarters, while the upper half remains on the body. There is a difference in the Army and Airforce service number and the Navy service number:
The tags contain the following information:
Norway.
Norwegian dog tags are designed to be broken in two like the Canadian version:
Poland.
The first dog tags were issued in Poland following the order of the General Staff of December 12, 1920. The earliest design (dubbed "kapala" in Polish, more properly called "kapsel legitymacyjny" - meaning "identification cap") consisted of a tin-made 30×50 mm rectangular frame and a rectangular cap fitting into the frame. Soldiers' details were filled in a small ID card placed inside the frame, as well as on the inside of the frame itself. The dog tag was similar to the tags used by the Austro-Hungarian Army during World War I. In case the soldier died, the frame was left with his body, while the lid was returned to his unit together with a note on his death. The ID card was handed over to the chaplain or the rabbi.
In 1928, a new type of dog tag was proposed by gen. bryg. Stanisław Rouppert, Poland's representative at the International Red Cross. It was slightly modified and adopted in 1931 under the name of Nieśmiertelnik wz. 1931 (literally, Immortalizer mark 1931). The new design consisted of an oval piece of metal (ideally steel, but in most cases aluminum alloy was used), roughly 40 by 50 millimeters. There were two notches on both sides of the tag, as well as two rectangular holes in the middle to allow for easier breaking of the tag in two halves. The halves contained the same set of data and were identical, except the upper half had two holes for a string or twine to go through. The data stamped on the dog tag included:
Sometimes the rank of the soldier was added to the reverse, and most members of the medical corps had a tiny cross stamped near the string holes, regardless of their religion.
Rhodesia.
The former Republic of Rhodesia used two WW2 British-style compressed asbestos fiber tags, a No. 1 octagonal (green) tag and a No. 2 circular (red) tag, stamped with identical information. The red tag was supposedly fireproof and the green tag rotproof. The following information was stamped on the tags: Number, Name, Initials, & Religion; Blood Type was stamped on reverse. The air force and BSAP often stamped their service on the reverse side above the blood group.
Many soldiers state they were issued blank tags and told to punch the information in themselves. 
Russian Federation.
The Russian Armed Forces use oval metal tags, similar to the dog tags of the Soviet Army. Each tag contains the title " () and the individual's alphanumeric number, as shown on the photo.
Singapore.
The Singapore Armed Forces-issued dog tags are inscribed (not embossed) with up to four items:
The dog tags consist of two metal pieces, one oval with two holes and one round with one hole. A synthetic lanyard is threaded through both holes in the oval piece and tied around the wearer's neck. The round piece is tied to the main loop on a shorter loop.
South Africa.
The former South African Defense Force used two long, rectangular aluminum tags with oval ends, stamped with serial number, name and initials, religion, and blood type.
South Korea.
The South Korean Army issues two long, rectangular tags with oval ends, stamped (in Korean lettering) with "Yuk-Gun" (English: Army) above a personal number, with the name below that and the blood group at the bottom.
South Vietnam.
The South Vietnamese Army used two American-style dog tags. Some tags added religion, e.g., Công Giáo for Catholic. They were stamped or inscribed with: 
Soviet Union.
During World War II, the Red Army did not issue metal dog tags to its troops. They were issued small ebony cylinders containing a slip of paper with a soldier's s particulars written on it. These do not hold up as well as metal dog tags.
After World War II, the Soviet Army used oval metal tags, similar to today's dog tags of the Russian Armed forces. Each tag contains the title " () and the individual's alphanumeric number.
Spain.
Issues a single metal oval, worn vertically, stamped "" above and below the 3-slot horizontal break line. It is stamped in 4 lines with:
Sweden.
Swedish identification tags are designed to be able to break apart. The information on them was prior to 2010:
Swedish dog tags issued to troops after 2010 are, for personal security reasons, only marked with personal identity number.
During the Cold War dog tags were issued to everyone, often soon after birth, since the threat of total war also meant the risk of severe civilian casualties. However in 2010, the Swedish government decided that the dog tags were not needed anymore.
Switzerland.
Swiss Armed Forces ID tag is an oval shaped non reflective plaque, containing the following information:
On the back side the letters CH standing for (Confoederatio Helvetica) are engraved next to a Swiss cross.
United States.
In the 1990s, the U.S. Army stopped using the term "dog tags", replacing it with the designation "ID tags".
A persistent rumor is that debossed (imprinted with stamped in letters) dog tags were issued from World War II till the end of the Vietnam War and that currently the U.S. Armed Forces is issuing embossed (imprinted with raised letters) dog tags. In actuality, the U.S. Armed Forces issues dog tags with both types of imprinting, depending on the machine used at a given facility. The military issued 95% of their identification tags up until recently (within the past 10 years) with debossed text.
The U.S. Armed Forces typically carry two identical oval dog tags containing:
Religious designation.
During World War II, an American dog tag could indicate only one of three religions through the inclusion of one letter: "P" for Protestant, "C" for Catholic, or "H" for Jewish (from the word, "Hebrew"), or (according to at least one source) "NO" to indicate no religious preference. Army regulations (606-5) soon included X and Y in addition to P, C, and H: the X indicating any religion not included in the first three, and the Y indicating either no religion or a choice not to list religion.
By the time of the Vietnam War, some IDs spelled out the broad religious choices such as PROTESTANT and CATHOLIC, rather than using initials, and also began to show individual denominations such as "METHODIST" or "BAPTIST." Tags did vary by service, however, such as the use of "CATH," not "CATHOLIC" on some Navy tags. For those with no religious affiliation and those who chose not to list an affiliation, either the space for religion was left blank or the words "NO PREFERENCE" or "NO RELIGIOUS PREF" were included.
Although American dog tags include the recipient's religion as a way of ensuring that religious needs will be met, some personnel have them reissued without religious affiliation listed—or keep two sets, one with the designation and one without—out of fear that identification as a member of a particular religion could increase the danger to their welfare or their lives if they fell into enemy hands. Some Jewish personnel avoided flying over German lines during WWII with ID tags that indicated their religion, and some Jewish personnel avoid the religious designation today out of concern that they could be captured by extremists who are anti-Semitic. Additionally, when American troops were first sent to Saudi Arabia during the Gulf War there were allegations that some U.S. military authorities were pressuring Jewish military personnel to avoid listing their religions on their ID tags.

</doc>
<doc id="7950" url="https://en.wikipedia.org/wiki?curid=7950" title="Drum">
Drum

The drum is a member of the percussion group of musical instruments. In the Hornbostel-Sachs classification system, it is a membranophone. Drums consist of at least one membrane, called a drumhead or drum skin, that is stretched over a shell and struck, either directly with the player's hands, or with a drum stick, to produce sound. There is usually a "resonance head" on the underside of the drum, typically tuned to a slightly lower pitch than the top drumhead. Other techniques have been used to cause drums to make sound, such as the thumb roll. Drums are the world's oldest and most ubiquitous musical instruments, and the basic design has remained virtually unchanged for thousands of years.
Drums may be played individually, with the player using a single drum, and some drums such as the djembe are almost always played in this way. Others are normally played in a set of two or more, all played by the one player, such as bongo drums and timpani. A number of different drums together with cymbals form the basic modern drum kit.
Uses.
Drums are usually played by striking with the hand, or with one or two sticks. In many traditional cultures, drums have a symbolic function and are used in religious ceremonies. Drums are often used in music therapy, especially hand drums, because of their tactile nature and easy use by a wide variety of people.
In popular music and jazz, "drums" usually refers to a drum kit or a set of drums (with some cymbals), and "drummer" to the person who plays them.
Drums acquired even divine status in places such as Burundi, where the "karyenda" was a symbol of the power of the king.
Construction.
The shell almost invariably has a circular opening over which the drumhead is stretched, but the shape of the remainder of the shell varies widely. In the western musical tradition, the most usual shape is a cylinder, although timpani, for example, use bowl-shaped shells. Other shapes include a frame design (tar, Bodhrán), truncated cones (bongo drums, Ashiko), goblet shaped (djembe), and joined truncated cones (talking drum).
Drums with cylindrical shells can be open at one end (as is the case with timbales), or can have two drum heads. Single-headed drums typically consist of a skin stretched over an enclosed space, or over one of the ends of a hollow vessel. Drums with two heads covering both ends of a cylindrical shell often have a small hole somewhat halfway between the two heads; the shell forms a resonating chamber for the resulting sound. Exceptions include the African slit drum, also known as a log drum as it is made from a hollowed-out tree trunk, and the Caribbean steel drum, made from a metal barrel. Drums with two heads can also have a set of wires, called snares, held across the bottom head, top head, or both heads, hence the name snare drum.
On modern band and orchestral drums, the drumhead is placed over the opening of the drum, which in turn is held onto the shell by a "counterhoop" (or "rim"), which is then held by means of a number of tuning screws called "tension rods" that screw into lugs placed evenly around the circumference. The head's tension can be adjusted by loosening or tightening the rods. Many such drums have six to ten tension rods. The sound of a drum depends on many variables—including shape, shell size and thickness, shell materials, counterhoop material, drumhead material, drumhead tension, drum position, location, and striking velocity and angle.
Prior to the invention of tension rods, drum skins were attached and tuned by rope systems—as on the Djembe—or pegs and ropes such as on Ewe Drums. These methods are rarely used today, though sometimes appear on regimental marching band snare drums. The head of a talking drum, for example, can be temporarily tightened by squeezing the ropes that connect the top and bottom heads. Similarly, the tabla is tuned by hammering a disc held in place around the drum by ropes stretching from the top to bottom head. Orchestral timpani can be quickly tuned to precise pitches by using a foot pedal.
Sound of a drum.
Several factors determine the sound a drum produces, including the type, shape and construction of the drum shell, the type of drum heads it has, and the tension of these drumheads. Different drum sounds have different uses in music. Take, for example, the modern Tom-tom drum. A jazz drummer may want drums that are high pitched, resonant and quiet whereas a rock drummer may prefer drums that are loud, dry and low-pitched. Since these drummers want different sounds, their drums are constructed a little differently.
The drum head has the most effect on how a drum sounds. Each type of drum head serves its own musical purpose and has its own unique sound. Double-ply drumheads dampen high frequency harmonics because they are heavier and they are suited to heavy playing. Drum heads with a white, textured coating on them muffle the overtones of the drum head slightly, producing a less diverse pitch. Drum heads with central silver or black dots tend to muffle the overtones even more. And drum heads with perimeter sound rings mostly eliminate overtones (Howie 2005). Some jazz drummers avoid using thick drum heads, preferring single ply drum heads or drum heads with no muffling. Rock drummers often prefer the thicker or coated drum heads.
The second biggest factor that affects drum sound is head tension against the shell. When the hoop is placed around the drum head and shell and tightened down with tension rods, the tension of the head can be adjusted. When the tension is increased, the amplitude of the sound is reduced and the frequency is increased, making the pitch higher and the volume lower.
The type of shell also affects the sound of a drum. Because the vibrations resonate in the shell of the drum, the shell can be used to increase the volume and to manipulate the type of sound produced. The larger the diameter of the shell, the lower the pitch. The larger the depth of the drum, the louder the volume. Shell thickness also determines the volume of drums. Thicker shells produce louder drums. Mahogany raises the frequency of low pitches and keeps higher frequencies at about the same speed. When choosing a set of shells, a jazz drummer may want smaller maple shells, while a rock drummer may want larger birch shells. For more information about tuning drums or the physics of a drum, visit the external links listed below.
History.
The drum is the oldest known instrument in the world dating back to 4000 BC in Egypt. Drums made with alligator skins have been found in Neolithic cultures located in China, dating to a period of 5500–2350 BC. In literary records, drums manifested shamanistic characteristics were often used in ritual ceremonies.
Bronze Dong Son drums are were fabricated by the Bronze Age Dong Son culture of northern Vietnam. They include the ornate Ngoc Lu drum.
Animal drumming.
Macaque monkeys drum objects in a rhythmic way to show social dominance and this has been shown to be processed in a similar way in their brains to vocalizations suggesting an evolutionary origin to drumming as part of social communication. Other primates make drumming sounds by chest beating or hand clapping, and rodents such as kangaroo rats also make similar sounds using their paws on the ground.
Talking drums.
Drums are used not only for their musical qualities, but also as a means of communication over great distances. The talking drums of Africa are used to imitate the tone patterns of spoken language. Throughout Sri Lankan history drums have been used for communication between the state and the community, and Sri Lankan drums have a history stretching back over 2500 years.
Drums in art.
Drumming may be a purposeful expression of emotion for entertainment, spiritualism and communication. Many cultures practice drumming as a spiritual or religious passage and interpret drummed rhythm similarly to spoken language or prayer. Drumming has developed over millennia to be a powerful art form. Drumming is commonly viewed as the root of music and is sometimes performed as a kinesthetic dance. As a discipline, drumming concentrates on training the body to punctuate, convey and interpret musical rhythmic intention to an audience and to the performer.
Military uses.
Chinese troops used tàigǔ drums to motivate troops, to help set a marching pace, and to call out orders or announcements. For example, during a war between Qi and Lu in 684 BC, the effect of drum on soldier's morale is employed to change the result of a major battle. Fife-and-drum corps of Swiss mercenary foot soldiers also used drums. They used an early version of the snare drum carried over the player's right shoulder, suspended by a strap (typically played with one hand using traditional grip). It is to this instrument that the English word "drum" was first used. Similarly, during the English Civil War rope-tension drums would be carried by junior officers as a means to relay commands from senior officers over the noise of battle. These were also hung over the shoulder of the drummer and typically played with two drum sticks. Different regiments and companies would have distinctive and unique drum beats only they recognized. In the mid-19th century, the Scottish military started incorporating pipe bands into their Highland Regiments.
During pre-Columbian warfare, Aztec nations were known to have used drums to send signals to the battling warriors. The Nahuatl word for drum is roughly translated as huehuetl.
The Rig Veda, one of the oldest religious scriptures in the world, contain several references to the use of Dundhubi (war drum). Arya tribes charged into battle to the beating of the war drum and chanting of a hymn that appears in Book VI of the Rig Veda and also the Atharva Veda where it is referred to as the "Hymn to the battle drum".

</doc>
<doc id="7951" url="https://en.wikipedia.org/wiki?curid=7951" title="Delphi">
Delphi

Delphi ( or ; , ) is both an archaeological site and a modern town in Greece on the south-western spur of Mount Parnassus in the valley of Phocis. In myths dating to the classical period of Ancient Greece (510-323 BC), the site of Delphi was believed to be determined by Zeus when he sought to find the centre of his "Grandmother Earth" (Ge, Gaea, or Gaia). He sent two eagles flying from the eastern and western extremities, and the path of the eagles crossed over Delphi where the omphalos, or navel of Gaia was found.
Earlier myths include traditions that Pythia, or the Delphic oracle, already was the site of an important oracle in the pre-classical Greek world (as early as 1400 BC) and, rededicated from about 800 BC, when it served as the major site during classical times for the worship of the god Apollo. Apollo was said to have slain Python, "a dragon" who lived there and protected the navel of the Earth. "Python" (derived from the verb πύθω ("pythō"), "to rot") is claimed by some to be the original name of the site in recognition of Python which Apollo defeated. The Homeric Hymn to Delphic Apollo recalled that the ancient name of this site had been "Krisa". Others relate that it was named Pytho and that Pythia, the priestess serving as the oracle, was chosen from their ranks by a group of priestesses who officiated at the temple.
Apollo's sacred precinct in Delphi was a panhellenic sanctuary, where every four years, starting in 586 BC athletes from all over the Greek world competed in the Pythian Games, one of the four panhellenic (or stephanitic) games, precursors of the Modern Olympics. The victors at Delphi were presented with a laurel crown ("stephanos") which was ceremonially cut from a tree by a boy who re-enacted the slaying of the Python. Delphi was set apart from the other games sites because it hosted the mousikos agon, musical competitions.
These Pythian Games rank second among the four stephanitic games chronologically and based on importance. These games, though, were different from the games at Olympia in that they were not of such vast importance to the city of Delphi as the games at Olympia were to the area surrounding Olympia. Delphi would have been a renowned city whether or not it hosted these games; it had other attractions that led to it being labeled the "omphalos" (navel) of the earth, in other words, the center of the world.
In the inner "hestia" ("hearth") of the Temple of Apollo, an eternal flame burned. After the battle of Plataea, the Greek cities extinguished their fires and brought new fire from the hearth of Greece, at Delphi; in the foundation stories of several Greek colonies, the founding colonists were first dedicated at Delphi.
Location.
The site of Delphi is located in upper central Greece, on multiple plateaux/terraces along the slope of Mount Parnassus, and includes the Sanctuary of Apollo, the site of the ancient Oracle. This semicircular spur is known as Phaedriades, and overlooks the Pleistos Valley.
Southwest of Delphi, about away, is the harbor-city of Kirrha on the Corinthian Gulf. Delphi was thought of by the Greeks as the middle of the entire earth.
The area of Delphi comprises a small downward plain crossed by the torrents Pleistos and Hylaithos, which at some points become very narrow and thus overflow by heavy rainfall. The area is planted with dense olive groves covering half its surface, whereas the other half is covered with low, bushy vegetation, suitable for grazing. The olive trees belong to the “Amfissis” species, created technically by the vaccination of oleasters. Most of the trees are over a hundred years old. The area was proclaimed a protected region in 1981. Furthermore, it was established as an archaeological zone A and B region in 1991 and 2012 respectively. Unfortunately, its declaration as protected zone did not avert the destruction of a large part of it by a fire in August 2013.
Dedication to Apollo.
The name "Delphoi" comes from the same root as δελφύς "delphys", "womb" and may indicate archaic veneration of Gaia at the site. Apollo is connected with the site by his epithet Δελφίνιος "Delphinios", "the Delphinian". The epithet is connected with dolphins (Greek δελφίς,-ῖνος) in the Homeric "Hymn to Apollo" (line 400), recounting the legend of how Apollo first came to Delphi in the shape of a dolphin, carrying Cretan priests on his back. The Homeric name of the oracle is "Pytho" ("Πυθώ").
Another legend held that Apollo walked to Delphi from the north and stopped at Tempe, a city in Thessaly, to pick laurel (also known as bay tree) which he considered to be a sacred plant. In commemoration of this legend, the winners at the Pythian Games received a wreath of laurel picked in the Temple.
Delphi became the site of a major temple to Phoebus Apollo, as well as the Pythian Games and the famous prehistoric oracle. Even in Roman times, hundreds of votive statues remained, described by Pliny the Younger and seen by Pausanias.
Carved into the temple were three phrases: ("gnōthi seautón" = "know thyself") and ("mēdén ágan" = "nothing in excess"), and ("eggýa pára d'atē" = "make a pledge and mischief is nigh"), In antiquity, the origin of these phrases was attributed to one or more of the Seven Sages of Greece by authors such as 
Plato and Pausanias.
Additionally, according to Plutarch's essay on the meaning of the "E at Delphi"—the only literary source for the inscription—there was also inscribed at the temple a large letter E. Among other things epsilon signifies the number 5.
However, ancient as well as modern scholars have doubted the legitimacy of such inscriptions. According to one pair of scholars, "The actual authorship of the three maxims set up on the Delphian temple may be left uncertain. Most likely they were popular proverbs, which tended later to be attributed to particular sages."
According to the Homeric-hymn to the Pythian Apollo, Apollo shot his first arrow as an infant which effectively slew the serpent Pytho, the son of Gaia, who guarded the spot. To atone the murder of Gaia's son, Apollo was forced to fly and spend eight years in menial service before he could return forgiven. A festival, the Septeria, was held every year, at which the whole story was represented: the slaying of the serpent, and the flight, atonement, and return of the god.
The Pythian Games took place every four years to commemorate Apollo's victory. Another regular Delphi festival was the "Theophania" (Θεοφάνεια), an annual festival in spring celebrating the return of Apollo from his winter quarters in Hyperborea. The culmination of the festival was a display of an image of the gods, usually hidden in the sanctuary, to worshippers.
The "Theoxenia" was held each summer, centred on a feast for "gods and ambassadors from other states". Myths indicate that Apollo killed the chthonic serpent Python, Pythia in older myths, but according to some later accounts his wife, Pythia, who lived beside the Castalian Spring. Some sources say it is because Python had attempted to rape Leto while she was pregnant with Apollo and Artemis.
This spring flowed toward the temple but disappeared beneath, creating a cleft which emitted chemical vapors that caused the Oracle at Delphi to reveal her prophecies. Apollo killed Python but had to be punished for it, since she was a child of Gaia. The shrine dedicated to Apollo was originally dedicated to Gaia and shared with Poseidon. The name Pythia remained as the title of the Delphic Oracle.
Erwin Rohde wrote that the Python was an earth spirit, who was conquered by Apollo, and buried under the Omphalos, and that it is a case of one deity setting up a temple on the grave of another. Another view holds that Apollo was a fairly recent addition to the Greek pantheon coming originally from Lydia . The Etruscans coming from northern Anatolia also worshipped Apollo, and it may be that he was originally identical with Mesopotamian Aplu, an Akkadian title meaning "son", originally given to the plague God Nergal, son of Enlil. Apollo Smintheus (Greek ), the mouse killer eliminates mice, a primary cause of disease, hence he promotes preventive medicine.
Oracle.
Delphi is perhaps best known for the oracle at the sanctuary that was dedicated to Apollo during the classical period. According to Aeschylus in the prologue of the "Eumenides", it had origins in prehistoric times and the worship of Gaea. In the last quarter of the 8th century BC there is a steady increase in artifacts found at the settlement site in Delphi, which was a new, post-Mycenaean settlement of the late 9th century. Pottery and bronze work as well as tripod dedications continue in a steady stream, in comparison to Olympia. Neither the range of objects nor the presence of prestigious dedications proves that Delphi was a focus of attention for a wide range of worshippers, but the large quantity of high value goods, found in no other mainland sanctuary, certainly encourages that view.
Apollo spoke through his oracle: the sibyl or priestess of the oracle at Delphi was known as the Pythia; she had to be an older woman of blameless life chosen from among the peasants of the area. She sat on a tripod seat over an opening in the earth (the "chasm"). When Apollo slew Python, its body fell into this fissure, according to legend, and fumes arose from its decomposing body. Intoxicated by the vapors, the sibyl would fall into a trance, allowing Apollo to possess her spirit. In this state she prophesied. It has been speculated that a gas high in ethylene, known to produce violent trances, came out of this opening, though this theory remains debatable.
One theory states that a goat herder fed his flocks on Parnassus. It happened one day the goats started playing with great agility upon nearing a chasm in the rock; the goat herd noticing this held his head over the chasm causing the fumes to go to his brain; throwing him into a strange trance.
While in a trance the Pythia "raved" – probably a form of ecstatic speech – and her ravings were "translated" by the priests of the temple into elegant hexameters.
A review of contemporary toxicological literature indicates that it is oleander that causes symptoms similar to those of the Pythia. Pythia used oleander as a complement during the oracular procedure, chewing its leaves and inhaling their smoke. The toxic substances of oleander resulted in symptoms similar to those of epilepsy, the “sacred disease,” which amounted to the possession of the Pythia by the spirit of Apollo, an event that made the Pythia his spokesperson, and subsequently, his prophetess. This explanation sheds light on the alleged spirit and chasm of Delphi, that have been the subject of intense debate and interdisciplinary research for the last hundred years.
People consulted the Delphic oracle on everything from important matters of public policy to personal affairs. The oracle could not be consulted during the winter months, for this was traditionally the time when Apollo would live among the Hyperboreans. Dionysus would inhabit the temple during his absence.
H.W. Parke writes that the foundation of Delphi and its oracle took place before recorded history and its origins are obscure, but dating to the worship of Gaia.
History.
The Delphic Oracle exerted considerable influence throughout the Greek world, and she was consulted before all major undertakings: wars, the founding of colonies, and so forth. She also was respected by the Greek-influenced countries around the periphery of the Greek world, such as Lydia, Caria, and even Egypt. The oracle was also known to the early Romans. Rome's seventh and last king, Lucius Tarquinius Superbus, after witnessing a snake near his palace, sent a delegation including two of his sons to consult the oracle.
The Oracle benefited from the patronage of the Macedonian kings. Later it was placed under the protection of the Aetolian League, which played the leading role in repelling the Gallic invasion of Brennus in 279 BC and saving the sanctuary. Delphi, under the domination of the Aetolians, enjoyed a period of peace until 168-167 BC, when the Romans were eventually able to permanently dominate the region. The Roman Republic protected the Oracle from a dangerous barbarian invasion in 109 BC and 105 BC. In 86 BC, during the Mithridatic Wars, the Roman general Sulla ordered the Amphictyons of Delphi to send him the precious metal offerings of the temple. They obeyed the order without objection, fearing the Roman power. Three years later, Medi, a Thracian tribe, raided Delphi, burned the temple, plundered the sanctuary and stole the “unquenchable fire” from the altar. During the raid, part of the temple roof collapsed. The same year, the Temple was severely damaged by an earthquake. Thus the Oracle fell in decay and the surrounding area became impoverished. The sparse local population led to difficulties in filling the posts required. The Oracle's credibility waned due to doubtful predictions.
When Nero came to Greece in AD 66, he took away over 500 of the best statues from Delphi to Rome. Subsequent Roman emperors of the Flavian dynasty contributed significantly towards its restoration. The oracle relatively flourished again during the rule of emperor Hadrian, who is believed to have visited the oracle twice. Also, Hadrian offered complete autonomy to the city. Plutarch was a significant factor too, by his presence as a chief priest. Barbarian raids commenced again during the reign of Marcus Aurelius.
By the 4th century, Delphi had acquired the status of a city, which was located to the west of the sanctuary grounds. Constantine the Great looted several monuments, most notably the Tripod of Plataea, which he used to decorate his new capital, Constantinople.
Despite the rise of Christianity across the Roman Empire, the oracle remained an active pagan centre throughout the 4th century, and the Pythian Games continued to be held at least until 424; however, the decline continued. The attempt of the emperor Julian to revive Hellenistic polytheism did not survive his reign. Excavations have revealed a large three-aisled basilica in the city, as well as traces of a church building in the sanctuary's gymnasium. The site was abandoned in the 6th or 7th centuries, although a single bishop of Delphi is attested in an episcopal list of the late 8th/early 9th centuries.
Time and natural disasters added to the picture of desolation of the once glorious place, and during the Ottoman period the small and insignificant village of Kastri was founded on the site.
The Delphic Sibyl.
The Delphic Sibyl was a legendary prophetic figure who was said to have given prophecies at Delphi shortly after the Trojan War. The prophecies attributed to her circulated in written collections of prophetic sayings, along with the oracles of figures such as Bakis. The Sibyl had no connection to the oracle of Apollo, and should not be confused with the Pythia.
Buildings and structures.
Occupation of the site at Delphi can be traced back to the Neolithic period with extensive occupation and use beginning in the Mycenaean period (1600–1100 BC). Most of the ruins that survive today date from the most intense period of activity at the site in the 6th century BC.
Temple of Apollo.
The ruins of the Temple of Delphi visible today date from the 4th century BC, and are of a peripteral Doric building. It was erected on the remains of an earlier temple, dated to the 6th century BC which itself was erected on the site of a 7th-century BC construction attributed to the architects Trophonios and Agamedes.
The 6th-century BC temple was named the "Temple of Alcmonidae" in tribute to the Athenian family who funded its reconstruction following a fire, which had destroyed the original structure. The new building was a Doric hexastyle temple of 6 by 15 columns. This temple was destroyed in 375 BC by an earthquake. The pediment sculptures are a tribute to Praxias and Androsthenes of Athens. Of a similar proportion to the second temple it retained the 6 by 15 column pattern around the stylobate. Inside was the adyton, the centre of the Delphic oracle and seat of Pythia. The temple had the statement "Know thyself", one of the Delphic maxims, carved into it (and some modern Greek writers say the rest were carved into it), and the maxims were attributed to Apollo and given through the oracle and/or the Seven Sages of Greece ("know thyself" perhaps also being attributed to other famous philosophers). The monument was partly restored during 1938(?)–1300.
The temple survived until AD 390, when the Roman emperor Theodosius I silenced the oracle by destroying the temple and most of the statues and works of art in the name of Christianity. The site was completely destroyed by zealous Christians in an attempt to remove all traces of Paganism.
Amphictyonic Council.
The Amphictyonic Council was a council of representatives from six Greek tribes that controlled Delphi and also the quadrennial Pythian Games. They met biannually and came from Thessaly and central Greece. Over time, the town of Delphi gained more control of itself and the council lost much of its influence.
Treasuries.
From the entrance of the site, continuing up the slope almost to the temple itself, are a large number of votive statues, and numerous treasuries. These were built by the various Greek city states — those overseas as well as those on the mainland — to commemorate victories and to thank the oracle for her advice, which was thought to have contributed to those victories. They are called "treasuries" because they held the offerings made to Apollo; these were frequently a "tithe" or tenth of the spoils of a battle. The most impressive is the now-restored Athenian Treasury, built to commemorate the Athenians' victory at the Battle of Marathon in 490 BC.
Several of the treasuries can be identified, among them the Siphnian Treasury, dedicated by the city of Siphnos whose citizens gave a tithe of the yield from their silver mines until the mines came to an abrupt end when the sea flooded the workings.
Other identifiable treasuries are those of the Sikyonians, the Boeotians, the Thebans, and the Athenians. One of the largest of the treasuries was that of Argos. Built in the late Doric period, the Argives took great pride in establishing their place amongst the other city states. Completed in 380, the treasury draws inspiration mostly from the Temple of Hera located in the Argolis, the acropolis of the city. However, recent analysis of the Archaic elements of the treasury suggest that its founding preceded this. Currently the rebuilt Treasury of the Athenians is the most impressive. Much of the architectural program is on display in the nearby museum.
Altar of the Chians.
Located in front of the Temple of Apollo, the main altar of the sanctuary was paid for and built by the people of Chios. It is dated to the 5th century BC by the inscription on its cornice. Made entirely of black marble, except for the base and cornice, the altar would have made a striking impression. It was restored in 1920.
Stoa of the Athenians.
The stoa leads off north-east from the main sanctuary. It was built in the Ionic order and consists of seven fluted columns, unusually carved from single pieces of stone (most columns were constructed from a series of discs joined together). The inscription on the stylobate indicates that it was built by the Athenians after their naval victory over the Persians in 478 BC, to house their war trophies. The stoa was attached to the existing Polygonal Wall.
Sibyl rock.
The Sibyl rock is a pulpit-like outcrop of rock between the Athenian Treasury and the Stoa of the Athenians upon the sacred way which leads up to the temple of Apollo in the archaeological area of Delphi. It is claimed to be where an ancient Sibyl pre-dating the Pythia of Apollo sat to deliver her prophecies.
Theatre.
The ancient theatre at Delphi was built further up the hill from the Temple of Apollo giving spectators a view of the entire sanctuary and the valley below. It was originally built in the 4th century BC but was remodeled on several occasions since. Its 35 rows can seat 5,000 spectators.
Tholos.
The Tholos at the sanctuary of Athena Pronoia (Ἀθηνᾶ Πρόνοια, "Athena of forethought") is a circular building that was constructed between 380 and 360 BC. It consisted of 20 Doric columns arranged with an exterior diameter of 14.76 meters, with 10 Corinthian columns in the interior.
The Tholos is located approximately a half a mile (800 m) from the main ruins at Delphi (at ). Three of the Doric columns have been restored, making it the most popular site at Delphi for tourists to take photographs. Vitruvius (vii, introduction) notes Theodorus of Samos as the architect of the Round Building which is at Delphi.
Gymnasium.
The gymnasium, which is half a mile away from the main sanctuary, was a series of buildings used by the youth of Delphi. The building consisted of two levels: a stoa on the upper level providing open space, and a palaestra, pool and baths on lower floor. These pools and baths were said to have magical powers, and imparted the ability to communicate to Apollo himself.
Stadium.
The stadium is located further up the hill, beyond the "via sacra" and the theatre. It was originally built in the 5th century BC but was altered in later centuries. The last major remodeling took place in the 2nd century AD under the patronage of Herodes Atticus when the stone seating was built and (arched) entrance. It could seat 6500 spectators and the track was 177 metres long and 25.5 metres wide.
Hippodrome.
The hippodrome of Delphi was the location where the running events took place during the Pythian Games. No trace of it has been found, but the location of the stadium and some remnants of retaining walls lead to the conclusion that it was set on a plain apart from the main part of the city and well away from the Peribolos of Apollo.
Polygonal wall.
The retaining wall was built to support the terrace housing the construction of the second temple of Apollo in 548 BC. Its name is taken from the polygonal masonry of which it is constructed. At a later date, from 200 BC onwards, the stones were inscribed with the manumission contracts of slaves who were consecrated to Apollo. Approximately a thousand manumissions are recorded on the wall.
Castalian spring.
The sacred spring of Delphi lies in the ravine of the Phaedriades. The preserved remains of two monumental fountains that received the water from the spring date to the Archaic period and the Roman, with the latter cut into the rock.
Athletic statues.
Delphi is famous for its many preserved athletic statues. It is known that Olympia originally housed far more of these statues, but time brought ruin to many of them, leaving Delphi as the main site of athletic statues. Kleobis and Biton, two brothers renowned for their strength, are modeled in two of the earliest known athletic statues at Delphi. The statues commemorate their feat of pulling their mother's cart several miles to the Sanctuary of Hera in the absence of oxen. The neighbors were most impressed and their mother asked Hera to grant them the greatest gift. When they entered Hera's temple, they fell into a slumber and never woke, dying at the height of their admiration, the perfect gift.
The Charioteer of Delphi is another ancient relic that has withstood the centuries. It is one of the best known statues from antiquity. The charioteer has lost many features, including his chariot and his left arm, but he stands as a tribute to athletic art of antiquity.
Architectural traditions.
Ancient tradition accounted for four temples that successively occupied the site before the 548/7 BC fire, following which the Alcmaeonids built a fifth. The poet Pindar celebrated the Alcmaeonid's temple in "Pythian" 7.8-9 and he also provided details of the third building ("Paean" 8. 65-75). Other details are given by Pausanias (10.5.9-13) and the Homeric Hymn to Apollo (294 ff.). The first temple was said to have been constructed out of olive branches from Tempe. The second was made by bees out of wax and wings but was miraculously carried off by a powerful wind and deposited among the Hyperboreans. The third, as described by Pindar, was created by the gods Hephaestus and Athena, but its architectural details included Siren-like figures or 'Enchantresses', whose baneful songs eventually provoked the Olympian gods to bury the temple in the earth (according to Pausanias, it was destroyed by earthquake and fire). In Pindar's words, addressed to the Muses:
<br>
The fourth temple was said to have been constructed from stone by Trophonius and Agamedes.
The Delphi Archaeological Museum.
The Delphi Archaeological Museum is at the foot of the main archaeological complex, on the east side of the village, and on the north side of the main road. The museum houses an impressive collection associated with ancient Delphi, including the earliest known notation of a melody, the famous Charioteer, golden treasures discovered beneath the Sacred Way, and fragments of reliefs from the Siphnian Treasury. Immediately adjacent to the exit (and overlooked by most tour guides) is the inscription that mentions the Roman proconsul Gallio.
Entries to the museum and to the main complex are separate and chargeable, and a reduced rate ticket gets entry to both. There is a small cafe, and a post office by the museum.
Excavations.
The site had been occupied by the village of Kastri since medieval times. Before a systematic excavation of the site could be undertaken, the village had to be relocated but the residents resisted. The opportunity to relocate the village occurred when it was substantially damaged by an earthquake, with villagers offered a completely new village in exchange for the old site. In 1893 the French Archaeological School removed vast quantities of soil from numerous landslides to reveal both the major buildings and structures of the sanctuary of Apollo and of Athena Pronoia along with thousands of objects, inscriptions and sculptures.
The site is now an archaeological one, and a very popular tourist destination. It is easily accessible from Athens as a day trip, and is often combined with the winter sports facilities available on Mount Parnassus, as well as the beaches and summer sports facilities of the nearby coast of Phocis.
The site is also protected as a site of extraordinary natural beauty, and the views from it are also protected: no industrial artefacts are to be seen from Delphi other than roads and traditional architecture residences (for example high voltage power lines and the like are routed so as to be invisible from the area of the sanctuary).
Depiction of Delphi in art.
From the 16th century onwards, the West develops an interest in Delphi. In th mid-15th century Strabo is first translated in Latin. The earliers depiction of Delphi is totally imaginary, created by the German N. Gerbel, who published in 1545 a text based on the map of Greece by N. Sofianos. The ancient sanctuary is depicted as a fortified city. 
The first travelers with archaeological interests, apart from the precursor Cyriacus of Ancona, are the British George Wheler and the French Jacob Spon, who visited Greece in a joint expedition in 1675-76. They published their impressions separately. In Wheler's "Journey into Greece", published in 1682, appears a sketch of the region of Dephi, where the settlement of Kastri and some ruins are depicted. The illustrations in Spon's publication "Voyage d'Italie, de Dalmatie, de Grèce et du Levant, 1678" are considered original and groundbreaking. 
Travelers continued to visit Delphi throughout the 19th century and published their books which contained diaries, sketches, views of the site as well as pictures of coins. The illustrations often reflected the spirit of romanticism, as evident by the works of Otto Magnus von Stackelberg, where, apart from the landscapes ("La Grèce. Vues pittoresques et topographiques", Paris 1834) are depicted also human types ("Costumes et usages des peuples de la Grèce moderne dessinés sur les lieux", Paris 1828). The philhellene painter W. Williams has comprised the landscape of Delphi in his themes (1829). important personalities such as F.Ch.-H.-L. Pouqueville, W.M. Leake, Chr. Wordsworth and Lord Byron are amongst the most important visitors of Delphi. 
After the foundation of the modern Greek state, the press becomes also interested in these travelers. Thus "Ephemeris" writes (17/03/1889): 
“In the "Revues des Deux Mondes" Paul Lefaivre published his memoirs from an excursion to Delphi. The French author relates in a charming style his adventures on the road, praising particularly the ability of an old woman to put back in its place the dismantled arm of one of his foreign traveling companions, who had fallen off the horse. In Arachova the Greek type is preserved intact. The men are rather athletes than farmers, built for running and wrestling, particularly elegant and slender under their mountain geer. Only briefly does he refer to the antiquities of Delphi, but he refers to a pelasgian wall 80 meters long, on which innumerable inscriptions are carved, decrees, conventions, manumissions". 
Gradually the first travelling guides appeared. The revolutionary "pocket" books invented by K. Baedeker, accompanied by maps useful for visiting archeological sites such as Delphi (1894) and the informed plans, the guides became practical and popular. 
The photographic lens revolutionized the way of depicting the landscape and the antiquities, particularly from 1893 onwards, when the systematic excavations of the French Archaeological School started. However, artists such as Vera Willoughby, continued to be inspired by the landscape.
Delphic themes inspired several graphic artists. Besides the landscape, Pythia/Sibylla become an illustration subject even on Tarot cards. A famous example constitutes Michelangelo's Delphic Sibyl (1509), the 19th-century German engraving Oracle of Apollo at Delphi, as well as the most recent The Oracle of Delphi, inc on paper, by the Swedish Malin Lind. 
Modern artists are inspired also by the Delphic Maxims. Examples of such works are displayed in the "Sculpture park of the European Cultural Center of Delphi" and in exhibitions taking place at the Archaeological Museum of Delphi.
Delphi in literature.
Delphi inspired literature as well. In 1814 W. Haygarth, friend of Lord Byron, refers to Delphi in his work "Greece, a Poem". In 1888 Charles Marie René Leconte de Lisle published his lyric drama L’Apollonide accompanied by music by Franz Servais. More recent French authors used Delphi as a source of inspiration such as Yves Bonnefoy (Delphes du second jour) or Jean Sulivan (nickname of Joseph Lemarchand) in L'Obsession de Delphes (1967), but also Rob MacGregor's Indiana Jones and the Peril at Delphi (1991).
The presence of Delphi in Greek literature is very intense. Poets such as Kostis Palamas (The Delphic Hymn, 1894), Kostas Karyotakis (Delphic festival, 1927), Nikephoros Vrettakos (return from Delphi, 1957), Yannis Ritsos (Delphi, 1961–62) and Kiki Dimoula (Gas omphalos and Appropriate terrain 1988), to mention only the most renowned ones. Angelos Sikelianos wrote The Dedication (of the Delphic speech) (1927), the Delphic Hymn (1927) and the tragedy Sibylla (1940), whereas in the context of the Delphic idea and the Delphic festivals he published an essay titled "The Delphic union" (1930). The nobelist George Seferis wrote an essay under the titel "Delphi", comprised in the book "Dokimes". 
The importance of Delphi for the Greeks is significant. The site has been recorded on the collective memory and have been expressed through tradition. Nikolaos Politis, the famous Greek ethnographer, in his Studies on the life and language of the Greek people - part A, offers two examples from Delphi: 
a) the priest of Apollo (176)
When Christ was born a priest of Apollo was sacrificing below the monastery of Panayia, on the road of Livadeia, on a site called Logari. Suddenly he abandoned the sacrifice and says to the people: "in this moment was born the son of God, who will be very powerful, like Apollo, but then Apollo will beat him". He didn't have time to finish his speech and a thunder came down and burnt him, opening the rock nearby into two. . 9
b)The Mylords (108)
The Mylords are not Christians, because nobody ever saw them cross themselves. They originate from the old pagan inhabitants of Delphi who kept their property in castle called Adelphi, named after the two brother princes who built it. When Christ and his mother came to the site, and all people around converted to Christianity they thought that they should better leave; thus the Mylords left for the West and took all their belongings with them. The Mylords come here now and worship theses stones. . 5
Modern Delphi.
Town.
Modern Delphi is situated immediately west of the archaeological site and hence is a popular tourist destination. It is on a major highway linking Amfissa along with Itea and Arachova. There are many hotels and guest houses in the town, and many taverns and bars. The main streets are narrow, and often one-way. The E4 European long distance path passes through the east end of the town. In addition to the archaeological interest, Delphi attracts tourists visiting the Parnassus Ski Center and the popular coastal towns of the region.
In the Middle Ages a town called Kastri was built on the archaeological site. The residents had used the marble columns and structures as support beams and roofs for their improvised houses, a usual way of rebuilding towns that were partially or totally destroyed, especially after the earthquake in 1580, which demolished several towns in Phocis. In 1893 archaeologists from the École française d'Athènes finally located the actual site of ancient Delphi and the village was moved to a new location, west of the site of the temples.
Municipality.
The municipality Delphi was formed at the 2011 local government reform by the merger of the following 8 former municipalities, that became municipal units:
The administrative seat of the municipality is in the largest town, Amfissa. The total population of the municipality is 32,263. The town Delphi has a population of 2,373 people while the population of the municipal unit of Delphi, including Chrisso (ancient Krissa), is 3,511.
General:

</doc>
<doc id="7952" url="https://en.wikipedia.org/wiki?curid=7952" title="Digital Equipment Corporation">
Digital Equipment Corporation

Digital Equipment Corporation, also known as DEC and using the trademark Digital, was a major American company in the computer industry from the 1960s to the 1990s. It was a leading vendor of computer systems, including computers, software, and peripherals, and its PDP and successor VAX products were the most successful of all minicomputers in terms of sales.
From 1957 until 1992 its headquarters were located in a former wool mill in Maynard, Massachusetts (since renamed Clock Tower Place and now home to multiple companies). DEC was acquired in June 1998 by Compaq, which subsequently merged with Hewlett-Packard in May 2002. Some parts of DEC, notably the compiler business and the Hudson, Massachusetts facility, were sold to Intel.
Digital Equipment Corporation should not be confused with the unrelated companies Digital Research, Inc or Western Digital, although the latter once manufactured the LSI-11 chipsets used in DEC's low end PDP-11/03 computers.
Overview.
Initially focusing on the small end of the computer market allowed DEC to grow without its potential competitors making serious efforts to compete with them. Their PDP series of machines became popular in the 1960s, especially the PDP-8, widely considered to be the first successful minicomputer. Looking to simplify and update their line, DEC replaced most of their smaller machines with the PDP-11 in 1970, eventually selling over 600,000 units and cementing DECs position in the industry. Originally designed as a follow-on to the PDP-11, DEC's VAX-11 series was the first widely used 32-bit minicomputer, sometimes referred to as "superminis". These were able to compete in many roles with larger mainframe computers, such as the IBM System/370. The VAX was a best-seller, with over 400,000 sold, and its sales through the 1980s propelled the company into the second largest in the industry. At its peak, DEC was the second largest employer in Massachusetts, second only to the state government.
The rapid rise of the business microcomputer in the late 1980s, and especially the introduction of powerful 32-bit systems in the 1990s, quickly eroded the value of DEC's systems. DEC's last major attempt to find a space in the rapidly changing market was the DEC Alpha 64-bit RISC processor architecture. DEC initially started work on Alpha as a way to re-implement their VAX series, but also employed it in a range of high-performance workstations. Although the Alpha processor family met both of these goals, and, for most of its lifetime, was the fastest processor family on the market, extremely high asking prices were outsold by lower priced x86 chips from Intel and clones such as AMD.
The company was acquired in June 1998 by Compaq, in what was at that time the largest merger in the history of the computer industry. At the time, Compaq was focused on the enterprise market and had recently purchased several other large vendors. DEC was a major player overseas where Compaq had less presence. However, Compaq had little idea what to do with its acquisitions, and soon found itself in financial difficulty of its own. The company subsequently merged with Hewlett-Packard in May 2002. some of DEC's product lines were still produced under the HP name.
History.
Origins.
Ken Olsen and Harlan Anderson were two engineers who had been working at MIT Lincoln Laboratory on the lab's various computer projects. The Lab is best known for their work on what would today be known as "interactivity", and their machines were among the first where operators had direct control over programs running in real time. These had started in 1944 with the famed Whirlwind which was originally developed to make a flight simulator for the US Navy, although this was never completed. Instead, this effort evolved into the SAGE system for the US Air Force, which used large screens and light guns to allow operators to interact with radar data stored in the computer.
When the Air Force project wound down, the Lab turned their attention to an effort to build a version of the Whirlwind using transistors in place of vacuum tubes. In order to test their new circuitry, they first built a small 18-bit machine known as TX-0 which first ran in 1956. When the TX-0 successfully proved the basic concepts, attention turned to a much larger system, the 36-bit TX-2 with a then-enormous 64 kWords of core memory. Core was so expensive that parts of TX-0's memory were stripped for the TX-2, and what remained of the TX-0 was then given to MIT on permanent loan.
At MIT, Olsen and Anderson noticed something odd: students would line up for hours to get a turn to use the stripped-down TX-0, while largely ignoring a faster IBM machine that was also available. The two decided that the draw of interactive computing was so strong that they felt there was a market for a small machine dedicated to this role, essentially a commercialized TX-0. They could sell this to users where graphical output or realtime operation would be more important than outright performance. Additionally, as the machine would cost much less than the larger systems then available, it would also be able to serve users that needed a lower-cost solution dedicated to a specific task, where a larger 36-bit machine would not be needed.
In 1957 when the pair and Ken's brother Stan went looking for capital, they found that the American business community was hostile to investing in computer companies. Many smaller computer companies had come and gone in the 1950s, wiped out when new technical developments rendered their platforms obsolete, and even large companies like RCA and General Electric were failing to make a profit in the market. The only serious expression of interest came from Georges Doriot and his American Research and Development Corporation (AR&D). Worried that a new computer company would find it difficult to arrange further financing, Doriot suggested the fledgling company change its business plan to focus less on computers, and even change their name from "Digital Computer Corporation".
The pair returned with an updated business plan that outlined two phases for the company's development. They would start by selling computer modules as stand-alone devices that could be purchased separately and wired together to produce a number of different digital systems for lab use. Then, if these "digital modules" were able to build a self-sustaining business, the company would be free to use them to develop a complete computer in their Phase II. The newly christened "Digital "Equipment" Corporation" received $70,000 from AR&D for a 70% share of the company, and began operations in a Civil War era textile mill in Maynard, Massachusetts, where plenty of inexpensive manufacturing space was available.
Digital modules.
In early 1958 DEC shipped its first products, the "Digital Laboratory Module" line. The Modules consisted of a number of individual electronic components and germanium transistors mounted to a circuit board, the actual circuits being based on those from the TX-2.
The Laboratory Modules were packaged in an extruded aluminum housing, intended to sit on an engineer's workbench, although a rack-mount bay was sold that held nine laboratory modules. They were then connected together using banana plug patch cords inserted at the front of the modules. Three versions were offered, running at 5 MHz (1957), 500 kHz (1959), or 10 MHz (1960). The Modules proved to be in high demand in other computer companies, who used them to build equipment to test their own systems. Despite the recession of the late 1950s, the company sold $94,000 worth of these modules during 1958 alone, turning a profit at the end of its first year.
The original Laboratory Modules were soon supplemented with the "Digital Systems Module" line, which were identical internally but packaged differently. The Systems Modules were designed with all of the connections at the back of the module using 22-pin Amphenol connectors, and were attached to each other by plugging them into a backplane that could be mounted in a 19-inch rack. The backplanes allowed 25 modules in a single 5-1/4 inch section of rack, and allowed the high densities needed to build a computer.
The original laboratory and system module lines were offered in 500 kilocycle, 5 megacycle and 10 megacycle versions. In all cases, the supply voltages were -15 and +10 volts, with logic levels of -3 volts (passive pull-down) and 0 volts (active pull-up).
DEC used the Systems Modules to build their "Memory Test" machine for testing core memory systems, selling about 50 of these pre-packaged units over the next eight years. The PDP-1 and LINC computers were also built using Systems Modules (see below).
Modules were part of DEC's product line into the 1970s, although they went through several evolutions during this time as technology changed. The same circuits were then packaged as the first "R" (red) series "Flip-Chip" modules. Later, other module series provided additional speed, much higher logic density, and industrial I/O capabilities. Digital published extensive data about the modules in free catalogs that became very popular.
PDP-1 family.
With the company established and a successful product on the market, DEC turned its attention to the computer market once again as part of its planned "Phase II". In August 1959, Ben Gurley started design of the company's first computer, the PDP-1. In keeping with Doriot's instructions, the name was an initialism for "Programmable Data Processor", leaving off the term "computer". As Gurley put it, "We aren't building computers, we're building 'Programmable Data Processors'." The prototype was first shown publicly at the Joint Computer Conference in Boston in December 1959. The first PDP-1 was delivered to Bolt, Beranek and Newman in November 1960, and formally accepted the next April. The PDP-1 sold in basic form for $120,000, or about $900,000 in 2011 US dollars. By the time production ended in 1969, 53 PDP-1s had been delivered.
The PDP-1 was supplied standard with 4096 words of core memory, 18-bits per word, and ran at a basic speed of 100,000 operations per second. It was constructed using many System Building Blocks that were packaged into several 19-inch racks. The racks were themselves packaged into a single large mainframe case, with a hexagonal control panel containing switches and lights mounted to lay at table-top height at one end of the mainframe. Above the control panel was the system's standard input/output solution, a punch tape reader and writer. Most systems were purchased with two peripherals, the Type 30 vector graphics display, and a Soroban Engineering modified IBM Model B Electric typewriter that was used as a printer. The Soroban system was notoriously unreliable, and often replaced with a modified Friden Flexowriter, which also contained its own punch tape system. A variety of more-expensive add-ons followed, including magnetic tape systems, punched card readers and punches, and faster punch tape and printer systems.
When DEC introduced the PDP-1, they also mentioned larger machines at 24, 30 and 36 bits, based on the same design. During construction of the prototype PDP-1, some design work was carried out on a 24-bit PDP-2, and the 36-bit PDP-3. Although the PDP-2 never proceeded beyond the initial design, the PDP-3 found some interest and was designed in full. Only one PDP-3 appears to have been built, in 1960, by the CIA's Scientific Engineering Institute (SEI) in Waltham, Massachusetts. According to the limited information available, they used it to process radar cross section data for the Lockheed A-12 reconnaissance aircraft. Gordon Bell remembered that it was being used in Oregon some time later, but could not recall who was using it.
In November 1962 DEC introduced the $65,000 PDP-4. The PDP-4 was similar to the PDP-1 and used a similar instruction set, but used slower memory and different packaging to lower the price. Like the PDP-1, about 54 PDP-4's were eventually sold, most to a customer base similar to the original PDP-1.
In 1964 DEC introduced its new Flip Chip module design, and used it to re-implement the PDP-4 as the PDP-7. The PDP-7 was introduced in December 1964, and about 120 were eventually produced. An upgrade to the Flip Chip led to the R series, which in turn led to the PDP-7A in 1965. The PDP-7 is most famous as the original machine for the Unix operating system, and until the Interdata 8/32 Unix only ran on DEC systems.
A more dramatic upgrade to the PDP-1 series was introduced in August 1966, the PDP-9. The PDP-9 was instruction compatible with the PDP-4 and −7, but ran about twice as fast as the −7 and was intended to be used in larger deployments. At only $19,900 in 1968, the PDP-9 was a big seller, eventually selling 445 machines, more than all of the earlier models combined.
Even while the PDP-9 was being introduced, its replacement was being designed, and was introduced as 1969's PDP-15, which re-implemented the PDP-9 using integrated circuits in place of modules. Much faster than the PDP-9 even in basic form, the PDP-15 also included a floating point unit and a separate input/output processor for further performance gains. Over 400 PDP-15's were ordered in the first eight months of production, and production eventually amounted to 790 examples in 12 basic models. However, by this time other machines in DEC's lineup could fill the same niche at even lower price points, and the PDP-15 would be the last of the 18-bit series.
PDP-8 family.
In 1962, Lincoln Laboratory used a selection of System Building Blocks to implement a small 12-bit machine, and attached it to a variety of analog-to-digital (A to D) input/output (I/O) devices that made it easy to interface with various analog lab equipment. The LINC proved to attract intense interest in the scientific community, and has since been referred to as the first real minicomputer, a machine that was small and inexpensive enough to be dedicated to a single task even in a small lab.
Seeing the success of the LINC, in 1963 DEC took the basic logic design but stripped away the extensive A to D systems to produce the PDP-5. The new machine, the first outside the PDP-1 mould, was introduced at WESTCON on 11 August 1963. A 1964 ad expressed the main advantage of the PDP-5, "Now you can own the PDP-5 computer for what a core memory alone used to cost: $27,000" 116 PDP-5s were produced until the lines were shut down in early 1967. Like the PDP-1 before it, the PDP-5 inspired a series of newer models based on the same basic design that would go on to be more famous than its parent.
On 22 March 1965, DEC introduced the PDP-8, which replaced the PDP-5's modules with the new R-series modules using Flip Chips. The machine was re-packaged into a small tabletop case, which remains distinctive for its use of smoked plastic over the CPU which allowed one to easily see the wire-wrapped internals of the CPU. Sold standard with 4 kWords of 12-bit core memory and a Teletype Model 33 ASR for basic input/output, the machine listed for only $18,000. The PDP-8 is referred to as the first "real" minicomputer because of its sub-$25,000 price. Sales were, unsurprisingly, very strong, and helped by the fact that several competitors had just entered the market with machines aimed directly at the PDP-5's market space, which the PDP-8 trounced. This gave the company two years of unrestricted leadership, and eventually 1450 "straight eight" machines were produced before it was replaced by newer implementations of the same basic design.
DEC hit an even lower price-point with the PDP-8/S, the S for "serial". As the name implies the /S used a serial arithmetic unit, which was much slower but reduced costs so much that the system sold for under $10,000. DEC then used the new PDP-8 design as the basis for a new LINC, the two-processor LINC-8. The LINC-8 used one PDP-8 CPU and a separate LINC CPU, and included instructions to switch from one to the other. This allowed customers to run their existing LINC programs, or "upgrade" to the PDP-8, all in software. Although not a huge seller, 142 LINC-8s were sold starting at $38,500. Like the original LINC to PDP-5 evolution, the LINC-8 was then modified into the single-processor PDP-12, adding another 1000 machines to the 12-bit family. Newer circuitry designs led to the PDP-8/I and PDP-8/L in 1968. In 1975, one year after an agreement between Digital and Intersil, the Intersil 6100 chip was launched, effectively a PDP-8 on a chip. This was a way to allow PDP-8 software to be run even after the official end-of-life announcement for the Digital PDP-8 product line.
PDP-10 family.
While the PDP-5 introduced a lower-cost line, 1963's PDP-6 was intended to take DEC into the mainframe market with a 36-bit machine. However, the PDP-6 proved to be a "hard sell" with customers, as it offered few obvious advantages over similar machines from the better-established vendors like IBM or Honeywell, in spite of its low cost around $300,000. Only 23 were sold, or 26 depending on the source, and unlike other models the low sales meant the PDP-6 was not improved with successor versions. However, the PDP-6 is historically important as the platform that introduced "Monitor", an early time-sharing operating system that would evolve into the widely used TOPS-10.
When newer Flip Chip packaging allowed the PDP-6 to be re-implemented at a much lower cost, DEC took the opportunity to refine their 36-bit design, introducing the PDP-10 in 1968. The PDP-10 was as much a success as the PDP-6 was a commercial failure; about 700 mainframe PDP-10s were sold before production ended in 1984. The PDP-10 was widely used in university settings, and thus was the basis of many advances in computing and operating system design during the 1970s. DEC later re-branded all of the models in the 36-bit series as the "DECsystem-10", and PDP-10s are generally referred to by the model of their CPU, starting with the "KA10", soon upgraded to the "KI10" (I:Integrated circuit); then to "KL10" (L:Large scale integration ECL logic); also the "KS10" (S: Small form factor). Unified product line upgrades produced the compatible DECSYSTEM-20, along with a TOPS-20 operating system that included virtual memory support. 
The Jupiter Project was supposed to continue the mainframe product line into the future by using "gate arrays" with an innovative Air Mover Cooling System, coupled with a built-in floating point processing engine called "FBOX". The design was intended for a top tier scientific computing niche, yet the critical performance measurement was based upon COBOL compilation which did not fully utilize the primary design features of Jupiter technology. When the Jupiter Project was cancelled in 1983, some of the engineers adapted aspects of the 36-bit design into a forthcoming 32-bit design, releasing the high-end VAX8600 in 1985.
DECtape.
One of the most unusual peripherals produced for the PDP-10 was the DECtape. The DECtape was a length of special 3/4-inch wide magnetic tape wound on 5-inch reels. The recording format was a highly-reliable redundant 10-track design using fixed-length numbered data "blocks" organized into a standard file structure, including a directory. Files could be written, read, changed, and deleted on a DECtape as though it were a disk drive. For greater efficiency, the DECtape drive could read and write to a DECtape in both directions.
In fact, some PDP-10 systems had no disks at all, using DECtapes alone for their primary data storage. The DECtape was also widely used on other PDP models, since it was much easier to use than hand-loading multiple paper tapes. Primitive early time-sharing systems could use DECtapes as system devices and swapping devices. Although superior to paper tape, DECtapes were relatively slow, and were supplanted once reliable disk drives became affordable.
Magnetic disk storage.
DEC was both a manufacturer and a buyer of magnetic disk storage, offering more than 100 different models of hard disk drive (HDD) and floppy disk drive (FDD) during its existence. In the 1970s, it was the single largest OEM purchaser of HDDs, procuring from Diablo, Control Data Corporation, Information Storage Systems, and Memorex, among others. 
DEC's first internally-developed HDD was the RS08, a 256 kWord fixed-head contact-start-stop drive using plated media; it shipped in 1969.
Beginning in the 1970s, DEC moved first its HDD manufacturing and then its mass storage development labs to Colorado Springs. 
DEC pioneered a number of HDD technologies, including sampled data servos (RL01, 1977) and serial HDD interfaces (Standard Disk Interconnect, 1983). The last internally-developed disk drive family (RA9x series) used plated media, departing from the HDD industry trend to carbon overcoated sputtered media. DEC designated a $400 million investment to bring this product line into production. The RA92 (1.5 GB) was introduced in 1992, using a 14-inch platter.
DEC purchased its FDDs from OEMs such as Shugart Associates, Toshiba, and Sony.
PDP-11.
The PDP-11 16-bit computer was designed in a crash program by Harold McFarland, Gordon Bell, Roger Cady, and others. The project was able to leap forward in design with the arrival of Harold McFarland, who had been researching 16-bit designs at Carnegie Mellon University. One of his simpler designs became the PDP-11, although when they first viewed the proposal, management was not impressed and almost cancelled it.
In particular, the new design did not include many of the addressing modes that were intended to make programs smaller in memory, a technique that was widely used on other DEC machines and CISC designs in general. This would mean the machine would spend more time accessing memory, which would slow it down. However, the machine also extended the idea of multiple "General Purpose Registers" (GPRs), which gave the programmer flexibility to use these high-speed memory caches as they needed, potentially addressing the performance issues.
A major advance in the PDP-11 design was Digital's Unibus, which supported all peripherals through memory mapping. This allowed a new device to be added easily, generally only requiring plugging a hardware interface board into the backplane and possibly adding a jumper to the wire wrapped backplane, and then installing software that read and wrote to the mapped memory to control it. The relative ease of interfacing spawned a huge market of third party add-ons for the PDP-11, which made the machine even more useful.
The combination of architectural innovations proved superior to competitors and the "11" architecture was soon the industry leader, propelling DEC back to a strong market position. The design was later expanded to allow paged physical memory and memory protection features, useful for multitasking and time-sharing. Some models supported separate instruction and data spaces for an effective virtual address size of 128 kB within a physical address size of up to 4 MB. Smaller PDP-11s, implemented as single-chip CPUs, continued to be produced until 1996, by which time over 600,000 had been sold.
The PDP-11 supported several operating systems, including Bell Labs' new Unix operating system as well as DEC's DOS-11, RSX-11, IAS, RT-11, DSM-11, and RSTS/E. Many early PDP-11 applications were developed using standalone paper-tape utilities. DOS-11 was the PDP-11's first disk operating system, but was soon supplanted by more capable systems. RSX provided a general-purpose multitasking environment and supported a wide variety of programming languages. IAS was a time-sharing version of RSX-11D. Both RSTS and Unix were time-sharing systems available to educational institutions at little or no cost, and these PDP-11 systems were destined to be the "sandbox" for a rising generation of engineers and computer scientists. Large numbers of PDP-11/70s were deployed in telecommunications and industrial control applications. AT&T Corporation became DEC's largest customer.
RT-11 provided a practical real-time operating system in minimal memory, allowing the PDP-11 to continue Digital's critical role as a computer supplier for embedded systems. Historically, RT-11 also served as the inspiration for many microcomputer OS's, as these were generally being written by programmers who cut their teeth on one of the many PDP-11 models. For example, CP/M used a command syntax similar to RT-11's, and even retained the awkward PIP program used to copy data from one computer device to another. As another historical footnote, DEC's use of "/" for "switches" (command-line options) would lead to the adoption of "\" for pathnames in MS-DOS and Microsoft Windows as opposed to "/" in Unix.
The evolution of the PDP-11 followed earlier systems, eventually including a single-user deskside personal computer form, the MicroPDP-11. In total, around 600,000 PDP-11s of all models were sold. and a wide variety of third-party peripheral vendors had also entered the computer product ecosystem.
VAX.
In 1976, DEC decided to extend the PDP-11 architecture to 32 bits while adding a complete virtual memory system to the simple paging and memory protection of the PDP-11. The result was the VAX architecture, where VAX stands for Virtual Address eXtension (from 16 to 32 bits). The first computer to use a VAX CPU was the VAX-11/780, which DEC referred to as a "superminicomputer". Although it was not the first 32-bit minicomputer, the VAX-11/780's combination of features, price, and marketing almost immediately propelled it to a leadership position in the market after it was released in 1978. VAX systems were so successful that in 1983, DEC canceled its Jupiter project, which had been intended to build a successor to the PDP-10 mainframe, and instead focused on promoting the VAX as the single computer architecture for the company.
Supporting the VAX's success was the VT52, one of the most successful smart terminals. Building on earlier less successful models (the VT05 and VT50), the VT52 was the first terminal that did everything one might want in a single chassis. The VT52 was followed by the even more successful VT100 and its follow-ons, making DEC one of the largest terminal vendors in the industry. With the VT series, DEC could now offer a complete top-to-bottom system from computer to all peripherals, which formerly required collecting the required devices from different suppliers.
The VAX processor architecture and family of systems evolved and expanded through several generations during the 1980s, culminating in the NVAX microprocessor implementation and VAX 7000/10000 series in the early 1990s.
Early microcomputers.
When a DEC research group demonstrated two prototype microcomputers in 1974—before the debut of the MITS Altair—Olsen chose to not proceed with the project. The company similarly rejected another personal computer proposal in 1977. At the time these systems were of limited utility, and Olsen famously derided them in 1977, stating "There is no reason for any individual to have a computer in his home." Unsurprisingly, DEC did not put much effort into the microcomputer area in the early days of the market. Interestingly in 1977, the Heathkit H11 was announced; a PDP-11 in kit form. At the beginning of the 1980s, DEC built the VT180 (codenamed "Robin"), which was a VT100 terminal with an added Z80-based microcomputer running CP/M, but this product was initially available only to DEC employees.
It was only after IBM had successfully launched the IBM PC in 1981 that DEC responded with their own systems. In 1982, Digital introduced not one, but three incompatible machines which were each tied to different proprietary architectures. The first, the DEC Professional, was based on the PDP-11/23 (and later, the 11/73) running the RSX-11M+ derived, but menu-driven, P/OS ("Professional Operating System"). This DEC machine easily outperformed the PC, but was more expensive than, and completely incompatible with IBM PC hardware and software, offering far fewer options for customizing a system.
Unlike CP/M and DOS microcomputers, every copy of every program for the Professional had to be provided with a unique key for the particular machine and CPU for which it was bought. At that time this was mainstream policy, because most computer software was either bought from the company that built the computer or custom-constructed for one client. However, the emerging third-party software industry disregarded the PDP-11/Professional line and concentrated on other microcomputers where distribution was easier. At DEC itself, creating better programs for the Professional was not a priority, perhaps from fear of cannibalizing the PDP-11 line. As a result, the Professional was a superior machine, running inferior software. In addition, a new user would have to learn an awkward, slow, and inflexible menu-based user interface which appeared to be radically different from PC DOS or CP/M, which were more commonly used on the 8080 and 8088 based microcomputers of the time. A second offering, the DECmate II was the latest version of the PDP-8 based word processors, but not really suited to general computing, nor competitive with Wang Laboratories' popular word processing equipment.
The most popular early DEC microcomputer was the dual-processor (Z80 and 8088) Rainbow 100, which ran the 8-bit CP/M operating system on the Z80 and the 16-bit CP/M-86 operating system on the Intel 8086 processor. It could also run a UNIX System III implementation called VENIX. Applications from standard CP/M could be re-compiled for the Rainbow, but by this time users were expecting custom-built (pre-compiled binary) applications such as Lotus 1-2-3, which was eventually ported along with MS-DOS 2.0 and introduced in late 1983. Although the Rainbow generated some press, it was unsuccessful due to its high price and lack of marketing and sales support. By late 1983 IBM was outselling DEC's personal computers by more than ten to one.
The way the DEC standard RX50 floppy disk drive supported DEC's initial offerings seemed to encapsulate their approach to the personal computer market. Although the mechanical drive hardware was nearly identical to other 5¼" floppy disk drives available on competing systems, DEC sought to differentiate their product by using a proprietary disk format for the data written on the disk. The DEC format had a higher capacity for data, but the RX50 drives were incompatible with other PC floppy drives. This required DEC owners to buy higher-priced, specially formatted floppy media, which was harder to obtain through standard distribution channels. DEC attempted to enforce exclusive control over its floppy media sales by copyrighting its proprietary disk format, and requiring a negotiated license agreement and royalty payments from anybody selling compatible media. The proprietary data format meant that RX50 floppies were not interchangeable with other PC floppies, further isolating DEC products from the developing de facto standard PC market. Hardware hackers and DEC enthusiasts eventually reverse-engineered the RX50 format, but the damage had already been done, in terms of market confusion and isolation.
A further system was introduced in 1986 as the VAXmate, which included Microsoft Windows 1.0 and used VAX/VMS-based file and print servers along with integration into DEC's own DECnet-family, providing LAN/WAN connection from PC to mainframe or supermini. The VAXmate replaced the Rainbow, and in its standard form was the first widely marketed diskless workstation.
Networking and clusters.
In 1984, DEC launched its first 10 Mbit/s Ethernet. Ethernet allowed scalable networking, and VAXcluster allowed scalable computing. Combined with DECnet and Ethernet-based terminal servers (LAT), DEC had produced a networked storage architecture which allowed them to compete directly with IBM. Ethernet replaced token ring, and went on to become the dominant networking model in use today.
In September 1985, DEC became the fifth company to register a .com domain name (dec.com).
Along with the hardware and protocols, DEC also introduced the VAXcluster concept, which allowed several VAX machines to be tied together into a single larger storage system. VAXclusters allowed a DEC-based company to scale their services by adding new machines to the cluster at any time, as opposed to buying a faster machine and using that to replace a slower one. The flexibility this offered was compelling, and allowed DEC to attack high-end markets formerly out of their reach.
Diversification.
Although their microcomputer efforts were eventually considered failures, the PDP-11 and VAX lines continued to sell in record numbers. Better yet, DEC was competing very well against the market leader, IBM, taking an estimated $2 billion away from them in the mid-80s. In 1986, Digital's profits rose 38% when the rest of the computer industry experienced a downturn, and by 1987 the company was threatening IBM's number one position in the computer industry.
At its peak, Digital was the second-largest computer company in the world, with over 100,000 employees. It was during this time that the company branched out development into a wide variety of projects that were far from its core business in computer equipment. The company invested heavily in custom software. In the 1970s and earlier most software was custom-written to serve a specific task, but by the 1980s the introduction of relational databases and similar systems allowed powerful software to be built in a modular fashion, potentially saving enormous amounts of development time. Software companies like Oracle became the new darlings of the industry, and DEC started their own efforts in every "hot" niche, in some cases several projects for the same niche. Some of these products competed with DEC's own partners, notably Rdb which competed with Oracle's products on the VAX, part of a major partnership only a few years earlier.
Although many of these products were well designed, most of them were DEC-only or DEC-centric, and customers frequently ignored them and used third-party products instead. This problem was further exacerbated by Olsen's aversion to traditional advertising and his belief that well-engineered products would sell themselves. Hundreds of millions of dollars were spent on these projects, at the same time that workstations using RISC microprocessors were starting to approach VAX CPUs in performance.
Faltering in the market.
As microprocessors continued to improve in the 1980s, it soon became clear that the next generation would offer performance and features equal to the best of DECs low-end minicomputer lineup. Worse, the Berkeley RISC and Stanford MIPS designs were aiming to introduce 32-bit designs that would outperform the fastest members of the VAX family, DEC's cash cow.
Constrained by the huge success of their VAX/VMS products, which followed the proprietary model, the company was very late to respond to these threats. In the early 1990s, DEC found its sales faltering and its first layoffs followed. The company that created the minicomputer, a dominant networking technology, and arguably the first computers for personal use, had abandoned the "low end" market, whose dominance with the PDP-8 had built the company in a previous generation. Decisions about what to do about this threat led to infighting within the company that seriously delayed their responses.
One group suggested that every possible development in the industry be poured into the construction of a new VAX family that would leapfrog the performance of the existing machines. This would limit the market erosion in the top-end segment, where profit margins were maximized and DEC could continue to survive as a minicomputer vendor. This line of thought led, eventually, to the VAX 9000 series, which were plagued with problems when they were first introduced in October 1989, already two years late. The problems took so long to work out, and the prices of the systems were so high, that DEC was never able to make the line the success they hoped.
Others within the company felt that the proper response was to introduce their own RISC designs and use those to build new machines. However, there was little official support for these efforts, and no less than four separate small projects ran in parallel at various labs around the US. Eventually these were gathered into the DEC PRISM project, which delivered a credible 32-bit design with some unique features allowing it to serve as the basis of a new VAX implementation. Infighting with teams dedicated to DEC's big iron made funding difficult, and the design was not finalized until April 1988, and then cancelled shortly thereafter.
Another group concluded that new workstations like those from Sun Microsystems and Silicon Graphics would take away a large part of DEC's existing customer base before the new VAX systems could address the issues, and that the company needed its own Unix workstation as soon as possible. Fed up with slow progress on both the RISC and VAX fronts, a group in Palo Alto started a skunkworks project to introduce their own systems. Selecting the MIPS processor, which was widely available, introducing the new DECstation series with the model 3100 on 11 January 1989. These systems would see some success in the market, but were later displaced by similar models running the Alpha.
32-bit MIPS and 64-bit Alpha systems.
Eventually, in 1992, DEC launched the DECchip 21064 processor, the first implementation of their Alpha instruction set architecture, initially named Alpha AXP (the "AXP" was a "non-acronym" and was later dropped). This was a 64-bit RISC architecture (as opposed to the 32-bit CISC architecture used in the VAX) and one of the first "pure" (not an extension of an earlier 32-bit architecture) 64-bit microprocessor architectures and implementations. The Alpha offered class-leading performance at its launch, and subsequent variants continued to do so into the 2000s. An AlphaServer SC45 supercomputer was still ranked No. 6 in the world in November 2004. Alpha-based computers (the DEC AXP series, later the AlphaStation and AlphaServer series) superseded both the VAX and MIPS architecture in DEC's product lines, and could run OpenVMS, DEC OSF/1 AXP (later, Digital Unix or Tru64 UNIX) and Microsoft's then-new operating system, Windows NT.
In 1998, following the takeover by Compaq Computers, a decision was made that Microsoft would no longer support and develop Windows NT for the Alpha series computers, a decision that was seen as the beginning of the end for the Alpha series computers.
StrongARM.
In the mid-1990s, Digital Semiconductor collaborated with ARM Limited to produce the StrongARM microprocessor. This was based in part on ARM7 and in part on DEC technologies like Alpha, and was targeted at embedded systems and portable devices. It was highly compatible with the ARMv4 architecture and was very successful, competing effectively against rivals such as the SuperH and MIPS architectures in the portable digital assistant market. Microsoft subsequently dropped support for these other architectures in their Pocket PC platform. In 1997, as part of a lawsuit settlement, the StrongARM intellectual property was sold to Intel. They continued to produce StrongARM, as well as developing it into the XScale architecture. Intel subsequently sold this business to Marvell Technology Group in 2006.
Video and Interactive Information Server.
The Video-on-Demand project at DEC started in 1992, following Ken Olsen’s retirement. At the time the company was rapidly downsizing under Robert Palmer, and it was difficult to gain funding for any new project. DEC’s Interactive Video Information Server architecture gained traction and excelled over those of other companies, in that it was highly scalable, using a gateway to set up interactive video delivery sessions on large numbers of video and information servers. Initially high-end VAXes were used, then Alphas.
The scalability feature allowed it to win contracts for many of the video-on-demand trials in the 1993-1995 timeframe, since the system could theoretically accommodate unlimited interactive video streams and other non-video content.
The design was proposed and incorporated into the MPEG2 international standard. Its object-oriented interface became the mandatory user-to user core interface in DSM-CC, widely used in video stream and file delivery for MPEG-2 compliant systems.
Commercially, DEC’s Digital and Interactive Information System was used by Adlink to distribute advertising to over 2 million subscribers.
Designing solutions.
Beyond DECsystem-10/20, PDP, VAX and Alpha, Digital was well respected for its communication subsystem designs, such as Ethernet, DNA (DIGITAL Network Architecture – predominantly DECnet products), DSA (Digital Storage Architecture – disks/tapes/controllers), and its "dumb terminal" subsystems including VT100 and DECserver products.
Final years.
At its peak in the late 1980s, Digital had $14 billion in sales and ranked among the most profitable companies in the US. With its strong staff of engineers, Digital was expected to usher in the age of personal computers, but the autocratic and trend-resistant Mr. Olsen was openly skeptical of the desktop machines, saying “the personal computer will fall flat on its face in business”, and regarding them as “toys” used for playing video games. Digital's fortunes declined after missing out on some critical market shifts, particularly toward the personal computer. The board forced Olsen to resign as president in July 1992.
In June 1992, Ken Olsen was replaced by Robert Palmer as the company's president. Digital's board of directors also granted Palmer the title of chief executive officer ("CEO"), a title that had never been used during Digital's 35-year existence. Palmer had joined DEC in 1985 to run Semiconductor Engineering and Manufacturing. His relentless campaign to be CEO, and success with the Alpha microprocessor family, made him a candidate to succeed Olsen. At the same time a more modern logo was designed
Palmer restructured Digital into nine business units that reported directly to him. Nonetheless, Digital continued to suffer record losses in recent quarters, including a loss of $260.5 million for the quarter that ended on September 30, 1992. It reported $2.8 billion in losses for its fiscal year 1992. January 5, 1993 saw the retirement of John F. Smith as senior vice president of operations, the second in command at Digital, and his position was not filled. A 35-year company veteran, he had joined Digital in 1958 as the company's 12th employee, passing up a chance to work for Bell Laboratories in New Jersey to work for Digital, then a tiny start-up company in the mill town of Maynard, Massachusetts. Smith rose to become one of the three senior vice presidents in 1987 and was widely considered among the potential successors to Ken Olsen, especially when Smith was appointed chief operating officer in 1991. Smith became a corporate spokesman on financial issues, and had filled in at trouble spots for which Olsen ordered more attention. However Smith was passed over in favor of Palmer when Olsen was forced to resign in July 1992, though Smith stayed on for a time to help turn around the struggling company.
In June 1993, Palmer and several of his top lieutenants presented their reorganization plans to applause from the board of directors, and several weeks later Digital reported its first profitable quarter in several years. However, on April 15, 1994, Digital reported a loss of $183 million—three to four times higher than the loss many people on Wall Street had predicted (compared with a loss of $30 million in the comparable period a year earlier), causing the stock price on the NYSE to plunge $5.875 to $23, a 20% drop. The losses at that point totaled $339 million for the current fiscal year. Sales of the VAX, long the company's biggest moneymaker, continued to decline, which in turn also hurt Digital's lucrative service and maintenance business (this made up more than a third of Digital Equipment's revenue of $14 billion in the 1993 fiscal year), which declined 11% year over year to $1.5 billion in the most recent quarter. 
Market acceptance of Digital Alpha computers and chips had been slower than the company had hoped, even though Alpha's sales for the quarter estimated at $275 million were up significantly from $165 million in the December quarter. Digital had also made a strong push into personal computers and workstations, which had even lower margins than Alpha computers and chips. Also, Digital was playing catchup with its own Unix offerings for client-server networks, as it long emphasized its own VMS software, while corporate computer users based their client-server networks on the industry-standard Unix software (of which Hewlett Packard was one of the market leaders). Digital's problems were similar to that of larger rival IBM, due to the fundamental shift in the computer industry that made it unlikely that Digital could ever again operate profitably at its former size of 120,000 employees, and while its workforce had been reduced to 92,000 people many analysts expected that they would have to cut another 20,000.
During the profitable years up until the early 1990s, DEC was a company that boasted that it never had a general layoff. Following the 1992 economic downturn, layoffs became regular events as the company continually downsized to try to stay afloat. Palmer was tasked with the goal of bringing DEC back to profitability, which he attempted to do by changing the established DEC business culture, hiring new executives from outside the company, and selling off various non-core business units:
By 1997, Digital had subsidiary companies in more than two dozen countries including Austria, Australia, Belgium, Brazil, Canada, China (People's Republic), Colombia, Cyprus, Czech Republic, Denmark, Finland, France, Germany, Ireland, Israel, Japan, Jersey States, New Zealand, Netherlands, Norway, Russia, Singapore, Spain, Sweden, Switzerland, Taiwan, and the United Kingdom.
Eventually, on 26 January 1998, what remained of the company (including Digital's multivendor global services organization and customer support centers) was sold to PC manufacturer Compaq in what was the largest merger up to that time in the computer industry. Several years earlier, Compaq had considered a bid for Digital but became seriously interested only after Digital's major divestments and refocusing on the Internet in 1997. At the time of Compaq's acquisition announcement, Digital had a total of 53,500 employees, down from a peak of 130,000 in the 1980s, but it still employed about 65% more people than Compaq to produce about half the volume of sales revenues. After the merger closed, Compaq moved aggressively to reduce Digital’s high selling, general, and administrative (SG&A) costs (equal to 24% of total 1997 revenues) and bring them more in line with Compaq’s SG&A expense ratio of 12% of revenues.
Compaq used the acquisition to move into enterprise services and compete with IBM, and by 2001 services made up over 20% of Compaq's revenues, largely due to the Digital employees inherited from the merger. Digital's own PC manufacturing was discontinued after the merger closed. As Compaq did not wish to compete with one of its key partner suppliers, the remainder of Digital Semiconductor (the Alpha microprocessor group) was sold to Intel, which placed those employees back in their Hudson (Massachusetts) office, which they had vacated when the site was sold to Intel in 1997.
Compaq struggled as a result of the merger with Digital, and was acquired by Hewlett-Packard in 2002. Compaq, and later HP, continued to sell many of the former Digital products but re-branded with their own logos. For example, HP now sells what were formerly Digital's StorageWorks disk/tape products, as a result of the Compaq acquisition.
The Digital logo survived for a while after the company ceased to exist, as the logo of Digital GlobalSoft, an IT services company in India (which was a 51% subsidiary of Compaq). Digital GlobalSoft was later renamed "HP GlobalSoft" (also known as the "HP Global Delivery India Center" or HP GDIC), and no longer uses the Digital logo.
Digital once held the Class A IP address block 16.0.0.0/8.
Digital Federal Credit Union (DCU), which was chartered in 1979 for employees of DEC, is now open to essentially everyone. DCU has over 700 different sponsors, including the companies that acquired pieces of DEC.
Research.
DEC's Research Laboratories (or Research Labs, as they were commonly known) conducted Digital's corporate research. Some of them were operated by Compaq and are still operated by Hewlett-Packard. The laboratories were:
Some of the former employees of Digital's Research Labs or Digital's R&D in general include:
Some of the former employees of Digital Equipment Corp who were responsible for developing Alpha and StrongARM
Some of the work of the Research Labs was published in the "Digital Technical Journal", which was in published from 1985 until 1998.
Accomplishments.
Digital supported the ANSI standards, especially the ASCII character set, which survives in Unicode and the ISO 8859 character set family. Digital's own Multinational Character Set also had a large influence on ISO 8859-1 (Latin-1) and, by extension, Unicode .
The first versions of the C language and the Unix operating system ran on Digital's PDP series of computers (first on a PDP-7, then the PDP-11's), which were among the first commercially viable minicomputers, although for several years Digital itself did not encourage the use of Unix.
Digital produced widely used and influential interactive operating systems, including OS-8, TOPS-10, TOPS-20, RSTS/E, RSX-11, RT-11, and OpenVMS. PDP computers, in particular the PDP-11 model, inspired a generation of programmers and software developers. Some PDP-11 systems more than 25 years old (software and hardware) are still being used to control and monitor factories, transportation systems and nuclear plants. Digital was an early champion of time-sharing systems.
The command-line interfaces found in Digital's systems, eventually codified as DCL, would look familiar to any user of modern microcomputer CLIs; those used in earlier systems, such as CTSS, IBM's JCL, or Univac's time-sharing systems, would look utterly alien. Many features of the CP/M and MS-DOS CLI show a recognizable family resemblance to Digital's OSes, including command names such as DIR and HELP and the "name-dot-extension" file naming conventions.
VAX and MicroVAX computers (very widespread in the 1980s) running VMS formed one of the most important proprietary networks, DECnet, which linked business and research facilities. The DECnet protocols formed one of the first peer-to-peer networking standards, with DECnet phase I being released in the mid-1970s. Email, file sharing, and distributed collaborative projects existed within the company long before their value was recognized in the market.
Digital, Intel and Xerox through their collaboration to create the DIX standard, were champions of Ethernet, but Digital is the company that made Ethernet commercially successful. Initially, Ethernet-based DECnet and LAT protocols interconnected VAXes with DECserver terminal servers. Starting with the Unibus to Ethernet adapter, multiple generations of Ethernet hardware from Digital were the de facto standard. The CI "computer interconnect" adapter was the industry's first network interface controller to use separate transmit and receive "rings".
Digital also invented clustering, an operating system technology that treated multiple machines as one logical entity. Clustering permitted sharing of pooled disk and tape storage via the HSC50/70/90 and later series of Hierarchical Storage Controllers (HSC). The HSCs delivered the first hardware RAID 0 and RAID 1 capabilities and the first serial interconnects of multiple storage technologies. This technology was the forerunner to architectures such as Network of Workstations which are used for massively cooperative tasks such as web-searches and drug research.
The LA36 and LA120 dot matrix printers became industry standards and may have hastened the demise of the Teletype Corporation.
The VT100 computer terminal became the industry standard, implementing a useful subset of the ANSI X3.64 standard, and even today terminal emulators such as HyperTerminal, PuTTY and Xterm still emulate a VT100 (or its more capable successor, the VT220).
The X Window System, the network transparent window system used on UNIX and Linux, and also available on other operating systems, was developed at MIT jointly between Project Athena and the Laboratory for Computer Science. Digital was the primary sponsor for this project, which was a contemporary of the GNU Project but not associated with it.
Microsoft was not exclusively bound to the Alpha chip so it pursued other processor makers such as IBM with the PowerPC architecture and eventually capitalized on the emerging strength of the Intel x86 based processors.
Notes-11 and its follow-on product, VAX Notes, were two of the first examples of online collaboration software, a category that has become to be known as groupware. Len Kawell, one of the original Notes-11 developers later joined Lotus Development Corporation and contributed to their Lotus Notes product.
Digital was one of the first businesses connected to the Internet, with "dec.com", registered in 1985, being one of the first of the now ubiquitous ".com" domains. DEC's "gatekeeper.dec.com" was a well-known software repository during the pre-World Wide Web days, and Digital was also the first computer vendor to open a public website, on 1 October 1993. The popular AltaVista, created by Digital, was one of the first comprehensive Internet search engines. (Although Lycos was earlier, it was much more limited.)
DEC invented Digital Linear Tape (DLT), formerly known as CompacTape, which began as a compact backup medium for MicroVAX systems, and later grew to capacities of 800 gigabytes.
Work on the first hard-disk-based MP3-player, the Personal Jukebox, started at the DEC Systems Research Center. (The project was started about a month before the merger into Compaq was completed.)
DEC's Western Research Lab created the Itsy Pocket Computer. This was developed into the Compaq iPaq line of PDAs, which replaced the Compaq Aero PDA.
User organizations.
Originally the users' group was called DECUS (Digital Equipment Computer User Society) during the 1960s to 1990s. When Compaq acquired Digital in 1998, the users group was renamed CUO, the Compaq Users' Organisation. When HP acquired Compaq in 2002, CUO became HP-Interex, although there are still DECUS groups in several countries. In the United States, the organization is represented by the Encompass organization; currently Connect.

</doc>
<doc id="7954" url="https://en.wikipedia.org/wiki?curid=7954" title="Dead Kennedys">
Dead Kennedys

Dead Kennedys are an American hardcore punk band formed in San Francisco, California in 1978. The band was one of the first American hardcore bands to make a significant impact in the United Kingdom. 
Dead Kennedys released four studio albums and one EP before disbanding in 1986. Since the band's dissolution, vocalist Jello Biafra has continued to collaborate and record with other artists. In 2001, the band reformed without Biafra, who had been in an acrimonious legal dispute with the remaining members over royalties.
History.
Formation of the band (1978).
Dead Kennedys formed in June 1978 in San Francisco, California, when East Bay Ray (Raymond Pepperell) advertised for bandmates in the newspaper "The Recycler", after seeing a ska-punk show at Mabuhay Gardens in San Francisco. The original band lineup consisted of Jello Biafra (Eric Reed Boucher) on vocals, East Bay Ray on guitar, Klaus Flouride (Geoffrey Lyall) on bass, and Ted (Bruce Slesinger) on drums and percussion. This lineup recorded their first demos. In early to mid July, the band recruited 6025 (Carlos Cadona) as a rhythm guitarist. Their first show was on July 19, 1978, at the Mabuhay Gardens in San Francisco, California.
Dead Kennedys played numerous shows at local venues afterwards. Due to the provocative name of the band, they sometimes played under pseudonyms, including "The DK's", "The Sharks", "The Creamsicles" and "The Pink Twinkies". The band's real name generated controversy. "San Francisco Chronicle" columnist Herb Caen wrote in November 1978, "Just when you think tastelessness has reached its nadir, along comes a punk rock group called The Dead Kennedys, which will play at Mabuhay Gardens on Nov. 22, the 15th anniversary of John F. Kennedy's assassination." Despite mounting protests, the owner of Mabuhay declared, "I can't cancel them NOW—there's a contract. Not, apparently, the kind of contract some people have in mind." However, despite popular belief, the name was not meant to insult the Kennedy family, but according to Biafra, "to bring attention to the end of the American Dream".
6025 left the band in March 1979 under somewhat unclear circumstances, generally considered to be musical differences. In June, the band released their first single, "California Über Alles", on Biafra and East Bay Ray's independent label, Alternative Tentacles. The band followed with a poorly attended East Coast tour, being a new and fairly unknown band at the time, without a full album release.
Bay Area Music Awards show (1980).
On March 25, 1980, Dead Kennedys were invited to perform at the Bay Area Music Awards in San Francisco to major record label artists to give the event some "new wave credibility", in the words of the organizers. The day of the performance was spent practicing the song they were asked to play, the underground hit, "California Über Alles". The band became the talking point of the ceremony when after about 15 seconds into the song, Biafra stopped the band—in a manner reminiscent of Elvis Costello's "Saturday Night Live" appearance—and said, "Hold it! We've gotta prove that we're adults now. We're not a punk rock band, we're a new wave band."
The band, who all wore white shirts with a big, black S painted on the front, pulled black ties from around the backs of their necks to form a dollar sign, then started playing a new song titled "Pull My Strings", a barbed, satirical attack on the ethics of the mainstream music industry, which contained the lyrics, "Is my cock big enough, is my brain small enough, for you to make me a star?". The song also referenced The Knack's song "My Sharona". "Pull My Strings" was never recorded for a studio release, though the performance at the Bay Area Music Awards, which was the only time the song was ever performed, was released on the band's compilation album "Give Me Convenience or Give Me Death".
"Holiday in Cambodia" and "Fresh Fruit for Rotting Vegetables" (1980–1981).
In early 1980, they recorded and released the single "Holiday in Cambodia". In June, the band recorded their debut album, "Fresh Fruit for Rotting Vegetables", released in September of that year. The album reached number 33 on the UK Albums Chart. In January 1981, Ted announced that he wanted to leave to pursue a career in architecture and would help look for a replacement. He played his last concert in February 1981. His replacement was D.H. Peligro (Darren Henley).
Around the same time, East Bay Ray had tried to pressure the rest of the band to sign to the major record label Polydor Records; Biafra stated that he was prepared to leave the group if the rest of the band wanted to sign to the label, though East Bay Ray asserts that he recommended against signing with Polydor. Polydor decided not to sign the band after they learned that Dead Kennedys' next single was to be entitled "Too Drunk to Fuck".
When "Too Drunk to Fuck" came out in May 1981, the song caused much controversy in the UK as the BBC feared the single would reach the Top 30; this would require a mention of the song on "Top of the Pops". It was never played although it was called "'Too Drunk' by the Kennedys" by presenter Tony Blackburn.
"In God We Trust, Inc.", "Plastic Surgery Disasters" and Alternative Tentacles Records (1981–1985).
After Peligro joined the band, the extended play "In God We Trust, Inc." (1981) saw them move toward a more aggressive hardcore/thrash sound. In addition to the EP's controversial artwork depicting a gold Christ figure on a cross of dollar bills, the lyrics contained Biafra's most biting social and political commentary yet, and songs such as "Moral Majority", "Nazi Punks Fuck Off!" and "We've Got a Bigger Problem Now" placed Dead Kennedys as the spokesmen of social protest, while "Dog Bite", a cover version of "Rawhide" and various joke introductions showed a much more whimsical side. In 1982, they released their second studio album, "Plastic Surgery Disasters". The album's cover features a withered starving African child's hand being held and dwarfed by a white man's hand, a picture that had won the World Press Photo award in 1980, taken in Karamoja district in Uganda by Mike Wells.
The band's music had evolved considerably in a short time, moving away from hardcore formulae toward a more innovative jazz-informed style, featuring musicianship and dynamics far beyond other bands in the genre (thus effectively removing the music from that genre). By now the group had become a de facto political force, pitting itself against rising elements of American social and political life such as the religious right, Ronald Reagan and the idle rich. The band continued touring all over the United States, as well as Europe and Australia, and gained a large underground following. While they continued to play live shows during 1983 and 1984, they took a break from releasing new records to concentrate on the Alternative Tentacles record label, which would become synonymous with DIY alternative culture. The band continued to write and perform new material during this time, which would appear on their next album (some of these early performances can be seen in the "DMPO's on Broadway" video, originally released by Dirk Dirksen and later reissued on Rhino).
"Frankenchrist" and obscenity trial (1985–1986).
The release of the album "Frankenchrist" in 1985 showed the band had grown in musical proficiency and lyrical maturity. While there were still a number of loud/fast songs, much of the music featured an eclectic mix of instruments including trumpets and synthesizers. Around this time Klaus Flouride released the similarly experimental solo EP "Cha Cha Cha With Mr. Flouride". Lyrically, the band continued their trademark social commentary, with songs such as "MTV Get Off The Air" and "Jock-O-Rama (Invasion of the Beef Patrol)" poking fun at mainstream America.
However, the controversy that erupted over H.R. Giger's "Penis Landscape", included as an insert with the album, dwarfed the notoriety of its music. The artwork caused a furor with the newly formed Parents Music Resource Center (PMRC). In December 1985 a teenage girl purchased the album at the Wherehouse Records store in Los Angeles County. The girl's mother wrote letters of complaint to the California Attorney General and to Los Angeles prosecutors. In 1986 members of the band, along with other parties involved in the distribution of "Frankenchrist", were charged criminally with distribution of harmful matter to minors. The store where the teen actually purchased the album was never named in the lawsuit. The criminal charges focused on an illustration by H.R. Giger, titled "Work 219: Landscape XX" (also known as "Penis Landscape"). Included as a poster with the album, "Penis Landscape" depicts nine copulating penises and vaginas.
Members of the band and others in the chain of distribution were charged with violating the California Penal Code on a misdemeanor charge carrying a maximum penalty of up to a year in county jail and a base fine of up to $2,000. Biafra says that during this time government agents invaded and searched his home. The prosecution tried to present the poster to the jury in isolation for consideration as obscene material, but Judge Susan Isacoff ruled that the poster must be considered along with the music and lyrics. The charges against three of the original defendants, Ruth Schwartz (owner of Mordam Records), Steve Boudreau (a distributor involved in supplying "Frankenchrist" to the Los Angeles Wherehouse store), and Salvatore Alberti (owner of the factory where the record was pressed), were dismissed for lack of evidence.
In August 1987, the case went to the jury with two remaining defendants: Jello Biafra and Michael Bonanno (former Alternative Tentacles label manager). However, the criminal trial ended with a hung jury, split 7 to 5 in favor of acquittal. District Attorneys Michael Guarino and Ira Riener made a motion for a retrial which was denied by Judge Isacoff, Superior Court Judge for the County of Los Angeles. The album, however, was banned from many record stores nationwide.
Jello Biafra brought up the court case after the break-up of the band on The Oprah Winfrey Show. Biafra was on the show with Tipper Gore as part of a panel discussion on the issues of "controversial music lyrics" and censorship.
"Bedtime for Democracy" and break-up (1986).
In addition to the obscenity lawsuit and being ignored by the mainstream media (MTV and most radio stations gave such groups scant notice, not to mention airplay), the band became increasingly disillusioned with the underground scene as well. The hardcore scene, which had been a haven for free-thinking intellectuals and downtrodden nonconformists, was attracting a more violent audience that imposed an increasing level of brutality on other concertgoers and began to alienate many of the bands and individuals who had helped pioneer the movement in the early 1980s. In earlier years the band had criticized neo-Nazi skinheads for trying to ruin the punk scene, but just as big a problem was the popularity of increasingly macho hardcore bands, which brought the group (and their genre) an audience that had little to do with the ideas/ideals they stood for. In January 1986, frustrated and alienated from their own scene, the Dead Kennedys decided to break up to pursue other interests and played their last concert on February 21. The band continued to work on songs, with Biafra penning songs such as "Chickenshit Conformist" and "Anarchy for Sale", which articulated their feelings about the "dumbing down" of punk rock.
During the summer they recorded these songs for their final album, "Bedtime for Democracy", which was released in November. The artwork, depicting a defaced Statue of Liberty overrun with Nazis, media, opportunists, Klan members, corrupt government officials, and religious zombies, echoed the idea that neither America itself or the punk scene were safe havens anymore for "your tired, your poor, your huddled masses yearning to breathe free". The album contains a number of fast/short songs interspersed with jazz ("D.M.S.O."), spoken word ("A Commercial") and psychedelia ("Cesspools In Eden"). The lyrical focus is more introspective and earnest ("Where Do Ya Draw The Line?"), with an anti-war, anti-violence ("Rambozo The Clown") bent, moving away from the violent imagery of their early records, while remaining as subversive as ever ("I Spy", "D.M.S.O."). In December, the band announced their split. Biafra went on to speak about his political beliefs on numerous television shows and he released a number of spoken-word albums. Ray, Flouride, and Peligro also went on to solo careers.
Legal conflicts.
Lawsuits over royalty payments.
In the late 1990s, former band members discovered problems with the amount of payments which each band member had received from their record label Alternative Tentacles. Former band members claimed that Jello Biafra had conspired to pay lower royalty rates to the band members. Although both sides agreed that the failure to pay these royalties was an accounting mistake, they were upset that Biafra failed to inform the band of the mistake after he and his co-workers discovered it.
Biafra claims that their lawyers had told him only to correspond through lawyers and not directly with the band, as the conflict over payment had apparently arisen before the accounting mistake was discovered. Both sides claim they attempted to resolve the matter without legal action, but the ultimately complicated legal dispute (involving royalties, publishing rights, and a number of other issues) soon led to the courts, where Biafra was found liable for the royalties after the jury determined that he had committed fraud and malice, and was ordered to pay damages of nearly $200,000, including $20,000 in punitive damages, to the band members.
Malice was defined for the jury as "conduct which is intended to cause injury or despicable conduct which is carried with a willful and conscious disregard for the rights of others". Biafra's appeal was denied; he had to pay the outstanding royalties and punitive damages, and was forced to hand over the rights to the majority of Dead Kennedys' back catalogue to the Decay Music partnership.
The jury and judges also noted, in their words, that Biafra “lacked credibility” on the song-writing issue and found from evidence presented by both sides that the song-writing credits were due to the entire band, using a clause in the band's written partnership giving a small share of every Dead Kennedys song royalty directly to the band partnership.
Biafra had received sole songwriting credit for most Dead Kennedys songs on all released albums for the last 20 years or so without complaints from the band, though a minority of songs had given credit to certain group members or the entire band as a whole, indicating a system designed to reflect the primary composers rather than a regimented system like the Jagger/Richards partnership; today, most Kennedys reissues list the songwriters as "Biafra, Dead Kennedys", indicating Biafra's lyrical contributions—which the band doesn't dispute, or else simply as "Dead Kennedys"). Ray, Flouride and Peligro found new distribution through another label, Manifesto Records.
This dispute was hotly contested by all concerned who felt passionately for their cause, and the case caused minor waves within punk circles. Biafra claims that guitarist East Bay Ray had long expressed displeasure with Alternative Tentacles and with the amount of money he received from them, thus the original incentive for the discovery of the back payments. It was found out that Alternative Tentacles was paying Dead Kennedys less per CD than all the other bands, including Biafra himself, and not informing his other bandmates, which was the fraud. Biafra accused the band of wanting to license the famous Dead Kennedys song "Holiday in Cambodia" for use in a Levi's jeans commercial, which the band denied.
Biafra apparently pushed this issue in court, although there was no hard evidence and the jurors were apparently unconcerned with corporate use of independently produced political music. Biafra would later complain that the jury was not sympathetic toward underground music and punk culture. The song never appeared in a Levi's commercial, although in interviews Biafra described the situation surrounding the commercial in detail and was able to give specifics about the advertisement, including the name of the advertising agency that had created the commercial's script.
Biafra's former bandmates maintain that they sued because of Jello Biafra's deliberate withholding of money, though when pressed they have acknowledged that the payment was an accounting mistake, but insist that Biafra was wrong in failing to inform the band directly. Details about this issue remain scarce. The band also maintains that the Levi's story was completely fictitious and invented by Biafra to discredit them. Ultimately, these issues have led to a souring of relationships with the erstwhile bandmates, who still have not resolved their personal differences as of 2012.
Disputes over new commercial activities.
Matters were stirred up even further when the three bandmates invited Jello Biafra to "bury the hatchet" in the form of a band reunion. Jello Biafra felt it was unprofessional because no one contacted him directly. In addition, Biafra was disdainful of the reunion, and having long expressed his disdain for nostalgia and rock reunion/oldies tours in particular, argued that the whole affair was motivated by greed.
Several DVDs, re-issues, and live albums have been released since the departure of Biafra most recently on Manifesto Records. According to Biafra, the live albums are "cash-ins" on Dead Kennedys' name and his music. Biafra also accused the releases of the new live material of having poor sound quality. Furthermore he has stated he is not receiving any royalties from the sale of any Manifesto Records releases. Consequently, he has discouraged fans from buying any Dead Kennedy reissues. The other band members denied Biafra's accusations regarding the live releases, and have defended the mixes as an effort of hard work. Biafra dismissed the new group as "the world's greediest karaoke band." Nevertheless, in 2003, Klaus Flouride said of performances without the band's former frontman: "There hasn't been a show yet that people didn't really like."
Biafra further criticized them for advertising shows using his own image taken from the original 1980s incarnation of the band, which he labeled as false advertising. He attacked the reformed Dead Kennedys in a song called "Those Dumb Punk Kids (Will Buy Anything)", which appears on his second collaboration with sludge metal band The Melvins, "Sieg Howdy!".
Biafra told an audience at a speaking gig in Trenton, New Jersey, that the remaining Dead Kennedys have licensed their single "Too Drunk to Fuck" to be used in a rape scene in a Robert Rodriguez movie. The reference is to a lounge cover of the song, recorded by the band Nouvelle Vague, played during a scene in the "Planet Terror" segment of "Grindhouse", although no rape takes place, and in fact the would-be rapist is killed by the would-be victim. The scene in "Planet Terror" has would-be rapist, "Rapist No. 1" (Quentin Tarantino) order one-legged stripper "Cherry Darlin" (Rose McGowan) to get up off the floor and dance. At this point Tarantino hits play on a cassette recorder and Nouvelle Vague's cover of "Too Drunk To Fuck" plays.
Jello, clearly disapproving of the situation, later wrote, "This is their lowest point since Levi's… This goes against everything the Dead Kennedys stands for in spades… The terrified woman later 'wins' by killing Tarantino, but that excuse does not rescue this at all. I wrote every note of that song and this is not what it was meant for…. Some people will do anything for money. I can't help but think back to how prudish Klaus Flouride was when he objected to H.R. Giger's painting on the "Frankenchrist" (sic) poster, saying he couldn't bear to show it to his parents. I'd sure love to be a fly on the wall when he tries to explain putting a song in a rape scene for money to his teenage daughter… The deal was pushed through by a new business manager the other three hired."
Reforming of new band line-up.
The reformed Dead Kennedys followed their court victory by announcing a number of tour dates, releasing reissues of all Dead Kennedys albums (except "Fresh Fruit for Rotting Vegetables", to which they did not have the rights until 2005), releasing several new archival concert DVDs, and licensing several songs to "The Manchurian Candidate" remake and the Tony Hawk Pro Skater video game. East Bay Ray claims he received a fax from Alternative Tentacles purporting Biafra approved the licensing for the game, which Biafra denies happening.
The band claims on their website that they still pay close attention to an anti-corporate ideology, despite performing on September 5, 2003 at a festival in Turkey that was sponsored by Coca-Cola, noting that they have since pulled out of a show in Los Angeles when they found that it was being sponsored by Coors. However, Biafra claims the above mentioned licensing deals prove otherwise. Some have found difficulty reconciling this claim when Biafra also licensed to major corporations, approving with the other band members use of Dead Kennedys’ songs in major studio film releases such as "Neighbors", "Freddy Got Fingered", and "Fear and Loathing in Las Vegas".
In 2001, Ray, Peligro, and Flouride chose former Dr. Know singer Brandon Cruz to replace Biafra on vocals. The band played under the name "DK Kennedys" for a few concerts, but later reverted to "Dead Kennedys" permanently. They played across the continental United States, Europe, Asia, South America, and Russia. Brandon Cruz left the band in May 2003 and was replaced by Jeff Penalty. The band has released two live albums of archival performances on Manifesto Records: "Mutiny on the Bay", compiled from various live shows including a recording from their last show with Biafra in 1986, and "Live at the Deaf Club", a recording of a 1979 performance at the Deaf Club in San Francisco which was greeted with more enthusiasm.
On October 9, 2007, a best of album titled "Milking the Sacred Cow" was released. It includes two previously unreleased live versions of "Soup Is Good Food" and "Jock-O-Rama", originally found on "Frankenchrist".
Jeff Penalty left the band in March 2008 in what he describes as a "not amicable split." He was replaced by former Wynona Riders singer Ron "Skip" Greer. D.H. also left the band to "take some personal time off". He was replaced for a tour by Translator drummer Dave Scheff.
Break from touring.
On August 21, 2008, the band announced an extended break from touring due to the health-related issues of Flouride and Peligro. They stated their plans to collaborate on new projects. The band performed a gig in Santa Rosa, California in June 2009, with Peligro returning to the drum kit.
In August 2010, the Dead Kennedys announced plans for a short East Coast tour. The lineup assembled for this tour contained East Bay Ray, Peligro, Greer, and bassist Greg Reeves replacing Flouride, who was taking "personal time off" from the band. The tour dates included performances in Philadelphia, New York City, Boston, Washington, D.C., Portland, Maine and Hawaii.
Back on the road.
Dead Kennedys had world tours in 2013 and in 2014, the latter mostly in North American cities.
Reworked material.
The band has played a reworked version of their song "MTV Get Off the Air", re-titled "MP3 Get Off the Web", with lyrics criticizing music piracy during their October 16, 2010, concert at the Rock and Roll Hotel in Washington, D.C..
"DK" logo.
The original logo was created by Winston Smith. He later contributed artwork for the covers of "In God We Trust, Inc.", "Plastic Surgery Disasters", "Frankenchrist", "Bedtime for Democracy", "Give Me Convenience or Give Me Death", the back cover of the "Kill the Poor" single and the Alternative Tentacles logo. When asked about the "DK" logo in an interview, Jello Biafra explained, "...I wanted to make sure it was something simple and easy to spray-paint so people would graffiti it all over the place, and then I showed it to Winston Smith. He played around with it, came back with a bunch of designs that had the circle and slightly 3-D looking letters and he had ones with different patterns behind it. I liked the one with bricks, but ultimately I thought simple red behind it was the boldest and the best."
Lyrics.
Dead Kennedys were noted for the harshness of their lyrics, which generally combined biting Juvenalian social satire while expressing a staunchly left-wing view of contemporary America. Unlike other leftist punk bands who use more direct sloganeering, Dead Kennedys' lyrics were often snide. For example, "Holiday in Cambodia" is a multi-layered satire targeting both yuppies and Cambodia's recently deposed Khmer Rouge regime.
Members.
Current members
Former members

</doc>
<doc id="7955" url="https://en.wikipedia.org/wiki?curid=7955" title="DNA">
DNA

Deoxyribonucleic acid (; DNA) is a molecule that carries most of the genetic instructions used in the development, functioning and reproduction of all known living organisms and many viruses. DNA is a nucleic acid; alongside proteins and carbohydrates, nucleic acids compose the three major macromolecules essential for all known forms of life. Most DNA molecules consist of two biopolymer strands coiled around each other to form a double helix. The two DNA strands are known as polynucleotides since they are composed of simpler units called nucleotides. Each nucleotide is composed of a nitrogen-containing nucleobase—either cytosine (C), guanine (G), adenine (A), or thymine (T)—as well as a monosaccharide sugar called deoxyribose and a phosphate group. The nucleotides are joined to one another in a chain by covalent bonds between the sugar of one nucleotide and the phosphate of the next, resulting in an alternating sugar-phosphate backbone. According to base pairing rules (A with T, and C with G), hydrogen bonds bind the nitrogenous bases of the two separate polynucleotide strands to make double-stranded DNA. The total amount of related DNA base pairs on Earth is estimated at 5.0 x 10, and weighs 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon).
DNA stores biological information. The DNA backbone is resistant to cleavage, and both strands of the double-stranded structure store the same biological information. Biological information is replicated as the two strands are separated. A significant portion of DNA (more than 98% for humans) is non-coding, meaning that these sections do not serve as patterns for protein sequences.
The two strands of DNA run in opposite directions to each other and are therefore anti-parallel. Attached to each sugar is one of four types of nucleobases (informally, "bases"). It is the sequence of these four nucleobases along the backbone that encodes biological information. Under the genetic code, RNA strands are translated to specify the sequence of amino acids within proteins. These RNA strands are initially created using DNA strands as a template in a process called transcription.
Within cells, DNA is organized into long structures called chromosomes. During cell division these chromosomes are duplicated in the process of DNA replication, providing each cell its own complete set of chromosomes. Eukaryotic organisms (animals, plants, fungi, and protists) store most of their DNA inside the cell nucleus and some of their DNA in organelles, such as mitochondria or chloroplasts. In contrast, prokaryotes (bacteria and archaea) store their DNA only in the cytoplasm. Within the chromosomes, chromatin proteins such as histones compact and organize DNA. These compact structures guide the interactions between DNA and other proteins, helping control which parts of the DNA are transcribed.
DNA was first isolated by Friedrich Miescher in 1869. Its molecular structure was identified by James Watson and Francis Crick in 1953, whose model-building efforts were guided by X-ray diffraction data acquired by Rosalind Franklin. DNA is used by researchers as a molecular tool to explore physical laws and theories, such as the ergodic theorem and the theory of elasticity. The unique material properties of DNA have made it an attractive molecule for material scientists and engineers interested in micro- and nano-fabrication. Among notable advances in this field are DNA origami and DNA-based hybrid materials.
Properties.
DNA is a long polymer made from repeating units called nucleotides. DNA was first identified and isolated by Friedrich Miescher in 1869 at the University of Tübingen, a substance he called "nuclein", and the double helix structure of DNA was first discovered in 1953 by Watson and Crick at the University of Cambridge, using experimental data collected by Rosalind Franklin and Maurice Wilkins. The structure of DNA is non-static, all species comprises two helical chains each coiled round the same axis, and each with a pitch of 34 ångströms (3.4 nanometres) and a radius of 10 ångströms (1.0 nanometre). According to another study, when measured in a particular solution, the DNA chain measured 22 to 26 ångströms wide (2.2 to 2.6 nanometres), and one nucleotide unit measured 3.3 Å (0.33 nm) long. Although each individual repeating unit is very small, DNA polymers can be very large molecules containing millions of nucleotides. For instance, the DNA in the largest human chromosome, chromosome number 1, consists of approximately 220 million base pairs and would be 85 mm long if straightened.
In living organisms DNA does not usually exist as a single molecule, but instead as a pair of molecules that are held tightly together. These two long strands entwine like vines, in the shape of a double helix. The nucleotide repeats contain both the segment of the backbone of the molecule, which holds the chain together, and a nucleobase, which interacts with the other DNA strand in the helix. A nucleobase linked to a sugar is called a nucleoside and a base linked to a sugar and one or more phosphate groups is called a nucleotide. A polymer comprising multiple linked nucleotides (as in DNA) is called a polynucleotide.
The backbone of the DNA strand is made from alternating phosphate and sugar residues. The sugar in DNA is 2-deoxyribose, which is a pentose (five-carbon) sugar. The sugars are joined together by phosphate groups that form phosphodiester bonds between the third and fifth carbon atoms of adjacent sugar rings. These asymmetric bonds mean a strand of DNA has a direction. In a double helix the direction of the nucleotides in one strand is opposite to their direction in the other strand: the strands are "antiparallel". The asymmetric ends of DNA strands are called the 5′ ("five prime") and 3′ ("three prime") ends, with the 5′ end having a terminal phosphate group and the 3′ end a terminal hydroxyl group. One major difference between DNA and RNA is the sugar, with the 2-deoxyribose in DNA being replaced by the alternative pentose sugar ribose in RNA.
The DNA double helix is stabilized primarily by two forces: hydrogen bonds between nucleotides and base-stacking interactions among aromatic nucleobases. In the aqueous environment of the cell, the conjugated π bonds of nucleotide bases align perpendicular to the axis of the DNA molecule, minimizing their interaction with the solvation shell and therefore, the Gibbs free energy. The four bases found in DNA are adenine (abbreviated A), cytosine (C), guanine (G) and thymine (T). These four bases are attached to the sugar/phosphate to form the complete nucleotide, as shown for adenosine monophosphate. Adenine pairs with thymine and guanine pairs with cytosine. It was represented by A-T base pairs and G-C base pairs.
Nucleobase classification.
The nucleobases are classified into two types: the purines, A and G, being fused five- and six-membered heterocyclic compounds, and the pyrimidines, the six-membered rings C and T. A fifth pyrimidine nucleobase, uracil (U), usually takes the place of thymine in RNA and differs from thymine by lacking a methyl group on its ring. In addition to RNA and DNA a large number of artificial nucleic acid analogues have also been created to study the properties of nucleic acids, or for use in biotechnology.
Uracil is not usually found in DNA, occurring only as a breakdown product of cytosine. However, in a number of bacteriophages – "Bacillus subtilis" bacteriophages PBS1 and PBS2 and "Yersinia" bacteriophage piR1-37 – thymine has been replaced by uracil. Another phage - Staphylococcal phage S6 - has been identified with a genome where thymine has been replaced by uracil.
Base J (beta-d-glucopyranosyloxymethyluracil), a modified form of uracil, is also found in a number of organisms: the flagellates "Diplonema" and "Euglena", and all the kinetoplastid genera. Biosynthesis of J occurs in two steps: in the first step a specific thymidine in DNA is converted into hydroxymethyldeoxyuridine; in the second HOMedU is glycosylated to form J. Proteins that bind specifically to this base have been identified. These proteins appear to be distant relatives of the Tet1 oncogene that is involved in the pathogenesis of acute myeloid leukemia. J appears to act as a termination signal for RNA polymerase II.
Grooves.
Twin helical strands form the DNA backbone. Another double helix may be found tracing the spaces, or grooves, between the strands. These voids are adjacent to the base pairs and may provide a binding site. As the strands are not symmetrically located with respect to each other, the grooves are unequally sized. One groove, the major groove, is 22  Å wide and the other, the minor groove, is 12 Å wide. The width of the major groove means that the edges of the bases are more accessible in the major groove than in the minor groove. As a result, proteins such as transcription factors that can bind to specific sequences in double-stranded DNA usually make contact with the sides of the bases exposed in the major groove. This situation varies in unusual conformations of DNA within the cell "(see below)", but the major and minor grooves are always named to reflect the differences in size that would be seen if the DNA is twisted back into the ordinary B form.
Base pairing.
In a DNA double helix, each type of nucleobase on one strand bonds with just one type of nucleobase on the other strand. This is called complementary base pairing. Here, purines form hydrogen bonds to pyrimidines, with adenine bonding only to thymine in two hydrogen bonds, and cytosine bonding only to guanine in three hydrogen bonds. This arrangement of two nucleotides binding together across the double helix is called a base pair. As hydrogen bonds are not covalent, they can be broken and rejoined relatively easily. The two strands of DNA in a double helix can therefore be pulled apart like a zipper, either by a mechanical force or high temperature. As a result of this complementarity, all the information in the double-stranded sequence of a DNA helix is duplicated on each strand, which is vital in DNA replication. Indeed, this reversible and specific interaction between complementary base pairs is critical for all the functions of DNA in living organisms.
The two types of base pairs form different numbers of hydrogen bonds, AT forming two hydrogen bonds, and GC forming three hydrogen bonds (see figures, right).
DNA with high GC-content is more stable than DNA with low GC-content.
As noted above, most DNA molecules are actually two polymer strands, bound together in a helical fashion by noncovalent bonds; this double stranded structure (dsDNA) is maintained largely by the intrastrand base stacking interactions, which are strongest for G,C stacks. The two strands can come apart – a process known as melting – to form two single-stranded DNA molecules (ssDNA) molecules. Melting occurs at high temperature, low salt and high pH (low pH also melts DNA, but since DNA is unstable due to acid depurination, low pH is rarely used).
The stability of the dsDNA form depends not only on the GC-content (% G,C basepairs) but also on sequence (since stacking is sequence specific) and also length (longer molecules are more stable). The stability can be measured in various ways; a common way is the "melting temperature", which is the temperature at which 50% of the ds molecules are converted to ss molecules; melting temperature is dependent on ionic strength and the concentration of DNA.
As a result, it is both the percentage of GC base pairs and the overall length of a DNA double helix that determines the strength of the association between the two strands of DNA. Long DNA helices with a high GC-content have stronger-interacting strands, while short helices with high AT content have weaker-interacting strands. In biology, parts of the DNA double helix that need to separate easily, such as the TATAAT Pribnow box in some promoters, tend to have a high AT content, making the strands easier to pull apart.
In the laboratory, the strength of this interaction can be measured by finding the temperature necessary to break the hydrogen bonds, their melting temperature (also called "T" value). When all the base pairs in a DNA double helix melt, the strands separate and exist in solution as two entirely independent molecules. These single-stranded DNA molecules ("ssDNA") have no single common shape, but some conformations are more stable than others.
Sense and antisense.
A DNA sequence is called "sense" if its sequence is the same as that of a messenger RNA copy that is translated into protein. The sequence on the opposite strand is called the "antisense" sequence. Both sense and antisense sequences can exist on different parts of the same strand of DNA (i.e. both strands can contain both sense and antisense sequences). In both prokaryotes and eukaryotes, antisense RNA sequences are produced, but the functions of these RNAs are not entirely clear. One proposal is that antisense RNAs are involved in regulating gene expression through RNA-RNA base pairing.
A few DNA sequences in prokaryotes and eukaryotes, and more in plasmids and viruses, blur the distinction between sense and antisense strands by having overlapping genes. In these cases, some DNA sequences do double duty, encoding one protein when read along one strand, and a second protein when read in the opposite direction along the other strand. In bacteria, this overlap may be involved in the regulation of gene transcription, while in viruses, overlapping genes increase the amount of information that can be encoded within the small viral genome.
Supercoiling.
DNA can be twisted like a rope in a process called DNA supercoiling. With DNA in its "relaxed" state, a strand usually circles the axis of the double helix once every 10.4 base pairs, but if the DNA is twisted the strands become more tightly or more loosely wound. If the DNA is twisted in the direction of the helix, this is positive supercoiling, and the bases are held more tightly together. If they are twisted in the opposite direction, this is negative supercoiling, and the bases come apart more easily. In nature, most DNA has slight negative supercoiling that is introduced by enzymes called topoisomerases. These enzymes are also needed to relieve the twisting stresses introduced into DNA strands during processes such as transcription and DNA replication.
Alternate DNA structures.
DNA exists in many possible conformations that include A-DNA, B-DNA, and Z-DNA forms, although, only B-DNA and Z-DNA have been directly observed in functional organisms. The conformation that DNA adopts depends on the hydration level, DNA sequence, the amount and direction of supercoiling, chemical modifications of the bases, the type and concentration of metal ions, as well as the presence of polyamines in solution.
The first published reports of A-DNA X-ray diffraction patterns—and also B-DNA—used analyses based on Patterson transforms that provided only a limited amount of structural information for oriented fibers of DNA. An alternate analysis was then proposed by Wilkins "et al.", in 1953, for the "in vivo" B-DNA X-ray diffraction/scattering patterns of highly hydrated DNA fibers in terms of squares of Bessel functions. In the same journal, James Watson and Francis Crick presented their molecular modeling analysis of the DNA X-ray diffraction patterns to suggest that the structure was a double-helix.
Although the "B-DNA form" is most common under the conditions found in cells, it is not a well-defined conformation but a family of related DNA conformations that occur at the high hydration levels present in living cells. Their corresponding X-ray diffraction and scattering patterns are characteristic of molecular paracrystals with a significant degree of disorder.
Compared to B-DNA, the A-DNA form is a wider right-handed spiral, with a shallow, wide minor groove and a narrower, deeper major groove. The A form occurs under non-physiological conditions in partially dehydrated samples of DNA, while in the cell it may be produced in hybrid pairings of DNA and RNA strands, as well as in enzyme-DNA complexes. Segments of DNA where the bases have been chemically modified by methylation may undergo a larger change in conformation and adopt the Z form. Here, the strands turn about the helical axis in a left-handed spiral, the opposite of the more common B form. These unusual structures can be recognized by specific Z-DNA binding proteins and may be involved in the regulation of transcription.
Alternative DNA chemistry.
For a number of years exobiologists have proposed the existence of a shadow biosphere, a postulated microbial biosphere of Earth that uses radically different biochemical and molecular processes than currently known life. One of the proposals was the existence of lifeforms that use arsenic instead of phosphorus in DNA. A report in 2010 of the possibility in the bacterium GFAJ-1, was announced, though the research was disputed, and evidence suggests the bacterium actively prevents the incorporation of arsenic into the DNA backbone and other biomolecules.
Quadruplex structures.
At the ends of the linear chromosomes are specialized regions of DNA called telomeres. The main function of these regions is to allow the cell to replicate chromosome ends using the enzyme telomerase, as the enzymes that normally replicate DNA cannot copy the extreme 3′ ends of chromosomes. These specialized chromosome caps also help protect the DNA ends, and stop the DNA repair systems in the cell from treating them as damage to be corrected. In human cells, telomeres are usually lengths of single-stranded DNA containing several thousand repeats of a simple TTAGGG sequence.
These guanine-rich sequences may stabilize chromosome ends by forming structures of stacked sets of four-base units, rather than the usual base pairs found in other DNA molecules. Here, four guanine bases form a flat plate and these flat four-base units then stack on top of each other, to form a stable G-quadruplex structure. These structures are stabilized by hydrogen bonding between the edges of the bases and chelation of a metal ion in the centre of each four-base unit. Other structures can also be formed, with the central set of four bases coming from either a single strand folded around the bases, or several different parallel strands, each contributing one base to the central structure.
In addition to these stacked structures, telomeres also form large loop structures called telomere loops, or T-loops. Here, the single-stranded DNA curls around in a long circle stabilized by telomere-binding proteins. At the very end of the T-loop, the single-stranded telomere DNA is held onto a region of double-stranded DNA by the telomere strand disrupting the double-helical DNA and base pairing to one of the two strands. This triple-stranded structure is called a displacement loop or D-loop.
Branched DNA.
In DNA fraying occurs when non-complementary regions exist at the end of an otherwise complementary double-strand of DNA. However, branched DNA can occur if a third strand of DNA is introduced and contains adjoining regions able to hybridize with the frayed regions of the pre-existing double-strand. Although the simplest example of branched DNA involves only three strands of DNA, complexes involving additional strands and multiple branches are also possible. Branched DNA can be used in nanotechnology to construct geometric shapes, see the section on uses in technology below.
Chemical modifications and altered DNA packaging.
Base modifications and DNA packaging.
The expression of genes is influenced by how the DNA is packaged in chromosomes, in a structure called chromatin. Base modifications can be involved in packaging, with regions that have low or no gene expression usually containing high levels of methylation of cytosine bases. DNA packaging and its influence on gene expression can also occur by covalent modifications of the histone protein core around which DNA is wrapped in the chromatin structure or else by remodeling carried out by chromatin remodeling complexes (see Chromatin remodeling). There is, further, crosstalk between DNA methylation and histone modification, so they can coordinately affect chromatin and gene expression.
For one example, cytosine methylation, produces 5-methylcytosine, which is important for X-chromosome inactivation. The average level of methylation varies between organisms – the worm "Caenorhabditis elegans" lacks cytosine methylation, while vertebrates have higher levels, with up to 1% of their DNA containing 5-methylcytosine. Despite the importance of 5-methylcytosine, it can deaminate to leave a thymine base, so methylated cytosines are particularly prone to mutations. Other base modifications include adenine methylation in bacteria, the presence of 5-hydroxymethylcytosine in the brain, and the glycosylation of uracil to produce the "J-base" in kinetoplastids.
Damage.
DNA can be damaged by many sorts of mutagens, which change the DNA sequence. Mutagens include oxidizing agents, alkylating agents and also high-energy electromagnetic radiation such as ultraviolet light and X-rays. The type of DNA damage produced depends on the type of mutagen. For example, UV light can damage DNA by producing thymine dimers, which are cross-links between pyrimidine bases. On the other hand, oxidants such as free radicals or hydrogen peroxide produce multiple forms of damage, including base modifications, particularly of guanosine, and double-strand breaks. A typical human cell contains about 150,000 bases that have suffered oxidative damage. Of these oxidative lesions, the most dangerous are double-strand breaks, as these are difficult to repair and can produce point mutations, insertions and deletions from the DNA sequence, as well as chromosomal translocations. These mutations can cause cancer. Because of inherent limitations in the DNA repair mechanisms, if humans lived long enough, they would all eventually develop cancer. DNA damages that are naturally occurring, due to normal cellular processes that produce reactive oxygen species, the hydrolytic activities of cellular water, etc., also occur frequently. Although most of these damages are repaired, in any cell some DNA damage may remain despite the action of repair processes. These remaining DNA damages accumulate with age in mammalian postmitotic tissues. This accumulation appears to be an important underlying cause of aging.
Many mutagens fit into the space between two adjacent base pairs, this is called "intercalation". Most intercalators are aromatic and planar molecules; examples include ethidium bromide, acridines, daunomycin, and doxorubicin. For an intercalator to fit between base pairs, the bases must separate, distorting the DNA strands by unwinding of the double helix. This inhibits both transcription and DNA replication, causing toxicity and mutations. As a result, DNA intercalators may be carcinogens, and in the case of thalidomide, a teratogen. Others such as benzo["a"]pyrene diol epoxide and aflatoxin form DNA adducts that induce errors in replication. Nevertheless, due to their ability to inhibit DNA transcription and replication, other similar toxins are also used in chemotherapy to inhibit rapidly growing cancer cells.
Biological functions.
DNA usually occurs as linear chromosomes in eukaryotes, and circular chromosomes in prokaryotes. The set of chromosomes in a cell makes up its genome; the human genome has approximately 3 billion base pairs of DNA arranged into 46 chromosomes. The information carried by DNA is held in the sequence of pieces of DNA called genes. Transmission of genetic information in genes is achieved via complementary base pairing. For example, in transcription, when a cell uses the information in a gene, the DNA sequence is copied into a complementary RNA sequence through the attraction between the DNA and the correct RNA nucleotides. Usually, this RNA copy is then used to make a matching protein sequence in a process called translation, which depends on the same interaction between RNA nucleotides. In alternative fashion, a cell may simply copy its genetic information in a process called DNA replication. The details of these functions are covered in other articles; here the focus is on the interactions between DNA and other molecules that mediate the function of the genome.
Genes and genomes.
Genomic DNA is tightly and orderly packed in the process called DNA condensation to fit the small available volumes of the cell. In eukaryotes, DNA is located in the cell nucleus, as well as small amounts in mitochondria and chloroplasts. In prokaryotes, the DNA is held within an irregularly shaped body in the cytoplasm called the nucleoid. The genetic information in a genome is held within genes, and the complete set of this information in an organism is called its genotype. A gene is a unit of heredity and is a region of DNA that influences a particular characteristic in an organism. Genes contain an open reading frame that can be transcribed, as well as regulatory sequences such as promoters and enhancers, which control the transcription of the open reading frame.
In many species, only a small fraction of the total sequence of the genome encodes protein. For example, only about 1.5% of the human genome consists of protein-coding exons, with over 50% of human DNA consisting of non-coding repetitive sequences. The reasons for the presence of so much noncoding DNA in eukaryotic genomes and the extraordinary differences in genome size, or "C-value", among species represent a long-standing puzzle known as the "C-value enigma". However, some DNA sequences that do not code protein may still encode functional non-coding RNA molecules, which are involved in the regulation of gene expression.
Some noncoding DNA sequences play structural roles in chromosomes. Telomeres and centromeres typically contain few genes, but are important for the function and stability of chromosomes. An abundant form of noncoding DNA in humans are pseudogenes, which are copies of genes that have been disabled by mutation. These sequences are usually just molecular fossils, although they can occasionally serve as raw genetic material for the creation of new genes through the process of gene duplication and divergence.
Transcription and translation.
A gene is a sequence of DNA that contains genetic information and can influence the phenotype of an organism. Within a gene, the sequence of bases along a DNA strand defines a messenger RNA sequence, which then defines one or more protein sequences. The relationship between the nucleotide sequences of genes and the amino-acid sequences of proteins is determined by the rules of translation, known collectively as the genetic code. The genetic code consists of three-letter 'words' called "codons" formed from a sequence of three nucleotides (e.g. ACT, CAG, TTT).
In transcription, the codons of a gene are copied into messenger RNA by RNA polymerase. This RNA copy is then decoded by a ribosome that reads the RNA sequence by base-pairing the messenger RNA to transfer RNA, which carries amino acids. Since there are 4 bases in 3-letter combinations, there are 64 possible codons (4 combinations). These encode the twenty standard amino acids, giving most amino acids more than one possible codon. There are also three 'stop' or 'nonsense' codons signifying the end of the coding region; these are the TAA, TGA, and TAG codons.
Replication.
Cell division is essential for an organism to grow, but, when a cell divides, it must replicate the DNA in its genome so that the two daughter cells have the same genetic information as their parent. The double-stranded structure of DNA provides a simple mechanism for DNA replication. Here, the two strands are separated and then each strand's complementary DNA sequence is recreated by an enzyme called DNA polymerase. This enzyme makes the complementary strand by finding the correct base through complementary base pairing, and bonding it onto the original strand. As DNA polymerases can only extend a DNA strand in a 5′ to 3′ direction, different mechanisms are used to copy the antiparallel strands of the double helix. In this way, the base on the old strand dictates which base appears on the new strand, and the cell ends up with a perfect copy of its DNA.
Extracellular nucleic acids.
Naked extracellular DNA (eDNA), most of it released by cell death, is nearly ubiquitous in the environment. Its concentration in soil may be as high as 2 μg/L, and its concentration in natural aquatic environments may be as high at 88 μg/L. Various possible functions have been proposed for eDNA: it may be involved in horizontal gene transfer; it may provide nutrients; and it may act as a buffer to recruit or titrate ions or antibiotics. Extracellular DNA acts as a functional extracellular matrix component in the biofilms of a number of bacterial species. It may act as a recognition factor to regulate the attachment and dispersal of specific cell types in the biofilm; it may contribute to biofilm formation; and it may contribute to the biofilm's physical strength and resistance to biological stress.
Interactions with proteins.
All the functions of DNA depend on interactions with proteins. These protein interactions can be non-specific, or the protein can bind specifically to a single DNA sequence. Enzymes can also bind to DNA and of these, the polymerases that copy the DNA base sequence in transcription and DNA replication are particularly important.
DNA-binding proteins.
Structural proteins that bind DNA are well-understood examples of non-specific DNA-protein interactions. Within chromosomes, DNA is held in complexes with structural proteins. These proteins organize the DNA into a compact structure called chromatin. In eukaryotes this structure involves DNA binding to a complex of small basic proteins called histones, while in prokaryotes multiple types of proteins are involved. The histones form a disk-shaped complex called a nucleosome, which contains two complete turns of double-stranded DNA wrapped around its surface. These non-specific interactions are formed through basic residues in the histones making ionic bonds to the acidic sugar-phosphate backbone of the DNA, and are therefore largely independent of the base sequence. Chemical modifications of these basic amino acid residues include methylation, phosphorylation and acetylation. These chemical changes alter the strength of the interaction between the DNA and the histones, making the DNA more or less accessible to transcription factors and changing the rate of transcription. Other non-specific DNA-binding proteins in chromatin include the high-mobility group proteins, which bind to bent or distorted DNA. These proteins are important in bending arrays of nucleosomes and arranging them into the larger structures that make up chromosomes.
A distinct group of DNA-binding proteins are the DNA-binding proteins that specifically bind single-stranded DNA. In humans, replication protein A is the best-understood member of this family and is used in processes where the double helix is separated, including DNA replication, recombination and DNA repair. These binding proteins seem to stabilize single-stranded DNA and protect it from forming stem-loops or being degraded by nucleases.
In contrast, other proteins have evolved to bind to particular DNA sequences. The most intensively studied of these are the various transcription factors, which are proteins that regulate transcription. Each transcription factor binds to one particular set of DNA sequences and activates or inhibits the transcription of genes that have these sequences close to their promoters. The transcription factors do this in two ways. Firstly, they can bind the RNA polymerase responsible for transcription, either directly or through other mediator proteins; this locates the polymerase at the promoter and allows it to begin transcription. Alternatively, transcription factors can bind enzymes that modify the histones at the promoter. This changes the accessibility of the DNA template to the polymerase.
As these DNA targets can occur throughout an organism's genome, changes in the activity of one type of transcription factor can affect thousands of genes. Consequently, these proteins are often the targets of the signal transduction processes that control responses to environmental changes or cellular differentiation and development. The specificity of these transcription factors' interactions with DNA come from the proteins making multiple contacts to the edges of the DNA bases, allowing them to "read" the DNA sequence. Most of these base-interactions are made in the major groove, where the bases are most accessible.
DNA-modifying enzymes.
Nucleases and ligases.
Nucleases are enzymes that cut DNA strands by catalyzing the hydrolysis of the phosphodiester bonds. Nucleases that hydrolyse nucleotides from the ends of DNA strands are called exonucleases, while endonucleases cut within strands. The most frequently used nucleases in molecular biology are the restriction endonucleases, which cut DNA at specific sequences. For instance, the EcoRV enzyme shown to the left recognizes the 6-base sequence 5′-GATATC-3′ and makes a cut at the vertical line. In nature, these enzymes protect bacteria against phage infection by digesting the phage DNA when it enters the bacterial cell, acting as part of the restriction modification system. In technology, these sequence-specific nucleases are used in molecular cloning and DNA fingerprinting.
Enzymes called DNA ligases can rejoin cut or broken DNA strands. Ligases are particularly important in lagging strand DNA replication, as they join together the short segments of DNA produced at the replication fork into a complete copy of the DNA template. They are also used in DNA repair and genetic recombination.
Topoisomerases and helicases.
Topoisomerases are enzymes with both nuclease and ligase activity. These proteins change the amount of supercoiling in DNA. Some of these enzymes work by cutting the DNA helix and allowing one section to rotate, thereby reducing its level of supercoiling; the enzyme then seals the DNA break. Other types of these enzymes are capable of cutting one DNA helix and then passing a second strand of DNA through this break, before rejoining the helix. Topoisomerases are required for many processes involving DNA, such as DNA replication and transcription.
Helicases are proteins that are a type of molecular motor. They use the chemical energy in nucleoside triphosphates, predominantly ATP, to break hydrogen bonds between bases and unwind the DNA double helix into single strands. These enzymes are essential for most processes where enzymes need to access the DNA bases.
Polymerases.
Polymerases are enzymes that synthesize polynucleotide chains from nucleoside triphosphates. The sequence of their products are created based on existing polynucleotide chains—which are called "templates". These enzymes function by repeatedly adding a nucleotide to the 3′ hydroxyl group at the end of the growing polynucleotide chain. As a consequence, all polymerases work in a 5′ to 3′ direction. In the active site of these enzymes, the incoming nucleoside triphosphate base-pairs to the template: this allows polymerases to accurately synthesize the complementary strand of their template. Polymerases are classified according to the type of template that they use.
In DNA replication, DNA-dependent DNA polymerases make copies of DNA polynucleotide chains. In order to preserve biological information, it is essential that the sequence of bases in each copy are precisely complementary to the sequence of bases in the template strand. Many DNA polymerases have a proofreading activity. Here, the polymerase recognizes the occasional mistakes in the synthesis reaction by the lack of base pairing between the mismatched nucleotides. If a mismatch is detected, a 3′ to 5′ exonuclease activity is activated and the incorrect base removed. In most organisms, DNA polymerases function in a large complex called the replisome that contains multiple accessory subunits, such as the DNA clamp or helicases.
RNA-dependent DNA polymerases are a specialized class of polymerases that copy the sequence of an RNA strand into DNA. They include reverse transcriptase, which is a viral enzyme involved in the infection of cells by retroviruses, and telomerase, which is required for the replication of telomeres. Telomerase is an unusual polymerase because it contains its own RNA template as part of its structure.
Transcription is carried out by a DNA-dependent RNA polymerase that copies the sequence of a DNA strand into RNA. To begin transcribing a gene, the RNA polymerase binds to a sequence of DNA called a promoter and separates the DNA strands. It then copies the gene sequence into a messenger RNA transcript until it reaches a region of DNA called the terminator, where it halts and detaches from the DNA. As with human DNA-dependent DNA polymerases, RNA polymerase II, the enzyme that transcribes most of the genes in the human genome, operates as part of a large protein complex with multiple regulatory and accessory subunits.
Genetic recombination.
A DNA helix usually does not interact with other segments of DNA, and in human cells the different chromosomes even occupy separate areas in the nucleus called "chromosome territories". This physical separation of different chromosomes is important for the ability of DNA to function as a stable repository for information, as one of the few times chromosomes interact is in chromosomal crossover which occurs during sexual reproduction, when genetic recombination occurs. Chromosomal crossover is when two DNA helices break, swap a section and then rejoin.
Recombination allows chromosomes to exchange genetic information and produces new combinations of genes, which increases the efficiency of natural selection and can be important in the rapid evolution of new proteins. Genetic recombination can also be involved in DNA repair, particularly in the cell's response to double-strand breaks.
The most common form of chromosomal crossover is homologous recombination, where the two chromosomes involved share very similar sequences. Non-homologous recombination can be damaging to cells, as it can produce chromosomal translocations and genetic abnormalities. The recombination reaction is catalyzed by enzymes known as recombinases, such as RAD51. The first step in recombination is a double-stranded break caused by either an endonuclease or damage to the DNA. A series of steps catalyzed in part by the recombinase then leads to joining of the two helices by at least one Holliday junction, in which a segment of a single strand in each helix is annealed to the complementary strand in the other helix. The Holliday junction is a tetrahedral junction structure that can be moved along the pair of chromosomes, swapping one strand for another. The recombination reaction is then halted by cleavage of the junction and re-ligation of the released DNA.
Evolution.
DNA contains the genetic information that allows all modern living things to function, grow and reproduce. However, it is unclear how long in the 4-billion-year history of life DNA has performed this function, as it has been proposed that the earliest forms of life may have used RNA as their genetic material. RNA may have acted as the central part of early cell metabolism as it can both transmit genetic information and carry out catalysis as part of ribozymes. This ancient RNA world where nucleic acid would have been used for both catalysis and genetics may have influenced the evolution of the current genetic code based on four nucleotide bases. This would occur, since the number of different bases in such an organism is a trade-off between a small number of bases increasing replication accuracy and a large number of bases increasing the catalytic efficiency of ribozymes. However, there is no direct evidence of ancient genetic systems, as recovery of DNA from most fossils is impossible because DNA survives in the environment for less than one million years, and slowly degrades into short fragments in solution. Claims for older DNA have been made, most notably a report of the isolation of a viable bacterium from a salt crystal 250 million years old, but these claims are controversial.
Building blocks of DNA (adenine, guanine and related organic molecules) may have been formed extraterrestrially in outer space. Complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have also been formed in the laboratory under conditions mimicking those found in outer space, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the universe, may have been formed in red giants or in interstellar dust and gas clouds.
Uses in technology.
Genetic engineering.
Methods have been developed to purify DNA from organisms, such as phenol-chloroform extraction, and to manipulate it in the laboratory, such as restriction digests and the polymerase chain reaction. Modern biology and biochemistry make intensive use of these techniques in recombinant DNA technology. Recombinant DNA is a man-made DNA sequence that has been assembled from other DNA sequences. They can be transformed into organisms in the form of plasmids or in the appropriate format, by using a viral vector. The genetically modified organisms produced can be used to produce products such as recombinant proteins, used in medical research, or be grown in agriculture.
DNA profiling.
Forensic scientists can use DNA in blood, semen, skin, saliva or hair found at a crime scene to identify a matching DNA of an individual, such as a perpetrator.This process is formally termed DNA profiling, but may also be called "genetic fingerprinting". In DNA profiling, the lengths of variable sections of repetitive DNA, such as short tandem repeats and minisatellites, are compared between people. This method is usually an extremely reliable technique for identifying a matching DNA. However, identification can be complicated if the scene is contaminated with DNA from several people. DNA profiling was developed in 1984 by British geneticist Sir Alec Jeffreys, and first used in forensic science to convict Colin Pitchfork in the 1988 Enderby murders case.
The development of forensic science, and the ability to now obtain genetic matching on minute samples of blood, skin, saliva or hair has led to a re-examination of a number of cases. Forensic scientists can analyze types of DNA: nuclear DNA or mitochondrial DNA; nuclear DNA can individualize evidence, while mitochondrial DNA, or mtDNA, can only classify the maternal inheritance of the sample taken in for analysis. Evidence can now be uncovered that was not scientifically possible at the time of the original examination. Combined with the removal of the double jeopardy law in some places, this can allow cases to be reopened where previous trials have failed to produce sufficient evidence to convince a jury. People charged with serious crimes may be required to provide a sample of DNA for matching purposes. The most obvious defence to DNA matches obtained forensically is to claim that cross-contamination of evidence has taken place. This has resulted in meticulous strict handling procedures with new cases of serious crime.
DNA profiling is also used to identify victims of mass casualty incidents. As well as positively identifying bodies or body parts in serious accidents, DNA profiling is being successfully used to identify individual victims in mass war graves – matching to family members.
DNA profiling is also used in DNA paternity testing in order to determine if someone is the biologicalparent or grandparent of a child with the probability of parentage is typically 99.99% when the alleged parent is biologically related to the child. Normal DNA sequencing methods happen after birth but there are new methods to test paternity while the mother is still pregnant.
DNA enzymes or catalytic DNA.
Deoxyribozymes, also called DNAzymes or catalytic DNA are first discovered in 1994. They are mostly single stranded DNA sequences isolated from a large pool of random DNA sequences through a combinatorial approach called in vitro selection or SELEX. DNAzymes catalyze variety of chemical reactions including RNA/DNA cleavage, RNA/DNA ligation, amino acids phosphorylation/dephosphorylation, carbon-carbon bond formation, and etc. DNAzymes can enhance catalytic rate of chemical reactions up to 100,000,000,000-fold over the uncatalyzed reaction. The most extensively studied class of DNAzymes are RNA-cleaving DNAzymes which have been used in detection of different metal ions and designing therapeutic agents. Several metal-specific DNAzymes have been reported including the GR-5 DNAzyme (lead-specific), the CA1-3 DNAzymes (copper-specific), the 39E DNAzyme (uranyl-specific) and the NaA43 DNAzyme (sodium-specific). The NaA43 DNAzyme, which is reported to be more than 10,000-fold selective for sodium over other metal ions, was used to make a real-time sodium sensor in living cells.
Bioinformatics.
Bioinformatics involves the development of techniques to store, data mine, search and manipulate biological data, including DNA nucleic acid sequence data. These have led to widely applied advances in computer science, especially string searching algorithms, machine learning and database theory. String searching or matching algorithms, which find an occurrence of a sequence of letters inside a larger sequence of letters, were developed to search for specific sequences of nucleotides. The DNA sequence may be aligned with other DNA sequences to identify homologous sequences and locate the specific mutations that make them distinct. These techniques, especially multiple sequence alignment, are used in studying phylogenetic relationships and protein function. Data sets representing entire genomes' worth of DNA sequences, such as those produced by the Human Genome Project, are difficult to use without the annotations that identify the locations of genes and regulatory elements on each chromosome. Regions of DNA sequence that have the characteristic patterns associated with protein- or RNA-coding genes can be identified by gene finding algorithms, which allow researchers to predict the presence of particular gene products and their possible functions in an organism even before they have been isolated experimentally. Entire genomes may also be compared, which can shed light on the evolutionary history of particular organism and permit the examination of complex evolutionary events.
DNA nanotechnology.
DNA nanotechnology uses the unique molecular recognition properties of DNA and other nucleic acids to create self-assembling branched DNA complexes with useful properties. DNA is thus used as a structural material rather than as a carrier of biological information. This has led to the creation of two-dimensional periodic lattices (both tile-based and using the "DNA origami" method) as well as three-dimensional structures in the shapes of polyhedra. Nanomechanical devices and algorithmic self-assembly have also been demonstrated, and these DNA structures have been used to template the arrangement of other molecules such as gold nanoparticles and streptavidin proteins.
History and anthropology.
Because DNA collects mutations over time, which are then inherited, it contains historical information, and, by comparing DNA sequences, geneticists can infer the evolutionary history of organisms, their phylogeny. This field of phylogenetics is a powerful tool in evolutionary biology. If DNA sequences within a species are compared, population geneticists can learn the history of particular populations. This can be used in studies ranging from ecological genetics to anthropology; For example, DNA evidence is being used to try to identify the Ten Lost Tribes of Israel.
Information storage.
In a paper published in "Nature" in January 2013, scientists from the European Bioinformatics Institute and Agilent Technologies proposed a mechanism to use DNA's ability to code information as a means of digital data storage. The group was able to encode 739 kilobytes of data into DNA code, synthesize the actual DNA, then sequence the DNA and decode the information back to its original form, with a reported 100% accuracy. The encoded information consisted of text files and audio files. A prior experiment was published in August 2012. It was conducted by researchers at Harvard University, where the text of a 54,000-word book was encoded in DNA.
History of DNA research.
DNA was first isolated by the Swiss physician Friedrich Miescher who, in 1869, discovered a microscopic substance in the pus of discarded surgical bandages. As it resided in the nuclei of cells, he called it "nuclein". In 1878, Albrecht Kossel isolated the non-protein component of "nuclein", nucleic acid, and later isolated its five primary nucleobases. In 1919, Phoebus Levene identified the base, sugar and phosphate nucleotide unit. Levene suggested that DNA consisted of a string of nucleotide units linked together through the phosphate groups. Levene thought the chain was short and the bases repeated in a fixed order. In 1937, William Astbury produced the first X-ray diffraction patterns that showed that DNA had a regular structure.
In 1927, Nikolai Koltsov proposed that inherited traits would be inherited via a "giant hereditary molecule" made up of "two mirror strands that would replicate in a semi-conservative fashion using each strand as a template". In 1928, Frederick Griffith in his experiment discovered that traits of the "smooth" form of "Pneumococcus" could be transferred to the "rough" form of the same bacteria by mixing killed "smooth" bacteria with the live "rough" form. This system provided the first clear suggestion that DNA carries genetic information—the Avery–MacLeod–McCarty experiment—when Oswald Avery, along with coworkers Colin MacLeod and Maclyn McCarty, identified DNA as the transforming principle in 1943. DNA's role in heredity was confirmed in 1952, when Alfred Hershey and Martha Chase in the Hershey–Chase experiment showed that DNA is the genetic material of the T2 phage.
In 1953, James Watson and Francis Crick suggested what is now accepted as the first correct double-helix model of DNA structure in the journal "Nature". Their double-helix, molecular model of DNA was then based on a single X-ray diffraction image (labeled as "Photo 51") taken by Rosalind Franklin and Raymond Gosling in May 1952, as well as the information that the DNA bases are paired—also obtained through private communications from Erwin Chargaff in the previous years.
Experimental evidence supporting the Watson and Crick model was published in a series of five articles in the same issue of "Nature". Of these, Franklin and Gosling's paper was the first publication of their own X-ray diffraction data and original analysis method that partially supported the Watson and Crick model; this issue also contained an article on DNA structure by Maurice Wilkins and two of his colleagues, whose analysis and "in vivo" B-DNA X-ray patterns also supported the presence "in vivo" of the double-helical DNA configurations as proposed by Crick and Watson for their double-helix molecular model of DNA in the previous two pages of "Nature". In 1962, after Franklin's death, Watson, Crick, and Wilkins jointly received the Nobel Prize in Physiology or Medicine. Nobel Prizes are awarded only to living recipients. A debate continues about who should receive credit for the discovery.
In an influential presentation in 1957, Crick laid out the central dogma of molecular biology, which foretold the relationship between DNA, RNA, and proteins, and articulated the "adaptor hypothesis". Final confirmation of the replication mechanism that was implied by the double-helical structure followed in 1958 through the Meselson–Stahl experiment. Further work by Crick and coworkers showed that the genetic code was based on non-overlapping triplets of bases, called codons, allowing Har Gobind Khorana, Robert W. Holley and Marshall Warren Nirenberg to decipher the genetic code. These findings represent the birth of molecular biology.

</doc>
<doc id="7957" url="https://en.wikipedia.org/wiki?curid=7957" title="Kennedy family">
Kennedy family

The Kennedy family is an American family of Irish descent that is prominent in American politics, government, public service, and business. The first Kennedys to reside in America were Patrick Kennedy and Bridget Murphy, who sailed from Ireland to America in 1849. Their son P. J. went into politics and business. P. J. and his wife Mary Hickey were the parents of businessman and politician Joseph P. Kennedy, Sr. The children of Joseph Sr. and Rose Fitzgerald were Joseph Jr., John, Kathleen, Rosemary, Eunice, Patricia, Robert, Jean, and Ted. John served as president, while Robert and Ted both became prominent senators. The Kennedys' political involvement has revolved around the Democratic Party. Harvard University educations have been common among them, and they have contributed heavily to that university's John F. Kennedy School of Government. The wealth and glamour of the family members, as well as their extensive and continuing involvement in public service, have elevated them to iconic status over the past half-century, with the Kennedys sometimes referred to as "America's Royal Family".
Joseph Sr. originally pinned his hopes on eldest son, Joseph Jr., to enter politics and be elected president. After Joseph Jr. was killed during World War II, however, those hopes later fell on his second son, John, to become president. Soon after John was elected in 1960, he, Robert, and Ted all held prominent positions in the federal government. They received intense publicity, often emphasizing their relative youth, allure, education, and future in politics. Between 1947 and 2011, there were 64 years with a Kennedy in elective office in Washington. This spans more than a quarter of the nation's existence.
The family has been at the center of many deaths, which contributed to the idea of the "Kennedy curse". Rosemary was forced to undergo a lobotomy which crippled her entire life; John and Robert were both assassinated during the 1960s; Ted was involved in the Chappaquiddick incident in 1969, which caused the death of his young colleague, Mary Jo Kopechne; and Joseph Jr., Kathleen, and John Jr. all died in plane crashes.

</doc>
<doc id="7958" url="https://en.wikipedia.org/wiki?curid=7958" title="Deflation (disambiguation)">
Deflation (disambiguation)

Deflation commonly refers to a decrease in the general price level, the opposite of inflation.
Deflation may also refer to:

</doc>
<doc id="7959" url="https://en.wikipedia.org/wiki?curid=7959" title="Democracy">
Democracy

Democracy, or democratic government, is "a system of government in which all the people of a state or polity ... are involved in making decisions about its affairs, typically by voting to elect representatives to a parliament or similar assembly," as defined by the Oxford English Dictionary. Democracy is further defined as (a:) "government by the people; especially : rule of the majority (b:) " a government in which the supreme power is vested in the people and exercised by them directly or indirectly through a system of representation usually involving periodically held free elections."
According to political scientist Larry Diamond, it consists of four key elements: (a) A political system for choosing and replacing the government through free and fair elections; (b) The active participation of the people, as citizens, in politics and civic life; (c) Protection of the human rights of all citizens, and (d) A rule of law, in which the laws and procedures apply equally to all citizens.
The term originates from the Greek ("") "rule of the people", which was found from δῆμος ("dêmos") "people" and κράτος ("krátos") "power" or "rule", in the 5th century BC to denote the political systems then existing in Greek city-states, notably Athens; the term is an antonym to ἀριστοκρατία ("aristokratía") "rule of an elite". While theoretically these definitions are in opposition, in practice the distinction has been blurred historically. The political system of Classical Athens, for example, granted democratic citizenship to an elite class of free men and excluded slaves and women from political participation. In virtually all democratic governments throughout ancient and modern history, democratic citizenship consisted of an elite class until full enfranchisement was won for all adult citizens in most modern democracies through the suffrage movements of the 19th and 20th centuries. The English word dates to the 16th century, from the older Middle French and Middle Latin equivalents.
Democracy contrasts with forms of government where power is either held by an individual, as in an absolute monarchy, or where power is held by a small number of individuals, as in an oligarchy. Nevertheless, these oppositions, inherited from Greek philosophy, are now ambiguous because contemporary governments have mixed democratic, oligarchic, and monarchic elements. Karl Popper defined democracy in contrast to dictatorship or tyranny, thus focusing on opportunities for the people to control their leaders and to oust them without the need for a revolution.
Characteristics.
No consensus exists on how to define democracy, but legal equality, political freedom and rule of law have been identified as important characteristics. These principles are reflected in all eligible citizens being equal before the law and having equal access to legislative processes. For example, in a representative democracy, every vote has equal weight, no unreasonable restrictions can apply to anyone seeking to become a representative, and the freedom of its eligible citizens is secured by legitimised rights and liberties which are typically protected by a constitution.
One theory holds that democracy requires three fundamental principles: (1) upward control, i.e. sovereignty residing at the lowest levels of authority, (2) political equality, and (3) social norms by which individuals and institutions only consider acceptable acts that reflect the first two principles of upward control and political equality.
The term "democracy" is sometimes used as shorthand for liberal democracy, which is a variant of representative democracy that may include elements such as political pluralism; equality before the law; the right to petition elected officials for redress of grievances; due process; civil liberties; human rights; and elements of civil society outside the government. Roger Scruton argues that democracy alone cannot provide personal and political freedom unless the institutions of civil society are also present.
In some countries, notably in the United Kingdom which originated the Westminster system, the dominant principle is that of parliamentary sovereignty, while maintaining judicial independence. In the United States, separation of powers is often cited as a central attribute. In India parliamentary sovereignty is subject to a Constitution which includes judicial review. Other uses of "democracy" include that of direct democracy. Though the term "democracy" is typically used in the context of a political state, the principles also are applicable to private organisations.
Majority rule is often listed as a characteristic of democracy. Hence, democracy allows for political minorities to be oppressed by the "tyranny of the majority" in the absence of legal protections of individual or group rights. An essential part of an "ideal" representative democracy is competitive elections that are fair both substantively and procedurally. Furthermore, freedom of political expression, freedom of speech, and freedom of the press are considered to be essential rights that allow eligible citizens to be adequately informed and able to vote according to their own interests.
It has also been suggested that a basic feature of democracy is the capacity of all voters to participate freely and fully in the life of their society. With its emphasis on notions of social contract and the collective will of all the voters, democracy can also be characterised as a form of political collectivism because it is defined as a form of government in which all eligible citizens have an equal say in lawmaking.
While representative democracy is sometimes equated with the republican form of government, the term "republic" classically has encompassed both democracies and aristocracies. Many democracies are constitutional monarchies, such as the United Kingdom.
History.
Ancient origins.
The term "democracy" first appeared in ancient Greek political and philosophical thought in the city-state of Athens during classical antiquity. Led by Cleisthenes, Athenians established what is generally held as the first democracy in 508–507 BC. Cleisthenes is referred to as "the father of Athenian democracy."
Athenian democracy took the form of a direct democracy, and it had two distinguishing features: the random selection of ordinary citizens to fill the few existing government administrative and judicial offices, and a legislative assembly consisting of all Athenian citizens. All eligible citizens were allowed to speak and vote in the assembly, which set the laws of the city state. However, Athenian citizenship excluded women, slaves, foreigners (μέτοικοι "métoikoi"), non-landowners, and males under 20 years old.
Of the estimated 200,000 to 400,000 inhabitants of Athens, there were between 30,000 and 60,000 citizens. The exclusion of large parts of the population from the citizen body is closely related to the ancient understanding of citizenship. In most of antiquity the benefit of citizenship was tied to the obligation to fight war campaigns.
Athenian democracy was not only "direct" in the sense that decisions were made by the assembled people, but also the "most direct" in the sense that the people through the assembly, boule and courts of law controlled the entire political process and a large proportion of citizens were involved constantly in the public business. Even though the rights of the individual were not secured by the Athenian constitution in the modern sense (the ancient Greeks had no word for "rights"), the Athenians enjoyed their liberties not in opposition to the government but by living in a city that was not subject to another power and by not being subjects themselves to the rule of another person.
Range voting appeared in Sparta as early as 700 BC. The Apella was an assembly of the people, held once a month, in which every male citizen of age 30 and above could participate. In the Apella, Spartans elected leaders and cast votes by range voting and shouting. Aristotle called this "childish," as compared with the stone voting ballots used by the Athenians. Sparta adopted it because of its simplicity, and to prevent any bias voting, buying, or cheating that was predominant in the early democratic elections.
Even though the Roman Republic contributed significantly to many aspects of democracy, only a minority of Romans were citizens with votes in elections for representatives. The votes of the powerful were given more weight through a system of gerrymandering, so most high officials, including members of the Senate, came from a few wealthy and noble families. In addition, the Roman Republic was the first government in the western world to have a Republic as a nation-state, although it didn't have much of a democracy. The Romans invented the concept of classics and many works from Ancient Greece were preserved. Additionally, the Roman model of governance inspired many political thinkers over the centuries, and today's modern representative democracies imitate more the Roman than the Greek models because it was a state in which supreme power was held by the people and their elected representatives, and which had an elected or nominated leader. Other cultures, such as the Iroquoi Nation in the Americas between around 1450 and 1600 AD also developed a form of democratic society before they came in contact with the Europeans. This indicates that forms of democracy may have been invented in other societies around the world.
Middle Ages.
During the Middle Ages, there were various systems involving elections or assemblies, although often only involving a small part of the population. These included:
Most regions in medieval Europe were ruled by clergy or feudal lords.
The Kouroukan Fouga divided the Mali Empire into ruling clans (lineages) that were represented at a great assembly called the "Gbara". However, the charter made Mali more similar to a constitutional monarchy than a democratic republic. A little closer to modern democracy were the Cossack republics of Ukraine in the 16th and 17th centuries: Cossack Hetmanate and Zaporizhian Sich. The highest post – the Hetman – was elected by the representatives from the country's districts.
The Parliament of England had its roots in the restrictions on the power of kings written into Magna Carta (1215), which explicitly protected certain rights of the King's subjects and implicitly supported what became the English writ of habeas corpus, safeguarding individual freedom against unlawful imprisonment with right to appeal. The first representative national assembly was Simon de Montfort's Parliament in England in 1265. The emergence of petitioning is some of the earliest evidence of parliament being used as a forum to address the general grievances of ordinary people. However, the power to call parliament remained at the pleasure of the monarch.
Modern era.
Early modern period.
During the early modern period, the power of the Parliament of England continually increased. Passage of the Petition of Right in 1628 and Habeas Corpus Act in 1679 established certain liberties and remain in effect. The idea of a political party took form with groups freely debating rights to political representation during the Putney Debates of 1647. After the English Civil Wars (1642–1651) and the Glorious Revolution of 1688, the Bill of Rights was enacted in 1689, which codified certain rights and liberties, and is still in effect. The Bill set out the requirement for regular elections, rules for freedom of speech in Parliament and limited the power of the monarch, ensuring that, unlike much of Europe at the time, royal absolutism would not prevail.
In North America, representative government began in Jamestown, Virginia, with the election of the House of Burgesses (forerunner of the Virginia General Assembly) in 1619. English Puritans who migrated from 1620 established colonies in New England whose local governance was democratic and which contributed to the democratic development of the United States; although these local assemblies had some small amounts of devolved power, the ultimate authority was held by the Crown and the English Parliament. The Puritans (Pilgrim Fathers), Baptists, and Quakers who founded these colonies applied the democratic organisation of their congregations also to the administration of their communities in worldly matters.
18th and 19th centuries.
The first Parliament of Great Britain was established in 1707, after the merger of the Kingdom of England and the Kingdom of Scotland under the Acts of Union. Although the monarch increasingly became a figurehead,
only a small minority actually had a voice; Parliament was elected by only a few percent of the population (less than 3% as late as 1780). During the Age of Liberty in Sweden (1718-1772), civil rights were expanded and power shifted from the monarch to parliament. The taxed peasantry was represented in parliament, although with little influence, but commoners without taxed property had no suffrage.
The creation of the short-lived Corsican Republic in 1755 marked the first nation in modern history to adopt a democratic constitution (all men and women above age of 25 could vote). This Corsican Constitution was the first based on Enlightenment principles and included female suffrage, something that was not granted in most other democracies until the 20th century.
In the American colonial period before 1776, and for some time after, often only adult white male property owners could vote; enslaved Africans, most free black people and most women were not extended the franchise. On the American frontier, democracy became a way of life, with more widespread social, economic and political equality. Although not described as a democracy by the founding fathers, they shared a determination to root the American experiment in the principles of natural freedom and equality.
The American Revolution led to the adoption of the United States Constitution in 1787, the oldest surviving, still active, governmental codified constitution. The Constitution provided for an elected government and protected civil rights and liberties for some, but did not end slavery nor extend voting rights in the United States beyond white male property owners (about 6% of the population). The Bill of Rights in 1791 set limits on government power to protect personal freedoms but had little impact on judgements by the courts for the first 130 years after ratification.
In 1789, Revolutionary France adopted the Declaration of the Rights of Man and of the Citizen and, although short-lived, the National Convention was elected by all males in 1792. However, in the early 19th century, little of democracy - as theory, practice, or even as word - remained in the North Atlantic world.
During this period, slavery remained a social and economic institution in places around the world. This was particularly the case in the eleven states of the American South. A variety of organisations were established advocating the movement of black people from the United States to locations where they would enjoy greater freedom and equality.
The United Kingdom's Slave Trade Act 1807 banned the trade across the British Empire, enforced internationally by the Royal Navy's West Africa Squadron under treaties Britain negotiated with other nations. As the voting franchise in the U.K. was increased, it also was made more uniform in a series of reforms beginning with the Reform Act of 1832. In 1833, the United Kingdom passed the Slavery Abolition Act which took effect across the British Empire.
Universal male suffrage was established in France in March 1848 in the wake of the French Revolution of 1848. In 1848, several revolutions broke out in Europe as rulers were confronted with popular demands for liberal constitutions and more democratic government.
In the 1860 United States Census, the slave population in the United States had grown to four million, and in Reconstruction after the Civil War (late 1860s), the newly freed slaves became citizens with a nominal right to vote for men. Full enfranchisement of citizens was not secured until after the African-American Civil Rights Movement (1955–1968) gained passage by the United States Congress of the Voting Rights Act of 1965.
20th and 21st centuries.
20th-century transitions to liberal democracy have come in successive "waves of democracy," variously resulting from wars, revolutions, decolonisation, and religious and economic circumstances. World War I and the dissolution of the Ottoman and Austro-Hungarian empires resulted in the creation of new nation-states from Europe, most of them at least nominally democratic.
In the 1920s democracy flourished and women's suffrage advanced, but the Great Depression brought disenchantment and most of the countries of Europe, Latin America, and Asia turned to strong-man rule or dictatorships. Fascism and dictatorships flourished in Nazi Germany, Italy, Spain and Portugal, as well as nondemocratic regimes in the Baltics, the Balkans, Brazil, Cuba, China, and Japan, among others.
World War II brought a definitive reversal of this trend in western Europe. The democratisation of the American, British, and French sectors of occupied Germany (disputed), Austria, Italy, and the occupied Japan served as a model for the later theory of regime change. However, most of Eastern Europe, including the Soviet sector of Germany fell into the non-democratic Soviet bloc.
The war was followed by decolonisation, and again most of the new independent states had nominally democratic constitutions. India emerged as the world's largest democracy and continues to be so. Countries that were once part of the British Empire often adopted the British Westminster system.
By 1960, the vast majority of country-states were nominally democracies, although most of the world's populations lived in nations that experienced sham elections, and other forms of subterfuge (particularly in Communist nations and the former colonies.)
A subsequent wave of democratisation brought substantial gains toward true liberal democracy for many nations. Spain, Portugal (1974), and several of the military dictatorships in South America returned to civilian rule in the late 1970s and early 1980s (Argentina in 1983, Bolivia, Uruguay in 1984, Brazil in 1985, and Chile in the early 1990s). This was followed by nations in East and South Asia by the mid-to-late 1980s.
Economic malaise in the 1980s, along with resentment of Soviet oppression, contributed to the collapse of the Soviet Union, the associated end of the Cold War, and the democratisation and liberalisation of the former Eastern bloc countries. The most successful of the new democracies were those geographically and culturally closest to western Europe, and they are now members or candidate members of the European Union.
The liberal trend spread to some nations in Africa in the 1990s, most prominently in South Africa. Some recent examples of attempts of liberalisation include the Indonesian Revolution of 1998, the Bulldozer Revolution in Yugoslavia, the Rose Revolution in Georgia, the Orange Revolution in Ukraine, the Cedar Revolution in Lebanon, the Tulip Revolution in Kyrgyzstan, and the Jasmine Revolution in Tunisia.
According to Freedom House, in 2007 there were 123 electoral democracies (up from 40 in 1972). According to "World Forum on Democracy", electoral democracies now represent 120 of the 192 existing countries and constitute 58.2 percent of the world's population. At the same time liberal democracies i.e. countries Freedom House regards as free and respectful of basic human rights and the rule of law are 85 in number and represent 38 percent of the global population.
In 2007 the United Nations declared September 15 the International Day of Democracy.
Measurement of democracy.
Several freedom indices are used to measure democracy:
Types of democracies.
Democracy has taken a number of forms, both in theory and practice. Some varieties of democracy provide better representation and more freedom for their citizens than others. However, if any democracy is not structured so as to prohibit the government from excluding the people from the legislative process, or any branch of government from altering the separation of powers in its own favour, then a branch of the system can accumulate too much power and destroy the democracy.
The following kinds of democracy are not exclusive of one another: many specify details of aspects that are independent of one another and can co-exist in a single system.
Basic forms.
Several variants of democracy exist, but there are two basic forms, both of which concern how the whole body of all eligible citizens executes its will. One form of democracy is direct democracy, in which all eligible citizens have active participation in the political decision making, for example voting on policy initiatives directly. In most modern democracies, the whole body of eligible citizens remain the sovereign power but political power is exercised indirectly through elected representatives; this is called a representative democracy.
Direct.
Direct democracy is a political system where the citizens participate in the decision-making personally, contrary to relying on intermediaries or representatives. The use of a lot system, a characteristic of Athenian democracy, is unique to direct democracies. In this system, important governmental and administrative tasks are performed by citizens picked from a lottery. A direct democracy gives the voting population the power to:
Within modern-day representative governments, certain electoral tools like referendums, citizens' initiatives and recall elections are referred to as forms of direct democracy. Direct democracy as a government system currently only exists in the Swiss cantons of Appenzell Innerrhoden and Glarus.
Representative.
Representative democracy involves the election of government officials by the people being represented. If the head of state is also democratically elected then it is called a democratic republic. The most common mechanisms involve election of the candidate with a majority or a plurality of the votes. Most western countries have representative systems.
Representatives may be elected or become diplomatic representatives by a particular district (or constituency), or represent the entire electorate through proportional systems, with some using a combination of the two. Some representative democracies also incorporate elements of direct democracy, such as referendums. A characteristic of representative democracy is that while the representatives are elected by the people to act in the people's interest, they retain the freedom to exercise their own judgement as how best to do so. Such reasons have driven criticism upon representative democracy, pointing out the contradictions of representation mechanisms' with democracy
Parliamentary.
Parliamentary democracy is a representative democracy where government is appointed by, or can be dismissed by, representatives as opposed to a "presidential rule" wherein the president is both head of state and the head of government and is elected by the voters. Under a parliamentary democracy, government is exercised by delegation to an executive ministry and subject to ongoing review, checks and balances by the legislative parliament elected by the people.
Parliamentary systems have the right to dismiss a Prime Minister at any point in time that they feel he or she is not doing their job to the expectations of the legislature. This is done through a Vote of No Confidence where the legislature decides whether or not to remove the Prime Minister from office by a majority support for his or her dismissal. In some countries, the Prime Minister can also call an election whenever he or she so chooses, and typically the Prime Minister will hold an election when he or she knows that they are in good favour with the public as to get re-elected. In other parliamentary democracies extra elections are virtually never held, a minority government being preferred until the next ordinary elections. An important feature of the parliamentary democracy is the concept of the "loyal opposition". The essence of the concept is that the second largest political party (or coalition) opposes the governing party (or coalition), while still remaining loyal to the state and its democratic principles.
Presidential.
Presidential Democracy is a system where the public elects the president through free and fair elections. The president serves as both the head of state and head of government controlling most of the executive powers. The president serves for a specific term and cannot exceed that amount of time. Elections typically have a fixed date and aren't easily changed. The president has direct control over the cabinet, specifically appointing the cabinet members.
The president cannot be easily removed from office by the legislature, but he or she cannot remove members of the legislative branch any more easily. This provides some measure of separation of powers. In consequence however, the president and the legislature may end up in the control of separate parties, allowing one to block the other and thereby interfere with the orderly operation of the state. This may be the reason why presidential democracy is not very common outside the Americas, Africa, and Central and Southeast Asia.
A semi-presidential system is a system of democracy in which the government includes both a prime minister and a president. The particular powers held by the prime minister and president vary by country.
Hybrid or semi-direct.
Some modern democracies that are predominantly representative in nature also heavily rely upon forms of political action that are directly democratic. These democracies, which combine elements of representative democracy and direct democracy, are termed "hybrid democracies", "semi-direct democracies" or "participatory democracies". Examples include Switzerland and some U.S. states, where frequent use is made of referendums and initiatives.
The Swiss confederation is a semi-direct democracy. At the federal level, citizens can propose changes to the constitution (federal popular initiative) or ask for a referendum to be held on any law voted by the parliament. Between January 1995 and June 2005, Swiss citizens voted 31 times, to answer 103 questions (during the same period, French citizens participated in only two referendums). Although in the past 120 years less than 250 initiatives have been put to referendum. The populace has been conservative, approving only about 10% of the initiatives put before them; in addition, they have often opted for a version of the initiative rewritten by government.
In the United States, no mechanisms of direct democracy exists at the federal level, but over half of the states and many localities provide for citizen-sponsored ballot initiatives (also called "ballot measures", "ballot questions" or "propositions"), and the vast majority of states allow for referendums. Examples include the extensive use of referendums in the US state of California, which is a state that has more than 20 million voters.
In New England Town meetings are often used, especially in rural areas, to manage local government. This creates a hybrid form of government, with a local direct democracy and a state government which is representative. For example, most Vermont towns hold annual town meetings in March in which town officers are elected, budgets for the town and schools are voted on, and citizens have the opportunity to speak and be heard on political matters.
Variants.
Constitutional monarchy.
Many countries such as the United Kingdom, Spain, the Netherlands, Belgium, Scandinavian countries, Thailand, Japan and Bhutan turned powerful monarchs into constitutional monarchs with limited or, often gradually, merely symbolic roles. For example, in the predecessor states to the United Kingdom, constitutional monarchy began to emerge and has continued uninterrupted since the Glorious Revolution of 1688 and passage of the Bill of Rights 1689.
In other countries, the monarchy was abolished along with the aristocratic system (as in France, China, Russia, Germany, Austria, Hungary, Italy, Greece and Egypt). An elected president, with or without significant powers, became the head of state in these countries.
Élite upper houses of legislatures, which often had lifetime or hereditary tenure, were common in many nations. Over time, these either had their powers limited (as with the British House of Lords) or else became elective and remained powerful (as with the Australian Senate).
Republic.
The term "republic" has many different meanings, but today often refers to a representative democracy with an elected head of state, such as a president, serving for a limited term, in contrast to states with a hereditary monarch as a head of state, even if these states also are representative democracies with an elected or appointed head of government such as a prime minister.
The Founding Fathers of the United States rarely praised and often criticised democracy, which in their time tended to specifically mean direct democracy, often without the protection of a Constitution enshrining basic rights; James Madison argued, especially in "The Federalist" No. 10, that what distinguished a "democracy" from a "republic" was that the former became weaker as it got larger and suffered more violently from the effects of faction, whereas a republic could get stronger as it got larger and combats faction by its very structure.
What was critical to American values, John Adams insisted, was that the government be "bound by fixed laws, which the people have a voice in making, and a right to defend." As Benjamin Franklin was exiting after writing the U.S. constitution, a woman asked him "Well, Doctor, what have we got—a republic or a monarchy?". He replied "A republic—if you can keep it."
Liberal democracy.
A liberal democracy is a representative democracy in which the ability of the elected representatives to exercise decision-making power is subject to the rule of law, and moderated by a constitution or laws that emphasise the protection of the rights and freedoms of individuals, and which places constraints on the leaders and on the extent to which the will of the majority can be exercised against the rights of minorities (see civil liberties).
In a liberal democracy, it is possible for some large-scale decisions to emerge from the many individual decisions that citizens are free to make. In other words, citizens can "vote with their feet" or "vote with their dollars", resulting in significant informal government-by-the-masses that exercises many "powers" associated with formal government elsewhere.
Socialist.
Socialist thought has several different views on democracy. Social democracy, democratic socialism, and the dictatorship of the proletariat (usually exercised through Soviet democracy) are some examples. Many democratic socialists and social democrats believe in a form of participatory, industrial, economic and/or workplace democracy combined with a representative democracy.
Within Marxist orthodoxy there is a hostility to what is commonly called "liberal democracy", which they simply refer to as parliamentary democracy because of its often centralised nature. Because of their desire to eliminate the political elitism they see in capitalism, Marxists, Leninists and Trotskyists believe in direct democracy implemented through a system of communes (which are sometimes called soviets). This system ultimately manifests itself as council democracy and begins with workplace democracy. (See Democracy in Marxism.)
Anarchist.
Anarchists are split in this domain, depending on whether they believe that a majority-rule is tyrannic or not.
The only form of democracy considered acceptable to many anarchists is direct democracy. Pierre-Joseph Proudhon argued that the only acceptable form of direct democracy is one in which it is recognised that majority decisions are not binding on the minority, even when unanimous. However, anarcho-communist Murray Bookchin criticised individualist anarchists for opposing democracy, and says "majority rule" is consistent with anarchism.
Some anarcho-communists oppose the majoritarian nature of direct democracy, feeling that it can impede individual liberty and opt in favour of a non-majoritarian form of consensus democracy, similar to Proudhon's position on direct democracy. Henry David Thoreau, who did not self-identify as an anarchist but argued for "a better government" and is cited as an inspiration by some anarchists, argued that people should not be in the position of ruling others or being ruled when there is no consent.
Anarcho-capitalists, voluntaryists and other right-anarchists oppose institutional democracy as they consider it in conflict with widely held moral values and ethical principles and their conception of individual rights. The "a priori" Rothbardian argument is that the state is a coercive institution which necessarily violates the non-aggression principle (NAP). Some right-anarchists also criticise democracy on "a posteriori" consequentialist grounds, in terms of inefficiency or disability in bringing about maximisation of individual liberty. They maintain the people who participate in democratic institutions are foremost driven by economic self-interest.
Sortition.
Sometimes called "democracy without elections", sortition chooses decision makers via a random process. The intention is that those chosen will be representative of the opinions and interests of the people at large, and be more fair and impartial than an elected official. The technique was in widespread use in Athenian Democracy and Renaissance Florence and is still used in modern jury selection.
Consociational.
A consociational democracy allows for simultaneous majority votes in two or more ethno-religious constituencies, and policies are enacted only if they gain majority support from both or all of them.
Consensus democracy.
A consensus democracy, in contrast, would not be dichotomous. Instead, decisions would be based on a multi-option approach, and policies would be enacted if they gained sufficient support, either in a purely verbal agreement, or via a consensus vote - a multi-option preference vote. If the threshold of support were at a sufficiently high level, minorities would be as it were protected automatically. Furthermore, any voting would be ethno-colour blind.
Supranational.
Qualified majority voting is designed by the Treaty of Rome to be the principal method of reaching decisions in the European Council of Ministers. This system allocates votes to member states in part according to their population, but heavily weighted in favour of the smaller states. This might be seen as a form of representative democracy, but representatives to the Council might be appointed rather than directly elected.
Inclusive.
Inclusive democracy is a political theory and political project that aims for direct democracy in all fields of social life: political democracy in the form of face-to-face assemblies which are confederated, economic democracy in a stateless, moneyless and marketless economy, democracy in the social realm, i.e. self-management in places of work and education, and ecological democracy which aims to reintegrate society and nature. The theoretical project of inclusive democracy emerged from the work of political philosopher Takis Fotopoulos in "Towards An Inclusive Democracy" and was further developed in the journal "Democracy & Nature" and its successor "The International Journal of Inclusive Democracy".
The basic unit of decision making in an inclusive democracy is the demotic assembly, i.e. the assembly of demos, the citizen body in a given geographical area which may encompass a town and the surrounding villages, or even neighbourhoods of large cities. An inclusive democracy today can only take the form of a confederal democracy that is based on a network of administrative councils whose members or delegates are elected from popular face-to-face democratic assemblies in the various demoi. Thus, their role is purely administrative and practical, not one of policy-making like that of representatives in representative democracy.
The citizen body is advised by experts but it is the citizen body which functions as the ultimate decision-taker . Authority can be delegated to a segment of the citizen body to carry out specific duties, for example to serve as members of popular courts, or of regional and confederal councils. Such delegation is made, in principle, by lot, on a rotation basis, and is always recallable by the citizen body. Delegates to regional and confederal bodies should have specific mandates.
Participatory politics.
A Parpolity or Participatory Polity is a theoretical form of democracy that is ruled by a Nested Council structure. The guiding philosophy is that people should have decision making power in proportion to how much they are affected by the decision. Local councils of 25–50 people are completely autonomous on issues that affect only them, and these councils send delegates to higher level councils who are again autonomous regarding issues that affect only the population affected by that council.
A council court of randomly chosen citizens serves as a check on the tyranny of the majority, and rules on which body gets to vote on which issue. Delegates may vote differently from how their sending council might wish, but are mandated to communicate the wishes of their sending council. Delegates are recallable at any time. Referendums are possible at any time via votes of most lower-level councils, however, not everything is a referendum as this is most likely a waste of time. A parpolity is meant to work in tandem with a participatory economy.
Cosmopolitan.
Cosmopolitan democracy, also known as "Global democracy" or "World Federalism", is a political system in which democracy is implemented on a global scale, either directly or through representatives. An important justification for this kind of system is that the decisions made in national or regional democracies often affect people outside the constituency who, by definition, cannot vote. By contrast, in a cosmopolitan democracy, the people who are affected by decisions also have a say in them.
According to its supporters, any attempt to solve global problems is undemocratic without some form of cosmopolitan democracy. The general principle of cosmopolitan democracy is to expand some or all of the values and norms of democracy, including the rule of law; the non-violent resolution of conflicts; and equality among citizens, beyond the limits of the state. To be fully implemented, this would require reforming existing international organisations, e.g. the United Nations, as well as the creation of new institutions such as a World Parliament, which ideally would enhance public control over, and accountability in, international politics.
Cosmopolitan Democracy has been promoted, among others, by physicist Albert Einstein, writer Kurt Vonnegut, columnist George Monbiot, and professors David Held and Daniele Archibugi. The creation of the International Criminal Court in 2003 was seen as a major step forward by many supporters of this type of cosmopolitan democracy.
Creative Democracy.
Creative Democracy is advocated by American philosopher John Dewey. The main idea about Creative Democracy is that democracy encourages individual capacity building and the interaction among the society. Dewey argues that democracy is a way of life in his work of ""Creative Democracy: The Task Before Us" and an experience built on faith in human nature, faith in human beings, and faith in working with others. Democracy, in Dewey's view, is a moral ideal requiring actual effort and work by people; it is not an institutional concept that exists outside of ourselves. "The task of democracy", Dewey concludes, "is forever that of creation of a freer and more humane experience in which all share and to which all contribute".
Non-governmental.
Aside from the public sphere, similar democratic principles and mechanisms of voting and representation have been used to govern other kinds of groups. Many non-governmental organisations decide policy and leadership by voting. Most trade unions and cooperatives are governed by democratic elections. Corporations are controlled by shareholders on the principle of one share, one vote. An analogous system, that fuses elements of democracy with sharia law, has been termed "islamocracy".
Theory.
Aristotle.
Aristotle contrasted rule by the many (democracy/polity), with rule by the few (oligarchy/aristocracy), and with rule by a single person (tyranny or today autocracy/absolute monarchy). He also thought that there was a good and a bad variant of each system (he considered democracy to be the degenerate counterpart to polity).
For Aristotle the underlying principle of democracy is freedom, since only in a democracy the citizens can have a share in freedom. In essence, he argues that this is what every democracy should make its aim. There are two main aspects of freedom: being ruled and ruling in turn, since everyone is equal according to number, not merit, and to be able to live as one pleases.
Rationale.
Among modern political theorists, there are three contending conceptions of the fundamental rationale for democracy: "aggregative democracy," "deliberative democracy," and "radical democracy."
Aggregative.
The theory of "aggregative democracy" claims that the aim of the democratic processes is to solicit citizens' preferences and aggregate them together to determine what social policies society should adopt. Therefore, proponents of this view hold that democratic participation should primarily focus on voting, where the policy with the most votes gets implemented.
Different variants of aggregative democracy exist. Under "minimalism", democracy is a system of government in which citizens have given teams of political leaders the right to rule in periodic elections. According to this minimalist conception, citizens cannot and should not "rule" because, for example, on most issues, most of the time, they have no clear views or their views are not well-founded. Joseph Schumpeter articulated this view most famously in his book "Capitalism, Socialism, and Democracy". Contemporary proponents of minimalism include William H. Riker, Adam Przeworski, Richard Posner.
According to the theory of direct democracy, on the other hand, citizens should vote directly, not through their representatives, on legislative proposals. Proponents of direct democracy offer varied reasons to support this view. Political activity can be valuable in itself, it socialises and educates citizens, and popular participation can check powerful elites. Most importantly, citizens do not really rule themselves unless they directly decide laws and policies.
Governments will tend to produce laws and policies that are close to the views of the median voter – with half to their left and the other half to their right. This is not actually a desirable outcome as it represents the action of self-interested and somewhat unaccountable political elites competing for votes. Anthony Downs suggests that ideological political parties are necessary to act as a mediating broker between individual and governments. Downs laid out this view in his 1957 book "An Economic Theory of Democracy".
Robert A. Dahl argues that the fundamental democratic principle is that, when it comes to binding collective decisions, each person in a political community is entitled to have his/her interests be given equal consideration (not necessarily that all people are equally satisfied by the collective decision). He uses the term polyarchy to refer to societies in which there exists a certain set of institutions and procedures which are perceived as leading to such democracy. First and foremost among these institutions is the regular occurrence of free and open elections which are used to select representatives who then manage all or most of the public policy of the society. However, these polyarchic procedures may not create a full democracy if, for example, poverty prevents political participation.
Deliberative.
"Deliberative democracy" is based on the notion that democracy is government by deliberation. Unlike aggregative democracy, deliberative democracy holds that, for a democratic decision to be legitimate, it must be preceded by authentic deliberation, not merely the aggregation of preferences that occurs in voting. "Authentic deliberation" is deliberation among decision-makers that is free from distortions of unequal political power, such as power a decision-maker obtained through economic wealth or the support of interest groups. If the decision-makers cannot reach consensus after authentically deliberating on a proposal, then they vote on the proposal using a form of majority rule.
Radical.
"Radical democracy" is based on the idea that there are hierarchical and oppressive power relations that exist in society. Democracy's role is to make visible and challenge those relations by allowing for difference, dissent and antagonisms in decision making processes.
Criticism.
Inefficiencies.
Some economists have criticized the efficiency of democracy. They base this on their premise of the irrational voter. Their argument is that voters are highly uninformed about many political issues, especially relating to economics, and have a strong bias about the few issues on which they are fairly knowledgeable. A common example often quoted to substantiate this point is the high economic development achieved by China (a non-democratic country) as compared to India (a democratic country).
Popular rule as a façade.
The 20th-century Italian thinkers Vilfredo Pareto and Gaetano Mosca (independently) argued that democracy was illusory, and served only to mask the reality of elite rule. Indeed, they argued that elite oligarchy is the unbendable law of human nature, due largely to the apathy and division of the masses (as opposed to the drive, initiative and unity of the elites), and that democratic institutions would do no more than shift the exercise of power from oppression to manipulation. As Louis Brandeis once professed, "We may have democracy, or we may have wealth concentrated in the hands of a few, but we can't have both."
Mob rule.
Plato's "The Republic" presents a critical view of democracy through the narration of Socrates: "Democracy, which is a charming form of government, full of variety and disorder, and dispensing a sort of equality to equals and unequaled alike." In his work, Plato lists 5 forms of government from best to worst. Assuming that "the Republic" was intended to be a serious critique of the political thought in Athens, Plato argues that only Kallipolis, an aristocracy led by the unwilling philosopher-kings (the wisest men), is a just form of government.
James Madison critiqued direct democracy (which he referred to simply as "democracy") in Federalist No. 10, arguing that representative democracy—which he described using the term "republic"—is a preferable form of government, saying: "... democracies have ever been spectacles of turbulence and contention; have ever been found incompatible with personal security or the rights of property; and have in general been as short in their lives as they have been violent in their deaths." Madison offered that republics were superior to democracies because republics safeguarded against tyranny of the majority, stating in Federalist No. 10: "the same advantage which a republic has over a democracy, in controlling the effects of faction, is enjoyed by a large over a small republic".
Political instability.
More recently, democracy is criticised for not offering enough political stability. As governments are frequently elected on and off there tends to be frequent changes in the policies of democratic countries both domestically and internationally. Even if a political party maintains power, vociferous, headline grabbing protests and harsh criticism from the mass media are often enough to force sudden, unexpected political change. Frequent policy changes with regard to business and immigration are likely to deter investment and so hinder economic growth. For this reason, many people have put forward the idea that democracy is undesirable for a developing country in which economic growth and the reduction of poverty are top priorities.
This opportunist alliance not only has the handicap of having to cater to too many ideologically opposing factions, but it is usually short lived since any perceived or actual imbalance in the treatment of coalition partners, or changes to leadership in the coalition partners themselves, can very easily result in the coalition partner withdrawing its support from the government.
Fraudulent elections.
In representative democracies, it may not benefit incumbents to conduct fair elections. A study showed that incumbents who rig elections stay in office 2.5 times as long as those who permit fair elections. Democracies in countries with high per capita income have been found to be less prone to violence, but in countries with low incomes the tendency is the reverse. Election misconduct is more likely in countries with low per capita incomes, small populations, rich in natural resources, and a lack of institutional checks and balances. Sub-Saharan countries, as well as Afghanistan, all tend to fall into that category.
Governments that have frequent elections tend to have significantly more stable economic policies than those governments who have infrequent elections. However, this trend does not apply to governments where fraudulent elections are common.
Opposition.
Democracy in modern times has almost always faced opposition from the previously existing government, and many times it has faced opposition from social elites. The implementation of a democratic government within a non-democratic state is typically brought about by democratic revolution.
Post-Enlightenment ideologies such as fascism, nazism, and neo-fundamentalism oppose democracy on different grounds, generally citing that the concept of democracy as a constant process is flawed and detrimental to a preferable course of development.
Development.
Several philosophers and researchers have outlined historical and social factors seen as supporting the evolution of democracy.
"Cultural factors" like Protestantism influenced the development of democracy, rule of law, human rights and political liberty (the faithful elected priests, religious freedom and tolerance has been practiced).
Other commentators have mentioned the influence of "wealth" (e.g. S. M. Lipset, 1959). In a related theory, Ronald Inglehart suggests that improved living-standards can convince people that they can take their basic survival for granted, leading to increased emphasis on self-expression values, which is highly correlated to democracy.
Carroll Quigley concludes that the characteristics of weapons are the main predictor of democracy: Democracy tends to emerge only when the best weapons available are easy for individuals to buy and use. By the 1800s, guns were the best personal weapons available, and in America, almost everyone could afford to buy a gun, and could learn how to use it fairly easily. Governments couldn't do any better: it became the age of mass armies of citizen soldiers with guns Similarly, Periclean Greece was an age of the citizen soldier and democracy.
Recent theories stress the relevance of "education" and of "human capital" - and within them of "cognitive ability" to increasing tolerance, rationality, political literacy and participation. Two effects of education and cognitive ability are distinguished: a cognitive effect (competence to make rational choices, better information-processing) and an ethical effect (support of democratic values, freedom, human rights etc.), which itself depends on intelligence.
Evidence that is consistent with conventional theories of why democracy emerges and is sustained has been hard to come by. Recent statistical analyses have challenged modernisation theory by demonstrating that there is no reliable evidence for the claim that democracy is more likely to emerge when countries become wealthier, more educated, or less unequal. Neither is there convincing evidence that increased reliance on oil revenues prevents democratisation, despite a vast theoretical literature on "the Resource Curse" that asserts that oil revenues sever the link between citizen taxation and government accountability, seen as the key to representative democracy. The lack of evidence for these conventional theories of democratisation have led researchers to search for the "deep" determinants of contemporary political institutions, be they geographical or demographic.
In the 21st century, democracy has become such a popular method of reaching decisions that its application beyond politics to other areas such as entertainment, food and fashion, consumerism, urban planning, education, art, literature, science and theology has been criticised as "the reigning dogma of our time". The argument suggests that applying a populist or market-driven approach to art and literature (for example), means that innovative creative work goes unpublished or unproduced. In education, the argument is that essential but more difficult studies are not undertaken. Science, as a truth-based discipline, is particularly corrupted by the idea that the correct conclusion can be arrived at by popular vote.
Robert Michels asserts that although democracy can never be fully realised, democracy may be developed automatically in the act of striving for democracy: "The peasant in the fable, when on his death-bed, tells his sons that a treasure is buried in the field. After the old man's death the sons dig everywhere in order to discover the treasure. They do not find it. But their indefatigable labor improves the soil and secures for them a comparative well-being. The treasure in the fable may well symbolise democracy."
Dr. Harald Wydra, in his book "Communism and The Emergence of Democracy" (2007), maintains that the development of democracy should not be viewed as a purely procedural or as a static concept but rather as an ongoing "process of meaning formation". Drawing on Claude Lefort's idea of the empty place of power, that "power emanates from the people . but is the power of nobody", he remarks that democracy is reverence to a symbolic mythical authority as in reality, there is no such thing as the people or "demos". Democratic political figures are not supreme rulers but rather temporary guardians of an empty place. Any claim to substance such as the collective good, the public interest or the will of the nation is subject to the competitive struggle and times of for gaining the authority of office and government. The essence of the democratic system is an empty place, void of real people which can only be temporarily filled and never be appropriated. The seat of power is there, but remains open to constant change. As such, what "democracy" is or what is "democratic" progresses throughout history as a continual and potentially never ending process of social construction.
In 2010 a study by a German military think-tank analyzed how peak oil might change the global economy. The study raises fears for the survival of democracy itself. It suggests that parts of the population could perceive the upheaval triggered by peak oil as a general systemic crisis. This would create "room for ideological and extremist alternatives to existing forms of government".

</doc>
<doc id="7960" url="https://en.wikipedia.org/wiki?curid=7960" title="Deduction and induction">
Deduction and induction

Deduction and induction may refer to:

</doc>
<doc id="7962" url="https://en.wikipedia.org/wiki?curid=7962" title="Logical disjunction">
Logical disjunction

In logic and mathematics, or is the truth-functional operator of (inclusive) disjunction, also known as alternation; the "or" of a set of operands is true if and only if "one or more" of its operands is true. The logical connective that represents this operator is typically written as ∨ or +.
""A" or "B"" is true if "A" is true, or if "B" is true, or if both "A" and "B" are true.
In logic, "or" by itself means the "inclusive" "or", distinguished from an exclusive or, which is false when both of its arguments are true, while an "or" is true in that case.
An operand of a disjunction is called a disjunct.
Related concepts in other fields are:
Notation.
Or is usually expressed with an infix operator: in mathematics and logic, ∨; in electronics, +; and in most programming languages, |, ||, or or. In Jan Łukasiewicz's prefix notation for logic, the operator is A, for Polish "alternatywa".
Definition.
Logical disjunction is an operation on two logical values, typically the values of two propositions, that has a value of "false" if and only if both of its operands are false. More generally, a disjunction is a logical formula that can have one or more literals separated only by ORs. A single literal is often considered to be a degenerate disjunction.
The disjunctive identity is false, which is to say that the "or" of an expression with false has the same value as the original expression. In keeping with the concept of vacuous truth, when disjunction is defined as an operator or function of arbitrary arity, the empty disjunction (OR-ing over an empty set of operands) is generally defined as false.
Truth table.
The truth table of formula_1:
When all inputs are true, the output is true.
When all inputs are false, the output is false.
If using binary values for true (1) and false (0), then "logical disjunction" works almost like binary addition. The only difference is that formula_2, while formula_3.
Symbol.
The mathematical symbol for logical disjunction varies in the literature. In addition to the word "or", and the formula "A"pq"", the symbol "formula_4", deriving from the Latin word "vel" (“either”, “or”) is commonly used for disjunction. For example: ""A" formula_4 "B" " is read as ""A" or "B" ". Such a disjunction is false if both "A" and "B" are false. In all other cases it is true.
All of the following are disjunctions:
The corresponding operation in set theory is the set-theoretic union.
Applications in computer science.
Operators corresponding to logical disjunction exist in most programming languages.
Bitwise operation.
Disjunction is often used for bitwise operations. Examples:
The codice_1 operator can be used to set bits in a bit field to 1, by codice_1-ing the field with a constant field with the relevant bits set to 1. For example, codice_3 will force the final bit to 1 while leaving other bits unchanged.
Logical operation.
Many languages distinguish between bitwise and logical disjunction by providing two distinct operators; in languages following C, bitwise disjunction is performed with the single pipe (codice_4) and logical disjunction with the double pipe (codice_5) operators.
Logical disjunction is usually short-circuited; that is, if the first (left) operand evaluates to codice_6 then the second (right) operand is not evaluated. The logical disjunction operator thus usually constitutes a sequence point.
In a parallel (concurrent) language, it is possible to short-circuit both sides: they are evaluated in parallel,
and if one terminates with value true, the other is interrupted. This operator is thus called the parallel or.
Although in most languages the type of a logical disjunction expression is boolean and thus can only have the value codice_6 or codice_8, in some (such as Python and JavaScript) the logical disjunction operator returns one of its operands: the first operand if it evaluates to a true value, and the second operand otherwise.
Constructive disjunction.
The Curry–Howard correspondence relates a constructivist form of disjunction to tagged union types.
Union.
The membership of an element of an union set in set theory is defined in terms of a logical disjunction: "x" ∈ "A" ∪ "B" if and only if ("x" ∈ "A") ∨ ("x" ∈ "B"). Because of this, logical disjunction satisfies many of the same identities as set-theoretic union, such as associativity, commutativity, distributivity, and de Morgan's laws.
Natural language.
As with other notions formalized in mathematical logic, the meaning of the natural-language coordinating conjunction "or" is closely related to, but different from the logical "or". For example, "Please ring me or send an email" likely means "do one or the other, but not both". On the other hand, "Her grades are so good that she's either very bright or studies hard" does not exclude the possibility of both. In other words, in ordinary language "or" can mean the inclusive or exclusive or.

</doc>
<doc id="7963" url="https://en.wikipedia.org/wiki?curid=7963" title="Disjunctive syllogism">
Disjunctive syllogism

In classical logic, disjunctive syllogism (historically known as modus tollendo ponens) is a valid argument form which is a syllogism having a disjunctive statement for one of its premises.
In propositional logic, disjunctive syllogism (also known as disjunction elimination, kneecapper's argument, and or elimination, or abbreviated ∨E), is a valid rule of inference. If we are told that at least one of two statements is true; and also told that it is not the former that is true; we can infer that it has to be the latter that is true. If either "P" or "Q" is true and "P" is false, then "Q" is true. The reason this is called "disjunctive syllogism" is that, first, it is a syllogism, a three-step argument, and second, it contains a logical disjunction, which simply means an "or" statement. "Either P or Q" is a disjunction; P and Q are called the statement's "disjuncts". The rule makes it possible to eliminate a disjunction from a logical proof. It is the rule that:
where the rule is that whenever instances of "formula_2", and "formula_3" appear on lines of a proof, "formula_4" can be placed on a subsequent line.
Disjunctive syllogism is closely related and similar to hypothetical syllogism, in that it is also type of syllogism, and also the name of a rule of inference. It is also related to the Law of noncontradiction and the Law of excluded middle, two of the .
Formal notation.
The "disjunctive syllogism" rule may be written in sequent notation:
where formula_6 is a metalogical symbol meaning that formula_4 is a syntactic consequence of formula_8, and formula_9 in some logical system;
and expressed as a truth-functional tautology or theorem of propositional logic:
where formula_11, and formula_4 are propositions expressed in some formal system.
Natural language examples.
Here is an example:
Here is another example:
Inclusive and exclusive disjunction.
Please observe that the disjunctive syllogism works whether 'or' is considered 'exclusive' or 'inclusive' disjunction. See below for the definitions of these terms.
There are two kinds of logical disjunction:
The widely used English language concept of "or" is often ambiguous between these two meanings, but the difference is pivotal in evaluating disjunctive arguments.
This argument:
is valid and indifferent between both meanings. However, only in the "exclusive" meaning is the following form valid:
however if the fact is true it does not commit the fallacy
With the "inclusive" meaning you could draw no conclusion from the first two premises of that argument. See affirming a disjunct.
Related argument forms.
Unlike modus ponendo ponens and modus ponendo tollens, with which it should not be confused, disjunctive syllogism is often not made an explicit rule or axiom of logical systems, as the above arguments can be proven with a (slightly devious) combination of reductio ad absurdum and disjunction elimination.
"Other forms of syllogism:" 
Disjunctive syllogism holds in classical propositional logic and intuitionistic logic, but not in some paraconsistent logics.

</doc>
<doc id="7964" url="https://en.wikipedia.org/wiki?curid=7964" title="Definition">
Definition

A definition is a statement of the meaning of a term (a word, phrase, or other set of symbols). Definitions can be classified into two large categories, intensional definitions (which try to give the essence of a term) and extensional definitions (which list every single object that a term describes). A term may have many different senses and multiple meanings, and thus require multiple definitions.
In mathematics, a definition is used to give a precise meaning to a new term, instead of describing a pre-existing term. Definitions and axioms are the basis on which all of mathematics is constructed.
Basic terminology.
In modern usage, a "definition" is something, typically expressed in words, that attaches a meaning to a word or group of words. The word or group of words that is to be defined is called the "definiendum", and the word, group of words, or action that defines it is called the "definiens". In the definition "An elephant is a large gray animal native to Asia and Africa", the word "elephant" is the "definiendum", and everything after the word "is" is the "definiens".
Note that the "definiens" is not "the meaning" of the word defined, but is instead something that "conveys the same meaning" as that word.
There are many sub-types of definitions, often specific to a given field of knowledge or study. These include, among many others, lexical definitions, or the common dictionary definitions of words already in a language; demonstrative definitions, which define something by pointing to an example of it (""This," aid while pointing to a large grey anima, "is an Asian elephant.""); and precising definitions, which reduce the vagueness of a word, typically in some special sense ("'Large', among female Asian elephants, is any individual weighing over 5,500 pounds.").
Intensional definitions vs extensional definitions.
An "Intensional definition", also called a "connotative" definition, specifies the necessary and sufficient conditions for a thing being a member of a specific set. Any definition that attempts to set out the essence of something, such as that by genus and differentia, is an intensional definition.
An "extensional definition", also called a "denotative" definition, of a concept or term specifies its "extension". It is a list naming every object that is a member of a specific set. 
Thus, the "seven deadly sins" can be defined "intensionally" as those singled out by Pope Gregory I as particularly destructive of the life of grace and charity within a person, thus creating the threat of eternal damnation. An "extensional" definition would be the list of wrath, greed, sloth, pride, lust, envy, and gluttony. In contrast, while an intensional definition of "Prime Minister" might be "the most senior minister of a cabinet in the executive branch of government in a parliamentary system", an extensional definition is not possible since it is not known who future prime ministers will be.
Classes of intensional definitions.
A genus–differentia definition is a type of intensional definition that takes a large category (the genus) and narrows it down to a smaller category by a distinguishing characteristic (i.e. the differentia).
More formally, a genus-differentia definition consists of:
For example, consider the following genus-differentia definitions:
Those definitions can be expressed as a genus ("a plane figure" and two differentiae ("that has three straight bounding sides" and "that has four straight bounding sides", respectively).
It is possible to have two different genus-differentia definitions that describe the same term, especially when the term describes the overlap of two large categories. For instance, both of these genus-differentia definitions of "square" are equally acceptable:
Thus, a "square" is a member of both the genus "rectangle" and the genus "rhombus".
Classes of extensional definitions.
One important form of the extensional definition is "ostensive definition". This gives the meaning of a term by pointing, in the case of an individual, to the thing itself, or in the case of a class, to examples of the right kind. So one can explain who "Alice" (an individual) is by pointing her out to another; or what a "rabbit" (a class) is by pointing at several and expecting another to understand. The process of ostensive definition itself was critically appraised by Ludwig Wittgenstein.
An "enumerative definition" of a concept or term is an "extensional definition" that gives an explicit and exhaustive listing of all the objects that fall under the concept or term in question. Enumerative definitions are only possible for finite sets and only practical for relatively small sets.
"Divisio" and "partitio".
"Divisio" and "partitio" are classical terms for definitions. A "partitio" is simply an intensional definition. A "divisio" is not an extensional definition, but an exhaustive list of subsets of a set, in the sense that every member of the "divided" set is a member of one of the subsets. An extreme form of "divisio" lists all sets whose only member is a member of the "divided" set. The difference between this and an extensional definition is that extensional definitions list "members", and not subsets.
Nominal definitions vs real definitions.
In classical thought, a definition was taken to be a statement of the essence of a thing. Aristotle had it that an object's essential attributes form its "essential nature", and that a definition of the object must include these essential attributes.
The idea that a definition should state the essence of a thing led to the distinction between "nominal" and "real" essence, originating with Aristotle. In a passage from the Posterior Analytics, he says that the meaning of a made-up name can be known (he gives the example "goat stag"), without knowing what he calls the "essential nature" of the thing that the name would denote, if there were such a thing. This led medieval logicians to distinguish between what they called the "quid nominis" or "whatness of the name", and the underlying nature common to all the things it names, which they called the "quid rei" or "whatness of the thing". (Early modern philosophers like Locke used the corresponding English terms "nominal essence" and "real essence"). The name "hobbit", for example, is perfectly meaningful. It has a "quid nominis". But one could not know the real nature of hobbits, even if there were such things, and so the real nature or "quid rei" of hobbits cannot be known. By contrast, the name "man" denotes real things (men) that have a certain "quid rei". The meaning of a name is distinct from the nature that thing must have in order that the name apply to it.
This leads to a corresponding distinction between "nominal" and "real" definitions. A nominal definition is the definition explaining what a word means, i.e. which says what the "nominal essence" is, and is definition in the classical sense as given above. A real definition, by contrast, is one expressing the real nature or "quid rei" of the thing.
This preoccupation with essence dissipated in much of modern philosophy. Analytic philosophy in particular is critical of attempts to elucidate the essence of a thing. Russell described it as "a hopelessly muddle-headed notion".
More recently Kripke's formalisation of possible world semantics in modal logic led to a new approach to essentialism. Insofar as the essential properties of a thing are "necessary" to it, they are those things it possesses in all possible worlds. Kripke refers to names used in this way as rigid designators.
Terms with multiple definitions.
Homonyms.
A homonym is, in the strict sense, one of a group of words that share the same spelling and pronunciation but have different meanings. Thus homonyms are simultaneously homographs (words that share the same spelling, regardless of their pronunciation) "and" homophones (words that share the same pronunciation, regardless of their spelling). The state of being a homonym is called homonymy. Examples of homonyms are the pair "stalk" (part of a plant) and "stalk" (follow/harass a person) and the pair "left" (past tense of leave) and "left" (opposite of right). A distinction is sometimes made between "true" homonyms, which are unrelated in origin, such as "skate" (glide on ice) and "skate" (the fish), and polysemous homonyms, or polysemes, which have a shared origin, such as "mouth" (of a river) and "mouth" (of an animal).
Polysemes.
Polysemy is the capacity for a sign (such as a word, phrase, or symbol) to have multiple meanings (that is, multiple semes or sememes and thus multiple senses), usually related by contiguity of meaning within a semantic field. It is thus usually regarded as distinct from homonymy, in which the multiple meanings of a word may be unconnected or unrelated.
In logic and mathematics.
In mathematics, definitions are generally not used to describe existing terms, but to give meaning to a new term. The meaning of a mathematical statement changes if definitions change. The precise meaning of a term given by a mathematical definition is often different than the English definition of the word used, which can lead to confusion for students who do not pay close attention to the definitions given.
Classification of mathematical definitions.
Authors have used different terms to classify definitions used in formal languages like mathematics. Norman Swartz classifies a definition as "stipulative" if it is intended to guide a specific discussion. A stipulative definition might be considered a temporary, working definition, and can only be disproved by showing a logical contradiction. In contrast, a "descriptive" definition can be shown to be "right" or "wrong" with reference to general usage.
Swartz defines a "precising definition" as one that extends the descriptive dictionary definition (lexical definition) for a specific purpose by including additional criteria. A precising definition narrows the set of things that meet the definition.
C.L. Stevenson has identified "persuasive definition" as a form of stipulative definition which purports to state the "true" or "commonly accepted" meaning of a term, while in reality stipulating an altered use (perhaps as an argument for some specific belief). Stevenson has also noted that some definitions are "legal" or "coercive" – their object is to create or alter rights, duties, or crimes.
Recursive definitions.
A recursive definition, sometimes also called an "inductive" definition, is one that defines a word in terms of itself, so to speak, albeit in a useful way. Normally this consists of three steps:
For instance, we could define a natural number as follows (after Peano): 
So "0" will have exactly one successor, which for convenience can be called "1". In turn, "1" will have exactly one successor, which could be called "2", and so on. Notice that the second condition in the definition itself refers to natural numbers, and hence involves self-reference. Although this sort of definition involves a form of circularity, it is not vicious, and the definition has been quite successful.
In the same way, we can define ancestor as follows:
Or simply: an ancestor is a parent or a parent of an ancestor.
In medicine.
In medical dictionaries, definitions should to the greatest extent possible be:
Issues with definitions.
Fallacies of definition.
Certain rules have traditionally been given for definitions (in particular, genus-differentia definitions).
Limitations of definition.
Given that a natural language such as English contains, at any given time, a finite number of words, any comprehensive list of definitions must either be circular or rely upon primitive notions. If every term of every "definiens" must itself be defined, "where at last should we stop?" A dictionary, for instance, insofar as it is a comprehensive list of lexical definitions, must resort to circularity.
Many philosophers have chosen instead to leave some terms undefined. The scholastic philosophers claimed that the highest genera (the so-called ten "generalissima") cannot be defined, since a higher genus cannot be assigned under which they may fall. Thus being, unity and similar concepts cannot be defined. Locke supposes in "An Essay Concerning Human Understanding" that the names of simple concepts do not admit of any definition. More recently Bertrand Russell sought to develop a formal language based on logical atoms. Other philosophers, notably Wittgenstein, rejected the need for any undefined simples. Wittgenstein pointed out in his "Philosophical Investigations" that what counts as a "simple" in one circumstance might not do so in another. He rejected the very idea that every explanation of the meaning of a term needed itself to be explained: "As though an explanation hung in the air unless supported by another one", claiming instead that explanation of a term is only needed to avoid misunderstanding.
Locke and Mill also argued that individuals cannot be defined. Names are learned by connecting an idea with a sound, so that speaker and hearer have the same idea when the same word is used. This is not possible when no one else is acquainted with the particular thing that has "fallen under our notice". Russell offered his theory of descriptions in part as a way of defining a proper name, the definition being given by a definite description that "picks out" exactly one individual. Saul Kripke pointed to difficulties with this approach, especially in relation to modality, in his book "Naming and Necessity".
There is a presumption in the classic example of a definition that the "definiens" can be stated. Wittgenstein argued that for some terms this is not the case. The examples he used include "game", "number" and "family". In such cases, he argued, there is no fixed boundary that can be used to provide a definition. Rather, the items are grouped together because of a family resemblance. For terms such as these it is not possible and indeed not necessary to state a definition; rather, one simply comes to understand the "use" of the term.

</doc>
<doc id="7965" url="https://en.wikipedia.org/wiki?curid=7965" title="Disruption">
Disruption

Disruption or Disruptive may refer to:

</doc>
<doc id="7966" url="https://en.wikipedia.org/wiki?curid=7966" title="Disco">
Disco

Disco is a genre of dance music containing elements of funk, soul, pop, and salsa that was most popular in the mid to late 1970s, though it has had brief resurgences. Its initial audiences were club-goers from the gay, African American, Italian American, Latino, and psychedelic communities in Philadelphia and then later New York City during the late 1960s and early 1970s. Disco also was a reaction against both the domination of rock music and the stigmatization of dance music by the counterculture during this period. Women embraced disco as well, and the music eventually expanded to several other marginalized communities of the time.
The disco sound has soaring vocals over a steady "four-on-the-floor" beat, an eighth note (quaver) or 16th note (semi-quaver) hi-hat pattern with an open hi-hat on the off-beat, and a prominent, syncopated electric bass line. In most disco tracks, strings, horns, electric pianos, and electric guitars create a lush background sound. Orchestral instruments such as the flute are often used for solo melodies, and lead guitar is less frequently used in disco than in rock. Many disco songs use electronic synthesizers.
Well-known 1970s disco performers included Donna Summer, the Bee Gees, Boney M, KC and the Sunshine Band, The Trammps, Gloria Gaynor and Chic. While performers and singers garnered much public attention, record producers working behind the scenes played an important role. Many non-disco artists recorded disco songs at the height of disco's popularity, and films such as "Saturday Night Fever" and "Thank God It's Friday" contributed to disco's rise in mainstream popularity. Disco was the last mass popular music movement that was driven by the baby boom generation. Disco was a worldwide phenomenon, but its popularity drastically declined in the United States in 1979 and 1980, and disco was no longer popular in the U.S. by 1981. Disco Demolition Night, an anti-disco protest held in Chicago on 12 July 1979, is commonly thought of as a factor in disco's fast and drastic decline.
By the late 1970s most major U.S. cities had thriving disco club scenes, with Studio 54 being a well-known example of a disco club. Popular dances included The Hustle, a sexually suggestive dance. Discotheque-goers often wore expensive and extravagant fashions. There was also a thriving drug subculture in the disco scene, particularly for drugs that would enhance the experience of dancing to the loud music and the flashing lights, such as cocaine.
Disco was a key influence on the 1980s electronic dance music style called House. A disco revival was seen, first in 2005 with Madonna's album "Confessions on a Dance Floor", and again in 2013, as disco-styled songs by artists like Daft Punk (with Nile Rodgers), Justin Timberlake, Breakbot, and Bruno Mars filled the pop charts in the UK and the US.
History.
Origins of the term and type of nightclub.
The term is derived from "discothèque" (French for "library of phonograph records", but it was subsequently used as a term for nightclubs in Paris). By the early 1940s, the terms "Disc Jockey" and "DJ" were in use to describe radio presenters. During WW II, because of restrictions set in place by the Nazi occupiers, jazz dance halls in Occupied France played records instead of using live music. Eventually more than one of these jazz venues had the proper name "discothèque". By 1959, the term was used in Paris to describe any of these type of nightclubs. That year a young reporter named Klaus Quirini started to select and introduce records at the Scotch-Club in Aachen, West Germany. By the following year the term was being used in the United States to describe that type of club, and a type of dancing in those clubs. By 1964, "discotheque" and the shorthand "disco" were used to describe a type of sleeveless dress worn when going out to nightclubs. In September 1964, "Playboy Magazine" used the word "disco" as a shorthand for a discothèque-styled nightclub.
Proto-disco and early history of disco music.
In Philadelphia , R&B musicians and audiences from the black, gay, Italian, and Latino communities adopted several traits from the hippies and psychedelia. They included venues with a loud, overwhelming sound, free-form dancing, trippy lighting, colorful costumes, and the use of hallucinogens. Psychedelic soul groups like the Chambers Brothers and especially Sly and The Family Stone influenced proto-disco acts such as Isaac Hayes, Willie Hutch and the soul style known as the Philadelphia Sound. In addition, the perceived positivity, lack of irony, and earnestness of the hippies informed proto-disco music like M.F.S.B.'s album "Love Is the Message". To the mainstream public M.F.S.B. stood for "Mother Father Sister Brother"; to the tough areas where they came from it was understood to stand for "Mother Fuckin' Son of a Bitch". Referring to their Playing Skill/Prowess.
A forerunner to disco-style clubs was the private parties held by New York City DJ David Mancuso in The Loft, a members-only club in his home in 1970.
The first article about disco was written in 1973 by Vince Aletti for "Rolling Stone" magazine. In 1974 New York City's WPIX-FM premiered the first disco radio show.
Philadelphia soul and New York soul were evolutions of the Motown sound, and were typified by the lavish percussion and lush strings that became a prominent part of mid-1970s disco songs. Early songs with disco elements include "You Keep Me Hangin' On" (The Supremes, 1966), "Only the Strong Survive" (Jerry Butler, 1968), "The Love You Save" by Jackson 5 (1970), "Soul Makossa" (Manu Dibango, 1972), "Superstition" by Stevie Wonder (1972) Eddie Kendricks' "Keep on Truckin'" (1973) and "The Love I Lost" by Harold Melvin & The Blue Notes (1973). "Love Train" by The O'Jays (1972), with M.F.S.B. as the backup band, hit Billboard Number 1 in March 1973, and has been called "disco".
Early disco was dominated with producers and labels such as Salsoul Records (Ken, Stanley, and Joseph Cayre), West End Records (Mel Cheren), Casablanca (Neil Bogart), and Prelude (Marvin Schlachter), to name a few. The genre was also shaped by Tom Moulton, who wanted to extend the enjoyment of dance songs — thus creating the extended mix or "remix". Other influential DJs and remixers who helped to establish what became known as the "disco sound" included David Mancuso, Nicky Siano, Shep Pettibone, Larry Levan, Walter Gibbons, and Chicago-based "Godfather of House" Frankie Knuckles.
Disco-era DJs would often remix (re-edit) existing songs using reel-to-reel tape machines, and add in percussion breaks, new sections, and new sounds. DJs would select songs and grooves according to what the dancers wanted, transitioning from one song to another with a DJ mixer and using a microphone to introduce songs and speak to the audiences. Other equipment was added to the basic DJ setup, providing unique sound manipulations, such as reverb, equalization, and echo. Using this equipment, a DJ could do effects such as cutting out all but the throbbing bassline of a song, and then slowly mixing in the beginning of another song using the DJ mixer's crossfader.
Disco hit the television airwaves with the music/dance variety show "Soul Train" in 1971 hosted by Don Cornelius, then Marty Angelo's "Disco Step-by-Step Television Show" in 1975, Steve Marcus' "Disco Magic/Disco 77", Eddie Rivera's "Soap Factory", and Merv Griffin's "Dance Fever", hosted by Deney Terrio, who is credited with teaching actor John Travolta to dance for his role in the hit movie "Saturday Night Fever", as well as "DANCE" based out of Columbia, South Carolina.
Rise to the mainstream.
From 1974 through 1977, disco music continued to increase in popularity as many disco songs topped the charts. In 1974, "Love's Theme" by Barry White's Love Unlimited Orchestra became the second disco song to reach number one on the "Billboard" Hot 100, after "Love Train". MFSB also released "TSOP (The Sound of Philadelphia)", featuring vocals by The Three Degrees, and this was the third disco song to hit number one; "TSOP" was written as the theme song for "Soul Train".
The Hues Corporation's 1974 "Rock the Boat", a U.S. #1 single and million-seller, was one of the early disco songs to hit #1. The same year saw the release of "Kung Fu Fighting", performed by Carl Douglas and produced by Biddu, which reached #1 in both the U.K. and U.S., and became the best-selling single of the year and one of the best-selling singles of all time with eleven million records sold worldwide, helping to popularize disco music to a great extent. Other chart-topping disco hits that year included "Rock Your Baby" by George McCrae.
In the northwestern sections of the United Kingdom, the Northern Soul explosion, which started in late 1960s and peaked in 1974, made the region receptive to Disco, which the region's Disc Jockeys were bringing back from New York City. George McCrae's "Rock Your Baby" became the United Kingdom's first number one disco single.
Also in 1974, Gloria Gaynor released the first side-long disco mix vinyl album, which included a remake of The Jackson 5's "Never Can Say Goodbye" and two other songs, "Honey Bee" and "Reach Out (I'll Be There)". Formed by Harry Wayne Casey ("KC") and Richard Finch, Miami's KC and the Sunshine Band had a string of disco-definitive top-five hits between 1975 and 1977, including "Get Down Tonight", "That's the Way (I Like It)", "(Shake, Shake, Shake) Shake Your Booty", "I'm Your Boogie Man" and "Keep It Comin' Love". Electric Light Orchestra's 1975 hit Evil Woman, although described as Orchestral Rock, featured a violin sound that became a staple of disco. In 1979, however, ELO did release two "true" disco songs: "Last Train To London" and "Shine A Little Love".
In 1975, American singer and songwriter Donna Summer recorded a song which she brought to her producer Giorgio Moroder entitled "Love to Love You Baby" which contained a series of simulated orgasms. The song was never intended for release but when Moroder played it in the clubs it caused a sensation. Moroder released it and it went to number 2. It has been described as the arrival of the expression of raw female sexual desire in pop music. A 17-minute 12 inch single was released. The 12" single became and remains a standard in discos today.
In 1978, a multi-million selling vinyl single disco version of "MacArthur Park" by Summer was number one on the "Billboard" Hot 100 chart for three weeks and was nominated for the Grammy Award for Best Female Pop Vocal Performance. Summer's recording, which was included as part of the "MacArthur Park Suite" on her double album Live and More, was eight minutes and forty seconds long on the album. The shorter seven-inch vinyl single version of the MacArthur Park was Summer's first single to reach number one on the Hot 100; it doesn't include the balladic second movement of the song, however. A 2013 remix of "Mac Arthur Park" by Summer hit #1 on the Billboard Dance Charts marking five consecutive decades with a #1 hit on the charts. From 1978 to 1979, Summer continued to release hits such as "Last Dance", "Bad Girls", "Heaven Knows", "No More Tears (Enough Is Enough)", "Hot Stuff" and "On the Radio", all very successful disco songs.
The Bee Gees used Barry Gibb's falsetto to garner hits such as "You Should Be Dancing", "Stayin' Alive", "Night Fever", "More Than A Woman" and "Love You Inside Out". Andy Gibb, a younger brother to the Bee Gees, followed with similarly-styled solo hits such as "I Just Want to Be Your Everything", "(Love Is) Thicker Than Water" and "Shadow Dancing". In 1975, hits such as Van McCoy's "The Hustle" and "Could It Be Magic" brought disco further into the mainstream. Other notable early disco hits include The Jackson 5's "Dancing Machine" (1974), Barry White's "You're the First, the Last, My Everything" (1974), LaBelle's "Lady Marmalade" (1975) and Silver Convention's "Fly Robin Fly" (1975).
Pop pre-eminence.
In December 1977, the film "Saturday Night Fever" was released. The film was marketed specifically to broaden disco's popularity beyond its primarily black and Latin audiences. It was a huge success and its soundtrack became one of the best-selling albums of all time. The idea for the film was sparked by a 1976 "New York" magazine article titled: "Tribal Rites Of The New Saturday Night" which chronicled the disco culture in mid-1970s New York City.
Chic was formed by Nile Rodgers — a self described "street hippie" from late 1960s New York — and Martin Dow, a DJ from Key West, Florida who pioneered the NYC sound across that state. "Le Freak" was a popular 1978 single that is regarded as an iconic song of the genre. Other hits by Chic include the often-sampled "Good Times" (1979) and "Everybody Dance". The group regarded themselves as the disco movement's rock band that made good on the hippie movement's ideals of peace, love, and freedom. Every song they wrote was written with an eye toward giving it "deep hidden meaning" or D.H.M.
The Jacksons (previously The Jackson 5) did many disco songs from 1975 to 1980, including "Shake Your Body (Down to the Ground)" (1978), "Blame it on the Boogie" (1978), "Lovely One" (1980), and "Can You Feel It" (1980)—all sung by Michael Jackson, whose 1979 solo album, "Off the Wall", included several disco hits, including the album's title song, "Rock with You", "Workin' Day and Night", and his second chart-topping solo hit in the disco genre, "Don't Stop 'til You Get Enough".
Crossover appeal.
Disco's popularity led many non-disco artists to record disco songs at the height of its popularity. Many of their songs were not "pure" disco, but were instead rock or pop songs with (sometimes inescapable) disco influence or overtones. Notable examples include Earth Wind & Fire's "Boogie Wonderland" with The Emotions (1979), Mike Oldfield's "Guilty" (1979) Blondie's "Heart of Glass" (1978), Cher's "Hell on Wheels" and "Take Me Home" (both 1979), Barry Manilow's "Copacabana" (1978), David Bowie's "John I'm Only Dancing (Again)" (1975), Rod Stewart's "Da Ya Think I'm Sexy?" (1979), Electric Light Orchestra's "Shine a Little Love" and "Last Train to London" (both 1979), George Benson's "Give Me the Night" (1980), Elton John and Kiki Dee's "Don't Go Breaking My Heart" (1976), M's "Pop Muzik" (1979), Barbra Streisand's "The Main Event"(1979) and Diana Ross' "Upside Down" (1980). The biggest hit by Ian Dury and the Blockheads, best known as a new wave band, was "Hit Me with Your Rhythm Stick" (1978), featuring a strong disco sound.
Even hard-core mainstream rockers mixed elements of disco with their typical rock 'n roll style in songs. Progressive rock group Pink Floyd, when creating their rock opera "The Wall", used disco-style components in their song, "Another Brick in the Wall, Part 2" (1979)—which became the group's only #1 hit single (in both the US and UK). The Eagles gave nods to disco with "One of These Nights" (1975) and "Disco Strangler" (1979), Paul McCartney & Wings did "Goodnight Tonight" (1979), Queen did "Another One Bites the Dust" (1980), The Rolling Stones did "Miss You" (1978) and "Emotional Rescue" (1980), Chicago did "Street Player" (1979), The Clash did "Ivan Meets G.I. Joe" (1980), The Beach Boys did "Here Comes the Night" (1979), The Kinks did "(Wish I Could Fly Like) Superman" (1979), and the J. Geils Band did "Come Back" (1980). Even heavy metal music group KISS jumped in with "I Was Made For Lovin' You" (1979). Ringo Starr's album "Ringo the 4th" (1978) features a strong disco influence.
The disco fad was also picked up even by "non-pop" artists, including the 1979 U.S. number one hit "No More Tears (Enough Is Enough)" by Easy listening singer Barbra Streisand in a duet with Donna Summer. Country music artist Connie Smith covered Andy Gibb's "I Just Want to Be Your Everything" in 1977, Bill Anderson did "Double S" in 1978, and Ronnie Milsap covered Tommy Tucker's "High Heel Sneakers" in 1979.
Disco revisions of songs.
Pre-existing non-disco songs and standards would frequently be "disco-ized" in the 1970s. The rich orchestral accompaniment that became identified with the disco era conjured up the memories of the big band era—which brought out several artists that recorded and disco-ized some big band arrangements including Perry Como, who re-recorded his 1929 and 1939 hit, "Temptation", in 1975, as well as Ethel Merman, who released an album of disco songs entitled "The Ethel Merman Disco Album" in 1979.
Myron Floren, second-in-command on "The Lawrence Welk Show", released a recording of the Clarinet Polka entitled "Disco Accordion." Similarly, Bobby Vinton adapted The Pennsylvania Polka into a song named "Disco Polka." Easy listening icon Percy Faith, in one of his last recordings, released an album entitled "Disco Party" (1975) and recorded a disco version of his famous "Theme from A Summer Place" in 1976. Classical music was even adapted for disco, notably Walter Murphy's "A Fifth of Beethoven" (1976, based on the first movement of Beethoven's 5th Symphony) and "Flight 76" (1976, based on Rimsky-Korsakov's "Flight of the Bumblebee"), and Louis Clark's "Hooked On Classics" series of albums and singles.
Notable disco hits based on movie and television themes included a medley from "Star Wars", "Star Wars Theme/Cantina Band" (1977) by Meco, and "Twilight Zone/Twilight Tone" (1979) by The Manhattan Transfer. Even the "I Love Lucy" theme wasn't spared from being disco-ized. Many original television theme songs of the era also showed a strong disco influence, such as "Keep Your Eye On The Sparrow" (theme from "Baretta", performed by Sammy Davis, Jr. and later a hit single for Rhythm Heritage), "Theme from "S.W.A.T."" (from "S.W.A.T", original and single versions by Rhythm Heritage), and Mike Post's "Theme from "Magnum, P.I."".
Parodies.
Several parodies of the disco style were created. Rick Dees, at the time a radio DJ in Memphis, Tennessee, recorded "Disco Duck" (1976) and "Dis-Gorilla" (1977); Frank Zappa parodied the lifestyles of disco dancers in "Disco Boy" on his 1976 "Zoot Allures" album, and in "Dancin' Fool" on his 1979 "Sheik Yerbouti" album; "Weird Al" Yankovic's eponymous 1983 debut album includes a disco song called "Gotta Boogie", an extended pun on the similarity of the disco move to the American slang word "booger".
Backlash and decline.
By the late 1970s, a strong anti-disco sentiment developed among rock fans and musicians, particularly in the United States. The slogans "disco sucks" and "death to disco" became common. Rock artists such as Rod Stewart and David Bowie who added disco elements to their music were accused of being sell outs.
The punk subculture in the United States and United Kingdom was often hostile to disco. Jello Biafra of the Dead Kennedys, in the song "Saturday Night Holocaust", likened disco to the cabaret culture of Weimar-era Germany for its apathy towards government policies and its escapism. Mark Mothersbaugh of Devo said that disco was "like a beautiful woman with a great body and no brains", and a product of political apathy of that era. New Jersey rock critic Jim Testa wrote "Put a Bullet Through the Jukebox", a vitriolic screed attacking disco that was considered a punk call to arms.
Anti-disco sentiment was expressed in some television shows and films. A recurring theme on the show "WKRP in Cincinnati" was a hostile attitude towards disco music. In one scene of the 1980 comedy film "Airplane!", a city skyline features a radio tower with a neon-lighted station callsign. A disc jockey voiceover says: "WZAZ in Chicago, where disco lives forever!" Then a wayward airplane slices the radio tower with its wing, the voiceover goes silent, and the lighted callsign goes dark.
July 12, 1979 became known as "the day disco died" because of Disco Demolition Night, an anti-disco demonstration in a baseball double-header at Comiskey Park in Chicago. Rock-station DJs Steve Dahl and Garry Meier, along with Michael Veeck, son of Chicago White Sox owner Bill Veeck, staged the promotional event for disgruntled rock fans between the games of a White Sox doubleheader. The event, which involved exploding disco records, ended with a riot, during which the raucous crowd tore out seats and pieces of turf, and caused other damage. The Chicago Police Department made numerous arrests, and the extensive damage to the field forced the White Sox to forfeit the second game to the Detroit Tigers, who had won the first game. Six months prior to the chaotic event, popular progressive rock radio station WDAI (WLS-FM) had suddenly switched to an all disco format, disenfranchising thousands of Chicago rock fans and leaving Dahl unemployed.
On July 21, 1979, the top six records on the U.S. music charts were disco songs.
By September 22, there were no vocal disco songs in the US Top 10 chart, with the exception of Herb Alpert's jazzy-disco instrumental "Rise".
Some in the media, in celebratory tones, declared disco "dead" and rock revived.
On December 27, 1980, disco's consecutive weekly streak of charting on the US Top 10 had reached its end, as no disco hits (of any kind) were present in the US Top 10 for the first time since the week ending, February 22, 1975. 
Impact on music industry.
The anti-disco backlash, combined with other societal and radio industry factors, changed the face of pop radio in the years following Disco Demolition Night. Starting in the 1980s, country music began a slow rise in American main pop charts. Emblematic of country music's rise to mainstream popularity was the commercially successful 1980 movie "Urban Cowboy". Somewhat ironically, the star of the film was John Travolta, who only three years before had starred in "Saturday Night Fever", a film that featured disco culture.
During this period of decline in disco's popularity, several record companies folded, were reorganized, or were sold. In 1979, MCA Records purchased ABC Records, absorbed some of its artists, and then shut the label down. RSO Records founder Robert Stigwood left the label in 1981 and TK Records closed in the same year. Salsoul Records continues to exist in the 2000s, but primarily is used as a reissue brand. Casablanca Records had been releasing fewer records in the 1980s, and was shut down in 1986 by parent company PolyGram.
Many groups that were popular during the disco period subsequently struggled to maintain their success—even those that tried to adapt to evolving musical tastes. The Bee Gees, for instance, only had four top-40 hits in the United States after the 1970s even though later songs they wrote and had "other" artists perform were successful. Of the handful of groups "not" taken down by disco's fall from favor, Kool and the Gang, The Jacksons—and Michael Jackson in particular—stand out: In spite of having helped define the disco sound early on, they continued to make popular and danceable, if more refined, songs for yet another generation of music fans in the 1980s and beyond.
Factors contributing to disco's decline.
Factors that have been cited as leading to the decline of disco in the United States include economic and political changes at the end of the 1970s as well as burnout from the hedonistic lifestyles led by participants. In the years since Disco Demolition Night, some social critics have described the backlash as implicitly macho and bigoted, and an attack on non-white and non-heterosexual cultures.
In January 1979, rock critic Robert Christgau argued that homophobia, and most likely racism, were reasons behind the backlash, a conclusion seconded by John Rockwell. Craig Werner wrote: "The Anti-disco movement represented an unholy alliance of funkateers and feminists, progressives and puritans, rockers and reactionaries. Nonetheless, the attacks on disco gave respectable voice to the ugliest kinds of unacknowledged racism, sexism and homophobia." Legs McNeil, founder of the fanzine "Punk", was quoted in an interview as saying, "the hippies always wanted to be black. We were going, 'fuck the blues, fuck the black experience'." He also said that disco was the result of an "unholy" union between homosexuals and blacks.
Steve Dahl, who had spearheaded Disco Demolition Night, denied any racist or homophobic undertones to the promotion, saying, "It's really easy to look at it historically, from this perspective, and attach all those things to it. But we weren't thinking like that." It has been noted that British punk rock critics of disco were very supportive of the pro-black/anti-racist reggae genre. Robert Christgau and Jim Testa have said that there were legitimate artistic reasons for being critical of disco.
In 1979 the music industry in the United States was undergoing its worst slump in decades, and disco, despite its mass popularity, was blamed. The producer-oriented sound was having difficulty mixing well with the industry's artist-oriented marketing system. Harold Childs, senior vice president at A&M Records, told the "Los Angeles Times" that "radio is really desperate for rock product" and "they're all looking for some white rock-n-roll". Gloria Gaynor argued that the music industry supported the destruction of disco because rock music producers were losing money and rock musicians were losing the spotlight. However, disco music remained relatively successful in the early 1980s, with big hits like Irene Cara's "Flashdance... What a Feeling", K.C. & The Sunshine Band's last major hit, "Give It Up", "Running With The Night" by Lionel Richie and Madonna's first album had strong disco influences. Record producer Giorgio Moroder's soundtracks to "American Gigolo", "Flashdance" and "Scarface" (which also had a heavy disco influence) proved to be successful. Also, Queen's 1982 album, "Hot Space" was inspired by the genre as well.
In the 1990s, disco and its legacy became more accepted by music artists and listeners alike, as more songs and films were released that referenced disco. Examples of songs during this time that were influenced by disco included Deee-Lite's "Groove Is in the Heart" (1990), U2's "Lemon" (1993), Blur's "Girls & Boys" (1994) & "Entertain Me" (1995), and Pulp's "Disco 2000" (1995), while films such as "Boogie Nights" (1997) and "The Last Days of Disco" (1998) featured primarily disco soundtracks. Even some heavy metal songs released during the later part of the 1990s utilized the hi-hat cymbal beat which was reminiscent of disco.
2000s - present: The success of nu-disco and disco revival.
In the early 2000s, an updated genre of disco called "nu-disco" began breaking into the mainstream. A few examples would be songs like Daft Punk's "One More Time" and Kylie Minogue's "Love At First Sight" became club favorites and commercial successes. Several nu-disco songs were crossovers with funky house, such as Spiller's "Groovejet (If This Ain't Love)" and Modjo's "Lady (Hear Me Tonight)", both songs sampling older disco songs and both reaching number 1 on the UK Singles Chart in 2000. Robbie Williams' disco hit "Rock DJ" was the UK's fourth best-selling single the same year. Rock band Manic Street Preachers released a disco song, "Miss Europa Disco Dancer", in 2001. The song's disco influence, which appears on "Know Your Enemy", was described as being "much-discussed". In 2005, Madonna immersed herself in the disco music of the 1970s, and released her album "Confessions on a Dance Floor" to rave reviews. In addition to that, her song "Hung Up" became a major top ten hit and club staple, and sampled ABBA's 1970s hit "Gimme! Gimme! Gimme! (A Man After Midnight)". In addition to her disco-influenced attire to award shows and interviews, her Confessions Tour also incorporated various elements of the 1970s, such as disco balls, a mirrored stage design, and the roller derby.
In 2013, several 1970s-style disco and funk songs charted, and the pop charts had more dance songs than at any other point since the late 1970s. The biggest disco hit of the year as of June was "Get Lucky" by Daft Punk, featuring Nile Rodgers on guitar. "Random Access Memories" also ended up winning Album of the Year at the 2014 Grammys. Other disco-styled songs that made it into the top 40 were Robin Thicke's "Blurred Lines" (No. 1), Justin Timberlake's "Take Back The Night" (No. 29), Bruno Mars' "Treasure" (No. 5) and Michael Jackson's posthumous release "Love Never Felt So Good" (No. 9). In addition, Arcade Fire's "Reflektor" featured strong disco elements. In 2014, disco music could be found in Lady Gaga's "Artpop" and Katy Perry's "Birthday". Other disco songs from 2014 include "I Want It All" By Karmin and 'Wrong Club" by The Ting Tings. Other top ten hits from 2015 like Mark Ronson's disco groove-infused "Uptown Funk", Maroon 5's "Sugar", The Weeknd's "Can't Feel My Face" and Jason Derulo's "Want To Want Me" also ascended the charts and have a strong disco influence. Disco mogul and producer Giorgio Moroder also re-appeared with his new album "Déjà Vu" in 2015 which has proved to be a modest success. Other songs from 2015 like "I Don't Like It, I Love It" by Flo Rida, "Adventure of a Lifetime" by Coldplay, "Back Together (Robin Thicke song)" by Robin Thicke and "Levels (Nick Jonas song)" by Nick Jonas feature disco elements as well.
Euro disco.
As disco's popularity sharply declined in the United States, abandoned by major U.S. record labels and producers, European disco continued evolving within the broad mainstream pop music scene. European acts Silver Convention, Love and Kisses, Munich Machine, and American acts Donna Summer and the Village People, were acts that defined the late 1970s Euro disco sound. Producers Giorgio Moroder, whom AllMusic described as "one of the principal architects of the disco sound" with the Donna Summer hit "I Feel Love" (1977), and Jean-Marc Cerrone were involved with Euro disco. The German group Kraftwerk also had an influence on Euro disco.
By far the most successful Euro disco act was ABBA. This Swedish quartet, which sang in English had hits as "Waterloo" (1974), "Fernando" (1976), "Take a Chance on Me" (1978), "Gimme! Gimme! Gimme! (A Man After Midnight)" (1979), and their signature smash "Dancing Queen" (1976)—ranks as the eighth best-selling act of all time. Other prominent European pop and disco groups were Luv' from the Netherlands and Boney M., a group of four West Indian singers and dancers masterminded by West German record producer Frank Farian. Boney M. charted worldwide hits with such songs as "Daddy Cool", "Ma Baker" and "Rivers of Babylon". Another Euro disco act was Amanda Lear, where euro-disco sound is most heard in Enigma ("Give a bit of Mmh to me") song (1978).
In France, Claude François who re-invented himself as the king of French disco, released "La plus belle chose du monde", a French version of the Bee Gees hit record, "Massachusetts", which became a big hit in Canada and Europe and "Alexandrie Alexandra" was posthumously released on the day of his burial and became a worldwide hit. Dalida released "J'attendrai", which became a big hit in Canada and Japan, and Cerrone's early hit songs, "Love in C Minor", "Give Me Love" and "Supernature" became major hits in the U.S. and Europe.
Role of Motown.
Diana Ross was one of the first Motown artists to embrace the disco sound with her successful 1976 outing "Love Hangover" from her self-titled album. Ross would continue to score disco hits for the rest of the disco era, including the 1980 dance classics "Upside Down" and "I'm Coming Out" (the latter immediately becoming a favorite in the gay community). The Supremes, the group that made Ross famous, scored a handful of hits in the disco clubs without Ross, most notably 1976's "I'm Gonna Let My Heart Do the Walking" and, their last charted single before disbanding, 1977's "You're My Driving Wheel".
Also noteworthy are Cheryl Lynn's "Got to Be Real" (1978), Evelyn "Champagne" King's "Shame" (1978), Cher's "Take Me Home" (1979), Sister Sledge's "We Are Family" (1979), Geraldine Hunt's "Can't Fake the Feeling" (1980), and Walter Murphy's various attempts to bring classical music to the mainstream, most notably his hit "A Fifth of Beethoven" (1976), which was inspired by Beethoven's fifth symphony.
Musical characteristics.
The music typically layered soaring, often-reverberated vocals, often doubled by horns, over a background "pad" of electric pianos and "chicken-scratch" rhythm guitars played on an electric guitar. "The “chicken scratch” sound is achieved by lightly pressing the strings against the fretboard and then quickly releasing them just enough to get a slightly muted scratching oun while constantly strumming very close to the bridge." Other backing keyboard instruments include the piano, electric organ (during early years), string synth, and electromechanical keyboards such as the Fender Rhodes electric piano, Wurlitzer electric piano, and Hohner Clavinet. Synthesizers are also fairly common in disco, especially in the late 1970s.
The rhythm is laid down by prominent, syncopated basslines (with heavy use of broken octaves, that is, octaves with the notes sounded one after the other) played on the bass guitar and by drummers using a drum kit, African/Latin percussion, and electronic drums such as Simmons and Roland drum modules. The sound was enriched with solo lines and harmony parts played by a variety of orchestral instruments, such as harp, violin, viola, cello, trumpet, saxophone, trombone, clarinet, flugelhorn, French horn, tuba, English horn, oboe, flute (sometimes especially the alto flute and occasionally bass flute), piccolo, timpani and synth strings, string section or a full string orchestra.
Most disco songs have a steady four-on-the-floor beat, a quaver or semi-quaver hi-hat pattern with an open hi-hat on the off-beat, and a heavy, syncopated bass line. Other Latin rhythms such as the rhumba, the samba and the cha-cha-cha are also found in disco recordings, and Latin polyrhythms, such as a rhumba beat layered over a merengue, are commonplace. The quaver pattern is often supported by other instruments such as the rhythm guitar and may be implied rather than explicitly present.
Songs often use syncopation, which is the accenting of unexpected beats. In general, the difference between a disco, or any dance song, and a rock or popular song is that in dance music the bass drum hits "four to the floor", at least once a beat (which in 4/4 time is 4 beats per measure), whereas in rock the bass hits on one and three and lets the snare take the lead on two and four (the "backbeat"). Disco is further characterized by a 16th note division of the quarter notes as shown in the second drum pattern below, after a typical rock drum pattern.
The orchestral sound usually known as "disco sound" relies heavily on string sections and horns playing linear phrases, in unison with the soaring, often reverberated vocals or playing instrumental fills, while electric pianos and chicken-scratch guitars create the background "pad" sound defining the harmony progression. Typically, all of the doubling of parts and use of additional instruments creates a rich "wall of sound". There are, however, more minimalistic flavors of disco with reduced, transparent instrumentation, pioneered by Chic (band).
In 1977, Giorgio Moroder again became responsible for a development in disco. Alongside Donna Summer and Pete Bellotte he wrote the song "I Feel Love" for Summer to perform. It became the first well-known disco hit to have a completely synthesised backing track. The song is still considered to have been well ahead of its time. Other disco producers, most famously Tom Moulton, grabbed ideas and techniques from dub music (which came with the increased Jamaican migration to New York City in the 1970s) to provide alternatives to the "four on the floor" style that dominated. DJ Larry Levan utilized styles from dub and jazz and remixing techniques to create early versions of house music that sparked the genre.
Production.
The "disco sound" was much more costly to produce than many of the other popular music genres from the 1970s. Unlike the simpler, four-piece band sound of the funk, soul of the late 1960s, or the small jazz organ trios, disco music often included a large pop band, with several chordal instruments (guitar, keyboards, synthesizer), several drum or percussion instruments (drumkit, Latin percussion, electronic drums), a horn section, a string orchestra, and a variety of "classical" solo instruments (for example, flute, piccolo, and so on).
Disco songs were arranged and composed by experienced arrangers and orchestrators, and record producers added their creative touches to the overall sound using multitrack recording techniques and effects units. Recording complex arrangements with such a large number of instruments and sections required a team that included a conductor, copyists, record producers, and mixing engineers. Mixing engineers had an important role in the disco production process, because disco songs used as many as 64 tracks of vocals and instruments. Mixing engineers and record producers, under the direction of arrangers, compiled these tracks into a fluid composition of verses, bridges, and refrains, complete with orchestral builds and breaks. Mixing engineers and record producers helped to develop the "disco sound" by creating a distinctive-sounding disco mix.
Early records were the "standard" 3 minute version until Tom Moulton came up with a way to make songs longer, wanting to take a crowd to another level and keep them dancing longer, he found that was impossible with 45-RPM vinyl discs of the time (which could usually hold no more than 5 minutes of good-quality music). With the help of José Rodriguez, his remasterer, he pressed a single on a 10" disc instead of 7". They cut the next single on a 12" disc, the same format as a standard album. This method fast became the standard format for all DJs of the genre.
Because record sales were often dependent on floor play in leading clubs, DJs were also important to the development and popularization of disco music. Notable DJs include Rex Potts (Loft Lounge, Sarasota, Florida), Karen Cook, Jim Burgess, Walter Gibbons, John "Jellybean" Benitez, Richie Kaczar of Studio 54, Rick Gianatos, Francis Grasso of Sanctuary, Larry Levan, Ian Levine, Neil "Raz" Rasmussen & Mike Pace of L'amour Disco in Brooklyn, Preston Powell of Magique, Jennie Costa of Lemontrees, Tee Scott, Tony Smith of Xenon, John Luongo, Robert Ouimet of The Limelight, and David Mancuso.
Disco clubs and culture.
By the late 1970s most major U.S. cities had thriving disco club scenes, but the largest scenes were in San Francisco, Miami, and most notably New York City. The scene was centered on discotheques, nightclubs, and private loft parties where DJs would play disco hits through powerful PA systems for the patrons who came to dance. The DJs played "... a smooth mix of long single records to keep people 'dancing all night long'". Some of the most prestigious clubs had elaborate lighting systems that throbbed to the beat of the music.
In October 1975 notable discos included "Studio One" in Los Angeles, "Leviticus" in New York, "Dugan's Bistro" in Chicago, and "The Library" in Atlanta. In the late 1970s, Studio 54 in New York City was arguably the most well known nightclub in the world. This club played a major formative role in the growth of disco music and nightclub culture in general.
Disco dancing.
In the early years dancers in discos danced in a "hang loose" style. Popular dances included "Bump", "Penguin", "Boogaloo", "Watergate" and the "Robot". By October 1975 The Hustle reigned. It was highly stylized, sophisticated and overtly sexual. Variations included the Brooklyn Hustle, New York Hustle and Latin Hustle.
During the disco era, many nightclubs would commonly host disco dance competitions or offer free instructional lessons. Some cities had disco dance instructors or dance schools, which taught people how to do popular disco dances such as ""touch dancing," ""the hustle," and "the cha cha." The pioneer of disco dance instruction was Karen Lustgarten in San Francisco in 1973. Her book "The Complete Guide to Disco Dancing" (Warner Books, 1978) was the first to name, break down and codify popular disco dances as a dance form and distinguish between disco freestyle, partner and line dances. The book hit the "New York Time" bestseller List for 13 weeks and was translated into Chinese, German and French.
In Chicago, Step By Step launched with the sponsorship support of the Coca-Cola company. Produced in the same studio that Don Cornelius used for the nationally syndicated television show, Soul Train, Step by Step's audience grew and became a success. The dynamic dance duo, Robin and Reggie spent the week teaching disco dancing in the disco clubs. The instructional show which aired on Saturday mornings had a following that would stay up all night on Fridays so they could be on the set the next morning, ready to return to the disco Saturday night equipped with the latest personalized dance steps. The producers of the show, John Reid and Greg Roselli, routinely made appearances at disco functions with Robin and Reggie to scout out dancing talent and promote upcoming events such as Disco Night at White Sox Park.
Some notable professional dance troupes of the 1970s included Pan's People and Hot Gossip. For many dancers, the primary influence of the 1970s disco age is still predominantly the film "Saturday Night Fever" (1977). This developed into the music and dance style of such films as "Fame" (1980), "Disco Dancer" (1982), "Flashdance" (1983), and "The Last Days of Disco" (1998). It also helped spawn dance competition TV shows such as Dance Fever (1979).
Disco fashion.
Disco fashions were very trendy in the late 1970s. Discothèque-goers often wore expensive and extravagant fashions for nights out at their local disco, such as sheer, flowing Halston dresses for women and shiny polyester Qiana shirts for men with pointy collars, preferably open at the chest, often worn with double-knit polyester shirt jackets with matching trousers known as the leisure suit. Necklaces and medallions were a common fashion accessory.
Drug subculture and sexual promiscuity.
In addition to the dance and fashion aspects of the disco club scene, there was also a thriving drug subculture, particularly for drugs that would enhance the experience of dancing to the loud music and the flashing lights, such as cocaine (nicknamed "blow"), amyl nitrite "poppers", and the "... other quintessential 1970s club drug Quaalude, which suspended motor coordination and gave the sensation that one's arms and legs had turned to "Jell-O." Paul Gootenberg states that "he relationship of cocaine to 1970s disco culture cannot be stressed enough..." 
According to Peter Braunstein, the "massive quantities of drugs ingested in discotheques produced the next cultural phenomenon of the disco era: rampant promiscuity and public sex. While the dance floor was the central arena of seduction, actual sex usually took place in the nether regions of the disco: bathroom stalls, exit stairwells, and so on. In other cases the disco became a kind of 'main course' in a hedonist's menu for a night out."
Famous disco bars included the Paradise Garage and Crisco Disco as well as "... cocaine-filled celeb hangouts such as Manhattan's Studio 54," which was operated by Steve Rubell and Ian Schrager. Studio 54 was notorious for the hedonism that went on within; the balconies were known for sexual encounters, and drug use was rampant. Its dance floor was decorated with an image of the "Man in the Moon" that included an animated cocaine spoon.
Influence on other music.
1982–1990: Post-disco and dance.
The transition from the late-1970s disco styles to the early-1980s dance styles was marked primarily by the change from complex arrangements performed by large ensembles of studio session musicians (including a horn section and an orchestral string section), to a leaner sound, in which one or two singers would perform to the accompaniment of synthesizer keyboards and drum machines.
In addition, dance music during the 1981–83 period borrowed elements from blues and jazz, creating a style different from the disco of the 1970s. This emerging music was still known as disco for a short time, as the word had become associated with any kind of dance music played in discothèques. Examples of early 1980s dance sound performers include D. Train, Kashif, and Patrice Rushen. These changes were influenced by some of the notable R&B and jazz musicians of the 1970s, such as Stevie Wonder, Kashif and Herbie Hancock, who had pioneered "one-man-band"-type keyboard techniques. Some of these influences had already begun to emerge during the mid-1970s, at the height of disco's popularity.
During the first years of the 1980s, the disco sound began to be phased out, and faster tempos and synthesized effects, accompanied by guitar and simplified backgrounds, moved dance music toward the funk and pop genres. This trend can be seen in singer Billy Ocean's recordings between 1979 and 1981. Whereas Ocean's 1979 song "American Hearts" was backed with an orchestral arrangement played by the Los Angeles Symphony Orchestra, his 1981 song "One of Those Nights (Feel Like Gettin' Down)" had a more bare, stripped-down sound, with no orchestration or symphonic arrangements. This drift from the original disco sound is called post-disco. In this music scene there are rooted subgenres, such as Italo disco, techno, house, dance-pop, boogie, and early alternative dance. During the early 1980s, dance music dropped the complicated song structure and orchestration that typified the disco sound.
TV themes.
During the 1970s, many TV theme songs were produced (or older themes updated) with disco influenced music. Examples include "S.W.A.T." (1975), "Wonder Woman" (1975), "Charlie's Angels" (1976), "NBC Saturday Night At The Movies" (1976), "The Love Boat" (1977), "The Donahue Show" (1977), "CHiPs" (1977), "The Professionals" (1977), "Three's Company" (1977), "Dallas" (1978), "Kojak" (1978), "The Hollywood Squares" (1979). The British Science Fiction program "" (1975) also featured a soundtrack strongly influenced by disco. This was especially evident in the show's second season.
DJ culture.
The rising popularity of disco came in tandem with developments in in the role of the DJ. DJing developed from the use of multiple record turntables and DJ mixers to create a continuous, seamless mix of songs, with one song transitioning to another with no break in the music to interrupt the dancing. The resulting DJ mix differed from previous forms of dance music in the 1960s, which were oriented towards live performances by musicians. This in turn affected the arrangement of dance music, since songs in the disco era typically contained beginnings and endings marked by a simple beat or riff that could be easily used to transition to a new song. The development of DJing was also influenced by new turntablism techniques, such as beatmatching, a process facilitated by the introduction of new turntable technologies such as the Techics SL-1200 MK 2, first sold in 1978, which had a precise variable pitch control and a direct drive motor. DJs were often avid record collectors, who would hunt through used record stores for obscure vintage soul and funk recordings. DJs helped to introduce rare records and new artists to audiences.
In the 1970s, individual DJs became more prominent, and some DJs, such as Larry Levan, the resident at Paradise Garage, Jim Burgess and Tee Scott became famous. Levan, for example, developed a cult following amongst club-goers, who referred to his DJ sets as "Saturday Mass". Some DJs would use reel to reel tape recorders to make remixes and tape edits of songs. Some DJs who were making remixes made the transition from the DJ booth to becoming a record producer, notably Burgess. Scott developed several innovations. He was the first disco DJ to use three turntables as sound sources, the first to simultaneously play two beatmatched records, the first user of electronic effects units in his mixes and an innovator in mixing dialogue in from well-known movies into his mixes, typically over a percussion break.
Rave culture.
As the Disco era came to a close in the late 1970s, rave culture began to see significant growth. Rave culture incorporated disco culture's same love of dance music, drug exploration, sexual promiscuity, and hedonism. Like disco, the rave scene was also based around DJs, dancing and club drugs. Although disco culture started out underground, it eventually thrived in the mainstream by the late 1970s; in contrast, the rave culture would make an effort to stay underground to avoid the animosity that was still surrounding disco and dance music. The rave scene also stayed underground to avoid law enforcement attention that was directed at the rave culture due to its use of secret, unauthorized warehouses for some dances and its association with illegal club drugs like Ecstasy.
Hip hop and electro.
The disco sound had a strong influence on early hip hop. Most of the early rap/hip-hop songs were created by isolating existing disco bass-guitar lines and dubbing over them with MC rhymes. The Sugarhill Gang used Chic's "Good Times" as the foundation for their 1979 hit "Rapper's Delight", generally considered to be the song that first popularized rap music in the United States and around the world. In 1982, Afrika Bambataa released the single "Planet Rock", which incorporated electronica elements from Kraftwerk's "Trans-Europe Express" and "Numbers" as well as YMO's "Riot in Lagos".
The Planet Rock sound also spawned a hip-hop electronic dance trend, electro music, which included songs such as Planet Patrol's "Play at Your Own Risk" (1982), C Bank's "One More Shot" (1982), Cerrone's "Club Underworld" (1984), Shannon's "Let the Music Play" (1983), Freeez's "I.O.U." (1983), Midnight Star's "Freak-a-Zoid" (1983), Chaka Khan's "I Feel For You" (1984).
Post-punk.
The post-punk movement that originated in the late 1970s both supported punk rock's rule breaking while rejecting its back to raw rock music element. Post-punk's mantra of constantly moving forward lent itself to both openness to and experimentation with elements of disco and other styles. Public Image Limited is considered the first post-punk group. The group's second album "Metal Box" fully embraced the "studio as instrument" methodology of disco. The group's founder John Lydon, the former lead singer for the Sex Pistols, told the press that disco was the only music he cared for at the time. No wave was a subgenre of post-punk centered in New York City.
For shock value, James Chance who was a notable member of the no wave scene, penned an article in the "East Village Eye" urging his readers to move uptown and get "trancin' with some superadioactive disco voodoo funk". His band James White and the Blacks wrote a disco album "Off White". Their performances resembled those of disco performers (horn section, dancers and so on). In 1981 ZE Records led the transition from no wave into the more subtle mutant disco (post-disco/punk) genre. Mutant disco acts such as Kid Creole and the Coconuts, Was Not Was, ESG and Liquid Liquid influenced several British post-punk acts such as New Order, Orange Juice and A Certain Ratio.
Dance-punk.
In the early 2000s the dance-punk (new rave in the United Kingdom) emerged as a part of a broader post punk revival. It fused elements of punk-related rock with different forms of dance music including disco. Klaxons, LCD Soundsystem, Death From Above 1979, The Rapture and Shitdisco were among acts associated with the genre.
Nu-disco.
Nu-disco is a 21st-century dance music genre associated with the renewed interest in 1970s and early 1980s disco, mid-1980s Italo disco, and the synthesizer-heavy Euro disco aesthetics. The moniker appeared in print as early as 2002, and by mid-2008 was used by record shops such as the online retailers Juno and Beatport. These vendors often associate it with re-edits of original-era disco music, as well as with music from European producers who make dance music inspired by original-era American disco, electro and other genres popular in the late 1970s and early 1980s. It is also used to describe the music on several American labels that were previously associated with the genres electroclash and french house.

</doc>
<doc id="7970" url="https://en.wikipedia.org/wiki?curid=7970" title="Darwin">
Darwin

Darwin most often refers to:
Darwin may also refer to the following:

</doc>
<doc id="7973" url="https://en.wikipedia.org/wiki?curid=7973" title="Donegal fiddle tradition">
Donegal fiddle tradition

The Donegal fiddle tradition is the way of playing the fiddle that is traditional in County Donegal, Ireland. It is one of the distinct fiddle traditions within Irish traditional music.
The distinctness of the Donegal tradition developed due to the close relations between Donegal and Scotland, and the Donegal repertoire and style has influences from Scottish fiddle music. For example, in addition to the standard tune types such as Jigs and Reels, the Donegal tradition also has Highlands (influenced by the Scottish Strathspey). The distinctiveness of the Donegal tradition led to some conflict between Donegal players and representatives of the mainstream tradition when Irish traditional music was organised in the 1960s.
The tradition has several distinguishing traits compared to other fiddle traditions such as the Sliabh Luachra style of southern ireland, most of which involves styles of bowing and the ornamentation of the music, and rhythm. Due to the frequency of double stops and the strong bowing it is often compared to the Cape Breton tradition. Another characteristic of the style is the rapid pace at which it tends to proceed. Modern players, such as the fiddle group Altan, continue to be popular due to a variety of reasons.
Among the most famous Donegal style players are John Doherty from the early twentieth century and James Byrne, Paddy Glackin, Tommy Peoples and Mairéad Ní Mhaonaigh in recent decades.
History.
The fiddle has ancient roots in Ireland, the first report of bowed instruments similar to the violin being in the Book of Leinster (ca. 1160). The modern violin was ubiquitous in Ireland by the early 1700s. However the first mention of the fiddle being in use in Donegal is from the blind harper Arthur O'Neill who in his 1760 memoirs described a wedding in Ardara as having "plenty of pipers and fiddlers". Donegal fiddlers participated in the development of the Irish music tradition in the 18th century during which jigs and slipjigs and later reels and hornpipes became the dominant musical forms. However, Donegal musicians, many of them being fishermen, also frequently travelled to Scotland, where they acquired tune types from the Scottish repertoire such as the Strathspey which was integrated into the Donegal tradition as "Highland" tunes. The Donegal tradition derives much of its unique character from the synthesis of Irish and Scottish stylistic features and repertoires. Aoidh notes however that while different types of art music were commonly played among the upper classes of Scottish society in the 18th century, the Donegal tradition drew exclusively from the popular types of Scottish music. Like some Scottish fiddlers (who, like Donegal fiddlers, tend to use a short bow and play in a straight-ahead fashion), some Donegal fiddlers worked at imitating the sound of the bagpipes. Workers from Donegal would bring their music to Scotland and also bring back Scottish tunes with them such music of J. Scott Skinner and Mackenzie Murdoch. Lilting, unaccompanied singing of wordless tunes, was also an important part of the Donegal musical tradition often performed by women in social settings. Describing the musical life of Arranmore Island in the late 19th century singer Róise Rua Nic Gríanna describes the most popular dances: "The Sets, the Lancers, the maggie Piggy, the Donkey, the Mazurka and the Barn dances". Among the travelling fiddlers of the late 19th century players such as John Mhosaí McGinley, Anthony Hlferty, the McConnells and the Dohertys are best known. As skill levels increased through apprenticeships several fiddle masters appeared such as the Cassidy's, Connie Haughey, Jimmy Lyons and Miock McShane of Teelin and Francie Dearg and Mickey Bán Byrne of Kilcar. These virtuosos played unaccompanied listening pieces in addition to the more common dance music.
The influences between Scotland and Donegal went both ways and were furthered by a wave of immigration from Donegal to Scotland in the 19th century (the regions share common names of dances), as can be heard in the volume of strathspeys, schottisches, marches, and Donegal's own strong piping tradition, has influenced and been influenced by music, and by the sounds, ornaments, and repertoire of the Píob Mhór, the traditional bagpipes of Ireland and Scotland. There are other differences between the Donegal style and the rest of Ireland. Instruments such as the tin whistle, flute, concertina and accordion were very rare in Donegal until modern times. Traditionally the píob mór and the fiddle were the only instruments used and the use of pipe or fiddle music was common in old wedding customs. Migrant workers carried their music to Scotland and also brought back a number of tunes of Scottish origin. The Donegal fiddlers may well have been the route by which Scottish tunes such as Lucy Campbell, Tarbolton Lodge (Tarbolton) and The Flagon (The Flogging Reel), that entered the Irish repertoire. These players prided themselves on their technical abilities, which included playing in higher positions (fairly uncommon among traditional Irish fiddlers), and sought out material which would demonstrate their skills.
As Irish music was consolidated and organised under the Comhaltas Ceoltóirí Éireann movement in the 1960s, both strengthened the interest in traditional music but sometimes conflicted with the Donegal tradition and its social conventions. The rigidly organised sessions of the Comhaltas reflected the traditions of Southern Ireland and Donegal fiddlers like John Doherty considered the National repertoire with its strong focus on reels to be less diverse than that of Donegal with its varied rhythms. Other old fiddlers dislike the ways comhaltas sessions were organised with a committee player, often not himself a musician, in charge. Sometimes Comhaltas representatives would even disparage the Donegal tradition, with its Scottish flavour, as being un-Irish, and prohibit them from playing local tunes with Scottish genealogies such as the "Highlands" at Comhaltas sessions. This sometimes cause antagonism between Donegal players and the main organisation of Traditinoal music in ireland.
Outside of the Comhaltas movement however, Donegal fiddling stood strong with Paddy Glackin of Ceoltorí Laighean and the Bothy Band and later Tommy Peoples also with the Bothy Band and Mairead Ni Mhaonaigh with Altan, who all drew attention and prestige to the Donegal tradition within folk music circles throughout Ireland.
Description of style.
The Donegal style of fiddling is a label often applied to music from this area, though one also might plausibly identify several different, but related, styles within the county. To the extent to which there is one common style in the county, it is characterised by a rapid pace; a tendency to be more un-swung in the playing of the fast dance tune types (reel and jigs); short (non-slurred), aggressive bowing, sparse ornamentation, the use of bowed triplets more often than trills as ornaments, the use of double stops and droning; and the occurrence of "playing the octave", with one player playing the melody and the other playing the melody an octave lower. None of these characteristics are universal, and there is some disagreement as to the extent to which there is a common style at all. In general, however, the style is rather aggressive.
Another feature of Donegal fiddling that makes it distinctive among Irish musical traditions is the variety of rare tune types that are played. Highlands, a type of tune in 4/4 time with some similarities to Scottish strathspeys, which are also played in Donegal, are one of the most commonly played types of tune in the county. Other tune types common solely in the county include barndances, also called "Germans," and mazurkas.
Fiddlers of the Donegal tradition.
Historical.
There are a number of different strands to the history of fiddle playing in County Donegal. Perhaps the best-known and, in the last half of the twentieth century, the most influential has been that of the Doherty family. Hugh Doherty is the first known musician of this family. Born in 1790, he headed an unbroken tradition of fiddlers and pipers in the Doherty family until the death, in 1980, of perhaps the best-known Donegal fiddler, John Doherty. John, a travelling tinsmith, was known for his extremely precise and fast finger- and bow-work and vast repertoire, and is considered to be one of the greatest Irish fiddlers ever recorded. John's older brother, Mickey, was also recorded and, though Mickey was another of the great Irish fiddlers, his reputation has been overshadowed by John's.
There is no single Donegal style but several distinctive styles. These styles traditionally come from the geographical isolated regions of Donegal including Inishowen, eastern Donegal, The Rosses and Gweedore, Croaghs, Teelin, Kilcar, Glencolmcille, Ballyshannon and Bundoran. Even with improved communications and transport, these regions still have recognisably different ways of fiddle playing. Notable deceased players of the older Donegal styles include Neillidh ("Neilly") Boyle, Francie Byrne, Con Cassidy,Frank Cassidy, James Byrne (1946–2008), and P.V. O'Donnell (2011). Currently living Donegal fiddlers, include, Vincent Campbell, John Gallagher, Paddy Glackin, Danny O'Donnell, and Tommy Peoples.
Modern.
Fiddle playing continues to be popular in Donegal. The three fiddlers of the Donegal "supergroup" Altan, Mairéad Ní Mhaonaigh, Paul O'Shaughnessy, and Ciarán Tourish, are generally admired within Donegal. An example of another fiddler-player from Donegal is Liz Doherty.
The fiddle, and traditional music in general, remained popular in Donegal not only because of the international coverage of certain artists but because of local pride in the music. Traditional music "Seisiúns" are still common place both in pubs and in houses. The Donegal fiddle music has been influenced by recorded music, but this is claimed to have had a positive impact on the tradition. Modern Donegal fiddle music is often played in concerts and recorded on albums.

</doc>
<doc id="7975" url="https://en.wikipedia.org/wiki?curid=7975" title="Double-barreled shotgun">
Double-barreled shotgun

A double-barreled shotgun is a shotgun with two parallel barrels, allowing two shots to be fired.
Construction.
Modern double-barreled shotguns, often known as "doubles", are almost universally break open actions, with the barrels tilting up at the rear to expose the breech ends of the barrels for unloading and reloading. Since there is no reciprocating action needed to eject and reload the shells, doubles are more compact than repeating designs such as pump action or lever-action shotguns.
Barrel configuration.
Double-barreled shotguns come in two basic configurations: the side-by-side shotgun (SxS) and the over/under shotgun ("over and under", O/U, etc.), indicating the arrangement of barrels. The original double-barreled guns were nearly all SxS designs, which was a more practical design of muzzle-loading firearms. Early cartridge shotguns also used the SxS action, because they kept the exposed hammers of the earlier muzzle-loading shotguns they evolved from. When hammerless designs started to become common, the O/U design was introduced, and most modern sporting doubles are O/U designs.
One significant advantage that doubles have over single barrel repeating shotguns is the ability to provide access to more than one choke at a time. Some shotgun sports, such as skeet, use crossing targets presented in a narrow range of distance, and only require one level of choke. Others, like sporting clays, give the shooter targets at differing ranges, and targets that might approach or recede from the shooter, and so must be engaged at differing ranges. Having two barrels lets the shooter use a more open choke for near targets, and a tighter choke for distant targets, providing the optimal shot pattern for each distance.
Their disadvantage lies in the fact that the barrels of a double-barreled shotgun, whether "O/U" or "SxS", are not parallel, but slightly angled, so that shots from the barrels converge, usually at "40 yards out". For the "SxS" configuration, the shotstring continues on its path to the opposite side of the rib after the converging point; for example, the left barrel's discharge travels on the left of the rib till it hits dead center at 40 yards out, after that, the discharge continues on to the right. In the "O/U" configuration with a parallel rib, both barrels' discharges will keep to the dead center, but the discharge from the "under" barrel will shoot higher than the discharge from the "over" barrel after 40 yards. Thus, double-barreled shotguns are accurate only at practical shotgun ranges, though the range of their ammunition easily exceeds four to six times that range.
"SxS" shotguns are often more expensive, and may take more practice to aim effectively than a "O/U". The off-center nature of the recoil in a SxS gun may make shooting the body-side barrel slightly more painful by comparison to an OU, single-shot, or pump/lever action shotgun. Gas-operated, and to a lesser extent recoil-operated, designs will recoil less than either. More "SxS" than "O/U" guns have traditional 'cast-off' stocks, where the end of the buttstock veers to the right, allowing a right-handed user to point the gun more easily.
Trigger mechanism.
The early doubles used two triggers, one for each barrel. These were located front to back inside the trigger guard, the index finger being used to pull either trigger, as having two fingers inside the trigger guard can cause a recoil induced double-discharge. Double trigger designs are typically set up for right-handed users. In double trigger designs, it is often possible to pull both triggers at once, firing both barrels simultaneously, though this is generally not recommended as it doubles the recoil, battering both shooter and shotgun. Discharging both barrels at the same time has long been a hunting trick employed by hunters using 8 gauge "elephant" shotguns, firing the two two-ounce slugs for sheer stopping power at close range.
Later models use a single trigger that alternately fires both barrels, called a "single selective trigger" or "SST". The SST does not allow firing both barrels at once, since the single trigger must be pulled twice in order to fire both barrels. The change from one barrel to the other may be done by a clockwork type system, where a cam alternates between barrels, or by an inertial system where the recoil of firing the first barrel toggles the trigger to the next barrel. A double-barreled shotgun with an inertial trigger works best with full power shotshells; shooting low recoil shotshells often will not reliably toggle the inertial trigger, causing an apparent failure to fire occasionally when attempting to depress the trigger a second time to fire the second barrel. Generally there is a method of selecting the order in which the barrels of an SST shotgun fire; commonly this is done through manipulation of the safety, pushing to one side to select top barrel first and the other side to select bottom barrel first. In the event that an inertial trigger does not toggle to the second barrel when firing low recoil shotshells, manually selecting the order to the second barrel will enable the second barrel to fire when the trigger is depressed again.
One of the advantages of the double, with double triggers or SST, is that a second shot can be taken almost immediately after the first, utilizing different chokes for the two shots. (Assuming, of course, that full power shotshells are fired, at least for a double-barreled shotgun with an inertial type SST, as needed to toggle the inertial trigger.)
Regulation.
Regulation is a term used for multi-barreled firearms that indicates how close to the same point of aim the barrels will shoot. Regulation is very important, because a poorly regulated gun may hit consistently with one barrel, but miss consistently with the other, making the gun nearly useless for anything requiring two shots. Fortunately, the short ranges and spread of shot provide a significant overlap, so a small error in regulation in a double will often be too small to be noticed. Generally the shotguns are regulated to hit the point of aim at a given distance, usually the maximum expected range since that is the range at which a full choke would be used, and where precise regulation matters most.
Regional use.
The double-barreled shotgun is seen as a weapon of prestige and authority in rural parts of India, where it is known as "dunali" (literally "two pipes"). It is especially common in Bihar, Purvanchal, Uttar Pradesh, Haryana and Punjab.

</doc>
<doc id="7976" url="https://en.wikipedia.org/wiki?curid=7976" title="Dessert">
Dessert

Dessert () is a course that concludes a main meal. The course usually consists of sweet foods and beverages, such as dessert wine or liqueurs, but may include coffee, cheeses, nuts, or other savory items. In some parts of the world, such as much of central and western Africa, there is no tradition of a dessert course to conclude a meal.
The term "dessert" can apply to many foods, such as cakes, tarts, cookies, biscuits, gelatins, pastries, ice creams, pies, puddings, custards, and sweet soups. Fruit is also commonly found in dessert courses because of its naturally occurring sweetness. Some cultures sweeten foods that are more commonly savory to create desserts.
Etymology.
The word "dessert" originated from the French word "desservir," meaning "to clear the table." Its first known use was in 1600, in a health education manual entitled "Naturall and artificial Directions for Health", which was written by William Vaughan. In his "A History of Dessert" (2013), Michael Krondl explains it refers to the fact dessert was served after the table had been cleared of other dishes. The term dates from the 14th century but attained its current meaning around the beginning of the 20th century when "service à la française" (setting a variety of dishes on the table at the same time) was replaced with "service à la russe" (presenting a meal in courses.)"
Usage.
The word dessert is most commonly used for this course in the United States, Canada, Australia, New Zealand and Ireland whilst Pudding is more commonly used in the United Kingdom. Alternatives such as "sweets" or "afters" are also used in the United Kingdom and some other Commonwealth countries, including Hong Kong, and India.
History.
Sweets were fed to the gods in ancient Mesopotamia and India and other ancient civilizations. Dried fruit and honey were likely the first sweeteners used in most of the world, but the spread of sugarcane around the world was essential to the development of dessert.
Sugarcane was grown and refined in India before 500 BCE and was crystallized, making it easy to transport, by 500 CE. Sugar and sugarcane were traded, making sugar available to Macedonia by 300 BCE and China by 600 CE. In South Asia, the Middle East and China, sugar has been a staple of cooking and desserts for over a thousand years. Sugarcane and sugar were little known and rare in Europe until the twelfth century or later, when the Crusades and then colonialization spread its use.
Europeans began to manufacture sugar in the Middle Ages, and more sweet desserts became available. Even then sugar was so expensive usually only the wealthy could indulge on special occasions. The first apple pie recipe was published in 1381. The earliest documentation of the term "cupcake" was in "Seventy-five Receipts for Pastry, Cakes, and Sweetmeats" in 1828 in Eliza Leslie's "Receipts" cookbook.
The Industrial Revolution in America and Europe caused desserts (and food in general) to be mass-produced, processed, preserved, canned, and packaged. Frozen foods became very popular starting in the 1920s when freezing emerged. These processed foods became a large part of diets in many industrialized nations. Many countries have desserts and foods distinctive to their nations or region.
Ingredients.
Sweet desserts usually contain cane sugar, palm sugar, honey or some type of syrup such as molasses, maple syrup, treacle, or corn syrup. Other common ingredients in Western-style desserts are flour or other starches, fats such as butter or lard, dairy, eggs, salt, acidic ingredients such as lemon juice, and spices and other flavoring agents such as chocolate, peanut butter, fruits, and nuts. The proportions of these ingredients, along with the preparation methods, play a major part in the consistency, texture, and flavor of the end product.
Sugars contribute moisture and tenderness to baked goods. Flour or starch components serves as a protein and gives the dessert structure. Fats contribute moisture and can enable the development of flaky layers in pastries and pie crusts. The dairy products in baked goods keep the desserts moist. Many desserts also contain eggs, in order to form custard or to aid in the rising and thickening of a cake-like substance. Egg yolks specifically contribute to the richness of desserts. Egg whites can act as a leavening agent or provide structure. Desserts can contain many spices and extracts to add a variety of flavors. Salt and acids are added to desserts to balance sweet flavors and create a contrast in flavors.
Some desserts are made with coffee, such as tiramisu, or a coffee-flavoured version of a dessert can be made, for example an iced coffee soufflé or coffee biscuits. Alcohol can also be used as an ingredient, to make alcoholic desserts.
Varieties.
Dessert consist of variations of flavors, textures, and appearances. Desserts can be defined as a usually sweeter course that concludes a meal. This definition includes a range of courses ranging from fruits or dried nuts to multi-ingredient cakes and pies. Many cultures have different variations of dessert. In modern times the variations of desserts have usually been passed down or come from geographical regions. This is one cause for the variation of desserts. These are some major categories in which desserts can be placed.
Cakes.
Cakes are sweet tender breads made with sugar and delicate flour. Cakes can vary from light, airy sponge cakes to dense cakes with less flour. Common flavourings include dried, candied or fresh fruit, nuts, cocoa or extracts. They may be filled with fruit preserves or dessert sauces (like pastry cream), iced with buttercream or other icings, and decorated with marzipan, piped borders, or candied fruit. Cake is often served as a celebratory dish on ceremonial occasions, for example weddings, anniversaries, and birthdays. Small-sized cakes have become popular, in the form of cupcakes and petits fours.
Chocolates and candies.
Chocolate is a typically sweet, usually brown, food preparation of "Theobroma cacao" seeds, roasted, ground, and often flavored. Pure, unsweetened chocolate contains primarily cocoa solids and cocoa butter in varying proportions. Much of the chocolate currently consumed is in the form of sweet chocolate, combining chocolate with sugar. Milk chocolate is sweet chocolate that additionally contains milk powder or condensed milk. White chocolate contains cocoa butter, sugar, and milk, but no cocoa solids. Dark chocolate is produced by adding fat and sugar to the cacao mixture, with no milk or much less than milk chocolate.
Candy, also called sweets or lollies, is a confection that features sugar as a principal ingredient. Many candies involve the crystallization of sugar which varies the texture of sugar crystals. Candies comprise many forms including caramel, marshmallows, and taffy.
Cookies or biscuits.
Cookies, (from the Dutch word "koekje" meaning little cake), also known as "biscuits" in many English-speaking countries, are flattish bite-sized or larger short pastries generally intended to be eaten out of the hand. Cookies can have a texture that is crispy, chewy, or soft. Examples include layered bars, crispy meringues, and soft chocolate chip cookies.
Custards and puddings.
These kinds of desserts usually include a thickened dairy base. Custards are cooked and thickened with eggs. Baked custards include crème brûlée and flan. Puddings are thickened with starches such as cornstarch or tapioca. Custards and puddings are often used as ingredients in other desserts, for instance as a filling for pastries or pies.
Deep-fried desserts.
Many cuisines include a dessert made of deep-fried starch-based batter or dough. In many countries a doughnut is a flour-based batter that has been deep-fried. It is sometimes filled with custard or jelly. Fritters are fruit pieces in a thick batter that have been deep fried. Gulab jamun is an Indian dessert made of milk solids kneaded into a dough, deep-fried, and soaked in honey. Churros are a deep-fried and sugared dough that is eaten as dessert or a snack in many countries.
Frozen desserts.
Ice cream, gelato, sorbet and shaved-ice desserts fit into this category. Ice cream is a cream base that is churned as it is frozen to create a creamy consistency. Gelato uses a milk base and has less air whipped in than ice cream, making it denser. Sorbet is made from churned fruit and is not dairy based. Shaved-ice desserts are made by shaving a block of ice and adding flavored syrup or juice to the ice shavings.
Jellied desserts.
Jellied desserts are made with a sweetened liquid thickened with gelatin or another thickening agent. They are traditional in many cultures. Grass jelly and annin tofu are Chinese jellied desserts. Yōkan is a Japanese jellied dessert. In English-speaking countries, many dessert recipes are based on gelatin with fruit and/or whipped cream added.
Pastries.
Pastries are sweet baked pastry products. Pastries can either take the form of light and flaky bread with an airy texture, such as a croissant or unleavened dough with a high fat content and crispy texture, such as shortbread. Pastries are often flavored or filled with fruits, chocolate, nuts, and spices. Pastries are sometimes eaten with tea or coffee as a breakfast food.
Pies, cobblers, and clafoutis.
Pies and cobblers are a crust with a filling. The crust can be either made from either a pastry or crumbs. Pie fillings range from fruits to puddings; cobbler fillings are generally fruit-based. Clafoutis are a batter with fruit-based filling poured over the top before baking.
Sweet soups.
Tong sui, literally translated as "sugar water" and also known as tim tong, is a collective term for any sweet, warm soup or custard served as a dessert at the end of a meal in Cantonese cuisine. "Tong sui" are a Cantonese specialty and are rarely found in other regional cuisines of China. Outside of Cantonese-speaking communities, soupy desserts generally are not recognized as a distinct category, and the term "tong sui" is not used.
Dessert wines.
Dessert wines are sweet wines typically served with dessert. There is no simple definition of a dessert wine. In the UK, a dessert wine is considered to be any sweet wine drunk with a meal, as opposed to the white fortified wines (fino and amontillado sherry) drunk before the meal, and the red fortified wines (port and madeira) drunk after it. Thus, most fortified wines are regarded as distinct from dessert wines, but some of the less strong fortified white wines, such as Pedro Ximénez sherry and Muscat de Beaumes-de-Venise, are regarded as honorary dessert wines. In the United States, by contrast, a dessert wine is legally defined as any wine over 14% alcohol by volume, which includes all fortified wines - and is taxed more highly as a result. Examples include Sauternes and Tokaji Aszú.
By continent.
Africa.
Throughout much of central and western Africa, there is no tradition of a dessert course following a meal. Fruit or fruit salad would be eaten instead, which may be spiced, or sweetened with a sauce. In some former colonies in the region, the colonial power has influenced desserts – for example, the Angolian "cocada amarela" (yellow coconut) resembles baked desserts in Portugal.
Asia.
In Asia, desserts are often eaten between meals as snacks rather than as a concluding course. There is widespread use of rice flour in East Asian desserts, which often include local ingredients such as coconut milk, palm sugar, and tropical fruit. In India, where sugarcane has been grown and refined since before 500 BCE, desserts have been an important part of the diet for thousands of years; types of desserts include burfis, halvahs, jalebis, and laddus.
Eurasia.
In Ukraine and Russia, breakfast foods such as nalysnyky or blintz or oladi (pancakes), and syrniki are served with honey and jam as desserts.
North America.
European colonization of the Americas yielded the introduction of a number of ingredients and cooking styles. The various styles continued expanding well into the 19th and 20th centuries, proportional to the influx of immigrants.
South America.
Dulce de leche is a very common confection in Argentina. In Bolivia, sugarcane, honey and coconut are traditionally used in desserts. "Tawa tawa" is a Bolivian sweet fritter prepared using sugar cane, and "helado de canela" is a dessert that is similar to sherbet which is prepared with cane sugar and cinnamon. Coconut tarts, puddings cookies and candies are also consumed in Bolivia. Brazil has a variety of candies such as brigadeiros (chocolate fudge balls), cocada (a coconut sweet), beijinhos (coconut truffles and clove) and romeu e julieta (cheese with a guava jam known as goiabada). Peanuts are used to make paçoca, rapadura and pé-de-moleque. Local common fruits are turned in juices and used to make chocolates, popsicles and ice cream. In Chile, "kuchen" has been described as a "trademark dessert." Several desserts in Chile are prepared with "manjar", (caramelized milk), including "alfajor", "flan", "cuchufli" and "arroz con leche". Desserts consumed in Colombia include dulce de leche, waffle cookies, puddings, nougat, coconut with syrup and thickened milk with sugarcane syrup. Desserts in Ecuador tend to be simple, and desserts are a moderate part of the cuisine. Desserts consumed in Ecuador include tres leches cake, flan, candies and various sweets.
Oceania.
Desserts are typically eaten in Australia, and most daily meals "end with simple desserts," which can include various fruits. More complex desserts include cakes, pies and cookies, which are sometimes served during special occasions.
Market.
The market for desserts has grown over the last few decades, which was greatly increased by the commercialism of baking desserts and the rise of food productions. Desserts are present in most restaurants as the popularity has increased. Many commercial stores have been established as solely desserts stores. Ice cream parlors have been around since before 1800. Many businesses started advertising campaigns focusing solely on desserts. The tactics used to market desserts are very different depending on the audience for example desserts can be advertised with popular movie characters to target children. The rise of companies like Food Network has marketed many shows which feature dessert and their creation. Shows like these have displayed extreme desserts and made a game show atmosphere which made desserts a more competitive field.
Desserts are a standard staple in restaurant menus, with different degrees of variety. Pie and cheesecake were among the most popular dessert courses ordered in U.S. restaurants in 2012.
Nutrition.
Dessert foods often contain relatively high amounts of sugar and/or fats and as a result, higher calorie counts per gram than other foods. Fresh or cooked fruit with minimal added sugar or fat is an exception.

</doc>
<doc id="7978" url="https://en.wikipedia.org/wiki?curid=7978" title="Data Encryption Standard">
Data Encryption Standard

The Data Encryption Standard (DES, or ) was once a predominant symmetric-key algorithm for the encryption of electronic data. It was highly influential in the advancement of modern cryptography in the academic world. Developed in the early 1970s at IBM and based on an earlier design by Horst Feistel, the algorithm was submitted to the National Bureau of Standards (NBS) following the agency's invitation to propose a candidate for the protection of sensitive, unclassified electronic government data. In 1976, after consultation with the National Security Agency (NSA), the NBS eventually selected a slightly modified version (strengthened against differential cryptanalysis, but weakened against brute force attacks), which was published as an official Federal Information Processing Standard (FIPS) for the United States in 1977. The publication of an NSA-approved encryption standard simultaneously resulted in its quick international adoption and widespread academic scrutiny. Controversies arose out of classified design elements, a relatively short key length of the symmetric-key block cipher design, and the involvement of the NSA, nourishing suspicions about a backdoor. The intense academic scrutiny the algorithm received over time led to the modern understanding of block ciphers and their cryptanalysis.
DES is now considered to be insecure for many applications. This is mainly due to the 56-bit key size being too small; in January 1999, distributed.net and the Electronic Frontier Foundation collaborated to publicly break a DES key in 22 hours and 15 minutes (see chronology). There are also some analytical results which demonstrate theoretical weaknesses in the cipher, although they are infeasible to mount in practice. The algorithm is believed to be practically secure in the form of Triple DES, although there are theoretical attacks. In recent years, the cipher has been superseded by the Advanced Encryption Standard (AES). Furthermore, DES has been withdrawn as a standard by the National Institute of Standards and Technology (formerly the National Bureau of Standards).
Some documentation makes a distinction between DES as a standard and DES as an algorithm, referring to the algorithm as the DEA (Data Encryption Algorithm).
History of DES.
The origins of DES go back to the early 1970s. In 1972, after concluding a study on the US government's computer security needs, the US standards body NBS (National Bureau of Standards)—now named NIST (National Institute of Standards and Technology)—identified a need for a government-wide standard for encrypting unclassified, sensitive information. Accordingly, on 15 May 1973, after consulting with the NSA, NBS solicited proposals for a cipher that would meet rigorous design criteria. None of the submissions, however, turned out to be suitable. A second request was issued on 27 August 1974. This time, IBM submitted a candidate which was deemed acceptable—a cipher developed during the period 1973–1974 based on an earlier algorithm, Horst Feistel's Lucifer cipher. The team at IBM involved in cipher design and analysis included Feistel, Walter Tuchman, Don Coppersmith, Alan Konheim, Carl Meyer, Mike Matyas, Roy Adler, Edna Grossman, Bill Notz, Lynn Smith, and Bryant Tuckerman.
NSA's involvement in the design.
On 17 March 1975, the proposed DES was published in the "Federal Register". Public comments were requested, and in the following year two open workshops were held to discuss the proposed standard. There was some criticism from various parties, including from public-key cryptography pioneers Martin Hellman and Whitfield Diffie, citing a shortened key length and the mysterious "S-boxes" as evidence of improper interference from the NSA. The suspicion was that the algorithm had been covertly weakened by the intelligence agency so that they—but no-one else—could easily read encrypted messages. Alan Konheim (one of the designers of DES) commented, "We sent the S-boxes off to Washington. They came back and were all different." The United States Senate Select Committee on Intelligence reviewed the NSA's actions to determine whether there had been any improper involvement. In the unclassified summary of their findings, published in 1978, the Committee wrote:
However, it also found that
Another member of the DES team, Walter Tuchman, stated "We developed the DES algorithm entirely within IBM using IBMers. The NSA did not dictate a single wire!"
In contrast, a declassified NSA book on cryptologic history states:
and
Some of the suspicions about hidden weaknesses in the S-boxes were allayed in 1990, with the independent discovery and open publication by Eli Biham and Adi Shamir of differential cryptanalysis, a general method for breaking block ciphers. The S-boxes of DES were much more resistant to the attack than if they had been chosen at random, strongly suggesting that IBM knew about the technique in the 1970s. This was indeed the case; in 1994, Don Coppersmith published some of the original design criteria for the S-boxes. According to Steven Levy, IBM Watson researchers discovered differential cryptanalytic attacks in 1974 and were asked by the NSA to keep the technique secret. Coppersmith explains IBM's secrecy decision by saying, "that was because ifferential cryptanalysi can be a very powerful tool, used against many schemes, and there was concern that such information in the public domain could adversely affect national security." Levy quotes Walter Tuchman: "hey asked us to stamp all our documents confidential... We actually put a number on each one and locked them up in safes, because they were considered U.S. government classified. They said do it. So I did it". Bruce Schneier observed that "It took the academic community two decades to figure out that the NSA 'tweaks' actually improved the security of DES."
The algorithm as a standard.
Despite the criticisms, DES was approved as a federal standard in November 1976, and published on 15 January 1977 as FIPS PUB 46, authorized for use on all unclassified data. It was subsequently reaffirmed as the standard in 1983, 1988 (revised as FIPS-46-1), 1993 (FIPS-46-2), and again in 1999 (FIPS-46-3), the latter prescribing "Triple DES" (see below). On 26 May 2002, DES was finally superseded by the Advanced Encryption Standard (AES), following a public competition. On 19 May 2005, FIPS 46-3 was officially withdrawn, but NIST has approved Triple DES through the year 2030 for sensitive government information.
The algorithm is also specified in ANSI X3.92 (Now, X3 is now known as INCITS and ANSI X3.92 as ANSI INCITS 92), NIST SP 800-67 and ISO/IEC 18033-3 (as a component of TDEA).
Another theoretical attack, linear cryptanalysis, was published in 1994, but it was the Electronic Frontier Foundation's DES cracker in 1998 that demonstrated that DES could be attacked very practically, and highlighted the need for a replacement algorithm. These and other methods of cryptanalysis are discussed in more detail later in this article.
The introduction of DES is considered to have been a catalyst for the academic study of cryptography, particularly of methods to crack block ciphers. According to a NIST retrospective about DES,
Description.
DES is the archetypal block cipher—an algorithm that takes a fixed-length string of plaintext bits and transforms it through a series of complicated operations into another ciphertext bitstring of the same length. In the case of DES, the block size is 64 bits. DES also uses a key to customize the transformation, so that decryption can supposedly only be performed by those who know the particular key used to encrypt. The key ostensibly consists of 64 bits; however, only 56 of these are actually used by the algorithm. Eight bits are used solely for checking parity, and are thereafter discarded. Hence the effective key length is 56 bits.
The key is nominally stored or transmitted as 8 bytes, each with odd parity. According to ANSI X3.92-1981 (Now, known as ANSI INCITS 92-1981), section 3.5:
Like other block ciphers, DES by itself is not a secure means of encryption but must instead be used in a mode of operation. FIPS-81 specifies several modes for use with DES. Further comments on the usage of DES are contained in FIPS-74.
Decryption uses the same structure as encryption but with the keys used in reverse order. (This has the advantage that the same hardware or software can be used in both directions.)
Overall structure.
The algorithm's overall structure is shown in Figure 1: there are 16 identical stages of processing, termed "rounds". There is also an initial and final permutation, termed "IP" and "FP", which are inverses (IP "undoes" the action of FP, and vice versa). IP and FP have no cryptographic significance, but were included in order to facilitate loading blocks in and out of mid-1970s 8-bit based hardware.
Before the main rounds, the block is divided into two 32-bit halves and processed alternately; this criss-crossing is known as the Feistel scheme. The Feistel structure ensures that decryption and encryption are very similar processes—the only difference is that the subkeys are applied in the reverse order when decrypting. The rest of the algorithm is identical. This greatly simplifies implementation, particularly in hardware, as there is no need for separate encryption and decryption algorithms.
The ⊕ symbol denotes the exclusive-OR (XOR) operation. The "F-function" scrambles half a block together with some of the key. The output from the F-function is then combined with the other half of the block, and the halves are swapped before the next round. After the final round, the halves are swapped; this is a feature of the Feistel structure which makes encryption and decryption similar processes.
The Feistel (F) function.
The F-function, depicted in Figure 2, operates on half a block (32 bits) at a time and consists of four stages:
The alternation of substitution from the S-boxes, and permutation of bits from the P-box and E-expansion provides so-called "confusion and diffusion" respectively, a concept identified by Claude Shannon in the 1940s as a necessary condition for a secure yet practical cipher.
Key schedule.
Figure 3 illustrates the "key schedule" for encryption—the algorithm which generates the subkeys. Initially, 56 bits of the key are selected from the initial 64 by "Permuted Choice 1" ("PC-1")—the remaining eight bits are either discarded or used as parity check bits. The 56 bits are then divided into two 28-bit halves; each half is thereafter treated separately. In successive rounds, both halves are rotated left by one or two bits (specified for each round), and then 48 subkey bits are selected by "Permuted Choice 2" ("PC-2")—24 bits from the left half, and 24 from the right. The rotations (denoted by "«<" in the diagram) mean that a different set of bits is used in each subkey; each bit is used in approximately 14 out of the 16 subkeys.
The key schedule for decryption is similar—the subkeys are in reverse order compared to encryption. Apart from that change, the process is the same as for encryption. The same 28 bits are passed to all rotation boxes.
Security and cryptanalysis.
Although more information has been published on the cryptanalysis of DES than any other block cipher, the most practical attack to date is still a brute force approach. Various minor cryptanalytic properties are known, and three theoretical attacks are possible which, while having a theoretical complexity less than a brute force attack, require an unrealistic number of known or chosen plaintexts to carry out, and are not a concern in practice.
Brute force attack.
For any cipher, the most basic method of attack is brute force—trying every possible key in turn. The length of the key determines the number of possible keys, and hence the feasibility of this approach. For DES, questions were raised about the adequacy of its key size early on, even before it was adopted as a standard, and it was the small key size, rather than theoretical cryptanalysis, which dictated a need for a replacement algorithm. As a result of discussions involving external consultants including the NSA, the key size was reduced from 128 bits to 56 bits to fit on a single chip.
In academia, various proposals for a DES-cracking machine were advanced. In 1977, Diffie and Hellman proposed a machine costing an estimated US$20 million which could find a DES key in a single day. By 1993, Wiener had proposed a key-search machine costing US$1 million which would find a key within 7 hours. However, none of these early proposals were ever implemented—or, at least, no implementations were publicly acknowledged. The vulnerability of DES was practically demonstrated in the late 1990s. In 1997, RSA Security sponsored a series of contests, offering a $10,000 prize to the first team that broke a message encrypted with DES for the contest. That contest was won by the DESCHALL Project, led by Rocke Verser, Matt Curtin, and Justin Dolske, using idle cycles of thousands of computers across the Internet. The feasibility of cracking DES quickly was demonstrated in 1998 when a custom DES-cracker was built by the Electronic Frontier Foundation (EFF), a cyberspace civil rights group, at the cost of approximately US$250,000 (see EFF DES cracker). Their motivation was to show that DES was breakable in practice as well as in theory: "There are many people who will not believe a truth until they can see it with their own eyes. Showing them a physical machine that can crack DES in a few days is the only way to convince some people that they really cannot trust their security to DES." The machine brute-forced a key in a little more than 2 days search.
The next confirmed DES cracker was the COPACOBANA machine built in 2006 by teams of the Universities of Bochum and Kiel, both in Germany. Unlike the EFF machine, COPACOBANA consists of commercially available, reconfigurable integrated circuits. 120 of these field-programmable gate arrays (FPGAs) of type XILINX Spartan-3 1000 run in parallel. They are grouped in 20 DIMM modules, each containing 6 FPGAs. The use of reconfigurable hardware makes the machine applicable to other code breaking tasks as well. One of the more interesting aspects of COPACOBANA is its cost factor. One machine can be built for approximately $10,000. The cost decrease by roughly a factor of 25 over the EFF machine is an example of the continuous improvement of digital hardware—see Moore's law. Adjusting for inflation over 8 years yields an even higher improvement of about 30x. Since 2007, SciEngines GmbH, a spin-off company of the two project partners of COPACOBANA has enhanced and developed successors of COPACOBANA. In 2008 their COPACOBANA RIVYERA reduced the time to break DES to less than one day, using 128 Spartan-3 5000's. Currently SciEngines RIVYERA holds the record in brute-force breaking DES, having utilized 128 Spartan-3 5000 FPGAs. Their 256 Spartan-6 LX150 model has further lowered this time.
Attacks faster than brute-force.
There are three attacks known that can break the full 16 rounds of DES with less complexity than a brute-force search: differential cryptanalysis (DC), linear cryptanalysis (LC), and Davies' attack. However, the attacks are theoretical and are unfeasible to mount in practice; these types of attack are sometimes termed certificational weaknesses.
There have also been attacks proposed against reduced-round versions of the cipher, that is, versions of DES with fewer than 16 rounds. Such analysis gives an insight into how many rounds are needed for safety, and how much of a "security margin" the full version retains. Differential-linear cryptanalysis was proposed by Langford and Hellman in 1994, and combines differential and linear cryptanalysis into a single attack. An enhanced version of the attack can break 9-round DES with 2 chosen plaintexts and has a 2 time complexity (Biham and others, 2002).
Minor cryptanalytic properties.
DES exhibits the complementation property, namely that
where formula_2 is the bitwise complement of formula_3 formula_4 denotes encryption with key formula_5 formula_6 and formula_7 denote plaintext and ciphertext blocks respectively. The complementation property means that the work for a brute force attack could be reduced by a factor of 2 (or a single bit) under a chosen-plaintext assumption. By definition, this property also applies to TDES cipher.
DES also has four so-called "weak keys". Encryption ("E") and decryption ("D") under a weak key have the same effect (see involution):
There are also six pairs of "semi-weak keys". Encryption with one of the pair of semiweak keys, formula_10, operates identically to decryption with the other, formula_11:
It is easy enough to avoid the weak and semiweak keys in an implementation, either by testing for them explicitly, or simply by choosing keys randomly; the odds of picking a weak or semiweak key by chance are negligible. The keys are not really any weaker than any other keys anyway, as they do not give an attack any advantage.
DES has also been proved not to be a group, or more precisely, the set formula_14 (for all possible keys formula_15) under functional composition is not a group, nor "close" to being a group. This was an open question for some time, and if it had been the case, it would have been possible to break DES, and multiple encryption modes such as Triple DES would not increase the security, because encryption under one key would be equivalent to decryption under another key.
It is known that the maximum cryptographic security of DES is limited to about 64 bits, even when independently choosing all round subkeys instead of deriving them from a key, which would otherwise permit a security of 768 bits. 
Simplified DES.
Simplified DES (SDES) was designed for educational purposes only, to help students learn about about modern cryptanalytic techniques.
SDES has similar properties and structure as DES, but has been simplified to make it much easier to perform encryption and decryption by hand with pencil and paper.
Some people feel that learning SDES gives insight into DES and other block ciphers, and insight into various cryptanalytic attacks against them.
Replacement algorithms.
Concerns about security and the relatively slow operation of DES in software motivated researchers to propose a variety of alternative block cipher designs, which started to appear in the late 1980s and early 1990s: examples include RC5, Blowfish, IDEA, NewDES, SAFER, CAST5 and FEAL. Most of these designs kept the 64-bit block size of DES, and could act as a "drop-in" replacement, although they typically used a 64-bit or 128-bit key. In the Soviet Union the GOST 28147-89 algorithm was introduced, with a 64-bit block size and a 256-bit key, which was also used in Russia later.
DES itself can be adapted and reused in a more secure scheme. Many former DES users now use Triple DES (TDES) which was described and analysed by one of DES's patentees (see FIPS Pub 46-3); it involves applying DES three times with two (2TDES) or three (3TDES) different keys. TDES is regarded as adequately secure, although it is quite slow. A less computationally expensive alternative is DES-X, which increases the key size by XORing extra key material before and after DES. GDES was a DES variant proposed as a way to speed up encryption, but it was shown to be susceptible to differential cryptanalysis.
On January 2, 1997, NIST announced that they wished to choose a successor to DES. In 2001, after an international competition, NIST selected a new cipher, the Advanced Encryption Standard (AES), as a replacement. The algorithm which was selected as the AES was submitted by its designers under the name Rijndael. Other finalists in the NIST AES competition included RC6, Serpent, MARS, and Twofish.

</doc>
<doc id="7983" url="https://en.wikipedia.org/wiki?curid=7983" title="Double-hulled tanker">
Double-hulled tanker

A double-hulled tanker refers to an oil tanker which has a double hull. They reduce the likelihood of leaks occurring than in single-hulled tankers, and their ability to prevent or reduce oil spills led to double hulls being standardized for oil tankers and other types of ships including by the International Convention for the Prevention of Pollution from Ships or MARPOL Convention. After the Exxon Valdez oil spill disaster in Alaska in 1989, the US Government required all new oil tankers built for use between US ports to be equipped with a full double hull.
Reasons for use.
A number of manufacturers have embraced oil tankers with a double hull because it strengthens the hull of ships, reducing the likelihood of oil disasters in low-impact collisions and groundings over single-hull ships. They reduce the likelihood of leaks occurring at low speed impacts in port areas when the ship is under pilotage. Research of impact damage of ships has revealed that double-hulled tankers are unlikely to perforate both hulls in a collision, preventing oil from seeping out. However, for smaller tankers, U shaped tanks might be susceptible to "free flooding" across the double bottom and up to the outside water level each side of the cargo tank. Salvors prefer to salvage doubled-hulled tankers because they permit the use of air pressure to vacuum out the flood water. In the 1960s, collision proof double hulls for nuclear ships were extensively investigated, due to escalating concerns over nuclear accidents.
The ability of double-hulled tankers to prevent or reduce oil spills led to double hulls being standardized for other types of ships including oil tankers by the International Convention for the Prevention of Pollution from Ships or MARPOL Convention. In 1992, MARPOL was amended, making it "mandatory for tankers of 5,000 dwt and more ordered after 6 July 1993 to be fitted with double hulls, or an alternative design approved by IMO". However, in the aftermath of the Erika incident of the coast off France in December 1999, members of IMO adopted a revised schedule for the phase-out of single-hull tankers, which came into effect on 1 September 2003, with further amendments validated on 5 April 2005.
After the Exxon Valdez oil spill disaster, when that ship grounded on Bligh Reef outside the port of Valdez, Alaska in 1989, the US Government required all new oil tankers built for use between US ports to be equipped with a full double hull. However, the damage to the Exxon Valdez penetrated sections of the hull (the slops oil tanks, or slop tanks) that were protected by a double bottom, or partial double hull.
Maintenance issues.
Although double-hulled tankers reduce the likelihood of ships grazing rocks and creating holes in the hull, a double hull does not protect against major, high-energy collisions or groundings which cause the majority of oil pollution, despite this being the reason that the double hull was mandated by United States legislation. Double-hulled tankers, if poorly designed, constructed, maintained and operated can be as problematic, if not more problematic than their single-hulled counterparts. Double-hulled tankers have a more complex design and structure than their single-hulled counterparts, which means that they require more maintenance and care in operating, which if not subject to responsible monitoring and policing, may cause problems. Double hulls often result in the weight of the hull increasing by at least 20%, and because the steel weight of doubled-hulled tanks should not be greater than that of single-hulled ships, the individual hull walls are typically thinner and theoretically less resistant to wear. Double hulls by no means eliminate the possibility of the hulls breaking apart. Due to the air space between the hulls, there is also a potential problem with volatile gases seeping out through worn areas of the internal hull, increasing the risk of an explosion.
Although several international conventions against pollution are in place, as of 2003 there was still no formal body setting international mandatory standards, although the International Safety Guide for Oil Tankers and Terminals (ISGOTT) does provide guidelines giving advise on optimum use and safety, such as recommending that ballast tanks are not entered while loaded with cargo, and that weekly samples are made of the atmosphere inside for hydrocarbon gas. Due to the difficulties of maintenance, ship builders have been competitive in producing double-hulled ships which are easier to inspect, such as ballast and cargo tanks which are easily accessible and easier to spot corrosion in the hull. The Tanker Structure Cooperative Forum (TSCF) published the "Guide to Inspection and Maintenance of Double-Hull Tanker Structures" in 1995 giving advice based on experience of operating double-hulled tankers.

</doc>
<doc id="7984" url="https://en.wikipedia.org/wiki?curid=7984" title="Drink">
Drink

A drink or beverage is a liquid intended for human consumption. In addition to basic needs, beverages form part of the culture of human society. Although all beverages, including juice, soft drinks, and carbonated drinks, have some form of water in them, water itself is often not classified as a beverage, and the word "beverage" has been recurrently defined as not referring to water.
An alcoholic beverage is a drink containing ethanol, commonly known as alcohol, although in chemistry the definition of an alcohol includes many other compounds. Alcoholic beverages, such as wine, beer, and liquor, have been part of human culture and development for 8,000 years.
Non-alcoholic beverages often signify drinks that would normally contain alcohol, such as beer and wine but are made with less than .5 percent alcohol by volume. The category includes drinks that have undergone an alcohol removal process such as non-alcoholic beers and de-alcoholized wines.
Biology.
When the human body becomes dehydrated it experiences the sensation of "thirst". This craving of fluids results in an instinctive need to drink. Thirst is regulated by the hypothalamus in response to subtle changes in the body's electrolyte levels, and also as a result of changes in the volume of blood circulating. The complete elimination of beverages, i.e. water, from the body will result in death faster than the removal of any other substance. Water and milk have been basic drinks throughout history. As water is essential for life, it has also been the carrier of many diseases.
As mankind evolved, new techniques were discovered to create drinks from the plants that were native to their areas. The earliest archaeological evidence of wine production yet found has been at sites in Georgia ( BCE) and Iran ( BCE). Beer may have been known in Neolithic Europe as far back as 3000 BCE, and was mainly brewed on a domestic scale. The invention of beer (and bread) has been argued to be responsible for humanity's ability to develop technology and build civilization. Tea likely originated in Yunnan, China during the Shang Dynasty (1500 BCE–1046 BCE) as a medicinal drink.
History.
Drinking has been a large part of socialising throughout the centuries. In Ancient Greece, a social gathering for the purpose of drinking was known as a symposium, where watered down wine would be drunk. The purpose of these gatherings could be anything from serious discussions to direct indulgence. In Ancient Rome, a similar concept of a "convivium" took place regularly.
Many early societies considered alcohol a gift from the gods, leading to the creation of gods such as Dionysus. Other religions forbid, discourage, or restrict the drinking of alcoholic beverages for various reasons. In some regions with a dominant religion the production, sale, and consumption of alcoholic beverages is forbidden to everybody, regardless of religion.
Toasting is a method of honouring a person or wishing good will by taking a drink. Another tradition is that of the loving cup, at weddings or other celebrations such as sports victories a group will share a drink in a large receptacle, shared by everyone until empty.
In East Africa and Yemen, coffee was used in native religious ceremonies. As these ceremonies conflicted with the beliefs of the Christian church, the Ethiopian Church banned the secular consumption of coffee until the reign of Emperor Menelik II. The beverage was also banned in Ottoman Turkey during the 17th century for political reasons and was associated with rebellious political activities in Europe.
Production.
A beverage or drink is a form of liquid which has been prepared for human consumption. This can include a number of different steps, some prior to transport, others immediately prior to consumption.
Purification of water.
Water is the chief constituent in all drinks, and the primary ingredient in most. Water is purified prior to drinking. Methods for purification include filtration and the addition of chemicals, such as chlorination. The importance of purified water is highlighted by the World Health Organisation, who point out 94% of deaths from diarrhea - the third biggest cause of infectious death worldwide at 1.8 million annually - could be prevented by improving the quality of the victim's environment, particularly safe water.
Pasteurisation.
Pasteurisation is the process of heating a liquid to for a period of time at a specified temperature, then immediately cooling. The process reduces the growth of micro-organisms within the liquid, thereby increasing the time before spoilage. It is primarily used on milk, which prior to pasteurisation is commonly infected with pathogenic bacteria and therefore the more likely than any other part of the common diet in the developed world to cause illness.
Juicing.
The process of extracting juice from fruits and vegetables can take a number of forms. Simple crushing of most fruits will provide a significant amount of liquid, though a more intense pressure can be applied to get the maximum amount of juice from the fruit. Both crushing and pressing are processes used in the production of wine.
Infusion.
Infusion is the process of extracting flavours from plant material by allowing the material to remain suspended within water. This process is used in the production of teas, herbal teas and can be used to prepare coffee (when using a coffee press).
Percolation.
The name is derived from the word "percolate" which means "to cause (a solvent) to pass through a permeable substance especially for extracting a soluble constituent".
In the case of coffee-brewing the solvent is water, the permeable substance is the coffee grounds, and the soluble constituents are the chemical compounds that give coffee its color, taste, aroma, and stimulating properties.
Carbonation.
Carbonation is the process of dissolving Carbon Dioxide into a liquid, such as water.
Fermentation.
Fermentation is a metabolic process that converts sugar to alcohol. Fermentation has been used by humans for the production of beverages since the Neolithic age. In winemaking, grape juice is combined with yeast in an anaerobic environment to allow the fermentation. The amount of sugar in the wine and the length of time given for fermentation determine the alcohol level and the sweetness of the wine.
When brewing beer, there are four primary ingredients - water, grain, yeast and hops. The grain is encouraged to germinate by soaking and drying in heat, a process known as malting. It is then milled before soaking again to create the sugars needed for fermentation. This process is known as mashing. Hops are added for flavouring, then the yeast is added to the mixture (now called wort) to start the fermentation process.
Distillation.
Distillation is a method of separating mixtures based on differences in volatility of components in a boiling liquid mixture. It is one of the methods used in the purification of water. It is also a method of producing spirits from milder alcoholic beverages.
Mixing.
An alcoholic mixed drink that contains two or more ingredients is referred to as a cocktail. Cocktails were originally a mixture of spirits, sugar, water, and bitters. The term is now often used for almost any mixed drink that contains alcohol, including mixers, mixed shots, etc. A cocktail today usually contains one or more kinds of spirit and one or more mixers, such as soda or fruit juice. Additional ingredients may be sugar, honey, milk, cream, and various herbs.
Types of drink.
Non-alcoholic drinks.
A non-alcoholic drink is one that contains little or no alcohol. This category includes low-alcohol beer, non-alcoholic wine, and apple cider if they contain less than 0.5% alcohol by volume. The term "soft drink" specifies the absence of alcohol in contrast to "hard drink" and "drink". The term "drink" is theoretically neutral, but often is used in a way that suggests alcoholic content. Beverages such as soda pop, sparkling water, iced tea, lemonade, root beer, fruit punch, milk, hot chocolate, tea, coffee, milkshakes, and tap water and energy drinks are all soft drinks.
Water.
Water is the world’s most consumed drink however, 97% of water on Earth is non-drinkable salt water. Fresh water is found in rivers, lakes, wetlands, groundwater, and frozen glaciers. Less than 1% of the Earth’s fresh water supplies are accessible through surface water and underground sources which are cost effect to retrieve.
Milk.
Regarded as one of the "original" drinks, milk is the primary source of nutrition for babies. In many cultures of the world, especially the Western world, humans continue to consume dairy milk beyond infancy, using the milk of other animals (especially cattle, goats and sheep) as a beverage. Plant milk, a general term for any milk-like product that is derived from a plant source, also has a long history of consumption in various countries and cultures. The most popular varieties internationally are soy milk, almond milk, rice milk and coconut milk.
Tea.
Tea, the second most consumed drink in the world, is produced from infusing dried leaves of the "camellia sinensis" shrub, in boiling water. There are many ways in which tea is prepared for consumption: lemon or milk and sugar are among the most common additives worldwide. Other additions include butter and salt in Bhutan, Nepal, and Tibet; bubble tea in Taiwan; fresh ginger in Indonesia, Malaysia and Singapore; mint in North Africa and Senegal; cardamom in Central Asia; rum to make Jagertee in Central Europe; and coffee to make yuanyang in Hong Kong. Tea is also served differently from country to country: in China and Japan tiny cups are used to serve tea; in Thailand and the United States tea is often served cold (as "iced tea") or with a lot of sweetener; Indians boil tea with milk and a blend of spices as masala chai; tea is brewed with a samovar in Iran, Kashmir, Russia and Turkey; and in the Australian Outback it is traditionally brewed in a billycan.
Tea leaves can be processed in different ways resulting in a drink which appears and tastes different. Chinese yellow and green tea are steamed, roasted and dried; Oolong tea is semi-fermented and appears green-black and black teas are fully fermented.
Around the world, people refer to other herbal infusions as "teas"; it is also argued that these were popular long before the "Camellia sinensis" shrub was used for tea making. Leaves, flowers, roots or bark can be used to make a herbal infusion and can be bought fresh, dried or powdered.
Coffee.
Coffee is a brewed beverage prepared from the roasted seeds of several species of an evergreen shrub of the genus "Coffea". The two most common sources of coffee beans are the highly regarded "Coffea arabica", and the "robusta" form of the hardier "Coffea canephora". Coffee plants are cultivated in more than 70 countries Once ripe, coffee "berries" are picked, processed, and dried to yield the seeds inside. The seeds are then roasted to varying degrees, depending on the desired flavor, before being ground and brewed to create coffee.
Coffee is slightly acidic (pH 5.0–5.1) and can have a stimulating effect on humans because of its caffeine content. It is one of the most popular drinks in the world. It can be prepared and presented in a variety of ways. The effect of coffee on human health has been a subject of many studies; however, results have varied in terms of coffee's relative benefit.
Coffee cultivation first took place in southern Arabia; the earliest credible evidence of coffee-drinking appears in the middle of the 15th century in the Sufi shrines of Yemen.
Carbonated drinks.
Carbonated drinks refer to drinks which have carbon dioxide dissolved into them. This can happen naturally through fermenting and in natural water spas or artificially by the dissolution of carbon dioxide under pressure. The first commercially available artificially carbonated drink is believed to have been produced by Thomas Henry in the late 1770s.
Cola, orange, various roots, ginger, and lemon/lime are commonly used to create non-alcoholic carbonated drinks; sugars and preservatives may be added later.
The most consumed carbonated soft drinks are produced by three major global brands: Coca-Cola, PepsiCo and the Dr Pepper Snapple Group.
Juice and juice drinks.
Fruit juice is a natural product that contains few or no additives. Citrus products such as orange juice and tangerine juice are familiar breakfast drinks, while grapefruit juice, pineapple, apple, grape, lime, and lemon juice are also common. Coconut water is a highly nutritious and refreshing juice. Many kinds of berries are crushed; their juices are mixed with water and sometimes sweetened. Raspberry, blackberry and currants are popular juices drinks but the percentage of water also determines their nutritive value. Grape juice allowed to ferment produces wine.
Fruits are highly perishable so the ability to extract juices and store them was of significant value. Some fruits are highly acidic and mixing them with water and sugars or honey was often necessary to make them palatable. Early storage of fruit juices was labor-intensive, requiring the crushing of the fruits and the mixing of the resulting pure juices with sugars before bottling.
Vegetable juices are usually served warm or cold. Different types of vegetables can be used to make vegetable juice such as carrots, tomatoes, cucumbers, celery and many more. Some vegetable juices are mixed with some fruit juice to make the vegetable juice taste better. Many popular vegetable juices, particularly ones with high tomato content, are high in sodium, and therefore consumption of them for health must be carefully considered. Some vegetable juices provide the same health benefits as whole vegetables in terms of reducing risks of cardiovascular disease and cancer.
Alcoholic Drinks.
An alcoholic beverage is a drink that contains ethanol, commonly known as alcohol (although in chemistry the definition of "alcohol" includes many other compounds). Beer has been a part of human culture for 8,000 years.
In many countries, drinking alcoholic beverages in a local bar or pub is a cultural tradition.
Beer.
Beer is an alcoholic beverage produced by the saccharification of starch and fermentation of the resulting sugar. The starch and saccharification enzymes are often derived from malted cereal grains, most commonly malted barley and malted wheat. Most beer is also flavoured with hops, which add bitterness and act as a natural preservative, though other flavourings such as herbs or fruit may occasionally be included. The preparation of beer is called brewing. Beer is the world's most widely consumed alcoholic beverage, and is the third-most popular drink overall, after water and tea. It is thought by some to be the oldest fermented beverage.
Some of humanity's earliest known writings refer to the production and distribution of beer: the Code of Hammurabi included laws regulating beer and beer parlours, and "The Hymn to Ninkasi", a prayer to the Mesopotamian goddess of beer, served as both a prayer and as a method of remembering the recipe for beer in a culture with few literate people. Today, the brewing industry is a global business, consisting of several dominant multinational companies and many thousands of smaller producers ranging from brewpubs to regional breweries.
Cider.
Cider is a fermented alcoholic beverage made from fruit juice, most commonly and traditionally apple juice, but also the juice of peaches, pears ("Perry" cider) or other fruit. Cider may be made from any variety of apple, but certain cultivars grown solely for use in cider are known as cider apples. The United Kingdom has the highest per capita consumption of cider, as well as the largest cider-producing companies in the world, , the U.K. produces 600 million litres of cider each year (130 million imperial gallons).
Wine.
Wine is an alcoholic beverage made from fermented grapes or other fruits. The natural chemical balance of grapes lets them ferment without the addition of sugars, acids, enzymes, water, or other nutrients. Yeast consumes the sugars in the grapes and converts them into alcohol and carbon dioxide. Different varieties of grapes and strains of yeasts produce different styles of wine. The well-known variations result from the very complex interactions between the biochemical development of the fruit, reactions involved in fermentation, terroir and subsequent appellation, along with human intervention in the overall process. The final product may contain tens of thousands of chemical compounds in amounts varying from a few percent to a few parts per billion.
Wines made from produce besides grapes are usually named after the product from which they are produced (for example, rice wine, pomegranate wine, apple wine and elderberry wine) and are generically called fruit wine. The term "wine" can also refer to starch-fermented or fortified beverages having higher alcohol content, such as barley wine, huangjiu, or sake.
Wine has a rich history dating back thousands of years, with the earliest production so far discovered having occurred  BC in Georgia. It had reached the Balkans by  BC and was consumed and celebrated in ancient Greece and Rome.
From its earliest appearance in written records, wine has also played an important role in religion. Red wine was closely associated with blood by the ancient Egyptians, who, according to Plutarch, avoided its free consumption as late as the 7th-century BC Saite dynasty, "thinking it to be the blood of those who had once battled against the gods". The Greek cult and mysteries of Dionysus, carried on by the Romans in their Bacchanalia, were the origins of western theater. Judaism incorporates it in the Kiddush and Christianity in its Eucharist, while alcohol consumption was forbidden in Islam.
Spirits.
The term spirit refers to a distilled beverage that contains no added sugar and has at least 20% alcohol by volume (ABV). Popular spirits include borovička, brandy, gin, rum, slivovitz, tequila, vodka, and whisky. Brandy is a spirit created by distilling wine, whilst vodka may be distilled from any starch- or sugar-rich plant matter; most vodka today is produced from grains such as sorghum, corn, rye or wheat.
In culture.
Places to drink.
Throughout history, people have come together in establishments to socialise whilst drinking. This includes cafés and coffeehouses, focus on providing hot drinks as well as light snacks. Many coffee houses in the Middle East, and in West Asian immigrant districts in the Western world, offer "shisha" ("nargile" in Turkish and Greek), flavored tobacco smoked through a hookah. Espresso bars, such as Starbucks and Costa Coffee are a type of coffeehouse that specialize in serving espresso and espresso-based drinks.
In China and Japan, the establishment would be a tea house, were people would socialise whilst drinking tea. Chinese scholars have used the teahouse for places of sharing ideas.
Alcoholic drinks are served in drinking establishments, which have different cultural connotations. For example, pubs are fundamental to the culture of Britain, Ireland, Australia, Atlantic Canada, New England, Metro Detroit, South Africa and New Zealand. In many places, especially in villages, a pub can be the focal point of the community. The writings of Samuel Pepys describe the pub as the heart of England. Many pubs are controlled by breweries, so cask ale or keg beer may be a better value than wines and spirits.
In contrast, types of bars range from seedy bars or nightclubs, sometimes termed "dive bars", to elegant places of entertainment for the elite. Bars provide stools or chairs that are placed at tables or counters for their patrons. The term "bar" is derived from the specialized counter on which drinks are served. Some bars have entertainment on a stage, such as a live band, comedians, go-go dancers, or strippers. Patrons may sit or stand at the bar and be served by the bartender, or they may sit at tables and be served by cocktail servers.
Matching with food.
Food and drink are often paired together to enhance the taste experience. This primarily happens with wine and a culture has grown up around the process. Weight, flavors and textures can either be contrasted or complemented. In recent years, food magazines began to suggest particular wines with recipes and restaurants would offer multi-course dinners matched with a specific wine for each course.
Presentation.
Different drinks have unique receptacles for their consumption. This is sometimes purely for presentations purposes, such as for cocktails. In other situations, the drinkware has practical application, such as coffee cups which are designed for insulation or brandy snifters which are designed to encourage evaporation but trap the aroma within the glass.
Many glasses include a stem, which allows the drinker to hold the glass without affecting the temperature of the drink. In champagne glasses, the bowl is designed to retain champagne's signature carbonation, by reducing the surface area at the opening of the bowl. Historically, champagne has been served in a champagne coupe, the shape of which allowed carbonation to dissipate even more rapidly than from a standard wine glass.
Commercial trade.
International exports and imports.
An important export commodity, coffee was the top agricultural export for twelve countries in 2004,
and it was the world's seventh-largest legal agricultural export by value in 2005. Green (unroasted) coffee is one of the most traded agricultural commodities in the world.
Investment.
Some drinks, such as wine, can be used as an alternative investment. This can be achieved by either purchasing and reselling individual bottles or cases of particular wines, or purchasing shares in an investment wine fund that pools investors' capital.

</doc>
<doc id="7985" url="https://en.wikipedia.org/wiki?curid=7985" title="Dill">
Dill

Dill ("Anethum graveolens") is an annual herb in the celery family Apiaceae.
It is the sole species of the genus "Anethum".
Growth.
Dill grows up to , with slender hollow stems and alternate, finely divided, softly delicate leaves long. The ultimate leaf divisions are broad, slightly broader than the similar leaves of fennel, which are threadlike, less than broad, but harder in texture. The flowers are white to yellow, in small umbels diameter. The seeds are long and thick, and straight to slightly curved with a longitudinally ridged surface.
Etymology.
"Dill" is a Germanic word whose origin is unknown.
Culinary use.
Fresh and dried dill leaves (sometimes called "dill weed" to distinguish it from dill seed) are widely used as herbs in Europe and central Asia.
Like caraway, the fernlike leaves of dill are aromatic and are used to flavor many foods such as gravlax (cured salmon) and other fish dishes, borscht and other soups, as well as pickles (where the dill flower is sometimes used). Dill is best when used fresh as it loses its flavor rapidly if dried; however, freeze-dried dill leaves retain their flavor relatively well for a few months.
Dill seed, having a flavor similar to caraway but also resembling that of fresh or dried dill weed, is used as a spice. Dill oil is extracted from the leaves, stems and seeds of the plant. The oil from the seeds is distilled and used in the manufacturing of soaps.
Dill is the eponymous ingredient in dill pickles: cucumbers preserved in salty brine and/or vinegar.
European cuisine.
In central and eastern Europe, Scandinavia, Russia and Finland, dill is a popular culinary herb used in the kitchen along with parsley. Fresh, finely cut dill leaves are used as topping in soups, especially the hot red borsht and the cold borsht mixed with curds, kefir, yoghurt, or sour cream, which is served during hot summer weather and is called okroshka. It is also popular in summer to drink fermented milk (curds, kefir, yoghurt, or buttermilk) mixed with dill (and sometimes other herbs).
In the same way, prepared dill is used as a topping for boiled potatoes covered with fresh butter – especially in summer when there are so-called "new," or young, potatoes. The dill leaves can be mixed with butter, making a dill butter, which can serve the same purpose. Dill leaves mixed with tvorog form one of the traditional cheese spreads used for sandwiches. Fresh dill leaves are used all year round as an ingredient in salads, "e.g.", one made of lettuce, fresh cucumbers and tomatoes, the way basil leaves are used in Italy and Greece.
In Poland, fresh dill leaves mixed with sour cream are the basis for dressings. It is especially popular to use this kind of sauce with freshly cut cucumbers, which practically are wholly immersed in the sauce, making a salad called "mizeria". The dill leaves serve as a basis for cooking dill sauce, used hot for baked freshwater fish and for chicken or turkey breast, or used hot or cold for hard-boiled eggs.
In south-eastern Poland it is popular to cook a dill-based soup (zupa koperkowa), served with potatoes and hard-boiled eggs. Whole stems including roots and flower buds are traditionally used to prepare Polish-style pickled cucumbers (ogórki kiszone), especially the so-called low-salt cucumbers ("ogórki małosolne"). Whole stems of dill (often including the roots) are also cooked with potatoes, especially the potatoes of autumn and winter, so they resemble the flavor of the newer potatoes found in summer. Some kinds of fish, especially trout and salmon, are traditionally baked with the stems and leaves of dill.
In the Czech Republic, white dill sauce made of cream (or milk), butter, flour, vinegar and dill is called "koprová omáčka" (also "koprovka" or "kopračka") and is served either with boiled eggs and potatoes or with dumplings and boiled beef. Another Czech dish with dill is a soup called "kulajda" that contains mushrooms (traditionally wild ones).
In Germany, dill is popular as a seasoning for fish and many other dishes, chopped as a garnish on potatoes, and a flavoring in pickles.
In Romania dill ("mărar") is widely used as an ingredient for soups such as "borş" (pronounced "borsh"), pickles and other dishes, especially those based on peas, beans and cabbage. It is popular for dishes based on potatoes and mushrooms and can be found in many summer salads (especially cucumber salad, cabbage salad and lettuce salad). During springtime, it is used with spring onions in omelets. It often complements sauces based on sour cream or yogurt and is mixed with salted cheese and used as a filling. Another popular dish with dill as a main ingredient is dill sauce, which is served with eggs and fried sausages.
In Hungary, dill is very widely used. It is popular as a sauce or filling, especially in Langos, and mixed with a type of cottage cheese. Dill is also used for pickling and in salads. The Hungarian name for dill is "kapor".
In Serbia, dill is known as "mirodjija" and is used as an addition to soups, potato and cucumber salads and French fries. It features in the Serbian proverb "бити мирођија у свакој чорби" /biti mirodjija u svakoj čorbi/ (to be a dill in every soup) which corresponds to the English proverb "to have a finger in every pie".
In Greece, dill is known as 'άνηθος' (anithos). In antiquity it was used as an add-in in wines, which they were called "anithites oinos" (wine with anithos-dill). In modern days, dill is used in salads, soups, sauces, and fish and vegetable dishes.
In Santa Maria, Azores, dill ("endro") is the most important ingredient of the traditional Holy Ghost soup ("sopa do Espírito Santo"). Dill is found practically everywhere in Santa Maria and is curiously rare in the other Azorean Islands.
In Sweden, dill is a common spice or herb. The top of fully grown dill is called "krondill" (English: Crown dill); this is used when cooking crayfish. The "krondill" is put in to the water after the crayfish is boiled, but still in hot and salt water. Then the entire dish is stored in refrigerator for at least 24 hours before eating (with toasted bread and butter). "Krondill" is also used for cucumber pickles. Small cucumbers, sliced or not, are put into a solution of hot water, mild acetic vinegar (not made from wine and without colour), sugar and "krondill". After a month or two, the cucumber pickles are ready to eat, for instance, with pork, brown sauce and potatoes, as a "sweetener". The thinner part of dill and young plants may be used with boiled fresh potatoes (as the first potatoes for the year, which usually are small and have a very thin skin). It is used together with, or instead of other green herbs, like parsley, chives and basil, in salads.
Asian cooking.
In Iran, dill is known as "shevid" and is sometimes used with rice and called "shevid-polo". It is also used in Iranian "aash" recipes, and is also called "sheved" in Persian.
In India, dill is known as "shepu" (शेपू) in Marathi and Konkani, "savaa" in Hindi or "soa" in Punjabi. In Telugu, it is called "Soa-kura" (for herb greens). It is also called "sabbasige soppu" (ಸಬ್ಬಸಿಗೆ ಸೊಪ್ಪು) in Kannada. In Tamil it is known as "sada kuppi"(சதகுப்பி). In Malayalam, it is ചതകുപ്പ ("chathakuppa") or ശതകുപ്പ ("sathakuppa"). In Sanskrit, this herb is called "shatapushpa". In Gujarati, it is known as "suva"(સૂવા). In India, dill is prepared in the manner of yellow "moong dal" as a main-course dish. It is considered to have very good antigas properties,so it is used as "mukhwas", or an after-meal digestive. It is also traditionally given to mothers immediately after childbirth. In the state of Uttar Pradesh in India, a smaller amount of fresh dill is cooked along with cut potatoes and fresh fenugreek leaves(Hindi आलू-मेथी-सोया).
In Manipur, dill, locally known as "pakhon", is an essential ingredient of "chagem pomba" – a traditional Manipuri dish made with fermented soybean and rice. In Sri Lanka dill is known in Sinhala as "asamodagam" (අසමෝදගම්).
In Laos and parts of northern Thailand, dill is known in English as Lao coriander (, ). In the Lao language, it is called "phak see", and in Thai, it is known as "phak chee Lao". In Lao cuisine, Lao coriander is used extensively in traditional Lao dishes such as "mok pa" (steamed fish in banana leaf) and several coconut milk-based curries that contain fish or prawns.
In China dill is colloquially called "huixiang" (茴香), or more properly "shiluo" (莳萝). It is a common filling in baozi and xianbing and can be used vegetarian, with rice vermicelli, or combined with either meat or eggs. Vegetarian dill baozi are a common part of a Beijing breakfast. In baozi and xianbing, it is often interchangeable with non-bulbing fennel and the term 茴香 can also refer to fennel, like caraway and coriander leaf share a name in Chinese as well. Dill is also stir fried as a potherb, often with egg, in the same manner as Chinese chives. It is commonly used in Taiwan as well.
In Vietnam, the use of dill in cooking is regional; it is used mainly in northern Vietnamese cuisine.
Middle East uses.
In Arab countries, dill seed, called "ain jaradeh" (grasshopper's eye), is used as a spice in cold dishes such as "fattoush" and pickles. In Arab countries of the Persian Gulf, dill is called "shibint" and is used mostly in fish dishes. In Egypt, dillweed is commonly used to flavor cabbage dishes, including "mahshi koronb" (stuffed cabbage leaves).
In Israel, dill seed is used to spice in salads and also to flavor omelette alongside parsley.
Other regional cooking.
In Canada, dill is a favorite herb to accompany poached salmon.
Traditional uses.
In Anglo-Saxon England, as prescribed in "Leechdoms, Wortcunning, and Starcraft of Early England" (also called "Læceboc", many of whose recipes were borrowed from Greek medicinal texts), dill was used in many traditional medicines, including those against jaundice, headache, boils, lack of appetite, stomach problems, nausea, liver problems, and many other ills. Dill seeds can also be used to prepare herbal tea.
In India the leaves of dill and other greens are used to prepare a variety of local dishes which are served as an accompaniment to rotis or chapatis.
In ancient Greece fragrance was made from the leaves of dill. Also, athletes used to spread essence of dill all over their body, as muscle toner.
Cultivation.
Successful cultivation requires warm to hot summers with high sunshine levels; even partial shade will reduce the yield substantially. It also prefers rich, well drained soil. The seeds are viable for three to ten years.
The seed is harvested by cutting the flower heads off the stalks when the seed is beginning to ripen. The seed heads are placed upside down in a paper bag and left in a warm, dry place for a week. The seeds then separate from the stems easily for storage in an airtight container.
Companion planting.
When used as a companion plant, dill attracts many beneficial insects as the umbrella flower heads go to seed. It makes a good companion plant for cucumbers. It is a poor companion for carrots and tomatoes.

</doc>
<doc id="7988" url="https://en.wikipedia.org/wiki?curid=7988" title="Dual space">
Dual space

In mathematics, any vector space "V" has a corresponding dual vector space (or just dual space for short) consisting of all linear functionals on "V" together with a naturally induced linear structure. Dual vector spaces for finite-dimensional vector spaces show up in tensor analysis. When applied to vector spaces of functions (which are typically infinite-dimensional), dual spaces are used to describe measures, distributions, and Hilbert spaces. Consequently, the dual space is an important concept in functional analysis.
There are two types of dual spaces: the "algebraic dual space", and the "continuous dual space". The algebraic dual space is defined for all vector spaces. When defined for a topological vector space there is a subspace of this dual space, corresponding to continuous linear functionals, which constitutes a continuous dual space.
Algebraic dual space.
Given any vector space "V" over a field "F", the dual space "V" is defined as the set of all linear maps (linear functionals). The dual space "V" itself becomes a vector space over "F" when equipped with an addition and scalar multiplication satisfying:
for all "φ" and , , and . Elements of the algebraic dual space "V" are sometimes called covectors or one-forms.
The pairing of a functional "φ" in the dual space "V" and an element "x" of "V" is sometimes denoted by a bracket: 
or . The pairing defines a nondegenerate bilinear mapping .
Finite-dimensional case.
If "V" is finite-dimensional, then "V" has the same dimension as "V". Given a basis in "V", it is possible to construct a specific basis in "V", called the dual basis. This dual basis is a set of linear functionals on "V", defined by the relation
for any choice of coefficients . In particular, letting in turn each one of those coefficients be equal to one and the other coefficients zero, gives the system of equations
where formula_4 is the Kronecker delta symbol. For example if "V" is R, and its basis chosen to be , then e and e are one-forms (functions that map a vector to a scalar) such that , , , and . (Note: The superscript here is the index, not an exponent).
In particular, if we interpret R as the space of columns of "n" real numbers, its dual space is typically written as the space of "rows" of "n" real numbers. Such a row acts on R as a linear functional by ordinary matrix multiplication. One way to see this is that a functional maps every "n"-vector "x" into a real number "y". Then, seeing this functional as a matrix "M", and "x", "y" as a matrix and a matrix (trivially, a real number) respectively, if we have , then, by dimension reasons, "M" must be a matrix, i.e., "M" must be a row vector.
If "V" consists of the space of geometrical vectors in the plane, then the level curves of an element of "V" form a family of parallel lines in "V", because the range is 1-dimensional, so that every point in the range is a multiple of any one nonzero element. So an element of "V" can be intuitively thought of as a particular family of parallel lines covering the plane. To compute the value of a functional on a given vector, one needs only to determine which of the lines the vector lies on. Or, informally, one "counts" how many lines the vector crosses. More generally, if "V" is a vector space of any dimension, then the level sets of a linear functional in "V" are parallel hyperplanes in "V", and the action of a linear functional on a vector can be visualized in terms of these hyperplanes.
Infinite-dimensional case.
If "V" is not finite-dimensional but has a basis e indexed by an infinite set "A", then the same construction as in the finite-dimensional case yields linearly independent elements e () of the dual space, but they will not form a basis.
Consider, for instance, the space R, whose elements are those sequences of real numbers that contain only finitely many non-zero entries, which has a basis indexed by the natural numbers N: for , e is the sequence consisting of all zeroes except in the "i"th position, which is "1". The dual space of R is R, the space of "all" sequences of real numbers: such a sequence ("a") is applied to an element ("x") of R to give the number ∑"ax", which is a finite sum because there are only finitely many nonzero "x". The dimension of R is countably infinite, whereas R does not have a countable basis.
This observation generalizes to any infinite-dimensional vector space "V" over any field "F": a choice of basis identifies "V" with the space ("F") of functions such that is nonzero for only finitely many , where such a function "f" is identified with the vector
in "V" (the sum is finite by the assumption on "f", and any may be written in this way by the definition of the basis).
The dual space of "V" may then be identified with the space "F" of "all" functions from "A" to "F": a linear functional "T" on "V" is uniquely determined by the values it takes on the basis of "V", and any function (with ) defines a linear functional "T" on "V" by
Again the sum is finite because "f" is nonzero for only finitely many "α".
Note that ("F") may be identified (essentially by definition) with the direct sum
of infinitely many copies of "F" (viewed as a 1-dimensional vector space over itself) indexed by "A", i.e., there are linear isomorphisms
On the other hand "F" is (again by definition), the direct product of infinitely many copies of "F" indexed by "A", and so the identification
is a special case of a general result relating direct sums (of modules) to direct products.
Thus if the basis is infinite, then the algebraic dual space is "always" of larger dimension (as a cardinal number) than the original vector space. This is in contrast to the case of the continuous dual space, discussed below, which may be isomorphic to the original vector space even if the latter is infinite-dimensional.
Bilinear products and dual spaces.
If "V" is finite-dimensional, then "V" is isomorphic to "V". But there is in general no natural isomorphism between these two spaces. Any bilinear form on "V" gives a mapping of "V" into its dual space via
where the right hand side is defined as the functional on "V" taking each to . In other words, the bilinear form determines a linear mapping
defined by
If the bilinear form is nondegenerate, then this is an isomorphism onto a subspace of "V". If "V" is finite-dimensional, then this is an isomorphism onto all of "V". Conversely, any isomorphism Φ from "V" to a subspace of "V" (resp., all of "V") defines a unique nondegenerate bilinear form on "V" by
Thus there is a one-to-one correspondence between isomorphisms of "V" to subspaces of (resp., all of) "V" and nondegenerate bilinear forms on "V".
If the vector space "V" is over the complex field, then sometimes it is more natural to consider sesquilinear forms instead of bilinear forms. In that case, a given sesquilinear form determines an isomorphism of "V" with the complex conjugate of the dual space
The conjugate space "V" can be identified with the set of all additive complex-valued functionals such that
Injection into the double-dual.
There is a natural homomorphism Ψ from "V" into the double dual "V", defined by for all , . This map Ψ is always injective; it is an isomorphism if and only if "V" is finite-dimensional. Indeed, the isomorphism of a finite-dimensional vector space with its double dual is an archetypal example of a natural isomorphism. Note that infinite-dimensional Hilbert spaces are not a counterexample to this, as they are isomorphic to their continuous duals, not to their algebraic duals.
Transpose of a linear map.
If is a linear map, then the "transpose" (or "dual") is defined by
for every . The resulting functional "f" ("φ") in "V" is called the "pullback" of "φ" along "f".
The following identity holds for all and :
where the bracket , on the left is the duality pairing of "V" with its dual space, and that on the right is the duality pairing of "W" with its dual. This identity characterizes the transpose, and is formally similar to the definition of the adjoint.
The assignment produces an injective linear map between the space of linear operators from "V" to "W" and the space of linear operators from "W" to "V"; this homomorphism is an isomorphism if and only if "W" is finite-dimensional. If then the space of linear maps is actually an algebra under composition of maps, and the assignment is then an antihomomorphism of algebras, meaning that . In the language of category theory, taking the dual of vector spaces and the transpose of linear maps is therefore a contravariant functor from the category of vector spaces over "F" to itself. Note that one can identify ("f" ) with "f" using the natural injection into the double dual.
If the linear map "f" is represented by the matrix "A" with respect to two bases of "V" and "W", then "f"  is represented by the transpose matrix "A" with respect to the dual bases of "W" and "V", hence the name. Alternatively, as "f" is represented by "A" acting on the left on column vectors, "f"  is represented by the same matrix acting on the right on row vectors. These points of view are related by the canonical inner product on R, which identifies the space of column vectors with the dual space of row vectors.
Quotient spaces and annihilators.
Let "S" be a subset of "V". The annihilator of "S" in "V", denoted here "S", is the collection of linear functionals such that for all . That is, "S" consists of all linear functionals such that the restriction to "S" vanishes: .
The annihilator of a subset is itself a vector space. In particular, is all of "V" (vacuously), whereas is the zero subspace. Furthermore, the assignment of an annihilator to a subset of "V" reverses inclusions, so that if , then
Moreover, if "A" and "B" are two subsets of "V", then
and equality holds provided "V" is finite-dimensional. If "A" is any family of subsets of "V" indexed by "i" belonging to some index set "I", then
In particular if "A" and "B" are subspaces of "V", it follows that
If "V" is finite-dimensional, and "W" is a vector subspace, then
after identifying "W" with its image in the second dual space under the double duality isomorphism . Thus, in particular, forming the annihilator is a Galois connection on the lattice of subsets of a finite-dimensional vector space.
If "W" is a subspace of "V" then the quotient space "V"/"W" is a vector space in its own right, and so has a dual. By the first isomorphism theorem, a functional factors through "V"/"W" if and only if "W" is in the kernel of "f". There is thus an isomorphism
As a particular consequence, if "V" is a direct sum of two subspaces "A" and "B", then "V" is a direct sum of "A" and "B".
Continuous dual space.
When dealing with topological vector spaces, one is typically only interested in the continuous linear functionals from the space into the base field formula_23 (or formula_24). This gives rise to the notion of the "continuous dual space" or "topological dual" which is a linear subspace of the algebraic dual space formula_25, denoted by formula_26. For any "finite-dimensional" normed vector space or topological vector space, such as Euclidean "n-"space, the continuous dual and the algebraic dual coincide. This is however false for any infinite-dimensional normed space, as shown by the example of discontinuous linear maps. Nevertheless in the theory of topological vector spaces the terms "continuous dual space" and "topological dual space" are often replaced by "dual space", since there is no serious need to consider discontinuous maps in this field.
For a topological vector space formula_27 its "continuous dual space", or "topological dual space", or just "dual space" (in the sense of the theory of topological vector spaces) formula_26 is defined as the space of all continuous linear functionals formula_29.
There is a standard construction for introducing a topology on the continuous dual formula_26 of a topological vector space formula_27. Fix a collection formula_32 of bounded subsets of formula_27. Then one has the topology on formula_27 of uniform convergence on sets from formula_32, or what is the same thing, the topology generated by seminorms of the form 
where formula_37 is a continuous linear functional on formula_27, and formula_39 runs over the class formula_32.
This means that a net of functionals formula_41 tends to a functional formula_37 in formula_26 if and only if 
Usually (but not necessarily) the class formula_32 is supposed to satisfy the following conditions:
If these requirements are fulfilled then the corresponding topology on formula_26 is Hausdorff and the sets 
form its local base.
Here are the three most important special cases.
Each of these three choices of topology on formula_26 leads to a variant of reflexivity property for topological vector spaces.
Examples.
Let 1 < "p" < ∞ be a real number and consider the Banach space "ℓ" of all sequences for which
is finite. Define the number "q" by . Then the continuous dual of "ℓ" is naturally identified with "ℓ": given an element , the corresponding element of is the sequence ("φ"(e)) where e denotes the sequence whose "n-"th term is 1 and all others are zero. Conversely, given an element , the corresponding continuous linear functional "φ" on is defined by for all (see Hölder's inequality).
In a similar manner, the continuous dual of is naturally identified with (the space of bounded sequences). Furthermore, the continuous duals of the Banach spaces "c" (consisting of all convergent sequences, with the supremum norm) and "c" (the sequences converging to zero) are both naturally identified with .
By the Riesz representation theorem, the continuous dual of a Hilbert space is again a Hilbert space which is anti-isomorphic to the original space. This gives rise to the bra–ket notation used by physicists in the mathematical formulation of quantum mechanics.
Transpose of a continuous linear map.
If is a continuous linear map between two topological vector spaces, then the (continuous) transpose is defined by the same formula as before:
The resulting functional is in. The assignment produces a linear map between the space of continuous linear maps from "V" to "W" and the space of linear maps from to . When "T" and "U" are composable continuous linear maps, then
When "V" and "W" are normed spaces, the norm of the transpose in is equal to that of "T" in. Several properties of transposition depend upon the Hahn–Banach theorem. For example, the bounded linear map "T" has dense range if and only if the transpose is injective.
When "T" is a compact linear map between two Banach spaces "V" and "W", then the transpose is compact. This can be proved using the Arzelà–Ascoli theorem.
When "V" is a Hilbert space, there is an antilinear isomorphism "i" from "V" onto its continuous dual. For every bounded linear map "T" on "V", the transpose and the adjoint operators are linked by
When "T" is a continuous linear map between two topological vector spaces "V" and "W", then the transpose is continuous when and are equipped with"compatible" topologies: for example when, for and , both duals have the strong topology of uniform convergence on bounded sets of "X", or both have the weak-∗ topology of pointwise convergence on "X". The transpose is continuous from to , or from to .
Annihilators.
Assume that "W" is a closed linear subspace of a normed space "V", and consider the annihilator of "W" in,
Then, the dual of the quotient can be identified with "W", and the dual of "W" can be identified with the quotient . Indeed, let "P" denote the canonical surjection from "V" onto the quotient ; then, the transpose is an isometric isomorphism from into, with range equal to "W". If "j" denotes the injection map from "W" into "V", then the kernel of the transpose is the annihilator of "W":
and it follows from the Hahn–Banach theorem that induces an isometric isomorphism
Further properties.
If the dual of a normed space "V" is separable, then so is the space "V" itself. The converse is not true: for example the space is separable, but its dual is not.
Topologies on the dual.
The topology of "V" and the topology of real or complex numbers can be used to induce on "V′" a dual space topology.
Double dual.
In analogy with the case of the algebraic double dual, there is always a naturally defined continuous linear operator from a normed space "V" into its continuous double dual, defined by
As a consequence of the Hahn–Banach theorem, this map is in fact an isometry, meaning for all "x" in "V". Normed spaces for which the map Ψ is a bijection are called reflexive.
When "V" is a topological vector space, one can still define Ψ("x") by the same formula, for every , however several difficulties arise. First, when "V" is not locally convex, the continuous dual may be equal to {0} and the map Ψ trivial. However, if "V" is Hausdorff and locally convex, the map Ψ is injective from "V" to the algebraic dual of the continuous dual, again as a consequence of the Hahn–Banach theorem.
Second, even in the locally convex setting, several natural vector space topologies can be defined on the continuous dual , so that the continuous double dual is not uniquely defined as a set. Saying that Ψ maps from "V" to , or in other words, that Ψ("x") is continuous on for every , is a reasonable minimal requirement on the topology of , namely that the evaluation mappings
be continuous for the chosen topology on . Further, there is still a choice of a topology on , and continuity of Ψ depends upon this choice. As a consequence, defining reflexivity in this framework is more involved than in the normed case.

</doc>
<doc id="7989" url="https://en.wikipedia.org/wiki?curid=7989" title="Dianetics">
Dianetics

Dianetics is a set of ideas and practices regarding the metaphysical relationship between the mind and body created by L. Ron Hubbard; Dianetics is practiced by followers of Scientology and separate independent Dianeticist groups. Hubbard coined "Dianetics" from the Greek stems "dia", meaning "through," and "nous", meaning "mind." Dianetics has achieved no acceptance as a scientific theory and is a widely accepted example of a pseudoscience.
Dianetics divides the mind into three parts: the conscious "analytical mind," the subconscious "reactive mind," and the somatic mind. The goal of Dianetics is to erase the content of the "reactive mind," which Scientologists believe interferes with a person's ethics, awareness, happiness, and sanity. The Dianetics procedure to achieve this erasure is called "auditing." In auditing, the Dianetic auditor asks a series of questions (or commands) and elicits answers to help a person locate and deal with painful experiences of the past, which Scientologists believe to be the content of the "reactive mind."
Practitioners of Dianetics believe that "the basic principle of existence is to survive" and that the basic personality of humans is sincere, intelligent, and good. The drive for goodness and survival is distorted and inhibited by aberrations "ranging from simple neuroses to different psychotic states to various kinds of sociopathic behavior patterns." Hubbard developed Dianetics, claiming that it could eradicate these aberrations.
When Hubbard formulated Dianetics, he described it as "a mix of Western technology and Oriental philosophy". He said that Dianetics "forms a bridge between" cybernetics and General Semantics (a set of ideas about education originated by Alfred Korzybski, which received much attention in the science fiction world in the 1940s) — a claim denied by scholars of General Semantics, including S. I. Hayakawa, who expressed strong criticism of Dianetics as early as 1951. Hubbard claimed that Dianetics could increase intelligence, eliminate unwanted emotions and alleviate a wide range of illnesses he believed to be psychosomatic. Among the conditions purportedly treated were arthritis, allergies, asthma, some coronary difficulties, eye trouble, ulcers, migraine headaches, "sexual deviation" (which for Hubbard included homosexuality), and even death. Hubbard asserted that "memories of painful physical and emotional experiences accumulate in a specific region of the mind, causing illness and mental problems." He taught that "once these experiences have been purged through cathartic procedures he developed, a person can achieve superior health and intelligence." Hubbard also variously defined Dianetics as "a spiritual healing technology" and "an organized science of thought."
Dianetics predates Hubbard's classification of Scientology as an "applied religious philosophy." Early in 1951, he expanded his writings to include teachings related to the soul, or "thetan." Dianetics is practiced by several independent Dianetics-only groups not connected with Scientology, and also Free Zone or Independent Scientologists. The Church of Scientology has prosecuted a number of people in court for unauthorized publication of Scientology and Dianetics copyrighted material.
History.
Hubbard always claimed that his ideas of Dianetics originated in the 1920s and 1930s. By his own account, he spent a great deal of time in the Oak Knoll Naval Hospital's library, where he encountered the work of Freud and other psychoanalysts. In April 1950, Hubbard and several others established the Hubbard Dianetic Research Foundation in Elizabeth, New Jersey to coordinate work related to the forthcoming publication. Hubbard first introduced Dianetics to the public in the article ' published in the May 1950 issue of the magazine "Astounding Science Fiction". Hubbard wrote ' at that time, allegedly completing the 180,000-word book in six weeks. The introduction of the book was the subject of an Associated Press article on March 29, 1950, with the lead "Discovery of a submind is claimed in a new book entitled "Dianetics"". 
When "Dianetics" was published in 1950, Hubbard announced in the opening pages, "The first contribution of Dianetics is the discovery that the problems of thought and mental function can be resolved within the bounds of the finite universe, which is to say that all data needful to the solution of mental action and Man’s endeavor can be measured, sensed and experienced as scientific truths independent of mysticism or metaphysics." This was in line with Hubbard’s initial presentation of Dianetics as a science, almost four years before he founded Scientology.
Publication of "Dianetics: The Modern Science of Mental Health" brought in a flood of money, which Hubbard used to establish Dianetics foundations in six major American cities. Dianetics shared the New York Times best-seller list with other self-help writings, including Norman Vincent Peale’s "The Art of Happiness" and Henry Overstreet’s "The Mature Mind". Scholar Hugh B. Urban asserted that the initial success of Dianetics was reflective of Hubbard’s “remarkable entrepreneurial skills.”
In January 1951, the New Jersey Board of Medical Examiners instituted proceedings against the Hubbard Dianetic Research Foundation in Elizabeth for teaching medicine without a licence. The Foundation closed its doors, causing the proceedings to be vacated, but its creditors began to demand settlement of its outstanding debts. Don Purcell, a millionaire Dianeticist from Wichita, Kansas, offered a brief respite from bankruptcy, but the Foundation's finances failed again in 1952.
In 1954, Hubbard defined Scientology as a religion focused on the spirit, differentiating it from Dianetics, which he defined as a science that addressed the physical being. He stated, “Dianetics is a science which applies to man, a living organism; and Scientology is a religion.”
Because of a sale of assets resulting from the bankruptcy, Hubbard no longer owned the rights to the name "Dianetics", but its philosophical framework still provided the seed for Scientology to grow. Scientologists refer to the book "Dianetics: The Modern Science of Mental Health" as "Book One." In 1952, Hubbard published a new set of teachings as "Scientology, a religious philosophy." Scientology did not replace Dianetics but extended it to cover new areas: Where the goal of Dianetics is to rid the individual of his reactive mind engrams, the stated goal of Scientology is to rehabilitate the individual's spiritual nature so that he may reach his full potential.
In 1963 and again in May 1969, Hubbard reorganized the material in Dianetics, the auditing commands, and E-meter use, naming the package "Standard Dianetics." In a 1969 bulletin, "This bulletin combines HCOB 27 April 1969 ‘R-3-R Restated’ with those parts of HCOB 24 June 1963 ‘Routine 3-R’ used in the new Standard Dianetic Course and its application. This gives the complete steps of Routine 3-R Revised." 
In 1978, Hubbard released "New Era Dianetics" (NED), a revised version supposed to produce better results in a shorter period of time. The course consists of 11 rundowns and requires a specifically trained auditor. It is similar to Standard Dianetics, but the person being audited is encouraged to find the decision or "postulate" he made during or as a result of the incident. ("Postulate" in Dianetics and Scientology has the meaning of "a conclusion, decision or resolution made by the individual himself; to conclude, decide or resolve a problem or to set a pattern for the future or to nullify a pattern of the past" in contrast to its conventional meanings.)
In the Church of Scientology, OTs study several levels of before reaching the highest level.
Basic concepts.
In the book, "", Hubbard describes techniques that he suggests can rid individuals of fears and psychosomatic illnesses. A basic idea in Dianetics is that the mind consists of two parts: the "analytical mind" and the "reactive mind." The "reactive mind", the mind which operates when a person is physically unconscious, acts as a record of shock, trauma, pain, and otherwise harmful memories. Experiences such as these, stored in the "reactive mind" are dubbed "engrams". Dianetics is proposed as a method to erase these engrams in the reactive mind to achieve a state of clear.
By his own admission, Hubbard made a great mistake when he used the biological definition of engram as a "trace on a cell", which was not in agreement with the standard biological definition.
Hubbard described Dianetics as "an organized science of thought built on definite axioms: statements of natural laws on the order of those of the physical sciences". These Dianetic axioms can be found in Hubbard books such as "Scientology 0-8: The Book of Basics" and "Advanced Procedure and Axioms". Unlike conventional therapies, Hubbard said, Dianetics would work every time if applied properly and "will invariably cure all psychosomatic ills and human aberrations." In April 1950, before the public release of Dianetics, he wrote: "To date, over two hundred patients have been treated; of those two hundred, two hundred cures have been obtained."
In Dianetics, the unconscious or reactive mind is described as a collection of "mental image pictures," which contain the recorded experience of past moments of unconsciousness, including all sensory perceptions and feelings involved, ranging from pre-natal experiences, infancy and childhood, to even the traumatic feelings associated events from past lives and extra-terrestrial cultures. The type of mental image picture created during a period of unconsciousness involves the exact recording of a painful experience. Hubbard called this phenomenon an engram, and defined it as "a complete recording of a moment of unconsciousness containing physical pain or painful emotion and all perceptions."
Hubbard proposed that painful physical or emotional traumas caused "aberrations" (deviations from rational thinking) in the mind, which produced lasting adverse physical and emotional effects, similar to conversion disorders. When the analytical (conscious) mind shut down during these moments, events and perceptions of this period were stored as engrams in the unconscious or reactive mind. (In Hubbard's earliest publications on the subject, engrams were variously referred to as "Norns", "Impediments," and "comanomes" before "engram" was adapted from its existing usage at the suggestion of Joseph Winter.) Some commentators noted Dianetics's blend of science fiction and occult orientations at the time.
Hubbard claimed that these engrams are the cause of almost all psychological and physical problems. In addition to physical pain, engrams could include words or phrases spoken in the vicinity while the patient was unconscious. For instance, Winter cites the example of a patient with a persistent headache supposedly tracing the problem to a doctor saying, "Take him now," during the patient's birth. Hubbard similarly claimed that leukemia is traceable to "an engram containing the phrase 'It turns my blood to water.'" While it is sometimes claimed that the Church of Scientology no longer stands by Hubbard's claims that Dianetics can treat physical conditions, it still publishes them: "... when the knee injuries of the past are located and discharged, the arthritis ceases, no other injury takes its place and the person is finished with arthritis of the knee." "he reactive min can give a man arthritis, bursitis, asthma, allergies, sinusitis, coronary trouble, high blood pressure ... And it is the only thing in the human being which can produce these effects ... Discharge the content of he reactive min and the arthritis vanishes, myopia gets better, heart illness decreases, asthma disappears, stomachs function properly and the whole catalog of ills goes away and stays away."
Some of the psychometric ideas in Dianetics, in particular the E-meter, can be traced to Carl Jung. Basic concepts, including conversion disorder, are derived from Sigmund Freud, whom Hubbard credited as an inspiration and source. Freud had speculated 40 years previously that traumas with similar content join together in "chains," embedded in the unconscious mind, to cause irrational responses in the individual. Such a chain would be relieved by inducing the patient to remember the earliest trauma, "with an accompanying expression of emotion."
According to Bent Corydon, Hubbard created the illusion that Dianetics was the first psychotherapy to address traumatic experiences in their own time, but others had done so as standard procedure.
One treatment method Hubbard drew from in developing Dianetics was abreaction therapy. Abreaction is a psychoanalytical term that means bringing to consciousness, and thus adequate expression, material that has been unconscious. "It includes not only the recollection of forgotten memories and experience, but also their reliving with appropriate emotional display and discharge of effect. This process is usually facilitated by the patient's gaining awareness of the causal relationship between the previously undischarged emotion and his symptoms."
According to Hubbard, before Dianetics psychotherapists had dealt with very light and superficial incidents (e.g. an incident that reminds the patient of a moment of loss), but with Dianetic therapy, the patient could actually erase moments of pain and unconsciousness. He emphasized: "The discovery of the engram is entirely the property of Dianetics. Methods of its erasure are also owned entirely by Dianetics..."
While 1950 style Dianetics was in some respects similar to older therapies, with the development of New Era Dianetics in 1978, the similarity vanished. New Era Dianetics uses an E-Meter and a rote procedure for running "chains" of related traumatic incidents.
Dianetics clarifies the understanding of psychosomatic illness in terms of "predisposition", "precipitation", and "prolongation". 
With the use of Dianetics techniques, Hubbard claimed, the reactive mind could be processed and all stored engrams could be refiled as experience. The central technique was "auditing," a two-person question-and-answer therapy designed to isolate and dissipate engrams (or "mental masses"). An auditor addresses questions to a subject, observes and records the subject's responses, and returns repeatedly to experiences or areas under discussion that appear painful until the troubling experience has been identified and confronted. Through repeated applications of this method, the reactive mind could be "cleared" of its content having outlived its usefulness in the process of evolution; a person who has completed this process would be "Clear".
The benefits of going Clear, according to Hubbard, were dramatic. A Clear would have no compulsions, repressions, psychoses or neuroses, and would enjoy a near-perfect memory as well as a rise in IQ of as much as 50 points. He also claimed that "the atheist is activated by engrams as thoroughly as the zealot". He further claimed that widespread application of Dianetics would result in "A world without insanity, without criminals and without war."
According to the Scientology journal "The Auditor", the total number of "Clears" as of May 2006 stands at 50,311.
Scientific evaluation and criticisms.
Hubbard's original book on Dianetics attracted highly critical reviews from science and medical writers and organizations. The American Psychological Association passed a resolution in 1950 calling "attention to the fact that these claims are not supported by empirical evidence of the sort required for the establishment of scientific generalizations." Subsequently, Dianetics has achieved no acceptance as a scientific theory and scientists cite Dianetics as an example of a pseudoscience.
In August 1950, amidst the success of "", Hubbard held a demonstration in Los Angeles' Shrine Auditorium where he presented a young woman called Sonya Bianca (a pseudonym) to a large audience including many reporters and photographers as 'the world's first Clear.' However, despite Hubbard's claim that she had "full and perfect recall of every moment of her life", Bianca proved unable to answer questions from the audience testing her memory and analytical abilities, including the question of the color of Hubbard's tie. Hubbard explained Bianca's failure to display her promised powers of recall to the audience by saying that he had used the word "now" in calling her to the stage, and thus inadvertently froze her in "present time," which blocked her abilities. Later, in the late 1950s, Hubbard would claim that several people had reached the state of Clear by the time he presented Bianca as the world's first; these others, Hubbard said, he had successfully cleared in the late 1940s while working "incognito" in Hollywood posing as a swami. In 1966, Hubbard declared South African Scientologist John McMaster to be the first true Clear. McMaster left the Sea Org in November 1969, expressing continuing belief in the Scientology Tech, but disapproval of the way Scientology was managed.
Few scientific investigations into the effectiveness of Dianetics have been published. Professor John A. Lee states in his 1970 evaluation of Dianetics:
The MEDLINE database records two independent scientific studies on Dianetics, both conducted in the 1950s under the auspices of New York University. Harvey Jay Fischer tested Dianetics therapy against three claims made by proponents and found it does not effect any significant changes in intellectual functioning, mathematical ability, or the degree of personality conflicts; Jack Fox tested Hubbard's thesis regarding recall of engrams, with the assistance of the Dianetic Research Foundation, and could not substantiate it.
Hubbard claimed, in an interview with the "New York Times" in November 1950, that "he had already submitted proof of claims made in the book to a number of scientists and associations." He added that the public as well as proper organizations were entitled to such proof and that he was ready and willing to give such proof in detail. In January 1951, the Hubbard Dianetic Research Foundation of Elizabeth, NJ published "Dianetic Processing: A Brief Survey of Research Projects and Preliminary Results", a booklet providing the results of psychometric tests conducted on 88 people undergoing Dianetics therapy. It presents case histories and a number of X-ray plates to support claims that Dianetics had cured "aberrations" including manic depression, asthma, arthritis, colitis and "overt homosexuality," and that after Dianetic processing, test subjects experienced significantly increased scores on a standardized IQ test. The report's subjects are not identified by name, but one of them is clearly Hubbard himself ("Case 1080A, R. L.").
The authors provide no qualifications, although they are described in Hubbard's book "Science of Survival" (where some results of the same study were reprinted) as psychotherapists. Critics of Dianetics are skeptical of this study, both because of the bias of the source and because the researchers appear to ascribe all physical benefits to Dianetics without considering possible outside factors; in other words, the report lacks any scientific controls. J.A. Winter, M.D., originally an associate of Hubbard and an early adopter of Dianetics, had by the end of 1950 cut his ties with Hubbard and written an account of his personal experiences with Dianetics. He described Hubbard as "absolutistic and authoritarian", and criticized the Hubbard Dianetic Research Foundation for failing to undertake "precise scientific research into the functioning of the mind". He also recommended that auditing be done by experts only and that it was dangerous for laymen to audit each other. Hubbard writes: "Again, Dianetics is not being released to a profession, for no profession could encompass it."
Commentators from a variety of backgrounds have described Dianetics as an example of pseudoscience—that is, information presented as scientific that fails to meet the criteria for science. For example, philosophy professor Robert Carroll points to Dianetics' lack of empirical evidence:
W. Sumner Davis similarly comments that
Procedure in practice.
The procedure of Dianetics therapy (known as "auditing") is a two-person activity. One person, the "auditor", guides the other person, the "pre-clear". The pre-Clear's job is to look at the mind and talk to the auditor. The auditor acknowledges what the pre-Clear says and controls the process so the pre-Clear may put his full attention on his work.
The auditor and pre-Clear sit down in chairs facing each other. The process then follows in eleven distinct steps:
Auditing sessions are kept confidential. This has come into question, though, as confidential information has been used to blackmail possible defectors (see Fair Game). A few transcripts of auditing sessions with confidential information removed have been published as demonstration examples. Some extracts can be found in Dr. J.A. Winter's book "". Other, more comprehensive, transcripts of auditing sessions carried out by Hubbard himself can be found in volume 1 of the "Research & Discovery Series" (Bridge Publications, 1980). Examples of public group processing sessions can be found throughout the "Congresses" lecture series. Recently, a rash of ex-Scientologists, most notably actor Jason Beghe, have come forward with claims that not only are audits verbally recorded and kept in a file, which was the concern of actor Leah Remini after her departure, but also videotaped by cameras placed sporadically around the auditing room.
According to Hubbard, auditing enables the pre-Clear to "contact" and "release" engrams stored in the reactive mind, relieving him of the physical and mental aberrations connected with them. The pre-Clear is asked to inspect and familiarize himself with the exact details of his own experience; the auditor may not tell him anything about his case or evaluate any of the information the pre-Clear finds.
The validity and practice of auditing have been questioned by a variety of non-Scientologist commentators. Commenting on the example cited by Winter, the science writer Martin Gardner asserts that "nothing could be clearer from the above dialogue than the fact that the dianetic explanation for the headache existed only in the mind of the therapist, and that it was with considerable difficulty that the patient was maneuvered into accepting it."
Other critics and medical experts have suggested that Dianetic auditing is a form of hypnosis, although the Church of Scientology has strongly denied that hypnosis forms any part of Dianetics. To the contrary, L. Ron Hubbard expressly warns not to use any hypnosis or hypnosis-like methods, because a person under hypnosis would be receptive to suggestions. This would decrease his self-determinism instead of increasing it, which is one of the prime goals of Dianetics. Winter 95 comments that the leading nature of the questions asked of a pre-Clear "encourage fantasy", a common issue also encountered with hypnosis, which can be used to form false memories. The auditor is instructed not to make any assessment of a recalled memory's reality or accuracy, but instead to treat it as if it were objectively real. Professor Richard J. Ofshe, a leading expert on false memories, suggests that the feeling of well-being reported by pre-Clear at the end of an auditing session may be induced by post-hypnotic suggestion. Other researchers have identified quotations in Hubbard's work suggesting evidence that false memories were created in "Dianetics," specifically in the form of birth and pre-birth memories. According to Hubbard: "Laughter is definitely the relief of painful emotion."
Autocontrol.
According to Hubbard, the majority of the people interested in the subject believed they could accomplish therapy alone. "It cannot be done" and he adds: "If a patient places himself in autohypnosis and regresses himself in an effort to reach illness or birth or prenatals, the only thing he will get is ill".

</doc>
<doc id="7990" url="https://en.wikipedia.org/wiki?curid=7990" title="Data warehouse">
Data warehouse

In computing, a data warehouse (DW or DWH), also known as an enterprise data warehouse (EDW), is a system used for reporting and data analysis. DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data and are used for creating analytical reports for knowledge workers throughout the enterprise. Examples of reports could range from annual and quarterly comparisons and trends to detailed daily sales analysis.
The data stored in the warehouse is uploaded from the operational systems (such as marketing, sales, etc., shown in the figure to the right). The data may pass through an operational data store for additional operations before it is used in the DW for reporting.
The difference between data warehouse and data mart 
Types of systems.
Types of data marts
Software tools.
The typical extract-transform-load (ETL)-based data warehouse uses staging, data integration, and access layers to house its key functions. The staging layer or staging database stores raw data extracted from each of the disparate source data systems. The integration layer integrates the disparate data sets by transforming the data from the staging layer often storing this transformed data in an operational data store (ODS) database. The integrated data are then moved to yet another database, often called the data warehouse database, where the data is arranged into hierarchical groups often called dimensions and into facts and aggregate facts. The combination of facts and dimensions is sometimes called a star schema. The access layer helps users retrieve data.
This definition of the data warehouse focuses on data storage. The main source of the data is cleaned, transformed, cataloged and made available for use by managers and other business professionals for data mining, online analytical processing, market research and decision support. However, the means to retrieve and analyze data, to extract, transform and load data, and to manage the data dictionary are also considered essential components of a data warehousing system. Many references to data warehousing use this broader context. Thus, an expanded definition for data warehousing includes business intelligence tools, tools to extract, transform and load data into the repository, and tools to manage and retrieve metadata.
Benefits.
A data warehouse maintains a copy of information from the source transaction systems. This architectural complexity provides the opportunity to :
Generic data warehouse environment.
The environment for data warehouses and marts includes the following:
In regards to source systems listed above, Rainer states, “A common source for the data in data warehouses is the company’s operational databases, which can be relational databases”.
Regarding data integration, Rainer states, “It is necessary to extract data from source systems, transform them, and load them into a data mart or warehouse”.
Rainer discusses storing data in an organization’s data warehouse or data marts.
Metadata are data about data. “IT personnel need information about data sources; database, table, and column names; refresh schedules; and data usage measures“.
Today, the most successful companies are those that can respond quickly and flexibly to market changes and opportunities. A key to this response is the effective and efficient use of data and information by analysts and managers. A “data warehouse” is a repository of historical data that are organized by subject to support decision makers in the organization. Once data are stored in a data mart or warehouse, they can be accessed.
History.
The concept of data warehousing dates back to the late 1980s when IBM researchers Barry Devlin and Paul Murphy developed the "business data warehouse". In essence, the data warehousing concept was intended to provide an architectural model for the flow of data from operational systems to decision support environments. The concept attempted to address the various problems associated with this flow, mainly the high costs associated with it. In the absence of a data warehousing architecture, an enormous amount of redundancy was required to support multiple decision support environments. In larger corporations it was typical for multiple decision support environments to operate independently. Though each environment served different users, they often required much of the same stored data. The process of gathering, cleaning and integrating data from various sources, usually from long-term existing operational systems (usually referred to as legacy systems), was typically in part replicated for each environment. Moreover, the operational systems were frequently reexamined as new decision support requirements emerged. Often new requirements necessitated gathering, cleaning and integrating new data from "data marts" that were tailored for ready access by users.
Key developments in early years of data warehousing were:
Information storage.
Facts.
A fact is a value or measurement, which represents a fact about the managed entity or system.
Facts as reported by the reporting entity are said to be at raw level. E.g. if a BTS (business transformation service) received 1,000 requests for traffic channel allocation, it allocates for 820 and rejects the remaining then it would report 3 facts or measurements to a management system:
Facts at raw level are further aggregated to higher levels in various dimensions to extract more service or business-relevant information out of it. These are called aggregates or summaries or aggregated facts.
E.g. if there are 3 BTSs in a city, then facts above can be aggregated from BTS to city level in network dimension.
E.g.
Dimensional vs. normalized approach for storage of data.
There are three or more leading approaches to storing data in a data warehouse — the most important approaches are the dimensional approach and the normalized approach.
The dimensional approach refers to Ralph Kimball’s approach in which it is stated that the data warehouse should be modeled using a Dimensional Model/star schema. The normalized approach, also called the 3NF model (Third Normal Form) refers to Bill Inmon's approach in which it is stated that the data warehouse should be modeled using an E-R model/normalized model.
In a dimensional approach, transaction data are partitioned into "facts", which are generally numeric transaction data, and "dimensions", which are the reference information that gives context to the facts. For example, a sales transaction can be broken up into facts such as the number of products ordered and the price paid for the products, and into dimensions such as order date, customer name, product number, order ship-to and bill-to locations, and salesperson responsible for receiving the order.
A key advantage of a dimensional approach is that the data warehouse is easier for the user to understand and to use. Also, the retrieval of data from the data warehouse tends to operate very quickly. Dimensional structures are easy to understand for business users, because the structure is divided into measurements/facts and context/dimensions. Facts are related to the organization’s business processes and operational system whereas the dimensions surrounding them contain context about the measurement (Kimball, Ralph 2008). Another advantage offered by dimensional model is that it does not involve a relational database every time. Thus,this type of modeling technique is very useful for end-user queries in data warehouse.
The main disadvantages of the dimensional approach are the following:
In the normalized approach, the data in the data warehouse are stored following, to a degree, database normalization rules. Tables are grouped together by "subject areas" that reflect general data categories (e.g., data on customers, products, finance, etc.). The normalized structure divides data into entities, which creates several tables in a relational database. When applied in large enterprises the result is dozens of tables that are linked together by a web of joins. Furthermore, each of the created entities is converted into separate physical tables when the database is implemented (Kimball, Ralph 2008).
The main advantage of this approach is that it is straightforward to add information into the database. Some disadvantages of this approach are that, because of the number of tables involved, it can be difficult for users to join data from different sources into meaningful information and to access the information without a precise understanding of the sources of data and of the data structure of the data warehouse.
Both normalized and dimensional models can be represented in entity-relationship diagrams as both contain joined relational tables. The difference between the two models is the degree of normalization (also known as Normal Forms). These approaches are not mutually exclusive, and there are other approaches. Dimensional approaches can involve normalizing data to a degree (Kimball, Ralph 2008).
In "Information-Driven Business", Robert Hillard proposes an approach to comparing the two approaches based on the information needs of the business problem. The technique shows that normalized models hold far more information than their dimensional equivalents (even when the same fields are used in both models) but this extra information comes at the cost of usability. The technique measures information quantity in terms of information entropy and usability in terms of the Small Worlds data transformation measure.
Design methodologies.
Bottom-up design.
In the "bottom-up" approach, data marts are first created to provide reporting and analytical capabilities for specific business processes. These data marts can then be integrated to create a comprehensive data warehouse. The data warehouse bus architecture is primarily an implementation of "the bus", a collection of conformed dimensions and conformed facts, which are dimensions that are shared (in a specific way) between facts in two or more data marts.
Top-down design.
The "top-down" approach is designed using a normalized enterprise data model. "Atomic" data, that is, data at the lowest level of detail, are stored in the data warehouse. Dimensional data marts containing data needed for specific business processes or specific departments are created from the data warehouse.
Hybrid design.
Data warehouses (DW) often resemble the hub and spokes architecture. Legacy systems feeding the warehouse often include customer relationship management and enterprise resource planning, generating large amounts of data. To consolidate these various data models, and facilitate the extract transform load process, data warehouses often make use of an operational data store, the information from which is parsed into the actual DW. To reduce data redundancy, larger systems often store the data in a normalized way. Data marts for specific reports can then be built on top of the DW.
The DW database in a hybrid solution is kept on third normal form to eliminate data redundancy. A normal relational database, however, is not efficient for business intelligence reports where dimensional modelling is prevalent. Small data marts can shop for data from the consolidated warehouse and use the filtered, specific data for the fact tables and dimensions required. The DW provides a single source of information from which the data marts can read, providing a wide range of business information. The hybrid architecture allows a DW to be replaced with a master data management solution where operational, not static information could reside.
The Data Vault Modeling components follow hub and spokes architecture. This modeling style is a hybrid design, consisting of the best practices from both third normal form and star schema. The Data Vault model is not a true third normal form, and breaks some of its rules, but it is a top-down architecture with a bottom up design. The Data Vault model is geared to be strictly a data warehouse. It is not geared to be end-user accessible, which when built, still requires the use of a data mart or star schema based release area for business purposes.
Data warehouses versus operational systems.
Operational systems are optimized for preservation of data integrity and speed of recording of business transactions through use of database normalization and an entity-relationship model. Operational system designers generally follow the Codd rules of database normalization in order to ensure data integrity. Codd defined five increasingly stringent rules of normalization. Fully normalized database designs (that is, those satisfying all five Codd rules) often result in information from a business transaction being stored in dozens to hundreds of tables. Relational databases are efficient at managing the relationships between these tables. The databases have very fast insert/update performance because only a small amount of data in those tables is affected each time a transaction is processed. Finally, in order to improve performance, older data are usually periodically purged from operational systems.
Data warehouses are optimized for analytic access patterns. Analytic access patterns generally involve selecting specific fields and rarely if ever 'select *' as is more common in operational databases. Because of these differences in access patterns, operational databases (loosely, OLTP) benefit from the use of a row-oriented DBMS whereas analytics databases (loosely, OLAP) benefit from the use of a column-oriented DBMS. Unlike operational systems which maintain a snapshot of the business, data warehouses generally maintain an infinite history which is implemented through ETL processes that periodically migrate data from the operational systems over to the data warehouse.
Evolution in organization use.
These terms refer to the level of sophistication of a data warehouse:

</doc>
<doc id="7991" url="https://en.wikipedia.org/wiki?curid=7991" title="Disperser">
Disperser

A disperser is a one-sided extractor. Where an extractor requires that every event gets the same probability under the uniform distribution and the extracted distribution, only the latter is required for a disperser. So for a disperser, an event formula_1 we have:
formula_2
Definition (Disperser): "A" formula_3"-disperser is a function"
formula_4
"such that for every distribution" formula_5 "on" formula_6 "with" formula_7 "the support of the distribution" formula_8 "is of size at least" formula_9.
Graph theory.
An ("N", "M", "D", "K", "e")-disperser is a bipartite graph with "N" vertices on the left side, each with degree "D", and "M" vertices on the right side, such that every subset of "K" vertices on the left side is connected to more than (1 − "e")"M" vertices on the right.
An extractor is a related type of graph that guarantees an even stronger property; every ("N", "M", "D", "K", "e")-extractor is also an ("N", "M", "D", "K", "e")-disperser.
Other meanings.
A disperser is a high-speed mixing device used to disperse or dissolve pigments and other solids into a liquid.

</doc>
<doc id="7992" url="https://en.wikipedia.org/wiki?curid=7992" title="Devonian">
Devonian

The Devonian is a geologic period and system of the Paleozoic Era spanning from the end of the Silurian Period, about Mya (million years ago), to the beginning of the Carboniferous Period, about . It is named after Devon, England, where rocks from this period were first studied.
The Devonian period experienced the first significant adaptive radiation of terrestrial life. Free-sporing vascular plants began to spread across dry land, forming extensive forests which covered the continents. By the middle of the Devonian, several groups of plants had evolved leaves and true roots, and by the end of the period the first seed-bearing plants appeared. Various terrestrial arthropods also became well-established. Fish reached substantial diversity during this time, leading the Devonian to often be dubbed the "Age of Fish". The first ray-finned and lobe-finned bony fish appeared, while the placoderms began dominating almost every known aquatic environment.
The ancestors of all tetrapods began adapting to walking on land, their strong pectoral and pelvic fins gradually evolving into legs. In the oceans, primitive sharks became more numerous than in the Silurian and the late Ordovician. The first ammonite mollusks appeared. Trilobites, the mollusk-like brachiopods and the great coral reefs, were still common. The Late Devonian extinction which started about 375 million years ago severely affected marine life, killing off all placoderms, and all trilobites, save for a few species of the order Proetida.
The paleogeography was dominated by the supercontinent of Gondwana to the south, the continent of Siberia to the north, and the early formation of the small continent of Euramerica in between.
History.
The period is named after Devon, a county in southwestern England, where a controversial argument in the 1830s over the age and structure of the rocks found distributed throughout the county was eventually resolved by the defining of the Devonian period in the geological timescale. The Great Devonian Controversy is a classic case of how the foundations of our present-day geological knowledge and classification of the rock record and geological timescale was socially as well as scientifically constructed. After a long period of vigorous argument and counter-argument between the main protagonists of Roderick Murchison with Adam Sedgwick against Henry de la Beche supported by George Bellas Greenough, Murchison and Sedgwick won the debate and named the period they proposed as the Devonian System.
While the rock beds that define the start and end of the Devonian period are well identified, the exact dates are uncertain. According to the International Commission on Stratigraphy (Ogg, 2004), the Devonian extends from the end of the Silurian Period Mya, to the beginning of the Carboniferous Period Mya (in North America, the beginning of the Mississippian subperiod of the Carboniferous).
In nineteenth-century texts the Devonian has been called the "Old Red Age", after the red and brown terrestrial deposits known in the United Kingdom as the Old Red Sandstone in which early fossil discoveries were found. Another common term is "Age of the Fishes", referring to the evolution of several major groups of fish that took place during the period. Older literature on the Anglo-Welsh basin divides it into the Downtonian, Dittonian, Breconian and Farlovian stages, the latter three of which are placed in the Devonian.
The Devonian has also erroneously been characterized as a "greenhouse age", due to sampling bias: most of the early Devonian-age discoveries came from the strata of western Europe and eastern North America, which at the time straddled the Equator as part of the supercontinent of Euramerica where fossil signatures of widespread reefs indicate tropical climates that were warm and moderately humid but in fact the climate in the Devonian differed greatly between epochs and geographic regions. For example, during the Early Devonian, arid conditions were prevalent through much of the world including Siberia, Australia, North America, and China, but Africa and South America had a warm temperate climate. In the Late Devonian, by contrast, arid conditions were less prevalent across the world and temperate climates were more common.
Subdivisions.
The Devonian Period is formally broken into Early, Middle and Late subdivisions. The rocks corresponding to these epochs are referred to as belonging to the Lower, Middle and Upper parts of the Devonian System.
The Early Devonian lasts from and begins with the Lochkovian stage, which lasts until the Pragian. This spans from , and is followed by the Emsian, which lasts until the Middle Devonian begins, .
The Middle Devonian comprises two subdivisions, the Eifelian giving way to the Givetian .
During this time the armoured jawless ostracoderm fish were declining in diversity; the jawed fish were thriving and increasing in diversity in both the oceans and freshwater. The shallow, warm, oxygen-depleted waters of Devonian inland lakes, surrounded by primitive plants, provided the environment necessary for certain early fish to develop essential characteristics such as well developed lungs, and the ability to crawl out of the water and onto the land for short periods of time.
Finally, the Late Devonian starts with the Frasnian, , during which the first forests were taking shape on land. The first tetrapods appear in the fossil record in the ensuing Famennian subdivision, the beginning and end of which are marked with extinction events. This lasted until the end of the Devonian, .
Climate.
The Devonian was a relatively warm period, and probably lacked any glaciers. The temperature gradient from the equator to the poles was not as large as it is today. The weather was also very arid, mostly along the equator where it was the driest. Reconstruction of tropical sea surface temperature from conodont apatite implies an average value of in the Early Devonian. levels dropped steeply throughout the Devonian period as the burial of the newly evolved forests drew carbon out of the atmosphere into sediments; this may be reflected by a Mid-Devonian cooling of around . The Late Devonian warmed to levels equivalent to the Early Devonian; while there is no corresponding increase in concentrations, continental weathering increases (as predicted by warmer temperatures); further, a range of evidence, such as plant distribution, points to Late Devonian warming. The climate would have affected the dominant organisms in reefs; microbes would have been the main reef-forming organisms in warm periods, with corals and stromatoporoid sponges taking the dominant role in cooler times. The warming at the end of the Devonian may even have contributed to the extinction of the stromatoporoids.
Paleogeography.
The Devonian period was a time of great tectonic activity, as Euramerica and Gondwana drew closer together.
The continent Euramerica (or Laurussia) was created in the early Devonian by the collision of Laurentia and Baltica, which rotated into the natural dry zone along the Tropic of Capricorn, which is formed as much in Paleozoic times as nowadays by the convergence of two great air-masses, the Hadley cell and the Ferrel cell. In these near-deserts, the Old Red Sandstone sedimentary beds formed, made red by the oxidized iron (hematite) characteristic of drought conditions.
Near the equator, the plate of Euramerica and Gondwana were starting to meet, beginning the early stages of assembling Pangaea. This activity further raised the northern Appalachian Mountains and formed the Caledonian Mountains in Great Britain and Scandinavia.
The west coast of Devonian North America, by contrast, was a passive margin with deep silty embayments, river deltas and estuaries, in today's Idaho and Nevada; an approaching volcanic island arc reached the steep slope of the continental shelf in Late Devonian times and began to uplift deep water deposits, a collision that was the prelude to the mountain-building episode of Mississippian times called the Antler orogeny.
Sea levels were high worldwide, and much of the land lay under shallow seas, where tropical reef organisms lived. The deep, enormous Panthalassa (the "universal ocean") covered the rest of the planet. Other minor oceans were Paleo-Tethys, Proto-Tethys, Rheic Ocean, and Ural Ocean (which was closed during the collision with Siberia and Baltica).
Biota.
Marine biota.
Sea levels in the Devonian were generally high. Marine faunas continued to be dominated by bryozoa, diverse and abundant brachiopods, the enigmatic hederelloids, microconchids and corals. Lily-like crinoids (animals, their resemblance to flowers notwithstanding) were abundant, and trilobites were still fairly common. Among vertebrates, jaw-less armored fish (ostracoderms) declined in diversity, while the jawed fish (gnathostomes) simultaneously increased in both the sea and fresh water. Armored placoderms were numerous during the lower stages of the Devonian Period and became extinct in the Late Devonian, perhaps because of competition for food against the other fish species. Early cartilaginous (Chondrichthyes) and bony fishes (Osteichthyes) also become diverse and played a large role within the Devonian seas. The first abundant genus of shark, "Cladoselache", appeared in the oceans during the Devonian Period. The great diversity of fish around at the time, have led to the Devonian being given the name "The Age of Fish" in popular culture.
The first ammonites also appeared during or slightly before the early Devonian Period around 400 Mya.
Reefs.
A now dry barrier reef, located in present-day Kimberley Basin of northwest Australia, once extended a thousand kilometers, fringing a Devonian continent. Reefs in general are built by various carbonate-secreting organisms that have the ability to erect wave-resistant frameworks close to sea level. The main contributors of the Devonian reefs were unlike modern reefs, which are constructed mainly by corals and calcareous algae. They were composed of calcareous algae and coral-like stromatoporoids, and tabulate and rugose corals, in that order of importance.
Terrestrial biota.
By the Devonian Period, life was well underway in its colonization of the land. The moss forests and bacterial and algal mats of the Silurian were joined early in the period by primitive rooted plants that created the first stable soils and harbored arthropods like mites, scorpions, trigonotarbids and myriapods (although arthropods appeared on land much earlier than in the Early Devonian and the existence of fossils such as "Climactichnites" suggest that land arthropods may have appeared as early as the Cambrian period). Also the first possible fossils of insects appeared around 416 Mya in the Early Devonian. The first tetrapods, evolving from lobe-finned fish, appeared in the coastal water no later than middle Devonian, and gave rise to the first Amphibians.
The greening of land.
Early Devonian plants did not have roots or leaves like the plants most common today and many had no vascular tissue at all. They probably spread largely by vegetative growth, and did not grow much more than a few centimeters tall. By far the largest land organism was "Prototaxites", the fruiting body of an enormous fungus that stood more than 8 meters tall, towering over the low, carpet-like vegetation. By the Middle Devonian, shrub-like forests of primitive plants existed: lycophytes, horsetails, ferns, and progymnosperms had evolved. Most of these plants had true roots and leaves, and many were quite tall. The earliest known trees, from the genus "Wattieza", appeared in the Late Devonian around 385 Ma. In the Late Devonian, the tree-like ancestral Progymnosperm "Archaeopteris" which had conifer-like true wood and fern-like foliage and the cladoxylopsids grew. (See also: lignin.) These are the oldest known trees of the world's first forests. By the end of the Devonian, the first seed-forming plants had appeared. This rapid appearance of so many plant groups and growth forms has been called the "Devonian Explosion".
The 'greening' of the continents acted as a carbon dioxide sink, and atmospheric levels of this greenhouse gas may have dropped. This may have cooled the climate and led to a massive extinction event. See Late Devonian extinction.
Animals and the first soils.
Primitive arthropods co-evolved with this diversified terrestrial vegetation structure. The evolving co-dependence of insects and seed-plants that characterizes a recognizably modern world had its genesis in the Late Devonian period. The development of soils and plant root systems probably led to changes in the speed and pattern of erosion and sediment deposition. The rapid evolution of a terrestrial ecosystem containing copious animals opened the way for the first vertebrates to seek out a terrestrial living. By the end of the Devonian, arthropods were solidly established on the land.
Late Devonian extinction.
A major extinction occurred at the beginning of the last phase of the Devonian period, the Famennian faunal stage (the Frasnian-Famennian boundary), about Mya, when all the fossil agnathan fishes, save for the psammosteid heterostracans, suddenly disappeared. A second strong pulse closed the Devonian period. The Late Devonian extinction was one of five major extinction events in the history of the Earth's biota, more drastic than the familiar extinction event that closed the Cretaceous.
The Devonian extinction crisis primarily affected the marine community, and selectively affected shallow warm-water organisms rather than cool-water organisms. The most important group to be affected by this extinction event were the reef-builders of the great Devonian reef-systems.
Amongst the severely affected marine groups were the brachiopods, trilobites, ammonites, conodonts, and acritarchs, as well as jawless fish, and all placoderms. Land plants as well as freshwater species, such as our tetrapod ancestors, were relatively unaffected by the Late Devonian extinction event.
The reasons for the Late Devonian extinctions are still unknown, and all explanations remain speculative. Canadian paleontologist Digby McLaren suggested in 1969 that the Devonian extinction events were caused by an asteroid impact. However, while there were Late Devonian collision events (see the Alamo bolide impact), little evidence supports the existence of a large enough Devonian crater.

</doc>
<doc id="7993" url="https://en.wikipedia.org/wiki?curid=7993" title="Dungeon Master (disambiguation)">
Dungeon Master (disambiguation)

Dungeon Master may mean:

</doc>
<doc id="7994" url="https://en.wikipedia.org/wiki?curid=7994" title="David Thompson (explorer)">
David Thompson (explorer)

David Thompson (30 April 1770 – 10 February 1857) was a British-Canadian fur trader, surveyor, and map-maker, known to some native peoples as "Koo-Koo-Sint" or "the Stargazer." Over his career he mapped over 3.9 million square kilometres of North America and for this has been described as the "greatest land geographer who ever lived."
Biography.
Thompson was born in Westminster to recent Welsh migrants, David and Ann Thompson. When Thompson was two, his father died and the financial hardship of this occurrence resulted in his and his brother's placement in the Grey Coat Hospital, a school for the disadvantaged of Westminster. He eventually graduated to the Grey Coat mathematical school and was introduced to basic navigation skills which would form the basis of his future career. In 1784, at the age of 14, he entered a seven-year apprenticeship with the Hudson's Bay Company. He set sail on 28 May of that year, and left England .
The Hudson's Bay Company (HBC).
He arrived in Churchill (now in Manitoba) and was put to work copying the personal papers of the governor of Fort Churchill, Samuel Hearne. The next year he was transferred to nearby York Factory, and over the next few years spent time as a clerk at Cumberland House, Saskatchewan and South Branch House before arriving at Manchester House in 1787. On 23 December 1788, Thompson seriously fractured his leg, forcing him to spend the next two winters at Cumberland House convalescing. It was during this time he greatly refined and expanded his mathematical, astronomical and surveying skills under the tutelage of Hudson's Bay Company surveyor Philip Turnor. It was also during this time that he lost sight in his right eye.
In 1790 with his apprenticeship nearing its end, Thompson made the unusual request of a set of surveying tools in place of the typical parting gift of fine clothes offered by the company to those completing their indenture. He received both. He then entered the employ of the Hudson's Bay Company as a fur trader and in 1792 completed his first significant survey, mapping a route to Lake Athabasca (where today's Alberta/Saskatchewan border is located). In recognition of his map-making skills, the company promoted him to surveyor in 1794. Thompson continued working for the Hudson's Bay Company until 23 May 1797 when, frustrated with the Hudson's Bay Company's policies over the use of alcohol in the fur trade, he left and walked 80 miles in the snow to enter the employ of the competition, the North West Company where he continued to work as a fur trader and surveyor.
North West Company.
Thompson's decision to defect to the North West Company in 1797 without providing the customary one-year notice was not well received by his former employers. However, joining the North West Company allowed Thompson to pursue his interest in surveying and work on mapping the interior of what was to become Canada. In 1797, Thompson was sent south by his employers to survey part of the Canada-US boundary along the water routes from Lake Superior to Lake of the Woods to satisfy unresolved questions of territory arising from the Jay Treaty between Great Britain and the United States. By 1798 Thompson had completed a survey of from Grand Portage, through Lake Winnipeg, to the headwaters of the Assiniboine and Mississippi Rivers, as well as two sides of Lake Superior. In 1798, the company sent him to Red Deer Lake (Lac La Biche in present-day Alberta) to establish a trading post. The English translation of Lac La Biche-Red Deer Lake-first appeared on the Mackenzie map of 1793. Thompson spent the next few seasons trading based in Fort George (now in Alberta), and during this time led several expeditions into the Rocky Mountains.
In 1804, at the annual meeting of the North West Company in Kaministiquia, Thompson was made a full partner of the company and spent the next few seasons based there managing the fur trading operations but still finding time to expand his surveys of the waterways around Lake Superior. However, a decision was made at the 1806 company meeting to send Thompson back out into the interior. Concern over the American-backed expedition of Lewis and Clark prompted the North West Company to charge Thompson with the task of finding a route to the Pacific to open up the lucrative trading territories of the Pacific Northwest.
Columbia travels.
After the general meeting in 1806, Thompson travelled to Rocky Mountain House and prepared for an expedition to follow the Columbia River to the Pacific. In June 1807 Thompson crossed the Rocky Mountains and spent the summer surveying the Columbia basin and continuing to survey the area over the next few seasons. Thompson mapped and established trading posts in Northwestern Montana, Idaho, Washington, and Western Canada. Trading posts he founded included Kootenae House, Kullyspell House and Saleesh House; the latter two of which were the first trading posts west of the Rockies in Idaho and Montana, respectively. These posts established by Thompson extended North West Company fur trading territory into the Columbia Basin drainage area. The maps he made of the Columbia River basin east of the Cascade Mountains were of such high quality and detail that they continued to be regarded as authoritative well into the mid-20th century. 
In early 1810, Thompson was returning eastward towards Montreal but while on route at Rainy Lake, received orders to return to the Rocky Mountains and establish a route to the mouth of the Columbia. This was a response by the North West Company to the plans of John Jacob Astor to send a ship around the Americas to establish a fur trading post. During his return, Thompson was delayed by an angry group of Peigan natives at Howse Pass which ultimately forced him to seek a new route across the Rocky Mountains through the Athabasca Pass.
David Thompson was the first European to navigate the full length of the Columbia River. During Thompson's 1811 voyage down the Columbia River he camped at the junction with the Snake River on 9 July 1811, and erected a pole and a notice claiming the country for Great Britain and stating the intention of the North West Company to build a trading post at the site. This notice was found later that year by Astorians looking to establish an inland fur post, contributing to their selection of a more northerly site at Fort Okanogan. The North West Company's Fort Nez Percés was established near the Snake River junction several years later. Continuing down the Columbia, Thompson passed the barrier of The Dalles with much less difficulty than experienced by Lewis and Clark, as high water obscured Celilo Falls and many of the rapids. On 14 July 1811, Thompson reached the partially constructed Fort Astoria at the mouth of the Columbia, arriving two months after the Pacific Fur Company's ship, the "Tonquin".
Before returning upriver and across the mountains, Thompson hired Naukane, a Native Hawaiian labourer brought to Fort Astoria by the Pacific Fur Company's ship "Tonquin". Naukane, known as Coxe to Thompson, accompanied Thompson across the continent to Lake Superior before journeying on to England.
Thompson wintered at Saleesh House before beginning his final journey back to Montreal in 1812.
In his published journals, Thompson recorded seeing large footprints near what is now Jasper, Alberta, in 1811. It has been suggested that these prints were similar to what has since been called the sasquatch. However, Thompson noted that these tracks showed "a small Nail at the end of each o", and stated that these tracks "very much resembles a large Bear's Track".
Appearance and personality.
In 1820, the English geologist, John Jeremiah Bigsby, attended a dinner party given by The Hon. William McGillivray at his home, Chateau St. Antoine, one of the early estates in Montreal's Golden Square Mile. He describes the party and some of the guests in his entertaining book "The Shoe and Canoe", giving an excellent description of David Thompson:
"I was well placed at table between one of the Miss McGillivray's and a singular-looking person of about fifty. He was plainly dressed, quiet, and observant. His figure was short and compact, and his black hair was worn long all round, and cut square, as if by one stroke of the shears, just above the eyebrows. His complexion was of the gardener's ruddy brown, while the expression of his deeply-furrowed features was friendly and intelligent, but his cut-short nose gave him an odd look. His speech betrayed the Welshman, although he left his native hills when very young. I might have been spared this description of Mr David Thompson by saying he greatly resembled Curran the Irish Orator..."
"I afterwards travelled much with him, and have now only to speak of him with great respect, or, I ought to say, with admiration... No living person possesses a tithe of his information respecting the Hudson's Bay countries... Never mind his Bunyan-like face and cropped hair; he has a very powerful mind, and a singular faculty of picture-making. He can create a wilderness and people it with warring savages, or climb the Rocky Mountains with you in a snow-storm, so clearly and palpably, that only shut your eyes and you hear the crack of the rifle, or feel the snow-flakes melt on your cheeks as he talks."
Marriage and children.
On 10 June 1799 at Île-à-la-Crosse, he married Charlotte Small, a thirteen-year-old Métis child of a Scottish fur trader Patrick Small and a Cree mother. Their marriage was formalised at the Scotch Presbyterian Church in Montreal on 30 October 1812. He and Charlotte had 13 children together; five of them were born before he left the fur trade. The family did not adjust easily to life in Eastern Canada and two of the children, John (aged 5) and Emma (aged 7) died of round worms, a common parasite. Their marriage lasted 58 years, the longest Canadian pre-Confederation marriage known.
Later years.
Upon his arrival back in Montreal, Thompson retired with a generous pension from the North West Company. He settled in nearby Terrebonne and worked on completing his great map, a summary of his lifetime of exploring and surveying the interior of North America. The map covered the wide area stretching from Lake Superior to the Pacific, and was given by Thompson to the North West Company. Thompson's 1814 map, his greatest achievement, was so accurate that 100 years later it was still the basis for many of the maps issued by the Canadian government. It now resides in the Archives of Ontario.
In 1815, Thompson moved his family to Williamstown, Upper Canada and a few years later was employed to survey the newly established borders with the United States from Lake of the Woods to the Eastern Townships of Quebec, established by Treaty of Ghent after the War of 1812. In 1843 Thompson completed his atlas of the region from Hudson Bay to the Pacific Ocean.
Afterwards, Thompson returned to a life as a land owner, but soon financial misfortune would ruin him. By 1831 he was so deeply in debt he was forced to take up a position as a surveyor for the British American Land Company to provide for his family. His luck continued to worsen and he was forced to move in with his daughter and son-in-law in 1845. He began work on a manuscript chronicling his life exploring the continent, but this project was left unfinished when his sight failed him completely in 1851.
Death and afterward.
The land mass mapped by Thompson amounted to 3.9 million square kilometres of wilderness (one-fifth of the continent). His contemporary, the great explorer Alexander Mackenzie, remarked that Thompson did more in ten months than he would have thought possible in two years.
Despite these significant achievements, Thompson died in Montreal in near obscurity on 10 February 1857, his accomplishments almost unrecognised. He never finished the book of his 28 years in the fur trade, based on his 77 field notebooks, before he died. In the 1890s geologist J.B. Tyrrell resurrected Thompson's notes and in 1916 published them as "David Thompson's Narrative", as part of the General Series of the Champlain Society. Further editions and re-examinations of Thompson's life and works were published in 1962 by Richard Glover, in 1971 by Victor Hopwood, and in 2015 by William Moreau.
Thompson's body was interred in Montreal's Mount Royal Cemetery in an unmarked grave. It was not until 1926 that efforts by J.B. Tyrell and the Canadian Historical Society resulted in the placing of a tombstone to mark his grave.
In 1957, one hundred years after his death, the Canadian government honoured him with his image on a Canadian postage stamp. The David Thompson Highway in Alberta was named in his honour, along with David Thompson High School situated on the side of the highway near Leslieville, Alberta. His prowess as a geographer is now well-recognized. He has been called "the greatest land geographer who ever lived."
There is a monument dedicated to David Thompson (maintained by the state of North Dakota) near the former town site of the ghost town, Verendrye, North Dakota, located approximately two miles north and one mile west of Karlsruhe, North Dakota. Thompson Falls, Montana and British Columbia's Thompson River are also named after the explorer.
The year 2007 marked the 150th year of Thompson's death and the 200th anniversary of his first crossing of the Rocky Mountains. Commemorative events and exhibits were planned across Canada and the United States from 2007 to 2011 as a celebration of his accomplishments.
Thompson was the subject of a 1964 National Film Board of Canada short film "David Thompson: The Great Mapmaker ", as well as the BBC2 programme "Ray Mears' Northern Wilderness" (Episode 5), broadcast in November 2009.

</doc>
<doc id="7995" url="https://en.wikipedia.org/wiki?curid=7995" title="Dioscoreales">
Dioscoreales

Dioscoreales (yam order) is a botanical name for an order of monocotyledon flowering plants in modern classification systems, such as the Angiosperm Phylogeny Group and the Angiosperm Phylogeny Web. Within the monocots Dioscoreales are grouped in the lilioid monocots where they are in a sister group relationship with the Pandanales. Of necessity the Dioscoreales contain the family Dioscoreaceae which includes the yam ("Dioscorea") that is used as an important food source in many regions around the globe. Older systems tended to place all lilioid monocots with reticulate veined leaves (such as Smilacaceae and Stemonaceae together with Dioscoraceae) in Dioscoreales. As currently circumscribed by phylogenetic analysis using combined morphology and molecular methods, Dioscreales contains many reticulate veined vines in Dioscoraceae, it also includes the myco-heterotrophic Burmanniaceae and the autotrophic Nartheciaceae. The order consists of three families, 21 genera and about 1,040 species.
Description.
Dioscoreales are vines or herbaceous forest floor plants. They may be achlorophyllous or saprophytic. Synapomorphies include tuberous roots, glandular hairs, seed coat characteristics and the presence of calcium oxalate crystals. Other characteristics of the order include the presence of saponin steroids, annular vascular bundles that are found in both the stem and leaf. The leaves are often unsheathed at the base, have a distinctive petiole and reticulate veined lamina. Alternatively they may be small and scale-like with a sheathed base. The flowers are actinomorphic, and may be bisexual or dioecious. The perianth may be conspicuous or reduced. Fruit is dry capsules or berries.
All of the species except the genera placed in Nartheciaceae express simultaneous microsporogenesis. Plants in Nartheciaceae show successive microsporogenesis which is one of the traits indicating that the family is sister to all the other members included in the order.
Taxonomy.
Earlier systems.
The Cronquist system, of 1981, did not recognise such an order, but placed most such plants in order Liliales in subclass Liliidae in class Liliopsida (monocotyledons) of division Magnoliophyta (angiosperms).
Under the Dahlgren system, Dioscoreales was placed in the superorder Lilianae in subclass Liliidae (monocotyledons) of class Magnoliopsida (angiosperms) and comprised the eight families Dioscoreaceae, Petermanniaceae, Ripogonaceae, Smilacaceae, Stemonaceae, Taccaceae, Trichopodaceae and Trilliaceae.
Angiosperm Phylogeny Group.
Under the Angiosperm Phylogeny Group system of 1998 (APG I), the order was placed in the monocot clade and comprised the five families Burmanniaceae, Dioscoreaceae, Taccaceae, Thismiaceae and Trichopodaceae
In APG II (2003), a number of changes were made to Dioscoreales, as a result of am extensive study by Caddick and colleagues (2000), using an analysis of three genes, "rbc"L, "atp"B and 18S rDNA, in addition to morphology. These studies resulted in a re-examination of the relationships between most of the genera within the order. Thismiaceae was shown to be a sister group to Burmanniaceae, and so was included in it. The monotypic families Taccaceae and Trichopodaceae were included in Dioscoreaceae, while Nartheciaceae could also be grouped within Dioscoreales. APG III (2009) did not change this, so the order now comprises three families Burmanniaceae, Dioscoreaceae and Nartheciaceae.
Evolution.
The data for the evolution of the order is collected from molecular analyses since there are no such fossils found. It is estimated that Dioscoreales and its sister clade Pandanales split up around 121 millions of years ago during Early Cretaceous when the stem group was formed. Then it took 3 to 6 millions of years for the crown group to differentiate in Mid Cretaceous.
Distribution and habitat.
Species from this order are distributed across all of the continents except Antarctica. They are mainly tropical or subtropical representatives but however there are members of Dioscoreaceae and Nartheciaceae families found in cooler regions of Europe and North America. Order Dioscoreales contains plants that are able to form an underground organ for reservation of nutritions as many other monocots. An exception is the family Burmanniaceae which is entirely myco-heterotrophic and contains species that lack photosynthetic abilities.
Ecology.
The three families included in order Dioscoreales also represent three different ecological groups of plants. Dioscoreaceae contains mainly vines ("Dioscorea") and other crawling species ("Epipetrum"). Nartheciaceae on the other hand is a family composed of herbeceous plants with a rather lily-like appearance ("Aletris") while Burmanniaceae is entirely myco-heterotrophic group.
Uses.
Many members of Dioscoreaceae produce tuberous starchy roots (yams) which form staple foods in tropical regions. They have also been the source of steroids for the pharmaceutical industry, including the production of oral contraceptives.

</doc>
<doc id="8000" url="https://en.wikipedia.org/wiki?curid=8000" title="Default">
Default

Default may refer to:

</doc>
<doc id="8002" url="https://en.wikipedia.org/wiki?curid=8002" title="Deposition">
Deposition

Deposition may refer to:
Deposition, in science, may refer to:
The Deposition can also refer to depictions of Christ's descent from the cross, as in:
The Deposition can also refer to:

</doc>
<doc id="8005" url="https://en.wikipedia.org/wiki?curid=8005" title="Dentistry">
Dentistry

Dentistry is a branch of medicine that is involved in the study, diagnosis, prevention, and treatment of diseases, disorders and conditions of the oral cavity, commonly in the dentition but also the oral mucosa, and of adjacent and related structures and tissues, particularly in the maxillofacial (jaw and facial) area. Although primarily associated with teeth among the general public, the field of dentistry or dental medicine is not limited to teeth but includes other aspects of the craniofacial complex including the temperomandibular and other supporting structures. The term dentistry comes from odontology (from Ancient Greek ὀδούς (odoús, "tooth")) – the study of the structure, development, and abnormalities of the teeth. Because of their substantial overlap in concept, dentistry is often also understood to subsume the now largely defunct medical specialty of stomatology (the study of the mouth and its disorders and diseases) for which reason the two terms are used interchangeably in certain regions.
Dentistry is widely considered important for overall health. Dental treatment is carried out by the dental team, which often consists of a dentist and dental auxiliaries (dental assistants, dental hygienists, dental technicians, and dental therapists). Most dentists work in private practices (primary care), although some work in dental hospitals and hospitals (secondary care) and institutions (prisons, armed forces bases, etc.).
The history of dentistry is almost as ancient as the history of humanity and civilization with the earliest evidence dating from 7000 BC. Remains from the early Harappan periods of the Indus Valley Civilization (c. 3300 BC) show evidence of teeth having been drilled dating back 9,000 years. It is thought that dental surgery was the first specialization from medicine.
Dental treatment.
Dentistry usually encompasses very important practices related to the oral cavity. Oral diseases are major public health problems due to their high incidence and prevalence across the globe with the disadvantaged affected more than other socio-economic groups.
The majority of dental treatments are carried out to prevent or treat the two most common oral diseases which are dental caries (tooth decay) and periodontal disease (gum disease or pyorrhea). Common treatments involve the restoration of teeth, extraction or surgical removal of teeth, scaling and root planing and endodontic root canal treatment.
All dentists in the United States undergo at least three years of undergraduate studies, but nearly all complete a bachelor's degree. This schooling is followed by four years of dental school to qualify as a "Doctor of Dental Surgery" (DDS) or "Doctor of Dental Medicine" (DMD). Dentists need to complete additional qualifications or continuing education to carry out more complex treatments such as sedation, oral and maxillofacial surgery, and dental implants.
By nature of their general training they can carry out the majority of dental treatments such as restorative (fillings, crowns, bridges), prosthetic (dentures), endodontic (root canal) therapy, periodontal (gum) therapy, and extraction of teeth, as well as performing examinations, radiographs (x-rays) and diagnosis. Dentists can also prescribe medications such as antibiotics, sedatives, and any other drugs used in patient management.
Dentists also encourage prevention of oral diseases through proper hygiene and regular, twice yearly, checkups for professional cleaning and evaluation. Conditions in the oral cavity may be indicative of systemic diseases such as osteoporosis, diabetes, or cancer. Many studies have also shown that gum disease is associated with an increased risk of diabetes, heart disease, and preterm birth. The concept that oral health can have an impact on systemic health and disease is referred to as "oral-systemic health".
Education and licensing.
Dr. John M. Harris started the world's first dental school in Bainbridge, Ohio, and helped to establish dentistry as a health profession. It opened on 21 February 1828, and today is a dental museum. The first dental college, Baltimore College of Dental Surgery, opened in Baltimore, Maryland, USA in 1840. The second in the United States was the Philadelphia College of Dental Surgery, established in 1852. In 1907 Temple University accepted a bid to incorporate the school.
Studies showed that dentists graduated from different countries, or even from different dental schools in one country, may have different clinical decisions for the same clinical condition. For example, dentists graduated from Israeli dental schools may recommend more often for the removal of asymptomatic impacted third molar (wisdom teeth) than dentists graduated from Latin American or Eastern European dental schools.
In the United Kingdom, the 1878 British Dentists Act and 1879 Dentists Register limited the title of "dentist" and "dental surgeon" to qualified and registered practitioners. However, others could legally describe themselves as "dental experts" or "dental consultants". The practice of dentistry in the United Kingdom became fully regulated with the 1921 Dentists Act, which required the registration of anyone practising dentistry. The British Dental Association, formed in 1880 with Sir John Tomes as president, played a major role in prosecuting dentists practising illegally.
In Korea, Taiwan, Japan, Finland, Sweden, Brazil, Chile, the United States, and Canada, a dentist is a healthcare professional qualified to practice dentistry after graduating with a degree of either Doctor of Dental Surgery (DDS) or Doctor of Dental Medicine (DMD). This is equivalent to the Bachelor of Dental Surgery/Baccalaureus Dentalis Chirurgiae (BDS, BDent, BChD, BDSc) that is awarded in the UK and British Commonwealth countries. In most western countries, to become a qualified dentist one must usually complete at least four years of postgraduate study; within the European Union the education has to be at least five years. Dentists usually complete between five and eight years of post-secondary education before practising. Though not mandatory, many dentists choose to complete an internship or residency focusing on specific aspects of dental care after they have received their dental degree.
Specialties.
Some dentists undertake further training after their initial degree in order to specialize. Exactly which subjects are recognized by dental registration bodies varies according to location. Examples include:
History.
Tooth decay was low in pre-agricultural societies, the growth in farming society about 10,000 years ago correlated with an increase with the rate of cavities. An infected tooth from Italy partially cleaned with flint tools, between 13,820 and 14,160 years old, represents the oldest known dentistry. The Indus Valley Civilization (IVC) has yielded evidence of dentistry being practised as far back as 7000 BC. An IVC site in Mehrgarh indicates that this form of dentistry involved curing tooth related disorders with bow drills operated, perhaps, by skilled bead crafters. The reconstruction of this ancient form of dentistry showed that the methods used were reliable and effective. The earliest dental filling, made of beeswax, was discovered in Slovenia and dates from 6500 years ago.
A Sumerian text from 5000 BC describes a "tooth worm" as the cause of dental caries. Evidence of this belief has also been found in ancient India, Egypt, Japan, and China. The legend of the worm is also found in the writings of Homer, and as late as the 14th century AD the surgeon Guy de Chauliac still promoted the belief that worms cause tooth decay.
Recipes for the treatment of toothache, infections and loose teeth are spread throughout the Ebers Papyrus, Kahun Papyri, Brugsch Papyrus, and Hearst papyrus of Ancient Egypt. The Edwin Smith Papyrus, written in the 17th century BC but which may reflect previous manuscripts from as early as 3000 BC, discusses the treatment of dislocated or fractured jaws. In the 18th century BC, the Code of Hammurabi referenced dental extraction twice as it related to punishment. Examination of the remains of some ancient Egyptians and Greco-Romans reveals early attempts at dental prosthetics. However, it is possible the prosthetics were prepared after death for aesthetic reasons.
Ancient Greek scholars Hippocrates and Aristotle wrote about dentistry, including the eruption pattern of teeth, treating decayed teeth and gum disease, extracting teeth with forceps, and using wires to stabilize loose teeth and fractured jaws. Some say the first use of dental appliances or bridges comes from the Etruscans from as early as 700 BC. In ancient Egypt, Hesi-Re is the first named "dentist" (greatest of the teeth). The Egyptians bound replacement teeth together with gold wire. Roman medical writer Cornelius Celsus wrote extensively of oral diseases as well as dental treatments such as narcotic-containing emollients and astringents. The earliest dental amalgams were first documented in a Tang Dynasty medical text written by the Chinese physician Su Kung in 659, and appeared in Germany in 1528.
Historically, dental extractions have been used to treat a variety of illnesses. During the Middle Ages and throughout the 19th century, dentistry was not a profession in itself, and often dental procedures were performed by barbers or general physicians. Barbers usually limited their practice to extracting teeth which alleviated pain and associated chronic tooth infection. Instruments used for dental extractions date back several centuries. In the 14th century, Guy de Chauliac invented the dental pelican (resembling a pelican's beak) which was used to perform dental extractions up until the late 18th century. The pelican was replaced by the dental key which, in turn, was replaced by modern forceps in the 20th century.
The first book focused solely on dentistry was the "Artzney Buchlein" in 1530, and the first dental textbook written in English was called "Operator for the Teeth" by Charles Allen in 1685.
In the United Kingdom there was no formal qualification for the providers of dental treatment until 1859 and it was only in 1921 that the practice of dentistry was limited to those who were professionally qualified. The Royal Commission on the National Health Service in 1979 reported that there were then more than twice as many registered dentists per 10,000 population in the UK than there were in 1921.
Modern dentistry.
It was between 1650 and 1800 that the science of modern dentistry developed. The English physician Thomas Browne in his "A Letter to a Friend" (circa 1656 pub. 1690) made an early dental observation with characteristic humour
The French surgeon Pierre Fauchard became known as the "father of modern dentistry". Despite the limitations of the primitive surgical instruments during the late 17th and early 18th century, Fauchard was a highly skilled surgeon who made remarkable improvisations of dental instruments, often adapting tools from watch makers, jewelers and even barbers, that he thought could be used in dentistry. He introduced dental fillings as treatment for dental cavities. He asserted that sugar derivate acids like tartaric acid were responsible for dental decay, and also suggested that tumors surrounding the teeth and in the gums could appear in the later stages of tooth decay.
Fauchard was the pioneer of dental prosthesis, and he discovered many methods to replace lost teeth. He suggested that substitutes could be made from carved blocks of ivory or bone. He also introduced dental braces, although they were initially made of gold, he discovered that the teeth position could be corrected as the teeth would follow the pattern of the wires. Waxed linen or silk threads were usually employed to fasten the braces. His contributions to the world of dental science consist primarily of his 1728 publication Le chirurgien dentiste or The Surgeon Dentist. The French text included "basic oral anatomy and function, dental construction, and various operative and restorative techniques, and effectively separated dentistry from the wider category of surgery".
After Fauchard, the study of dentistry rapidly expanded. Two important books, "Natural History of Human Teeth" (1771 ) and "Practical Treatise on the Diseases of the Teeth" (1778), were published by British surgeon John Hunter. In 1763 he entered into a period of collaboration with the London-based dentist James Spence. He began to theorise about the possibility of tooth transplants from one person to another. He realised that the chances of an (initially, at least) successful tooth transplant would be improved if the donor tooth was as fresh as possible and was matched for size with the recipient. These principles are still used in the transplantation of internal organs. Hunter conducted a series of pioneering operations, in which he attempted a tooth transplant. Although the donated teeth never properly bonded with the recipients' gums, one of Hunter's patients stated that he had three which lasted for six years, a remarkable achievement for the period.
Major advances were made in the 19th century, and dentistry evolved from a trade to a profession. The profession came under government regulation by the end of the 19th century. In the UK the Dentist Act was passed in 1878 and the British Dental Association formed in 1879. In the same year, Francis Brodie Imlach was the first ever dentist to be elected President of the Royal College of Surgeons (Edinburgh), raising dentistry onto a par with clinical surgery for the first time.
Priority patients.
UK NHS priority patients include patients with congenital abnormalities (such as cleft palates and hypodontia), patients who have suffered orofacial trauma and those being treated for cancer in the head and neck region. These are treated in a multidisciplinary team approach with other hospital based dental specialities orthodontics and maxillofacial surgery. Other priority patients include those with infections (either third molars or necrotic teeth) or avulsed permanent teeth, as well as patients with a history of smoking or smokeless tobacco with ulcers in the oral cavity.

</doc>
<doc id="8007" url="https://en.wikipedia.org/wiki?curid=8007" title="Diameter">
Diameter

In geometry, a diameter of a circle is any straight line segment that passes through the center of the circle and whose endpoints lie on the circle. It can also be defined as the longest chord of the circle. Both definitions are also valid for the diameter of a sphere. The word "diameter" is derived from Greek "διάμετρος" ("diametros"), "diameter of a circle", from "δια-" ("dia-"), "across, through" + "μέτρον" ("metron"), "measure". It is often abbreviated DIA, dia, d, or ⌀.
In more modern usage, the length of a diameter is also called the diameter. In this sense one speaks of "the" diameter rather than "a" diameter (which refers to the line itself), because all diameters of a circle or sphere have the same length, this being twice the radius r.
For a convex shape in the plane, the diameter is defined to be the largest distance that can be formed between two opposite parallel lines tangent to its boundary, and the "width" is defined to be the smallest such distance. Both quantities can be calculated efficiently using rotating calipers. For a curve of constant width such as the Reuleaux triangle, the width and diameter are the same because all such pairs of parallel tangent lines have the same distance.
For an ellipse, the standard terminology is different. A diameter of an ellipse is any chord passing through the midpoint of the ellipse. For example, conjugate diameters have the property that a tangent line to the ellipse at the endpoint of one of them is parallel to the other one. The longest diameter is called the major axis.
Generalizations.
The definitions given above are only valid for circles, spheres and convex shapes. However, they are special cases of a more general definition that is valid for any kind of "n"-dimensional convex or non-convex object, such as a hypercube or a set of scattered points. The diameter of a subset of a metric space is the least upper bound of the set of all distances between pairs of points in the subset. So, if "A" is the subset, the diameter is
If the distance function d is viewed here as having codomain R (the set of all real numbers), this implies that the diameter of the empty set (the case ) equals −∞ (negative infinity). Some authors prefer to treat the empty set as a special case, assigning it a diameter equal to 0, which corresponds to taking the codomain of d to be the set of nonnegative reals.
For any solid object or set of scattered points in n-dimensional Euclidean space, the diameter of the object or set is the same as the diameter of its convex hull. In medical parlance concerning a lesion or in geology concerning a rock, the diameter of an object is the supremum of the set of all distances between pairs of points in the object.
In differential geometry, the diameter is an important global Riemannian invariant.
In plane geometry, a diameter of a conic section is typically defined as any chord which passes through the conic's centre; such diameters are not necessarily of uniform length, except in the case of the circle, which has eccentricity "e" = 0.
Diameter symbol.
The symbol or variable for diameter, , is similar in size and design to ø, the Latin small letter o with stroke. In Unicode it is defined as . On an Apple Macintosh, the diameter symbol can be entered via the character palette (this is opened by pressing in most applications), where it can be found in the Technical Symbols category.
The character will sometimes not display correctly, however, since many fonts do not include it. In many situations the letter ø is an acceptable substitute, which in Unicode is . It can be obtained in UNIX-like operating systems using a Compose key by pressing, in sequence, and on a Macintosh by pressing (in both cases, that is the letter o, not the number 0).
In LaTeX the diameter symbol can be obtained with the command \diameter from the wasysym package.
The diameter symbol is distinct from the empty set symbol , from an (italic) uppercase phi "Φ", and from the Nordic vowel Ø.

</doc>
<doc id="8008" url="https://en.wikipedia.org/wiki?curid=8008" title="Direct examination">
Direct examination

The direct examination or examination-in-chief is one stage in the process of adducing evidence from witnesses in a court of law. Direct examination is the questioning of a witness by the party who called him or her, in a trial. Direct examination is usually performed to elicit evidence in support of facts which will satisfy a required element of a party's claim or defense.
In direct examination, one is generally prohibited from asking leading questions. This prevents a lawyer from feeding answers to a favorable witness. An exception to this rule occurs if one side has called a witness, but it is either understood, or becomes clear, that the witness is hostile to the questioner's side of the controversy. The lawyer may then ask the court to declare the person he or she has called to the stand a hostile witness. If the court does so, the lawyer may thereafter ask witness leading questions during direct examination. 
The techniques of direct examination are taught in courses on trial advocacy. Each direct examination is integrated with the overall case strategy through either a theme and theory or, with more advanced strategies, a line of effort.

</doc>
<doc id="8011" url="https://en.wikipedia.org/wiki?curid=8011" title="Alcohol intoxication">
Alcohol intoxication

Alcohol intoxication (also known as drunkenness or inebriation) is a physiological state (that may also include psychological alterations of consciousness) induced by the ingestion of ethanol (alcohol).
Alcohol intoxication is the result of alcohol entering the bloodstream faster than it can be metabolized by the liver, which breaks down the ethanol into non-intoxicating byproducts. Some effects of alcohol intoxication (such as euphoria and lowered social inhibitions) are central to alcohol's desirability as a beverage and its history as one of the world's most widespread recreational drugs. Despite this widespread use and alcohol's legality in most countries, many medical sources tend to describe any level of alcohol intoxication as a form of poisoning due to ethanol's damaging effects on the body in large doses; some religions consider alcohol intoxication to be a sin while others utilize it in sacrament.
Symptoms of alcohol intoxication include euphoria, flushed skin and decreased social inhibition at lower doses, with larger doses producing progressively severe impairments of balance, muscle coordination (ataxia), and decision-making ability (potentially leading to violent or erratic behavior) as well as nausea or vomiting from alcohol's disruptive effect on the semicircular canals of the inner ear and chemical irritation of the gastric mucosa. Sufficiently high levels of blood-borne alcohol will cause coma and death from the depressive effects of alcohol upon the central nervous system.
Pathophysiology.
Alcohol is metabolized by a normal liver at the rate of about of spirits (roughly a typical drink-size serving of beer, wine, or spirits) every 90 minutes. An "abnormal" liver with conditions such as hepatitis, cirrhosis, gall bladder disease, and cancer are likely to result in a slower rate of metabolism.
Ethanol is metabolised to acetaldehyde by alcohol dehydrogenase (ADH), which is found in many tissues, including the gastric mucosa. Acetaldehyde is metabolised to acetate by acetaldehyde dehydrogenase (ALDH), which is found predominantly in liver mitochondria. Acetate is used by the muscle cells to produce acetyl-CoA using the enzyme acetyl-CoA synthetase, and the acetyl-CoA is then used in the citric acid cycle. It takes roughly 90 minutes for a healthy liver to metabolize a , approximately one hour per standard unit.
Ethanol's acute effects are due largely to its nature as a central nervous system depressant, and are dependent on blood alcohol concentrations:
As drinking increases, people become sleepy, or fall into a stupor. After a very high level of consumption, the respiratory system becomes depressed and the person will stop breathing. Comatose patients may aspirate their vomit (resulting in vomitus in the lungs, which may cause "drowning" and later pneumonia if survived). CNS depression and impaired motor co-ordination along with poor judgement increases the likelihood of accidental injury occurring. It is estimated that about one third of alcohol-related deaths are due to accidents (32%), and another 14% are from intentional injury.
In addition to respiratory failure and accidents caused by effects on the central nervous system, alcohol causes significant metabolic derangements. Hypoglycaemia occurs due to ethanol's inhibition of gluconeogenesis, especially in children, and may cause lactic acidosis, ketoacidosis, and acute renal failure. Metabolic acidosis is compounded by respiratory failure. Patients may also present with hypothermia.
Pharmacology.
In the past, alcohol was believed to be a non-specific pharmacological agent affecting many neurotransmitter systems in the brain. However, molecular pharmacology studies have shown that alcohol has only a few primary targets. In some systems, these effects are facilitatory and in others inhibitory.
Among the neurotransmitter systems with enhanced functions are: GABA, 5-HT receptor agonism (responsible for GABAergic (GABA receptor PAM), glycinergic, and cholinergic effects), nicotinic acetylcholine receptors.
Among those that are inhibited are: NMDA, dihydropyridine-sensitive L-type Ca2+ channels and G-protein-activated inwardly rectifying K+ channels.
The result of these direct effects is a wave of further indirect effects involving a variety of other neurotransmitter and neuropeptide systems, leading finally to the behavioural or symptomatic effects of alcohol intoxication.
GABA receptors.
Many of the effects of activating GABA receptors have the same effects as that of ethanol consumption. Some of these effects include anxiolytic, anticonvulsant, sedative and hypnotic effects, cognitive impairment, and motor incoordination. This correlation between activating GABA receptors and the effects of ethanol consumption has led to the study of ethanol and its effects on GABA receptors. It has been shown that ethanol does in fact exhibit positive allosteric binding properties to GABA receptors. However, binding is only limited to pentamers containing the δ-subunit rather than the γ-subunit. GABA receptors containing the δ-subunit have been shown to be located exterior to the synapse and are involved with tonic inhibition rather than its γ-subunit counterpart, which is involved in phasic inhibition. The δ-subunit has been shown to be able to form the allosteric binding site which makes GABA receptors containing the δ-subunit more sensitive to ethanol concentrations, even to moderate social ethanol consumption levels (30mM). While it has been shown by Santhakumar et al. that GABA receptors containing the δ-subunit are sensitive to ethanol modulation, depending on subunit combinations receptors, could be more or less sensitive to ethanol. It has been shown that GABA receptors that contain both δ and β3-subunits display increased sensitivity to ethanol. One such receptor that exhibits ethanol insensitivity is α3-β6-δ GABA. It has also been shown that subunit combination is not the only thing that contributes to ethanol sensitivity. Location of GABA receptors within the synapse may also contribute to ethanol sensitivity.
Acute alcohol poisoning.
"Acute alcohol poisoning" is a related medical term used to indicate a dangerously high concentration of alcohol in the blood, high enough to induce coma, respiratory depression, or even death. It is considered a medical emergency. The term is mostly used by healthcare providers. Toxicologists use the term "alcohol intoxication" to discriminate between alcohol and other toxins.
The signs and symptoms of acute alcohol poisoning include:
Diagnosis.
Definitive diagnosis relies on a blood test for alcohol, usually performed as part of a toxicology screen.
Law enforcement officers often use breathalyzer units and field sobriety tests as more convenient and rapid alternatives to blood tests.
There are also various models of breathalyzer units that are available for consumer use. Because these may have varying reliability and may produce different results than the tests used for law-enforcement purposes, the results from such devices should be conservatively interpreted.
Many informal intoxication tests exist, which, in general, are unreliable and not recommended as deterrents to excessive intoxication or as indicators of the safety of activities such as motor vehicle driving, heavy equipment operation, machine tool use, etc.
For determining whether someone is intoxicated by alcohol by some means other than a blood-alcohol test, it is necessary to rule out other conditions such as hypoglycemia, stroke, usage of other intoxicants, mental health issues, and so on. It is best if his/her behavior has been observed while the subject is sober to establish a baseline. Several well-known criteria can be used to establish a probable diagnosis. For a physician in the acute-treatment setting, acute alcohol intoxication can mimic other acute neurological disorders, or is frequently combined with other recreational drugs that complicate diagnosis and treatment.
Management.
Acute alcohol poisoning is a medical emergency due to the risk of death from respiratory depression and/or inhalation of vomit if emesis occurs while the patient is unconscious and unresponsive. Emergency treatment for acute alcohol poisoning strives to stabilize the patient and maintain a patent airway and respiration, while waiting for the alcohol to metabolize. This can be done by removal of any vomitus or, if patient is unconscious or has impaired gag reflex, intubation of the trachea using an endotracheal tube to maintain adequate airway:
Also:
Additional medication may be indicated for treatment of nausea, tremor, and anxiety.
Prognosis.
A normal liver detoxifies the blood of alcohol over a period of time that depends on the initial level and the patient's overall physical condition. An abnormal liver will take longer but still succeeds, provided the alcohol does not cause liver failure.
People having drunk heavily for several days or weeks may have withdrawal symptoms after the acute intoxication has subsided.
A person consuming a dangerous amount of alcohol persistently can develop memory blackouts and idiosyncratic intoxication or pathological drunkenness symptoms.
Long-term persistent consumption of excessive amounts of alcohol can cause liver damage and have other deleterious health effects.
Society and culture.
Alcohol intoxication is a risk factor in some cases of catastrophic injury, in particular for unsupervised recreational activity. A study in the province of Ontario based on epidemiological data from 1986, 1989, 1992, and 1995 states that 79.2% of the 2,154 catastrophic injuries recorded for the study were preventable, of which 346 involved alcohol consumption. The activities most commonly associated with alcohol-related catastrophic injury were snowmobiling (124), fishing (41), diving (40), boating (31) and canoeing (7), swimming (31), riding an all-terrain vehicle (24), and cycling (23). These events are often associated with unsupervised young males, often inexperienced in the activity, and many result in drowning.
Legal issues.
Laws on drunkenness vary. In the United States, it is a criminal offence for a person to be drunk while driving a motorized vehicle, except in Wisconsin, where it is only a fine for the first offence. It is also a criminal offence to fly an aircraft or (in some American states) to assemble or operate an amusement park ride while drunk. Similar laws also exist in the United Kingdom and most other countries.
In some countries, it is also an offence to serve alcohol to an already-intoxicated person, and, often, alcohol can be sold only by persons qualified to serve responsibly through alcohol server training.
The (BAC) for legal operation of a vehicle is typically measured as a percentage of a unit volume of blood. This percentage ranges from 0.00% in Romania and the United Arab Emirates; to 0.05% in Australia, South Africa, Germany and Scotland; to 0.08% in England and Wales, the United States, Canada, and New Zealand.
The United States Federal Aviation Administration prohibits crew members from performing their duties with a BAC greater than 0.04% within eight hours of consuming an alcoholic beverage, or while under the influence of alcohol.
In the United States, the United Kingdom, and Australia, people are arrested for public intoxication, called "being drunk and disorderly" or "being drunk and incapable."
In some countries, there are special facilities, sometimes known as "drunk tanks", for the temporary detention of persons found to be drunk.
Religious views.
Some religious groups permit the consumption of alcohol. Some permit consumption but prohibit intoxication, while others prohibit alcohol consumption altogether. Most Christian denominations such as Catholic and Orthodox use wine as apart of the Eucharist and permit the drinking of alcohol but consider it sinful to become intoxicated. 
In the Qur'an, there is a prohibition on the consumption of grape-based alcoholic beverages, and intoxication is considered as an abomination in the Hadith. Islamic schools of law (Madh'hab) have interpreted this as a strict prohibition of the consumption of all types of alcohol and declared it to be haraam ("forbidden"), although other uses may be permitted.
Some Protestant Christian denominations prohibit the drinking of alcohol based upon Biblical passages that condemn drunkenness (such as Proverbs 23:21, Isaiah 28:1, Habakkuk 2:15), but others allow moderate use of alcohol. While Proverbs 31:4, warns against kings and rulers drinking wine and strong drink, Proverbs 31:6–7 promotes giving strong drink to the perishing and wine to those whose lives are bitter, to forget their poverty and troubles. In some Christian groups, a small amount of wine is part of the rite of communion. In The Church of Jesus Christ of Latter-day Saints, alcohol consumption is forbidden, and teetotalism has become a distinguishing feature of its members. Jehovah's Witnesses allow moderate alcohol consumption among its members.
In Buddhism, in general, the consumption of intoxicants is discouraged for both monastics and lay followers. Many followers of Buddhism observe a code of conduct known as the Five Precepts, of which the fifth precept is an undertaking to refrain from the consumption of intoxicating substances (except for medical reasons). In the Bodhisattva Vows of the "Brahma Net Sutra", observed by some monastic communities and some lay followers, distribution of intoxicants is likewise discouraged as well as consumption.
In the branch of Hinduism known as Gaudiya Vaishnavism, one of the four regulative principles forbids the taking of intoxicants, including alcohol.
In Judaism, the Babylonian Talmud says in Megillah 7b that "Rava said: A person is obligated to become intoxicated on Purim until he is unaware of the difference between 'Cursed be Haman' and 'Blessed be Mordechai.'" This is taken to mean that on the Jewish festival of Purim one is commanded to drink alcohol to the point of intoxication. During all other times of year, though, Judaism stresses moderation—not cutting out alcohol entirely, but not getting too drunk either.

</doc>
<doc id="8013" url="https://en.wikipedia.org/wiki?curid=8013" title="Data compression">
Data compression

In signal processing, data compression, source coding,
or bit-rate reduction involves encoding information using fewer bits than the original representation. Compression can be either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by identifying unnecessary information and removing it.
The process of reducing the size of a data file is referred to as data compression. In the context of data transmission, it is called source coding (encoding done at the source of the data before it is stored or transmitted) in opposition to channel coding.
Compression is useful because it helps reduce resource usage, such as data storage space or transmission capacity. Because compressed data must be decompressed to use, this extra processing imposes computational or other costs through decompression; this situation is far from being a free lunch. Data compression is subject to a space–time complexity trade-off. For instance, a compression scheme for video may require expensive hardware for the video to be decompressed fast enough to be viewed as it is being decompressed, and the option to decompress the video in full before watching it may be inconvenient or require additional storage. The design of data compression schemes involves trade-offs among various factors, including the degree of compression, the amount of distortion introduced (when using lossy data compression), and the computational resources required to compress and decompress the data.
Lossless.
Lossless data compression algorithms usually exploit statistical redundancy to represent data without losing any information, so that the process is reversible. Lossless compression is possible because most real-world data exhibits statistical redundancy. For example, an image may have areas of colour that do not change over several pixels; instead of coding "red pixel, red pixel, ..." the data may be encoded as "279 red pixels". This is a basic example of run-length encoding; there are many schemes to reduce file size by eliminating redundancy.
The Lempel–Ziv (LZ) compression methods are among the most popular algorithms for lossless storage. DEFLATE is a variation on LZ optimized for decompression speed and compression ratio, but compression can be slow. DEFLATE is used in PKZIP, Gzip and PNG. LZW (Lempel–Ziv–Welch) is used in GIF images. Also noteworthy is the LZR (Lempel-Ziv–Renau) algorithm, which serves as the basis for the Zip method. LZ methods use a table-based compression model where table entries are substituted for repeated strings of data. For most LZ methods, this table is generated dynamically from earlier data in the input. The table itself is often Huffman encoded (e.g. SHRI, LZX).
Current LZ-based coding schemes that perform well are Brotli and LZX. LZX is used in Microsoft's CAB format.
The best modern lossless compressors use probabilistic models, such as prediction by partial matching. The Burrows–Wheeler transform can also be viewed as an indirect form of statistical modelling.
The class of grammar-based codes are gaining popularity because they can compress "highly repetitive" input extremely effectively, for instance, a biological data collection of the same or closely related species, a huge versioned document collection, internet archival, etc. The basic task of grammar-based codes is constructing a context-free grammar deriving a single string. Sequitur and Re-Pair are practical grammar compression algorithms for which software is publicly available.
In a further refinement of the direct use of probabilistic modelling, statistical estimates can be coupled to an algorithm called arithmetic coding. Arithmetic coding is a more modern coding technique that uses the mathematical calculations of a finite-state machine to produce a string of encoded bits from a series of input data symbols. It can achieve superior compression to other techniques such as the better-known Huffman algorithm. It uses an internal memory state to avoid the need to perform a one-to-one mapping of individual input symbols to distinct representations that use an integer number of bits, and it clears out the internal memory only after encoding the entire string of data symbols. Arithmetic coding applies especially well to adaptive data compression tasks where the statistics vary and are context-dependent, as it can be easily coupled with an adaptive model of the probability distribution of the input data. An early example of the use of arithmetic coding was its use as an optional (but not widely used) feature of the JPEG image coding standard. It has since been applied in various other designs including H.264/MPEG-4 AVC and HEVC for video coding.
Lossy.
Lossy data compression is the converse of lossless data compression. In these schemes, some loss of information is acceptable. Dropping nonessential detail from the data source can save storage space. Lossy data compression schemes are designed by research on how people perceive the data in question. For example, the human eye is more sensitive to subtle variations in luminance than it is to the variations in color. JPEG image compression works in part by rounding off nonessential bits of information. There is a corresponding trade-off between preserving information and reducing size. A number of popular compression formats exploit these perceptual differences, including those used in music files, images, and video.
Lossy image compression can be used in digital cameras, to increase storage capacities with minimal degradation of picture quality. Similarly, DVDs use the lossy MPEG-2 video coding format for video compression.
In lossy audio compression, methods of psychoacoustics are used to remove non-audible (or less audible) components of the audio signal. Compression of human speech is often performed with even more specialized techniques; speech coding, or voice coding, is sometimes distinguished as a separate discipline from "audio compression". Different audio and speech compression standards are listed under audio coding formats. "Voice compression" is used in internet telephony, for example, audio compression is used for CD ripping and is decoded by the audio players.
Theory.
The theoretical background of compression is provided by information theory (which is closely related to algorithmic information theory) for lossless compression and rate–distortion theory for lossy compression. These areas of study were essentially forged by Claude Shannon, who published fundamental papers on the topic in the late 1940s and early 1950s. Coding theory is also related to this. The idea of data compression is also deeply connected with statistical inference.
Machine learning.
There is a close connection between machine learning and compression: a system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution) while an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for "general intelligence."
Data differencing.
Data compression can be viewed as a special case of data differencing: Data differencing consists of producing a "difference" given a "source" and a "target," with patching producing a "target" given a "source" and a "difference," while data compression consists of producing a compressed file given a target, and decompression consists of producing a target given only a compressed file. Thus, one can consider data compression as data differencing with empty source data, the compressed file corresponding to a "difference from nothing." This is the same as considering absolute entropy (corresponding to data compression) as a special case of relative entropy (corresponding to data differencing) with no initial data.
When one wishes to emphasize the connection, one may use the term "differential compression" to refer to data differencing.
Uses.
Audio.
Audio data compression, not to be confused with dynamic range compression, has the potential to reduce the transmission bandwidth and storage requirements of audio data. Audio compression algorithms are implemented in software as audio codecs. Lossy audio compression algorithms provide higher compression at the cost of fidelity and are used in numerous audio applications. These algorithms almost all rely on psychoacoustics to eliminate less audible or meaningful sounds, thereby reducing the space required to store or transmit them.
In both lossy and lossless compression, information redundancy is reduced, using methods such as coding, pattern recognition, and linear prediction to reduce the amount of information used to represent the uncompressed data.
The acceptable trade-off between loss of audio quality and transmission or storage size depends upon the application. For example, one 640MB compact disc (CD) holds approximately one hour of uncompressed high fidelity music, less than 2 hours of music compressed losslessly, or 7 hours of music compressed in the MP3 format at a medium bit rate. A digital sound recorder can typically store around 200 hours of clearly intelligible speech in 640MB.
Lossless audio compression produces a representation of digital data that decompress to an exact digital duplicate of the original audio stream, unlike playback from lossy compression techniques such as Vorbis and MP3. Compression ratios are around 50–60% of original size, which is similar to those for generic lossless data compression. Lossless compression is unable to attain high compression ratios due to the complexity of waveforms and the rapid changes in sound forms. Codecs like FLAC, Shorten and TTA use linear prediction to estimate the spectrum of the signal. Many of these algorithms use convolution with the filter 1 to slightly whiten or flatten the spectrum, thereby allowing traditional lossless compression to work more efficiently. The process is reversed upon decompression.
When audio files are to be processed, either by further compression or for editing, it is desirable to work from an unchanged original (uncompressed or losslessly compressed). Processing of a lossily compressed file for some purpose usually produces a final result inferior to the creation of the same compressed file from an uncompressed original. In addition to sound editing or mixing, lossless audio compression is often used for archival storage, or as master copies.
A number of lossless audio compression formats exist. Shorten was an early lossless format. Newer ones include Free Lossless Audio Codec (FLAC), Apple's Apple Lossless (ALAC), MPEG-4 ALS, Microsoft's Windows Media Audio 9 Lossless (WMA Lossless), Monkey's Audio, TTA, and WavPack. See list of lossless codecs for a complete listing.
Some audio formats feature a combination of a lossy format and a lossless correction; this allows stripping the correction to easily obtain a lossy file. Such formats include MPEG-4 SLS (Scalable to Lossless), WavPack, and OptimFROG DualStream.
Other formats are associated with a distinct system, such as:
Lossy audio compression.
Lossy audio compression is used in a wide range of applications. In addition to the direct applications (mp3 players or computers), digitally compressed audio streams are used in most video DVDs, digital television, streaming media on the internet, satellite and cable radio, and increasingly in terrestrial radio broadcasts. Lossy compression typically achieves far greater compression than lossless compression (data of 5 percent to 20 percent of the original stream, rather than 50 percent to 60 percent), by discarding less-critical data.
The innovation of lossy audio compression was to use psychoacoustics to recognize that not all data in an audio stream can be perceived by the human auditory system. Most lossy compression reduces perceptual redundancy by first identifying perceptually irrelevant sounds, that is, sounds that are very hard to hear. Typical examples include high frequencies or sounds that occur at the same time as louder sounds. Those sounds are coded with decreased accuracy or not at all.
Due to the nature of lossy algorithms, audio quality suffers when a file is decompressed and recompressed (digital generation loss). This makes lossy compression unsuitable for storing the intermediate results in professional audio engineering applications, such as sound editing and multitrack recording. However, they are very popular with end users (particularly MP3) as a megabyte can store about a minute's worth of music at adequate quality.
Coding methods.
To determine what information in an audio signal is perceptually irrelevant, most lossy compression algorithms use transforms such as the modified discrete cosine transform (MDCT) to convert time domain sampled waveforms into a transform domain. Once transformed, typically into the frequency domain, component frequencies can be allocated bits according to how audible they are. Audibility of spectral components calculated using the absolute threshold of hearing and the principles of simultaneous masking—the phenomenon wherein a signal is masked by another signal separated by frequency—and, in some cases, temporal masking—where a signal is masked by another signal separated by time. Equal-loudness contours may also be used to weight the perceptual importance of components. Models of the human ear-brain combination incorporating such effects are often called psychoacoustic models.
Other types of lossy compressors, such as the linear predictive coding (LPC) used with speech, are source-based coders. These coders use a model of the sound's generator (such as the human vocal tract with LPC) to whiten the audio signal (i.e., flatten its spectrum) before quantization. LPC may be thought of as a basic perceptual coding technique: reconstruction of an audio signal using a linear predictor shapes the coder's quantization noise into the spectrum of the target signal, partially masking it.
Lossy formats are often used for the distribution of streaming audio or interactive applications (such as the coding of speech for digital transmission in cell phone networks). In such applications, the data must be decompressed as the data flows, rather than after the entire data stream has been transmitted. Not all audio codecs can be used for streaming applications, and for such applications a codec designed to stream data effectively will usually be chosen.
Latency results from the methods used to encode and decode the data. Some codecs will analyze a longer segment of the data to optimize efficiency, and then code it in a manner that requires a larger segment of data at one time to decode. (Often codecs create segments called a "frame" to create discrete data segments for encoding and decoding.) The inherent latency of the coding algorithm can be critical; for example, when there is a two-way transmission of data, such as with a telephone conversation, significant delays may seriously degrade the perceived quality.
In contrast to the speed of compression, which is proportional to the number of operations required by the algorithm, here latency refers to the number of samples that must be analysed before a block of audio is processed. In the minimum case, latency is zero samples (e.g., if the coder/decoder simply reduces the number of bits used to quantize the signal). Time domain algorithms such as LPC also often have low latencies, hence their popularity in speech coding for telephony. In algorithms such as MP3, however, a large number of samples have to be analyzed to implement a psychoacoustic model in the frequency domain, and latency is on the order of 23 ms (46 ms for two-way communication)).
Speech encoding.
Speech encoding is an important category of audio data compression. The perceptual models used to estimate what a human ear can hear are generally somewhat different from those used for music. The range of frequencies needed to convey the sounds of a human voice are normally far narrower than that needed for music, and the sound is normally less complex. As a result, speech can be encoded at high quality using a relatively low bit rate.
If the data to be compressed is analog (such as a voltage that varies with time), quantization is employed to digitize it into numbers (normally integers). This is referred to as analog-to-digital (A/D) conversion. If the integers generated by quantization are 8 bits each, then the entire range of the analog signal is divided into 256 intervals and all the signal values within an interval are quantized to the same number. If 16-bit integers are generated, then the range of the analog signal is divided into 65,536 intervals.
This relation illustrates the compromise between high resolution (a large number of analog intervals) and high compression (small integers generated). This application of quantization is used by several speech compression methods. This is accomplished, in general, by some combination of two approaches:
Perhaps the earliest algorithms used in speech encoding (and audio data compression in general) were the A-law algorithm and the µ-law algorithm.
History.
A literature compendium for a large variety of audio coding systems was published in the IEEE Journal on Selected Areas in Communications (JSAC), February 1988. While there were some papers from before that time, this collection documented an entire variety of finished, working audio coders, nearly all of them using perceptual (i.e. masking) techniques and some kind of frequency analysis and back-end noiseless coding. Several of these papers remarked on the difficulty of obtaining good, clean digital audio for research purposes. Most, if not all, of the authors in the JSAC edition were also active in the MPEG-1 Audio committee.
The world's first commercial broadcast automation audio compression system was developed by Oscar Bonello, an engineering professor at the University of Buenos Aires. In 1983, using the psychoacoustic principle of the masking of critical bands first published in 1967, he started developing a practical application based on the recently developed IBM PC computer, and the broadcast automation system was launched in 1987 under the name Audicom. Twenty years later, almost all the radio stations in the world were using similar technology manufactured by a number of companies.
Video.
Video compression uses modern coding techniques to reduce redundancy in video data. Most video compression algorithms and codecs combine spatial image compression and temporal motion compensation. Video compression is a practical implementation of source coding in information theory. In practice, most video codecs also use audio compression techniques in parallel to compress the separate, but combined data streams as one package.
The majority of video compression algorithms use lossy compression. Uncompressed video requires a very high data rate. Although lossless video compression codecs perform at a compression factor of 5-12, a typical MPEG-4 lossy compression video has a compression factor between 20 and 200. As in all lossy compression, there is a trade-off between video quality, cost of processing the compression and decompression, and system requirements. Highly compressed video may present visible or distracting artifacts.
Some video compression schemes typically operate on square-shaped groups of neighboring pixels, often called macroblocks. These pixel groups or blocks of pixels are compared from one frame to the next, and the video compression codec sends only the differences within those blocks. In areas of video with more motion, the compression must encode more data to keep up with the larger number of pixels that are changing. Commonly during explosions, flames, flocks of animals, and in some panning shots, the high-frequency detail leads to quality decreases or to increases in the variable bitrate.
Encoding theory.
Video data may be represented as a series of still image frames. The sequence of frames contains spatial and temporal redundancy that video compression algorithms attempt to eliminate or code in a smaller size. Similarities can be encoded by only storing differences between frames, or by using perceptual features of human vision. For example, small differences in color are more difficult to perceive than are changes in brightness. Compression algorithms can average a color across these similar areas to reduce space, in a manner similar to those used in JPEG image compression. Some of these methods are inherently lossy while others may preserve all relevant information from the original, uncompressed video.
One of the most powerful techniques for compressing video is interframe compression. Interframe compression uses one or more earlier or later frames in a sequence to compress the current frame, while intraframe compression uses only the current frame, effectively being image compression.
The most powerful used method works by comparing each frame in the video with the previous one. If the frame contains areas where nothing has moved, the system simply issues a short command that copies that part of the previous frame, bit-for-bit, into the next one. If sections of the frame move in a simple manner, the compressor emits a (slightly longer) command that tells the decompressor to shift, rotate, lighten, or darken the copy. This longer command still remains much shorter than intraframe compression. Interframe compression works well for programs that will simply be played back by the viewer, but can cause problems if the video sequence needs to be edited.
Because interframe compression copies data from one frame to another, if the original frame is simply cut out (or lost in transmission), the following frames cannot be reconstructed properly. Some video formats, such as DV, compress each frame independently using intraframe compression. Making 'cuts' in intraframe-compressed video is almost as easy as editing uncompressed video: one finds the beginning and ending of each frame, and simply copies bit-for-bit each frame that one wants to keep, and discards the frames one doesn't want. Another difference between intraframe and interframe compression is that, with intraframe systems, each frame uses a similar amount of data. In most interframe systems, certain frames (such as "I frames" in MPEG-2) aren't allowed to copy data from other frames, so they require much more data than other frames nearby.
It is possible to build a computer-based video editor that spots problems caused when I frames are edited out while other frames need them. This has allowed newer formats like HDV to be used for editing. However, this process demands a lot more computing power than editing intraframe compressed video with the same picture quality.
Today, nearly all commonly used video compression methods (e.g., those in standards approved by the ITU-T or ISO) apply a discrete cosine transform (DCT) for spatial redundancy reduction. The DCT that is widely used in this regard was introduced by N. Ahmed, T. Natarajan and K. R. Rao in 1974. Other methods, such as fractal compression, matching pursuit and the use of a discrete wavelet transform (DWT) have been the subject of some research, but are typically not used in practical products (except for the use of wavelet coding as still-image coders without motion compensation). Interest in fractal compression seems to be waning, due to recent theoretical analysis showing a comparative lack of effectiveness of such methods.
Timeline.
The following table is a partial history of international video compression standards.
Genetics.
Genetics compression algorithms are the latest generation of lossless algorithms that compress data (typically sequences of nucleotides) using both conventional compression algorithms and genetic algorithms adapted to the specific datatype. In 2012, a team of scientists from Johns Hopkins University published a genetic compression algorithm that does not use a reference genome for compression. HAPZIPPER was tailored for HapMap data and achieves over 20-fold compression (95% reduction in file size), providing 2- to 4-fold better compression and in much faster time than the leading general-purpose compression utilities. For this, Chanda, Elhaik, and Bader introduced MAF based encoding (MAFE), which reduces the heterogeneity of the dataset by sorting SNPs by their minor allele frequency, thus homogenizing the dataset. Other algorithms in 2009 and 2013 (DNAZip and GenomeZip) have compression ratios of up to 1200-fold—allowing 6 billion basepair diploid human genomes to be stored in 2.5 megabytes (relative to a reference genome or averaged over many genomes).
Outlook and currently unused potential.
It is estimated that the total amount of data that is stored on the world's storage devices could be further compressed with existing compression algorithms by a remaining average factor of 4.5:1. It is estimated that the combined technological capacity of the world to store information provides 1,300 exabytes of hardware digits in 2007, but when the corresponding content is optimally compressed, this only represents 295 exabytes of Shannon information.

</doc>
<doc id="8022" url="https://en.wikipedia.org/wiki?curid=8022" title="History of the Democratic Republic of the Congo">
History of the Democratic Republic of the Congo

The region that is now the Democratic Republic of the Congo was first settled about 80,000 years ago. Bantu migration arrived in the region from Nigeria in the 7th century AD. The Kingdom of Kongo remained present in the region between the 14th and the early 19th centuries. Belgian colonization began when King Leopold II founded the Congo Free State, a corporate state run solely by King Leopold. Reports of widespread murder and torture in the rubber plantations led the Belgian government to seize the Congo from Leopold II and establish the Belgian Congo. Under Belgian rule, the colony was run with the presence of numerous Christian organizations that wanted to Westernize the Congolese people.
After an uprising by the Congolese people, Belgium granted the Congo its independence in 1960. However, the Congo was left unstable because tribal leaders had more power than the central government. Prime Minister Patrice Lumumba tried to restore order with the aid of the Soviet Union as part of the Cold War, causing the United States to support a coup led by Colonel Joseph Mobutu in 1965. Mobutu quickly seized complete power of the Congo and renamed the country Zaire. He sought to Africanize the country, changing his own name to Mobuto Sese Seko, and demanded that African citizens change their Western names to traditional African names. Mobuto sought to repress any opposition to his rule, in which he successfully did throughout the 1980s. However, with his regime weakened during the early 1990s, Mobuto was forced to agree to a power-sharing government with the opposition party. Mobuto remained the head of state and promised elections for the next two years that never happened.
In the First Congo War, Rwanda invaded Zaire, which overthrow Mobuto during the process. Laurent-Desire Kabila later took power and renamed the Democratic Republic of the Congo. After a disappointing rule under Kabila, the Second Congo War broke out, resulting in a regional war with many different African nations taking part. Kabila was assassinated by his bodyguard in 2001, and his son, Joseph, succeeded him and later elected president by the Congolese government in 2006. Upon taking office, Kabila quickly sought peace, ending the era of war in Africa. Soldiers were left in the Congo for a few years and a power-sharing government between Kabila and the opposition party was set up. Kabila later resumed complete control over the Congo and was re-elected in a disputed election in 2011. Today, the Congo remains dangerously unstable.
Early history.
The area now known as the Democratic Republic of the Congo was populated as early as 80,000 years ago, as shown by the 1988 discovery of the Semliki harpoon at Katanda, one of the oldest barbed harpoons ever found, and which is believed to have been used to catch giant river catfish. Congo was settled in the 7th and 8th centuries A.D. by Bantus from present-day Nigeria. During its history, the area has also been known as "Congo", "Congo Free State", "Belgian Congo", and "Zaire". The Kingdom of Kongo was a powerful kingdom that existed between the 14th and the early 19th century. It was the dominant force in the region until the arrival of the Portuguese. Second in importance was the Anziku Kingdom.
Colonial rule.
Congo Free State (1885–1908).
The Congo Free State was a corporate state privately controlled by Leopold II of Belgium through the "Association internationale africaine", a non-governmental organization. Leopold was the sole shareholder and chairman. The state included the entire area of the present Democratic Republic of the Congo. Under Leopold II's administration, the Congo Free State became the site of one of the most infamous international scandals of the turn of the twentieth century. The report of the British Consul Roger Casement led to the arrest and punishment of white officials who had been responsible for cold-blooded killings during a rubber-collecting expedition in 1903, including one Belgian national for causing the shooting of at least 122 Congolese natives. Estimates of the total death toll vary considerably. In the absence of a census, the first was made in 1924, it is even more difficult to quantify the population loss of the period. Roger Casement's famous 1904 report estimated ten million people. According to Casement's report, indiscriminate "war", starvation, reduction of births and tropical diseases caused the country's depopulation. The European and U.S. press agencies exposed the conditions in the Congo Free State to the public in 1900. By 1908 public and diplomatic pressure led Leopold II to annex the Congo as the Belgian Congo colony.
Belgian Congo (1908–1960).
On the 15 of November 1908 King Léopold II of Belgium formally relinquished personal control of the Congo Free State. The renamed Belgian Congo was put under the direct administration of the Belgian government and its Ministry of Colonies.
Belgian rule in the Congo was based around the "colonial trinity" ("trinité coloniale") of state, missionary and private company interests. The privileging of Belgian commercial interests meant that large amounts of capital flowed into the Congo and that individual regions became specialised. On many occasions, the interests of the government and private enterprise became closely tied and the state helped companies break strikes and remove other barriers imposed by the indigenous population. The country was split into nesting, hierarchically organised administrative subdivisions, and run uniformly according to a set "native policy" ("politique indigène")—in contrast to the British and the French, who generally favoured the system of indirect rule whereby traditional leaders were retained in positions of authority under colonial oversight. There was also a high degree of racial segregation. Large numbers of white immigrants who moved to the Congo after the end of World War II came from across the social spectrum, but were nonetheless always treated as superior to blacks.
During the 1940s and 1950s, the Congo experienced an unprecedented level of urbanisation and the colonial administration began various development programmes aimed at making the territory into a "model colony". Notable advances were made in treating diseases such as African trypanosomiasis. One of the results of these measures was the development of a new middle class of Europeanised African "évolué" in the cities. By the 1950s the Congo had a wage labour force twice as large as that in any other African colony. The Congo's rich natural resources, including uranium—much of the uranium used by the U.S. nuclear programme during World War II was Congolese—led to substantial interest in the region from both the Soviet Union and the United States as the Cold War developed.
The Congo Crisis (1960–1965).
Following riots in Leopoldville between 4–7 January 1959, and Stanleyville on 31 October 1959, the Belgians realised they could not maintain control of such a vast country in the face of rising demands for independence. The Belgians and Congolese political leaders held a Round Table Conference in Brussels beginning on 18 January 1960. At the end of the Conference on 27 January 1960 it was announced that elections would be held in the Congo on 22 May 1960, and full independence granted on 30 June 1960. The Congo was indeed granted its independence on 30 June 1960, adopting the name "Republic of the Congo" (République du Congo). As the French colony of Middle Congo (Moyen Congo) also chose the name Republic of Congo upon receiving its independence, the two countries were more commonly known as Congo-Léopoldville and Congo-Brazzaville, after their capital cities. President Mobutu changed the country's official name to Zaire in 1966.
In 1960, the country was in a very unstable state—regional tribal leaders held far more power than the central government—and with the departure of the Belgian administrators, there were almost no skilled bureaucrats left in the country. The first Congolese university graduate was only in 1956, and virtually no one in the new nation had any idea of how to manage a country of such size.
Parliamentary elections in 1960 produced the nationalist Patrice Lumumba as prime minister and pro-Western Joseph Kasavubu as president of the renamed Democratic Republic of the Congo.
Even from this fleeting moment of independence democracy began to unravel. On 5 July 1960 a military mutiny by Congolese soldiers against their European officers broke out in the capital and rampant looting began. On 11 July 1960 the richest province of the country, Katanga, seceded under Moise Tshombe. The United Nations sent 20,000 peacekeepers to protect Europeans in the country and try to restore order. Western paramilitaries and mercenaries, often hired by mining companies to protect their interests, also began to pour into the country. In this same period Congo's second richest province, Kasai, also announced its independence on 8 August 1960.
Prime Minister Lumumba turned to the USSR for assistance. Nikita Khrushchev agreed to help, offering advanced weaponry and technical advisors. The United States viewed the Soviet presence as an attempt to take advantage of the situation and gain a proxy state in sub-Saharan Africa. UN forces were ordered to block any shipments of arms into the country. The United States also looked for a way to replace Lumumba as leader. President Kasavubu had clashed with Prime Minister Lumumba and advocated an alliance with the West rather than the Soviets. The U.S. sent weapons and CIA personnel to aid forces allied with Kasavubu and combat the Soviet presence. On 14 September 1960, with U.S. and CIA support, Colonel Joseph Mobutu overthrew the government and arrested Lumumba.
On 17 January 1961 Mobutu sent Lumumba to Élisabethville (now Lubumbashi), capital of Katanga. In full view of the press he was beaten and forced to eat copies of his own speeches. For the next three weeks, he was not seen or heard from. Then Katangan radio announced implausibly that he had escaped and been killed by some villagers. In fact he had been tortured and killed along with two others shortly after his arrival. It was soon clear that he had been murdered in custody. In 2001, a Belgian inquiry established that he had been shot by Katangan gendarmes in the presence of Belgian officers, under Katangan command. Lumumba was beaten, placed in front of a firing squad with 2 other allies, cut up, buried, dug up and what remained was dissolved in acid.
In Stanleyville, those loyal to the deposed Lumumba set up a rival government under Antoine Gizenga which lasted from 31 March 1961 until it was reintegrated on 5 August 1961. After some reverses, UN and Congolese government forces succeeded in recapturing the breakaway provinces of South Kasai on 30 December 1961, and Katanga on 15 January 1963.
A new crisis erupted in the Simba Rebellion of 1964-1965 which saw half the country taken by the rebels. European mercenaries, US, and Belgian troops were called in by the Congolese government to defeat the rebellion.
Zaire (1965–1997).
Unrest and rebellion plagued the government until November 1965, when Lieutenant General Mobutu, by then commander in chief of the national army, seized control of the country and declared himself president for five years. Mobutu quickly consolidated his power and was elected unopposed as president in 1970. Embarking on a campaign of cultural awareness, Mobutu renamed the country the Republic of Zaire in 1971 and required citizens to adopt African names as well as drop their French-language ones. Relative peace and stability prevailed until 1977 and 1978 when Katangan rebels, based in Angola, launched a series of invasions (Shaba I and II) into the Shaba (Katanga) region. The rebels were driven out with the aid of Belgian paratroopers.
Zaire remained a one-party state in the 1980s. Although Mobutu successfully maintained control during this period, opposition parties, most notably the Union pour la Démocratie et le Progrès Social (UDPS), were active. Mobutu's attempts to quell these groups drew significant international criticism. 
As the Cold War came to a close, internal and external pressures on Mobutu increased. In late 1989 and early 1990, Mobutu was weakened by a series of domestic protests, by heightened international criticism of his regime's human rights practices, by a faltering economy, and by government corruption, most notably his massive embezzlement of government funds for personal use.
In April 1990, Mobutu declared the Third Republic, agreeing to a limited multi-party system with elections and a constitution. As details of a reform package were delayed, soldiers in September 1991 began looting Kinshasa to protest their unpaid wages. Two thousand French and Belgian troops, some of whom were flown in on U.S. Air Force planes, arrived to evacuate the 20,000 endangered foreign nationals in Kinshasa.
In 1992, after previous similar attempts, the long-promised Sovereign National Conference was staged, encompassing over 2,000 representatives from various political parties. The conference gave itself a legislative mandate and elected Archbishop Laurent Monsengwo as its chairman, along with Étienne Tshisekedi wa Mulumba, leader of the UDPS, as prime minister. By the end of the year Mobutu had created a rival government with its own prime minister. The ensuing stalemate produced a compromise merger of the two governments into the High Council of Republic-Parliament of Transition (HCR-PT) in 1994, with Mobutu as head of state and Kengo Wa Dondo as prime minister. Although presidential and legislative elections were scheduled repeatedly over the next two years, they never took place.
First Congo War (1996–1997).
By 1996, tensions from the neighboring Rwanda war and genocide had spilled over to Zaire: "see History of Rwanda". Rwandan Hutu militia forces (Interahamwe), who had fled Rwanda following the ascension of a Tutsi-led government, had been using Hutu refugees camps in eastern Zaire as a basis for incursion against Rwanda. These Hutu militia forces soon allied with the Zairian armed forces (FAZ) to launch a campaign against Congolese ethnic Tutsis in eastern Zaire. In turn, these Tutsis formed a militia to defend themselves against attacks. When the Zairian government began to escalate its massacres in November 1996, the Tutsi militias erupted in rebellion against Mobutu.
The Tutsi militia was soon joined by various opposition groups and supported by several countries, including Rwanda and Uganda. This coalition, led by Laurent-Desire Kabila, became known as the Alliance des Forces Démocratiques pour la Libération du Congo-Zaïre (AFDL). The AFDL, now seeking the broader goal of ousting Mobutu, made significant military gains in early 1997. They were soon joined by various Zairean politicians, who had been unsuccessfully opposing the dictatorship of Mobutu for many years, and now saw an opportunity for them in the invasion of Zaire by two of the region's strongest military forces. Following failed peace talks between Mobutu and Kabila in May 1997, Mobutu left the country, and Kabila marched unopposed to Kinshasa on 20 May. Kabila named himself president, consolidated power around himself and the AFDL, and reverted the name of the country to the Democratic Republic of Congo.
Second Congo War (1998–2003).
Kabila demonstrated little ability to manage the problems of his country, and lost his allies. To counterbalance the power and influence of Rwanda in DRC, the Ugandan troops instigated the creation of another rebel movement called the Movement for the Liberation of Congo (MLC), led by the Congolese warlord Jean-Pierre Bemba. They attacked in August 1998, backed by Rwandan and Ugandan troops. Soon afterwards, Angola, Namibia, and Zimbabwe became involved militarily in the Congo, with Angola and Zimbabwe supporting the government. While the six African governments involved in the war signed a ceasefire accord in Lusaka in July 1999, the Congolese rebels did not and the ceasefire broke down within months. However, Kabila was assassinated in 2001 by one of his bodyguards and was succeeded by his son, Joseph. Upon taking office, Kabila called for multilateral peace talks to end the war. Kabila partly succeeded when a further peace deal was brokered between him, Uganda, and Rwanda leading to the apparent withdrawal of foreign troops.
Currently, the Ugandans and the MLC still hold a wide section of the north of the country; Rwandan forces and its front, the Rassemblement Congolais pour la Démocratie (RCD) control a large section of the east; and government forces or their allies hold the west and south of the country. There were reports that the conflict is being prolonged as a cover for extensive looting of the substantial natural resources in the country, including diamonds, copper, zinc, and coltan. The conflict was reignited in January 2002 by ethnic clashes in the northeast and both Uganda and Rwanda then halted their withdrawal and sent in more troops. Talks between Kabila and the rebel leaders, held in Sun City, lasted a full six weeks, beginning in April 2002. In June, they signed a peace accord in which Kabila would share power with former rebels. By June 2003, all foreign armies except those of Rwanda had pulled out of Congo. Few people in the Congo have been unaffected by the armed conflict. A survey conducted in 2009 by the ICRC and Ipsos shows that three quarters (76%) of the people interviewed have been affected in some way–either personally or due to the wider consequences of armed conflict.
The response of the international community has been incommensurate with the scale of the disaster resulting from the war in the Congo. Its support for political and diplomatic efforts to end the war has been relatively consistent, but it has taken no effective steps to abide by repeated pledges to demand accountability for the war crimes and crimes against humanity that were routinely committed in Congo. United Nations Security Council and the U.N. Secretary-General have frequently denounced human rights abuses and the humanitarian disaster that the war unleashed on the local population. But they had shown little will to tackle the responsibility of occupying powers for the atrocities taking place in areas under their control, areas where the worst violence in the country took place. Hence Rwanda, like Uganda, has escaped any significant sanction for its role.
Transitional government (2003–2006).
DR Congo had a transitional government in July 2003 until the election was over. A constitution was approved by voters and on 30 July 2006 the Congo held its first multi-party elections since independence in 1960. After this Joseph Kabila took 45% of the votes and his opponent Jean-Pierre Bemba took 20%. That was the origin of a fight between the two parts from 20–22 August 2006 in the streets of the capital, Kinshasa. Sixteen people died before policemen and UN mission MONUC took control of the city. A new election was held on 29 October 2006, which Kabila won with 70% of the vote. Bemba has publicly commented on election "irregularities," despite the fact that every neutral observer has praised the elections. On 6 December 2006 the Transitional Government came to an end as Joseph Kabila was sworn in as President.
Continued conflicts.
The fragility of the state has allowed continued violence and human rights abuses in the east. There are three significant centers of conflict.
Ituri, where MONUC has proved unable to contain the numerous militia and groups driving the Ituri conflict
Northern Katanga, where Mai-Mai created by Laurent Kabila slipped out of the control of Kinshasa.
In October 2009 a new conflict started in Dongo, Sud-Ubangi District where clashes had broken out over access to fishing ponds.
Kivu conflict.
North Kivu and South Kivu, where Democratic Forces for the Liberation of Rwanda (FDLR) continues to threaten the Rwandan border and the Banyamulenge, Rwanda supported RCD-Goma rebels (see Kivu war).
In April 2012, ethnic Tutsi soldiers mutinied against the government of the Democratic Republic of the Congo. Mutineers formed a rebel group called the March 23 Movement (M23), composed of former members of the rebel National Congress for the Defence of the People (CNDP). On 20 November 2012, M23 took control of Goma, a provincial capital with a population of one million people.
Re-election of Joseph Kabila.
In December 2011, Joseph Kabila was re-elected for a second term as president. After the results were announced on 9 December, there was violent unrest in Kinshasa and Mbuji-Mayi, where official tallies showed that a strong majority had voted for the opposition candidate Etienne Tshisekedi. Official observers from the Carter Center reported that returns from almost 2,000 polling stations in areas where support for Tshisekedi was strong had been lost and not included in the official results. They described the election as lacking credibility. On 20 December, Kabila was sworn in for a second term, promising to invest in infrastructure and public services. However, Tshisekedi maintained that the result of the election was illegitimate and said that he intended also to "swear himself in" as president.
On 19 January 2015 protests led by students at the University of Kinshasa broke out. The protests began following the announcement of a proposed law that would allow Kabila to remain in power until a national census can be conducted (elections had been planned for 2016). By Wednesday 21 January clashes between police and protesters had claimed at least 42 lives (although the government claimed only 15 people had been killed).

</doc>
<doc id="8023" url="https://en.wikipedia.org/wiki?curid=8023" title="Geography of the Democratic Republic of the Congo">
Geography of the Democratic Republic of the Congo

The Democratic Republic of the Congo is by the Congo River Basin, which covers an area of almost . The country's only outlet to the Atlantic Ocean is a narrow strip of land on the north bank of the Congo River.
The vast, low-lying central area is a plateau-shaped basin sloping toward the west, covered by tropical rainforest and criss-crossed by rivers, a large area of this has been categorized by the World Wildlife Fund as the Central Congolian lowland forests ecoregion. The forest center is surrounded by mountainous terraces in the west, plateaus merging into savannahs in the south and southwest. 
Dense grasslands extend beyond the Congo River in the north. High mountains of the Ruwenzori Range (some above ) are found on the eastern borders with Rwanda and Uganda (see Albertine Rift montane forests for a description of this area).
Climate.
The Democratic Republic of the Congo lies on the Equator, with one-third of the country to the north and two-thirds to the south. The climate is hot and humid in the river basin and cool and dry in the southern highlands, with a cold, alpine climate in the Rwenzori Mountains. 
South of the Equator, the rainy season lasts from October to May and north of the Equator, from April to November. Along the Equator, rainfall is fairly regular throughout the year. During the wet season, thunderstorms often are violent but seldom last more than a few hours. The average rainfall for the entire country is about .
Data.
Location of Congo:
Central Africa, northeast of Angola
Geographic coordinates: 
Map references:
Africa
Area:
"total:"
2,344,858 km
"land:"
2,267,048 km
"water:"
77,810 km
Area - comparative:
slightly less than one-fourth the size of the US
Land boundaries:
"total:"
10,481 km
"border countries:"
Angola 2,646 km, Burundi 236 km, Central African Republic 1,747 km, Republic of the Congo 1,229 km, Rwanda 221 km, South Sudan 714 km, Tanzania 479 km, Uganda 877 km, Zambia 2,332 km
Coastline:
Maritime claims:
"territorial sea:"
"exclusive economic zone:"
boundaries with neighbors
Climate:
tropical; hot and humid in equatorial river basin; cooler and drier in southern highlands; cooler-cold and wetter in eastern highlands and the Ruwenzori Range; north of Equator - wet season April to October, dry season December to February; south of Equator - wet season November to March, dry season April to October
Terrain:
vast central plateau covered by tropical rainforest, surrounded by mountains in the west, plains and savanna in the south/southwest, and grasslands in the north. The high mountains of the Ruwenzori Range on the eastern borders.
Elevation extremes:
"lowest point:"
Atlantic Ocean 0 m
"highest point:"
Pic Marguerite on Mont Ngaliema (Mount Stanley) 5,110 m
Natural resources:
cobalt, copper, niobium, petroleum, industrial and gem diamonds, gold, silver, zinc, manganese, tin, uranium, coal, hydropower, timber
Land use:
"arable land:"
3.09% 
"permanent crops:"
0.36%
96.55 (2012 est.)
Irrigated land:
105 km (2003)
Total renewable water resources:
1,283 km (2011)
Freshwater withdrawal (domestic/industrial/agricultural):
"total:"
0.68 km/yr (68%/21%/11%)
"per capita:"
11.25 m/yr (2005)
Natural hazards:
periodic droughts in south; Congo River floods (seasonal); in the east, in the Albertine Rift, there are active volcanoes
Environment - current issues:
Poaching threatens wildlife populations (for example, the painted hunting dog, "Lycaon pictus", is now considered extirpated from the Congo due to human overpopulation and poaching); water pollution; deforestation (chiefly due to land conversion to agriculture by indigenous farmers); refugees responsible for significant deforestation, soil erosion, and wildlife poaching; mining of minerals (coltan — a mineral used in creating capacitors, diamonds, and gold) causing environmental damage
Environment - international agreements:
"party to:"
Biodiversity, Climate Change, Desertification, Endangered Species, Hazardous Wastes, Law of the Sea, Marine Dumping, Nuclear Test Ban, Ozone Layer Protection, Tropical Timber 83, Tropical Timber 94, Wetlands
"signed, but not ratified:"
Environmental Modification
Geography:
D.R. Congo is one of 6 African states that straddles the Equator; it's the largest African state that has the Equator passing through it. Very narrow strip of land that controls the lower Congo River and is the only outlet to South Atlantic Ocean; dense tropical rainforest in central river basin and eastern highlands.
Extreme points.
This is a list of the extreme points of the Democratic Republic of the Congo, the points that are farther north, south, east or west than any other location.

</doc>
<doc id="8024" url="https://en.wikipedia.org/wiki?curid=8024" title="Demographics of the Democratic Republic of the Congo">
Demographics of the Democratic Republic of the Congo

This article is about the demographic features of the population of the Democratic Republic of the Congo, including ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.
As many as 250 ethnic groups have been distinguished and named. The most numerous people are the Luba, Mongo, and Bakongo. 
Although 700 local languages and dialects are spoken, the linguistic variety is bridged both by the use of French and the intermediary languages Kongo, Luba-Kasai, Swahili, and Lingala.
Population.
According to the 2010 revison of the World Population Prospects the total population was 65 966 000 in 2010, compared to only 12 184 000 in 1950. The proportion of children below the age of 15 in 2010 was 46.3%, 51.1% was between 15 and 65 years of age, while 2.7% was 65 years or older
Structure of the population (DHS 2013-2014) (Males 45 548, Females 49 134 = 94 682) :
Vital statistics.
Registration of vital events in the Democratic Republic of the Congo is incomplete. The Population Departement of the United Nations prepared the following estimates.
Fertility and Births.
Total Fertility Rate (TFR) (Wanted Fertility Rate) and Crude Birth Rate (CBR):
Ethnic groups.
More than 250 ethnic groups have been identified and named of which the majority are Bantu. The four largest groups - Mongo, Luba, Kongo (all Bantu), and the Mangbetu-Azande make up about 45% of the population. The country has also 60,000 White Congolese most of Belgian ancestry who remained after independence.
Bantu peoples (80%):
Central Sudanic/Ubangian :
Nilotic peoples :
Pygmy peoples :
More than 600,000 pygmies (around 1% of the total population) are believed to live in the DR Congo's huge forests, where they survive by hunting wild animals and gathering fruits.
Languages.
The four major languages in the DRC are French (official), Lingala (a lingua franca trade language), Kingwana (a dialect of Swahili), Kikongo, and Tshiluba. There are over 200 ethnic languages.
French is generally the medium of instruction in schools. English is taught as a compulsory foreign language in Secondary and High School around the country. It is a required subject in the Faculty of Economics at major universities around the country and there are numerous language schools in the country that teach it. In the town of Beni, for instance, there is a Bilingual University that offer courses in both French and English. President Kabila himself is fluent in both English and French, as was his father.
Religions.
Roman Catholic 50%, Protestant 20%, Kimbanguist 10%, Muslim 10%, other (includes syncretic sects and indigenous beliefs) 10% official report according to the CIA The World Factbook
Roman Catholic 43.9%, Protestant 24.8%, Other Christian 23.7%, Muslim 1.6%, Non-religious 0.6%, Hindu 0.1% other syncretic sects and indigenous beliefs 5.3% according to Joshua project
CIA World Factbook demographic statistics.
The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.
Net migration rate.
-0.54 migrant(s)/1,000 population
"note": fighting between the Congolese Government and Uganda- and Rwanda-backed Congolese rebels spawned a regional war in DRC in August 1998, which left 2.33 million Congolese internally displaced and caused 412,000 Congolese refugees to flee to surrounding countries (2011 est.)
Given the situation in the country and the condition of state structures, it is extremely difficult to obtain reliable data however evidence suggests that DRC continues to be a destination country for immigrants in spite of recent declines. Immigration is seen to be very diverse in nature, with refugees and asylum-seekers - products of the numerous and violent conflicts in the Great Lakes Region - constituting an important subset of the population in the country.
Additionally, the country’s large mine operations attract migrant workers from Africa and beyond and there is considerable migration for commercial activities from other African countries and the rest of the world, but these movements are not well studied. Transit migration towards South Africa and Europe also plays a role. Immigration in the DRC has decreased steadily over the past two decades, most likely as a result of the armed violence that the country has experienced.
According to the International Organization for Migration, the number of immigrants in the DRC has declined from just over 1 million in 1960, to 754,000 in 1990, to 480,000 in 2005, to an estimated 445,000 in 2010. Valid figures are not available on migrant workers in particular, partly due to the predominance of the informal economy in the DRC. Data are also lacking on irregular immigrants, however given neighbouring country ethnic links to nationals of the DRC, irregular migration is assumed to be a significant phenomenon in the country. 
Figures on the number of Congolese nationals abroad vary greatly depending on the source, from 3 to 6 million. This discrepancy is due to a lack of official, reliable data. Emigrants from the DRC are above all long-term emigrants, the majority of which live within Africa and to a lesser extent in Europe; 79.7% and 15.3% respectively, according to estimates on 2000 data. Most Congolese emigrants however, remain in Africa, with new destination countries including South Africa and various points en route to Europe.
In addition to being a host country, the DRC has also produced a considerable number of refugees and asylum-seekers located in the region and beyond. These numbers peaked in 2004 when, according to UNHCR, there were more than 460,000 refugees from the DRC; in 2008, Congolese refugees numbered 367,995 in total, 68% of which were living in other African countries.
Congolese diaspora.
The table below shows DRC born people who have emigrated abroad (although it excludes their descendants).
These are only estimates and do not account for Congolese migrants residing illegally in these countries. 

</doc>
<doc id="8025" url="https://en.wikipedia.org/wiki?curid=8025" title="Economy of the Democratic Republic of the Congo">
Economy of the Democratic Republic of the Congo

Sparsely populated in relation to its area, the Democratic Republic of the Congo is home to a vast potential of natural resources and mineral wealth. Its untapped deposits of raw minerals are estimated to be worth in excess of US$24 trillion. Despite this, the economy has declined drastically since the mid-1980s.
At the time of its independence in 1960, the Democratic Republic of the Congo was the second most industrialized country in Africa after South Africa. It boasted a thriving mining sector and its agriculture sector was relatively productive. Since then, corruption, war and political instability have been a severe detriment to further growth, today leaving DRC with a GDP per capita among the world's lowest.
Economic Implications of Conflicts.
The two recent conflicts (the First and Second Congo Wars), which began in 1996, have dramatically reduced national output and government revenue, have increased external debt, and have resulted in deaths of more than five million people from war, and associated famine and disease. Malnutrition affects approximately two thirds of the country's population.
Agriculture is the mainstay of the economy, accounting for 57.9% of GDP in 1997. In 1996, agriculture employed 66% of the work force.
Rich in minerals, the Democratic Republic of the Congo has a difficult history of predatory mineral extraction, which has been at the heart of many struggles within the country for many decades, but particularly in the 1990s. The economy of the third largest country in Africa relies heavily on mining. However, much economic activity occurs in the informal sector and is not reflected in GDP data.
In 2006 Transparency International ranked the Democratic Republic of the Congo 156 out of 163 countries in the Corruption Perception Index, tying Bangladesh, Chad, and Sudan with a 2.0 rating. President Joseph Kabila established the Commission of Repression of Economic Crimes upon his ascension to power in 2001.
The conflicts in the DRC were over water, minerals, and other resources. Political agendas have worsened the economy because very few people are benefiting in times of crisis while they let the people they are leading suffer. Made worse because of national and international corporations which are corrupt. The corporations allow the fighting for resources to continue simply because they continue to benefit from it. Many deaths are the result of not having basic needs which shows how the economy affects its people who are also treated like slaves. There have been lots of refugees since the fighting in 1998 which doesn’t help the poverty issue in the country. Taxes are not used for they are supposed to be used for because of those corrupt leaders fulfilling their own objectives instead of the country’s needs. The DRC is consistently rated the lowest on the UN Human Development Index.
Economic history.
Before Leopold.
Portuguese traders showed up around the 1480s and found that the people would be good slaves which led them to stopping any political power that could stop the slave trade. They traded weapons for the slaves which led to anarchy all across the nation.
After Leopold.
Forced labor was important for the rural sector. The corporations that dominated the economy were mostly owned by Belgium, but British capital also played an important role. Independence caused the Congo to become the most industrialized country in Sub-Saharan Africa, after South Africa. 1950s was a period of rising income and expectations. Called the best public health system in Africa, huge wealth disparity. The Belgian companies favored workers in certain areas more and exported them to work in different areas and they took away opportunities from other people. The favored groups also got better education and were able to secure jobs to people in the same ethnic group which caused tensions to rise. In 1960 there were only 16 university graduates out of 20 million people in the country. Belgium still had economic power and independence gave little opportunity for improvement. “no elite, no trouble”, “before independence = after independence”, When the Belgians left they left nobody in the country with the skills to run the government or the economy. Before independence there were just 3 out of 5000 government jobs held by Congolese people.
Zaire.
After the Congo crisis Mobutu arose as the country's sole ruler and stabilized the country politically. Economically, however, the situation continued to decline and by 1979 the purchasing power was only 4% of that from 1960. Starting in 1976 the IMF provided stabilizing loans to the dictatorship. Much of the money was embezzled by Mobutu and his circle. This was not a secret as the 1982 report by IMF’s envoy Erwin Blumenthal documented. He stated, it is “alarmingly clear that the corruptive system in Zaire with all its wicked and ugly manifestations, its mismanagement and fraud will destroy all endeavors of international institutions, of friendly governments, and of the commercial banks towards recovery and rehabilitation of Zaire’s economy". Blumenthal indicated that there was “no chance” that creditors would ever recover their loans. Yet the IMF and the World Bank continued to lend money that was either embezzled, stolen, or "wasted on elephant projects". “Structural adjustment programmes” implemented as a condition of IMF loans cut support for health care, education, and infrastructure.
1990s.
International Bank for Reconstruction and Development (IBRD) Trust Fund for the Congo.
Poor infrastructure, an uncertain legal framework, corruption, and lack of openness in government economic policy and financial operations remain a brake on investment and growth. A number of International Monetary Fund (IMF) and World Bank missions have met with the new government to help it develop a coherent economic plan but associated reforms are on hold.
Faced with continued currency depreciation, the government resorted to more drastic measures and in January 1999 banned the widespread use of U.S. dollars for all domestic commercial transactions, a position it later adjusted. The government has been unable to provide foreign exchange for economic transactions, while it has resorted to printing money to finance its expenditure. Growth was negative in 2000 because of the difficulty of meeting the conditions of international donors, continued low prices of key exports, and post-coup instability.
Although depreciated, congolese francs have been stable for few years (Ndonda, 2014)
2000s.
Conditions improved in late 2002 with the withdrawal of a large portion of the invading foreign troops. A number of IMF and World Bank missions have met with the government to help it develop a coherent economic plan, and President Kabila has begun implementing reforms.
Special Economic Zone.
The DRC is embarking on the establishment of special economic zones (SEZ) to encourage the revival of its industry. The first SEZ was planned to come into being in 2012 in N'Sele, a commune of Kinshasa, and will focus on agro-industries. The Congolese authorities also planned to open another zone dedicated to mining (Katanga) and a third dedicated to cement (in the Bas-Congo). There are three phases to the program that each have their own objectives. Phase I was the precursor to the actual investment in the Special Economic Zone where policymakers agreed to the framework, the framework was studied for its establishment, and to predict the potential market demand for the land. Stage one of Phase II involved submitting laws for the Special Economic Zone, finding good sites for businesses, and currently there is an effort to help the government attract foreign investment. Stage two of Phase II hasn’t been started yet and it involves assisting the government in creating framework for the country, creating an overall plan for the site, figuring out what the environmental impact of the project will be, and guessing how much it will cost and what the return can be made on the investment. Phase III involves the World Bank creating a transaction phase that will keep everything competitive. The program is looking for options to hand over the program to the World Bank which could be very beneficial for the western part of the country.
Implications of Instability on Economy.
Ongoing conflicts dramatically reduced government revenue 
increased external debt. As Reyntjens wrote, “Entrepreneurs of insecurity are engaged in extractive activities that would be impossible in a stable state environment. The criminalization context in which these activities occur offers avenues for considerable factional and personal enrichment through the trafficking of arms, illegal drugs, toxic products, mineral resources and dirty money.”16 Ethnic rivalries were made worse because of economic interests and there was lots of looting and smuggling of coltan. Illegal monopolies started forming in the country where they used forced labor for children to mine or work as soldiers. National parks were overrun with people looking to exploit minerals and resources. Increased poverty and hunger from the war and that increased the hunting of rare wildlife. Education was denied when the country was under foreign control and very few people make money of the minerals in the country. The national resources are not the root cause for the continued fighting in the region, however the competition has become an incentive to keep fighting. The DRC’s level of economic freedom is one of the lowest in the world putting it in the repressed category. The armed militias fight with the government in the Eastern section of the country over the mining sector or the corruption of the government and weak policies lead to the instabilities of the economy. The abuse of human rights also ruins economic activity considering that the DRC has a 7% unemployment rate, but still has one of the lowest GDP’s per capita in the world. A major problem for people trying to start their own companies is that the minimum amount of capital needed to launch the company is 5 times the average annual income and prices are regulated by the government which almost forces people to have to work for the larger, more corrupt businesses otherwise they won’t have work. It is hard for the DRC to encourage foreign trade because of the barriers of regulation.
International Relations.
International Bank for Reconstruction and Development (IBRD) Trust Fund for the Congo.
Poor infrastructure, an uncertain legal framework, corruption, and lack of openness in government economic policy and financial operations remain a brake on investment and growth. A number of International Monetary Fund (IMF) and World Bank missions have met with the new government to help it develop a coherent economic plan but associated reforms are on hold.
Faced with continued currency depreciation, the government resorted to more drastic measures and in January 1999 banned the widespread use of U.S. dollars for all domestic commercial transactions, a position it later adjusted. The government has been unable to provide foreign exchange for economic transactions, while it has resorted to printing money to finance its expenditure. Growth was negative in 2000 because of the difficulty of meeting the conditions of international donors, continued low prices of key exports, and post-coup instability. 125 companies in 2003 contributed to the conflict in DRC showing the corruption.
World Bank.
With the help of the International Development Association the DRC has worked toward the reestablishment of social services. This is done by giving 15 million people access to basic health services and giving bed nets to prevent malaria from spreading to people. With the Emergency Demobilization and Reintegration Program more than 107,000 adults and 34,000 child soldiers stood down their militarized posture. The travel time from Lubumbashi to Kasomeno in Katanga went down from seven days to two hours because of the improved roads which led to the decrease of prices of main goods by 60%. With the help of the IFC, KfW, and the EU the DRC improved its businesses by reducing the time it took to create a business by 51%, reducing the time it took to get construction permits by 54%, and reducing the number of taxes from 118 to 30. Improvements in health have been noticeable specifically that deliveries attended by trained staff jumped from 47 to 80%. In education 14 million textbooks were provided to children, completion rates of school have increased, and higher education was made available to students that chose to pursue it.
Ease of Doing Business Rank (EDBR).
The Democratic Republic of Congo ranks 183 on the low end of the ease of doing business scale as ranked by the World Bank. This measures the difficulties of starting a business, enforcing contracts, paying taxes, resolving insolvency, protecting investors, trading across borders, getting credit, getting electricity, dealing with construction permits and registering property (World Bank 2014:8).
International Monetary Fund.
The IMF plans on giving the DRC a $1 billion loan after its two-year suspension after it failed to give details about a mining deal from one of its state owned mines and an Israeli billionaire, Dan Gertler. The loan may be necessary for the country because there will be elections in December 2016 for the next president and the cost of funding this would range around $1.1 billion. The biggest problem with the vote is getting a country of 68 million people the size of Western Europe to polling stations with less than 1,860 miles of paved roads.
Sectors.
Agriculture.
Agriculture is the mainstay of the economy, accounting for 57.9% of the GDP in 1997. Main cash crops include coffee, palm oil, rubber, cotton, sugar, tea, and cocoa. Food crops include cassava, plantains, maize, groundnuts, and rice. In 1996, agriculture employed 66% of the work force.
Fishing.
The Democratic Republic of Congo also possesses 50 percent of Africa’s forests and a river system that could provide hydro-electric power to the entire continent, according to a United Nations report on the country’s strategic significance and its potential role as an economic power in central Africa. Fish are the single most important source of animal protein in the DRC. Total production of marine, river, and lake fisheries in 2003 was estimated at 222,965 tons, all but 5,000 tons from inland waters. PEMARZA, a state agency, carries on marine fishing.
Forestry.
Forests cover 60 percent of the total land area. There are vast timber resources, and commercial development of the country’s 61 million hectares (150 million acres) of exploitable wooded area is only beginning. The Mayumbe area of Bas-Congo was once the major center of timber exploitation, but forests in this area were nearly
depleted. The more extensive forest regions of the central cuvette and of the Ubangi River valley have increasingly been tapped.
Roundwood removals were estimated at 72,170,000 m in 2003, about 95 percent for fuel. Some 14 species are presently being harvested. Exports of forest products in 2003 totalled $25.7 million. Foreign capital is necessary in order for forestry to expand, and the government recognizes that changes in tax structure and export procedures will be needed to facilitate economic growth.
Mining.
Rich in minerals, the DRC has a difficult history of predatory mineral extraction, which has been at the heart of many struggles within the country for many decades, but particularly in the 1990s. Although the economy of the Democratic Republic of the Congo, the second largest country in Africa has historically relied heavily on mining, this is no longer reflected in the GDP data as the mining industry has suffered from long-term "uncertain legal framework, corruption, and a lack of transparency in government policy." The informal sector .
In her book entitled "The Real Economy of Zaire", MacGaffey described a second, often illegal economy, "system D," which is outside the official economy (MacGaffey 1991:27). and therefore is not reflected in the GDP.
exploitation of mineral substances as MIBA EMAXON and De Beers 
The economy of the second largest country in Africa relies heavily on mining. The Congo is the world's largest producer of cobalt ore, and a major producer of copper and industrial diamonds. The Congo has 70% of the world’s coltan, and more than 30% of the world’s diamond reserves., mostly in the form of small, industrial diamonds. The coltan is a major source of tantalum, which is used in the fabrication of electronic components in computers and mobile phones. In 2002, tin was discovered in the east of the country, but, to date, mining has been on a small scale.
Copper and Cobalt.
Katanga Mining Limited, a London-based company, owns the Luilu Metallurgical Plant, which has a capacity of 175,000 tonnes of copper and 8,000 tonnes of cobalt per year, making it the largest cobalt refinery in the world. After a major rehabilitation program, the company restarted copper production in December 2007 and cobalt production in May 2008.
Informal sector.
Much economic activity occurs in the informal sector and is not reflected in GDP data.
Transport.
Ground transport in the Democratic Republic of Congo has always been difficult. The terrain and climate of the Congo Basin present serious barriers to road and rail construction, and the distances are enormous across this vast country. Furthermore, chronic economic mismanagement and internal conflict has led to serious under-investment over many years.
On the other hand, the Democratic Republic of Congo has thousands of kilometres of navigable waterways, and traditionally water transport has been the dominant means of moving around approximately two-thirds of the country.

</doc>
<doc id="8026" url="https://en.wikipedia.org/wiki?curid=8026" title="Politics of the Democratic Republic of the Congo">
Politics of the Democratic Republic of the Congo

Politics of the Democratic Republic of Congo take place in a framework of a republic in transition from a civil war to a semi-presidential republic.
On 18 and 19 December 2005, a successful nationwide referendum was carried out on a draft constitution, which set the stage for elections in 2006. The voting process, though technically difficult due to the lack of infrastructure, was facilitated and organized by the Congolese Independent Electoral Commission with support from the UN mission to the Congo (MONUC). Early UN reports indicate that the voting was for the most part peaceful, but spurred violence in many parts of the war-torn east and the Kasais.
In 2006, many Congolese complained that the constitution was a rather ambiguous document and were unaware of its contents. This is due in part to the high rates of illiteracy in the country. However, interim President Kabila urged Congolese to vote 'Yes', saying the constitution is the country's best hope for peace in the future. 25 million Congolese turned out for the two-day balloting. According to results released in January 2006, the constitution was approved by 84% of voters. . The new constitution also aims to decentralize authority, dividing the vast nation into 25 semi-autonomous provinces, drawn along ethnic and cultural lines.
The country's first democratic elections in four decades were held on 30 July 2006, with a run-off between the incumbent, President Kabila, and his rival Bemba held on 29 October 2006. Polling was once again facilitated - yet not run - by UN peacekeepers. .
Political history.
From the day King Leopold II established colonial authority in what is now the Democratic Republic of Congo to today, the country's government has been unstable. This is reflected in its seven name changes since 1885:
From the day of the arguably ill-prepared independence of the Democratic Republic of the Congo, the tensions between the powerful leaders of the political elite, such as Joseph Kasa Vubu, Patrice Lumumba, Moise Tshombe, Joseph Mobutu and others, jeopardize the political stability of the new state. From Tshombe's secession of the Katanga, to the assassination of Lumumba, to the two coups d'état of Mobutu, the country has known periods of true nationwide peace, but virtually no period of genuine democratic rule.
The Mobutu era.
The Regime of Marshall Mobutu Sese Seko lasted 32 years (1965–1997), during which all but the first seven years the country was named Zaire. The dictatorial regime operated as a one-party-state, which saw most of the powers concentrated between President Mobutu, who was simultaneously the head of the state-party (Popular Movement of the Revolution), and a series of essentially rubber-stamping institutions.
One particularity of the Regime was the claim to be thriving for an "authentic" system, different from Western, or Soviet influences. This lasted roughly between the establishment of Zaire in 1971, and the official beginning of the transition towards democracy, on 24 April 1990. This was true at the regular people's level as everywhere else. People were ordered by law to drop their Western Christian names; the titles Mr. and Mrs. were abandoned for the male and female versions of the French word for "citizen"; Men were forbidden to wear suits, and women to wear pants. At the institutional level, many of the institutions also changed denominations, but the end result was a system that borrowed from both systems:
Every corporation, whether financial or union, as well as every division of the administration, were set up as branches of the party, the CEOs, Union leaders, and division directors being sworn-in as section presidents of the party. Every aspect of life was regulated to some degree by the party, and the will of its founding-president, Mobutu Sese Seko.
Most of the petty aspects of the regime disappeared after 1990, and the beginning of the democratic transition. The latter was intended to be fairly short-lived, but Mobutu's power plays dragged it in length, to ultimately 1997, when the forces-led by Laurent Kabila eventually toppled the regime, after a 9-month-long successful military campaign.
The Kabilas' governments and war.
The government of former president Mobutu Sese Seko was toppled by a rebellion led by Laurent Kabila in May 1997, with the support of Rwanda and Uganda. They were later to turn against Kabila and backed a rebellion against him in August 1998. Troops from Zimbabwe, Angola, Namibia, Chad, and Sudan intervened to support the Kinshasa regime. A cease-fire was signed on 10 July 1999 by the DROC, Zimbabwe, Angola, Uganda, Namibia, Rwanda, and Congolese armed rebel groups, but fighting continued.
Under Laurent Kabila's regime, all executive, legislative, and military powers were first vested in the President, Laurent-Désiré Kabila. The judiciary was independent, with the president having the power to dismiss or appoint. The president was first head of a 26-member cabinet dominated by the Alliance of Democratic Forces for the Liberation of Congo (ADFL). Towards the end of the 90s, Laurent Kabila created and appointed a Transitional Parliament, with a seat in the buildings of the former Katanga Parliament, in the southern town of Lubumbashi, in a move to unite the country, and to legitimate his regime. Kabila was assassinated on 16 January 2001 and his son Joseph Kabila was named head of state ten days later.
The younger Kabila continued with his father's Transitional Parliament, but overhauled his entire cabinet, replacing it with a group of technocrats, with the stated aim of putting the country back on the track of development, and coming to a decisive end of the Second Congo War. In October 2002, the new president was successful in getting occupying Rwandan forces to withdraw from eastern Congo; two months later, an agreement was signed by all remaining warring parties to end the fighting and set up a Transition Government, the make-up of which would allow representation for all negotiating parties. Two founding documents emerged from this: The , and the Global and Inclusive Agreement, both of which describe and determine the make-up and organization of the Congolese institutions, until planned elections in July 2006, at which time the provisions of the new constitution, democratically approved by referendum in December 2005, will take full effect and that is how it happened.
Under the Global and All-Inclusive Agreement, signed on 17 December 2002, in Pretoria, there was to be one President and four Vice-Presidents, one from the government, one from the Rally for Congolese Democracy, one from the MLC, and one from civil society. The position of Vice-President expired after the 2006 elections.
Present situation.
After being for three years (2003–06) in the interregnum between two constitutions, the Democratic Republic of the Congo is now under the regime of the Constitution of the Third Republic. The constitution, adopted by referendum in 2005, and promulgated by President Joseph Kabila in February 2006, establishes a decentralized semi-presidential republic, with a separation of powers between the three branches of government - executive, legislative and judiciary, and a distribution of prerogatives between the central government and the provinces.
Executive branch.
Since the July 2006 elections, the country is led by a semi-presidential, strongly-decentralized state. The executive at the central level, is divided between the President, and a Prime Minister appointed by him/her from the party having the majority of seats in Parliament. Should there be no clear majority, the President can appoint a "government former" that will then have the task to win the confidence of the National Assembly. The President appoints the government members (ministers) at the proposal of the Prime Minister. In coordination, the President and the government have the charge of the executive. The Prime minister and the government are responsible to the lower-house of Parliament, the National Assembly.
At the province level, the Provincial legislature (Provincial Assembly) elects a governor, and the governor, with his government of up to 10 ministers, is in charge of the provincial executive. Some domains of government power are of the exclusive provision of the Province, and some are held concurrently with the Central government. This is not a Federal state however, simply a decentralized one, as the majority of the domains of power are still vested in the Central government. The governor is responsible to the Provincial Assembly.
Criticisms.
The semi-presidential system has been described by some as "conflictogenic" and "dictatogenic", as it ensures frictions, and a reduction of pace in government life, should the President and the Prime Minister be from different sides of the political arena. This was seen several times in France, a country that shares the semi-presidential model. It was also, arguably, in the first steps of the Congo into independence, the underlying cause of the crisis between Prime Minister Patrice Lumumba and President Joseph Kasa Vubu, who ultimately dismissed each other, in 1960.
In January 2015 the 2015 Congolese protests broke out in the country's capital following the release of a draft law that would extend the presidential term limits and allow Joseph Kabila to run again for office.
Legislative branch.
Under the Transition Constitution.
The Inter-Congolese dialogue, that set-up the transitional institutions, created a bicameral parliament, with a National Assembly and Senate, made up of appointed representatives of the parties to the dialogue. These parties included the preceding government, the rebel groups that were fighting against the government, with heavy Rwandan and Ugandan support, the internal opposition parties, and the Civil Society. At the beginning of the transition, and up until recently, the National Assembly is headed by the MLC with Speaker Hon. Olivier Kamitatu, while the Senate is headed by a representative of the Civil Society, namely the head of the Church of Christ in Congo, Mgr. Pierre Marini Bodho. Hon. Kamitatu has since left both the MLC and the Parliament to create his own party, and ally with current President Joseph Kabila. Since then, the position of Speaker is held by Hon. Thomas Luhaka, of the MLC.
Aside from the regular legislative duties, the Senate had the charge to draft a new constitution for the country. That constitution was adopted by referendum in December 2005, and decreed into law on 18 February 2006.
Under the New Constitution.
The Parliament of the third republic is also bicameral, with a National Assembly and a Senate. Members of the National Assembly, the lower - but the most powerful - house, are elected by direct suffrage. Senators are elected by the legislatures of the 26 provinces.
Judicial branch.
Under the New Constitution.
The Congolese Judicial Branch Consists of a Supreme Court, which handles federal crimes.
Administrative divisions.
Under the Transition Constitution.
10 provinces (provinces, singular - province) and one city* (ville): Bandundu, Bas-Congo, Équateur, Kasai-Occidental, Kasai-Oriental, Katanga, Kinshasa*, Maniema, North Kivu, Orientale.
Each province is divided into districts.
Under the New Constitution.
25 provinces (provinces, singular - province) and city* (ville): Bas-Uele | Équateur | Haut-Lomami | Haut-Katanga | Haut-Uele | Ituri | Kasaï | Kasaï oriental | Kongo central | Kwango | Kwilu | Lomami | Lualaba | Lulua | Mai-Ndombe | Maniema | Mongala | North Kivu | Nord-Ubangi | Sankuru | South Kivu | Sud-Ubangi | Tanganyika | Tshopo | Tshuapa | Kinshasa*
International organization participation.
ACCT, ACP, AfDB, AU, CEEAC, CEPGL, ECA, FAO, G-19, G-24, G-77, IAEA, IBRD, ICAO, ICC, ICRM, IDA, IFAD, IFC, IFRCS, IHO, ILO, IMF, International Maritime Organization, Intelsat, Interpol, IOC, IOM, ITU, ITUC, NAM, OPCW (signatory), PCA, SADC, UN, UNCTAD, UNESCO, UNHCR, UNIDO, UPU, WCO WFTU, WHO, WIPO, WMO, WToO, WTrO

</doc>
<doc id="8027" url="https://en.wikipedia.org/wiki?curid=8027" title="Telecommunications in the Democratic Republic of the Congo">
Telecommunications in the Democratic Republic of the Congo

Telecommunications in the Democratic Republic of the Congo include radio, television, fixed and mobile telephones, and the Internet.
Radio is the dominant medium; a handful of stations, including state-run Radio-Télévision Nationale Congolaise (RTNC), broadcast across the country. The United Nations Mission (MONUSCO) and a Swiss-based NGO, Fondation Hirondelle, operate one of country's leading stations, Radio Okapi. The network employs mostly-Congolese staff and aims to bridge political divisions. Radio France Internationale (RFI), which is widely available on FM, is the most popular news station. The BBC broadcasts on FM in Kinshasa (92.7), Lubumbashi (92.0), Kisangani (92.0), Goma (93.3) and Bukavu (102.2).

</doc>
<doc id="8028" url="https://en.wikipedia.org/wiki?curid=8028" title="Transport in the Democratic Republic of the Congo">
Transport in the Democratic Republic of the Congo

Ground transport in the Democratic Republic of Congo (DRC) has always been difficult. The terrain and climate of the Congo Basin present serious barriers to road and rail construction, and the distances are enormous across this vast country. Furthermore, chronic economic mismanagement and internal conflict has led to serious under-investment over many years.
On the other hand, the DRC has thousands of kilometres of navigable waterways, and traditionally water transport has been the dominant means of moving around approximately two-thirds of the country.
Transport problems.
As an illustration of transport difficulties in the DRC, even before wars damaged the infrastructure, the so-called "national" route, used to get supplies to Bukavu from the seaport of Matadi, consisted of the following:
In other words, goods had to be loaded and unloaded eight times and the total journey would take many months.
Many of the routes listed below are in poor condition and may be operating at only a fraction of their original capacity (if at all), despite recent attempts to make improvements. Up to 2006 the United Nations Joint Logistics Centre (UNJLC) had an operation in Congo to support humanitarian relief agencies working there, and its bulletins and maps about the transport situation are archived on the UNJLC web site.
The First and Second Congo Wars saw great destruction of transport infrastructure from which the country has not yet recovered. Many vehicles were destroyed or commandeered by militias, especially in the north and east of the country, and the fuel supply system was also badly affected. Consequently, outside of Kinshasa, Matadi and Lubumbashi, private and commercial road transport is almost non-existent and traffic is scarce even where roads are in good condition. The few vehicles in use outside these cities are run by the United Nations, aid agencies, the DRC government, and a few larger companies such as those in the mining and energy sectors. It is notable that high-resolution satellite photos on the Internet show large cities such as Bukavu, Butembo and Kikwit virtually devoid of traffic, compared to similar photos of towns in neighbouring countries.
Air transport is the only effective means of moving between many places within the country. The Congolese government, the United Nations, aid organisations and large companies use air rather than ground transport to move personnel and freight. The UN operates a large fleet of aircraft and helicopters, and compared to other African countries the DRC has a large number of small domestic airlines and air charter companies. The transport (and smuggling) of minerals with a high value for weight is also carried out by air, and in the east, some stretches of paved road isolated by destroyed bridges or impassable sections have been turned into airstrips.
For the ordinary citizen though, especially in rural areas, often the only options are to cycle, walk or go by dugout canoe.
Some parts of the DRC are more accessible from neighbouring countries than from Kinshasa. For example Bukavu itself and Goma and other north-eastern towns are linked by paved road from the DRC border to the Kenyan port of Mombasa, and most goods for these cities have been brought via this route in recent years. Similarly, Lubumbashi and the rest of Katanga Province is linked to Zambia, through which the paved highway and rail networks of Southern Africa can be accessed. Such links through neighbouring countries are generally more important for the east and south-east of the country, and are more heavily used, than surface links to the capital.
Major infrastructure programs.
In 2007 China agreed to lend the DRC US$5bn for two major transport infrastructure projects to link mineral-rich Katanga, specifically Lubumbashi, by rail to an ocean port (Matadi) and by road to the Kisangani river port, and to improve its links to the transport network of Southern Africa in Zambia. The two projects would also link the major parts of the country not served by water transport, and the main centres of the economy. Loan repayments will be from concessions for raw materials which China desperately needs: copper, cobalt, gold and nickel, as well as by toll revenues from the road and railway. In the face of reluctance by the international business community to invest in DRC, this represents a revitalisation of DRC's infrastructure much needed by its government.
The China Railway Seventh Group Co. Ltd will be in charge of the contract, under signed by the China Railway Engineering Corporation, with construction to be started from June 2008.
Highways.
The Democratic Republic of the Congo has fewer all-weather paved highways than any country of its population and size in Africa — a total of 2250 km, of which only 1226 km is in good condition (see below). To put this in perspective, the road distance across the country in any direction is more than 2500 km (e.g. Matadi to Lubumbushi, 2700 km by road). The figure of 2250 km converts to 35 km of paved road per 1,000,000 of population. Comparative figures for Zambia and Botswana are 721 km and 3427 km respectively.
Categories.
The road network is theoretically divided into four categories (national roads, priority regional roads, secondary regional roads and local roads), however, the United Nations Joint Logistics Centre (UNJLC) reports that this classification is of little practical use because some roads simply do not exist. For example, National Road 9 is not operational and cannot be detected by remote sensing methods.
The two principal highways are:
Inventory.
The total road network in 2005, according to the UNJLC, consisted of:
The UNJLC also points out that the pre-Second Congo War network no longer exists, and is dependent upon 20,000 bridges and 325 ferries, most of which are in need of repair or replacement. In contrast, a Democratic Republic of the Congo government document shows that, also in 2005, the network of main highways in good condition was as follows:
The 2000 Michelin "Motoring and Tourist Map 955 of Southern and Central Africa", which categorizes roads as "surfaced", "improved" (generally unsurfaced but with gravel added and graded), "partially improved" and "earth roads" and "tracks" shows that there were 2694 km of paved highway in 2000. These figures indicate that, compared to the more recent figures above, there has been a deterioration this decade, rather than improvement.
International highways.
Three routes in the Trans-African Highway network pass through DR Congo:
Waterways.
The DRC has more navigable rivers and moves more passengers and goods by boat and ferry than any other country in Africa. Kinshasa, with 7 km of river frontage occupied by wharfs and jetties, is the largest inland waterways port on the continent. However, much of the infrastructure — vessels and port handling facilities — has, like the railways, suffered from poor maintenance and internal conflict.
The total length of waterways is estimated at 15,000 km including the Congo River, its tributaries, and unconnected lakes.
The 1000-kilometre Kinshasa-Kisangani route on the Congo River is the longest and best-known. It is operated by river tugs pushing several barges lashed together, and for the hundreds of passengers and traders these function like small floating towns. Rather than mooring at riverside communities along the route, traders come out by canoe and small boat alongside the river barges and transfer goods on the move.
Most waterway routes do not operate to regular schedules. It is common for an operator to moor a barge at a riverside town and collect freight and passengers over a period of weeks before hiring a river tug to tow or push the barge to its destination.
Domestic links via inland waterways.
The middle Congo River and its tributaries from the east are the principal domestic waterways in the DRC. The two principal river routes are:
See the diagrammatic transport map above for other river waterways.
The most-used domestic lake waterways are:
Most large Congo river ferry boats were destroyed during the civil war. Only smaller boats are running and they are irregular.
Pipelines.
petroleum products 390 km
Merchant marine.
1 petroleum tanker
Airports.
Kemal Saiki, a United Nations spokesman, said that the Democratic Republic of the Congo does not "even have 2,000 miles of roads" and that many people traveling around the country fly on aircraft.
The main passenger airlines of the country are flyCongo, Compagnie Africaine d'Aviation and Korongo Airlines. All of their hubs are at Kinshasa's N'djili Airport
The country had 229 airports in 2002 and 232 around 1999.
Airports - with paved runways.
<br>"total:"
24
<br>"over 3,047 m:"
4
<br>"2,438 to 3,047 m:"
2
<br>"1,524 to 2,437 m:"
16
<br>"914 to 1,523 m:"
2 (2002 est.)
Airports - with unpaved runways.
<br>"total:"
205
<br>"1,524 to 2,437 m:"
19
<br>"914 to 1,523 m:"
95
<br>"under 914 m:"
91 (2002 est.)
Transport safety and incidents.
All air carriers certified by the Democratic Republic of the Congo have been banned from operating at airports in the European Community by the European Commission because of inadequate safety standards.
Rocketry.
The Democratic Republic of the Congo has a rocketry program called Troposphere.

</doc>
<doc id="8029" url="https://en.wikipedia.org/wiki?curid=8029" title="Armed Forces of the Democratic Republic of the Congo">
Armed Forces of the Democratic Republic of the Congo

The Armed Forces of the Democratic Republic of Congo () is the state organisation responsible for defending the Democratic Republic of the Congo. The FARDC is being rebuilt as part of the peace process which followed the end of the Second Congo War in July 2003.
The majority of FARDC members are land forces, but it also has a small air force and an even smaller navy. Together the three services may number between 144,000 and 159,000 personnel. In addition, there is a presidential force called the Republican Guard, but it and the National Congolese Police (PNC) are not part of the Armed Forces.
The government in the capital city Kinshasa, the United Nations, the European Union, and bilateral partners which include Angola, South Africa, and Belgium are attempting to create a viable force with the ability to provide the Democratic Republic of Congo with stability and security. However, this process is being hampered by corruption, inadequate donor coordination, and competition between donors. The various military units now grouped under the FARDC banner are some of the most unstable in Africa after years of war and underfunding.
To assist the new government, since February 2000 the United Nations has had the United Nations Mission in the Democratic Republic of Congo (now called MONUSCO), which currently has a strength of over 16,000 peacekeepers in the country. Its principal tasks are to provide security in key areas, such as the South Kivu and North Kivu in the east, and to assist the government in reconstruction. Foreign rebel groups are also in the Congo, as they have been for most of the last half-century. The most important is the Democratic Forces for the Liberation of Rwanda (FDLR), against which Laurent Nkunda's troops were fighting, but other smaller groups such as the anti-Ugandan Lord's Resistance Army are also present.
The legal standing of the FARDC was laid down in the Transitional Constitution, articles 118 and 188. This was then superseded by provisions in the 2006 Constitution, articles 187 to 192. Law 04/023 of November 12, 2004 establishes the General Organisation of Defence and the Armed Forces. In mid-2010, the Congolese Parliament was debating a new defence law, provisionally designated Organic Law 130.
History.
The first organized Congolese troops, known as the , were created in 1888 when King Leopold II of Belgium, who held the Congo Free State as his private property, ordered his Secretary of the Interior to create military and police forces for the state. In 1908, under international pressure, Leopold ceded administration of the colony to the government of Belgium as the Belgian Congo. It remained under the command of a Belgian officer corps through to the independence of the colony in 1960. The "Force Publique" saw combat in Cameroun, and successfully invaded and conquered areas of German East Africa, notably present day Rwanda, during World War I. Elements of the "Force Publique" were also used to form Belgian colonial units that fought in the East African Campaign during World War II.
At independence on 30 June 1960, the army suffered from a dramatic deficit of trained leaders, particularly in the officer corps. This was because the "Force Publique" had always only been officered by Belgian or other expatriate whites. The Belgian Government made no effort to train Congolese commissioned officers until the very end of the colonial period and there were only about 20 African cadets in training on the eve of independence. Ill-advised actions by Belgian officers led to an enlisted ranks' rebellion on 5 July 1960, which helped spark the Congo Crisis. Lieutenant General Émile Janssens, the "Force Publique" commander, wrote during a meeting of soldiers that 'Before independence=After Independence', pouring cold water on the soldiers' desires for an immediate raise in their status.
Vanderstraeten says that on the morning of 8 July 1960, following a night during which all control had been lost over the soldiers, numerous ministers arrived at Camp Leopold with the aim of calming the situation. Both Lumumba and Kasa-Vubu eventually arrived, and the soldiers listened to Kasa-Vubu "religiously." After his speech, Kasa-Vubu and the ministers present retired into the camp canteen to hear a delegation from the soldiers. Vanderstraeten says that, according to Joseph Ileo, their demands ("revendications") included the following:
The "laborious" discussions which then followed were later retrospectively given the label of an "extraordinary ministerial council." Gérald-Libois writes that '..the special meeting of the council of ministers took steps for the immediate Africanisation of the officer corps and ..named Victor Lundula, who was born in Kasai and was burgomaster of Jadotville, as Commander-in-Chief of the "Armée Nationale Congolaise" (ANC); Colonel Joseph-Désiré Mobutu as chief of staff; and the Belgian, Colonel Henniquiau, as chief advisor to the ANC.' Thus General Janssens was dismissed. Both Lundula and Mobutu were former sergeants of the "Force Publique". It appears that Maurice Mpolo, Minister of Youth and Sports, was given the defence portfolio.
On 8–9 July 1960, the soldiers were invited to appoint black officers, and 'command of the army passed securely into the hands of former sergeants,' as the soldiers in general chose the most-educated and highest-ranked Congolese army soldiers as their new officers. Most of the Belgian officers were retained as advisors to the new Congolese hierarchy, and calm returned to the two main garrisons at Leopoldville and Thysville. The "Force Publique" was renamed the "Armée nationale congolaise" (ANC), or Congolese National Armed Forces. However, in Katanga Belgian officers resisted the Africanisation of the army.
On 9 July 1960, there was an "Force Publique" mutiny at Camp Massart at Elizabethville; five or seven Europeans were killed. The army revolt and resulting rumours caused severe panic across the country, and Belgium despatched troops and the naval Task Group 218.2 to protect its citizens. Belgian troops intervened in Elisabethville and Luluabourg (10 July), Matadi (11 July), Leopoldville (13 July) and elsewhere. There were immediate suspicions that Belgium planned to re-seize the country while doing so. Large numbers of Belgian colonists fled the country. At the same time, on 9 July, Albert Kalonji proclaimed the independence of South Kasai. Two days later on 11 July, Moise Tshombe declared the independence of Katanga province in the south-east, closely backed by remaining Belgian administrators and soldiers.
On 14 July 1960, in response to requests by Prime Minister Lumumba, the UN Security Council adopted United Nations Security Council Resolution 143. This called upon Belgium to remove its troops and for the UN to provide 'military assistance' to the Congolese forces to allow them 'to meet fully their tasks'. Lumumba demanded that Belgium remove its troops immediately, threatening to seek help from the Soviet Union if they did not leave within two days. The UN reacted quickly and established the United Nations Operation in the Congo (ONUC). The first UN troops arrived the next day but there was instant disagreement between Lumumba and the UN over the new force's mandate. Because the Congolese army had been in disarray since the mutiny, Lumumba wanted to use the UN troops to subdue Katanga by force. Referring to the resolution, Lumumba wrote to UN Secretary General Dag Hammarskjöld, 'From these texts it is clear that, contrary to your personal interpretation, the UN force may be used to subdue the rebel government of Katanga.' Secretary General Hammarskjöld refused. To Hammarskjöld, the secession of Katanga was an internal Congolese matter and the UN was forbidden to intervene by Article 2 of the United Nations Charter. Disagreements over what the UN force could and could not do continued throughout its deployment.
The last Belgian troops left the country by 23 July, as United Nations forces continued to deploy throughout the Congo.
During the crucial period of July–August 1960, Joseph-Désiré Mobutu built up "his" national army by channeling foreign aid to units loyal to him, by exiling unreliable units to remote areas, and by absorbing or dispersing rival armies. He tied individual officers to him by controlling their promotion and the flow of money for payrolls. Researchers working from the 1990s have concluded that money was directly funnelled to the army by the U.S. Central Intelligence Agency, the UN, and Belgium. Despite this, by September 1960, following the four-way division of the country, there were four separate armed forces: Mobotu's ANC itself, numbering about 12,000, the South Kasai Constabulary loyal to Albert Kalonji (3,000 or less), the Katanga Gendarmerie which were part of Moise Tshombe's regime (totalling about 10,000), and the Stanleyville dissident ANC loyal to Antoine Gizenga (numbering about 8,000).
In August 1960, due to rejection of requests to the UN for aid to suppress the South Kasai and Katanga revolts, Lumumba's government decided to request Soviet help. de Witte writes that 'Leopoldville asked the Soviet Union for planes, lorries, arms, and equipment. .. Shortly afterwards, on 22 or 23 August, about 1,000 soldiers left for Kasai.' de Witte goes on to write that on 26–27 August, the ANC seized Bakwanga, Albert Kalonji's capital in South Kasai, without serious resistance. 'In the next two days it temporarily put an end to the secession of Kasai.'
The Library of Congress Country Study for the Congo says at this point that:
"n 5 September 196 Kasavubu also appointed Mobutu as head of the ANC. Joseph Ileo was chosen as the new prime minister and began trying to form a new government. Lumumba and his cabinet responded by accusing Kasa-Vubu of high treason and voted to dismiss him. Parliament refused to confirm the dismissal of either Lumumba or Kasavubu and sought to bring about a reconciliation between them. After a week's deadlock, Mobutu announced on September 14 that he was assuming power until December 31, 1960, in order to "neutralize" both Kasavubu and Lumumba."
In early January 1961, ANC units loyal to Lumumba invaded northern Katanga to support a revolt of Baluba tribesmen against Tshombe's secessionist regime.
United Nations Security Council Resolution 161 of 21 February 1961, called for the withdrawal of Belgian officers from command positions in the ANC, and the training of new Congolese officers with UN help. The various efforts made by ONUC to retrain the ANC from August 1960 to their effective end in June 1963 are described in Arthur House's book The UN in the Congo : The Civilian Operations, pages 145-155. By March 1963 however, after the visit of Colonel Michael Greene of the United States Army, and the resulting 'Greene Plan,' the pattern of bilaterally agreed military assistance to various Congolese military components, instead of a single unified effort, was already taking shape.
In early 1964, a new crisis broke out as Congolese rebels calling themselves "Simba" (Swahili for "Lion") rebelled against the government. They were led by Pierre Mulele, Gaston Soumialot and Christophe Gbenye who were former members of Gizenga's Parti Solidaire Africain (PSA). The rebellion affected Kivu and Eastern (Orientale) provinces. By August they had captured Stanleyville and set up a rebel government there. As the rebel movement spread, discipline became more difficult to maintain, and acts of violence and terror increased. Thousands of Congolese were executed, including government officials, political leaders of opposition parties, provincial and local police, school teachers, and others believed to have been Westernized. Many of the executions were carried out with extreme cruelty, in front of a monument to Lumumba in Stanleyville. Tshombe decided to use foreign mercenaries as well as the ANC to suppress the rebellion. Mike Hoare was employed to created the English-speaking 5 Commando ANC at Kamina, with the assistance of a Belgian officer, Colonel Frederic Vanderwalle, while 6 Commando ANC was French-speaking and originally under the command of a Belgian Army colonel, Lamouline. By August 1964, the mercenaries, with the assistance of other ANC troops, were making headway against the Simba rebellion. Fearing defeat, the rebels started taking hostages of the local white population in areas under their control. These hostages were rescued in Belgian airdrops (Dragon Rouge and Dragon Noir) over Stanleyville and Paulis with U.S. airlift support. The operation coincided with the arrival of mercenary units (seemingly including the hurriedly formed 5th Mechanised Brigade) at Stanleyville which was quickly captured. It took until the end of the year to completely put down the remaining areas of rebellion.
After five years of turbulence, in 1965 Mobutu used his position as ANC Chief of Staff to seize power in the Congo. Although Mobutu succeeded in taking power, his position was soon threatened by the Kisangani Mutinies, also known as the Stanleyville Mutinies or Mercenaries' Mutinies, which were eventually suppressed.
As a general rule, since that time, the armed forces have not intervened in politics as a body, rather being tossed and turned as ambitious men have shaken the country. In reality, the larger problem has been the misuse and sometimes abuse of the military and police by political and ethnic leaders.
On 16 May 1968 a parachute brigade of two regiments (each of three battalions) was formed which eventually was to grow in size to a full division.
Zaire 1971–1997.
The country was renamed Zaire in 1971 and the army was consequently designated the (FAZ). In 1971 the army's force consisted of the 1st Groupement at Kananga, with one guard battalion, two infantry battalions, and a gendarmerie battalion attached, and the 2nd Groupement (Kinshasa), the 3rd Groupement (Kisangani), the 4th Groupement (Lubumbashi), the 5th Groupement (Bukavu), the 6th Groupement (Mbandaka), and the 7th Groupement (Boma). Each was about the size of a brigade, and commanded by 'aging generals who have had no military training, and often not much positive experience, since they were NCOs in the Belgian Force Publique.' By the late 1970s the number of groupements reached nine, one per administrative region. The parachute division (Division des Troupes Aéroportées Renforcées de Choc, DITRAC) operated semi-independently from the rest of the army.
In July 1972 a number of the aging generals commanding the "groupements" were retired. Général d'armée Louis Bobozo, and Generaux de Corps d'Armée Nyamaseko Mata Bokongo, Nzoigba Yeu Ngoli, Muke Massaku, Ingila Grima, Itambo Kambala Wa Mukina, Tshinyama Mpemba, and General de Division Yossa Yi Ayira, the last having been commander of the Kamina base, were all retired on 25 July 1972. Taking over as military commander-in-chief, now titled Captain General, was newly promoted General de Division Bumba Moaso, former commander of the parachute division.
A large number of countries supported the FAZ in the early 1970s. Three hundred Belgian personnel were serving as staff officers and advisors throughout the Ministry of Defence, Italians were supporting the Air Force, Americans were assisting with transport and communications, Israelis with airborne forces training, and there were British advisors with the engineers.
On 11 June 1975 several military officers were arrested in what became known as the "coup monté et manqué." Amongst those arrested were Générals Daniel KATSUVA wa Katsuvira, Land Forces Chief of Staff, UTSHUDI Wembolenga, Commandant of the 2nd Military Region at Kalemie; FALLU Sumbu, Military Attaché of Zaïre in Washington, Colonel MUDIAYI wa Mudiayi, the military attaché of Zaïre in Paris, the military attache in Brussels, a paracommando battalion commander, and several others. The regime alleged these officers and others (including Mobutu's civil "secrétaire particulier") had plotted the assassination of Mobutu, high treason, and disclosure of military secrets, among other offences. The alleged coup was investigated by a revolutionary commission headed by Boyenge Mosambay Singa, at that time head of the Gendarmerie. Writing in 1988, Michael Schatzberg said the full details of the coup had yet to emerge.
In late 1975, Mobutu, in a bid to install a pro-Kinshasa government in Angola and thwart the Marxist Popular Movement for the Liberation of Angola (MPLA)'s drive for power, deployed FAZ armored cars, paratroopers, and three infantry battalions to Angola in support of the National Liberation Front of Angola (FNLA).
On 10 November 1975, an anti-Communist force made up of 1,500 FNLA fighters, 100 Portuguese Angolan soldiers, and two FAZ battalions passed near the city of Quifangondo, only 30 km north of Luanda, at dawn on 10 November. The force, supported by South African aircraft and three 140 mm artillery pieces, marched in a single line along the Bengo River to face an 800-strong Cuban force across the river. Thus the Battle of Quifangondo began. The Cubans and MPLA fighters bombarded the FNLA with mortar and 122 mm rockets, destroying most of the FNLA's armored cars and six Jeeps carrying antitank rockets in the first hour of fighting.
Mobutu's support for the FNLA policy backfired when the MPLA won in Angola. The MPLA, then, acting ostensibly at least as the (Front for the National Liberation of the Congo), occupied Zaire's Katanga Province, then known as Shaba, in March 1977, facing little resistance from the FAZ. This invasion is sometimes known as Shaba I. Mobutu had to request assistance, which was provided by Morocco in the form of regular troops who routed the MPLA and their Cuban advisors out of Katanga. The humiliation of this episode led to civil unrest in Zaire in early 1978, which the FAZ had to put down.
The poor performance of Zaire's military during Shaba I gave evidence of chronic weaknesses (which extend to this day). One problem was that some of the Zairian soldiers in the area had not received pay for extended periods. Senior officers often kept the money intended for the soldiers, typifying a generally disreputable and inept senior leadership in the FAZ. As a result, many soldiers simply deserted rather than fight. Others stayed with their units but were ineffective. During the months following the Shaba invasion, Mobutu sought solutions to the military problems that had contributed to the army's dismal performance. He implemented sweeping reforms of the command structure, including wholesale firings of high-ranking officers. He merged the military general staff with his own presidential staff and appointed himself chief of staff again, in addition to the positions of minister of defence and supreme commander that he already held. He also redeployed his forces throughout the country instead of keeping them close to Kinshasa, as had previously been the case. The Kamanyola Division, at the time considered the army's best formation, and considered the president's own, was assigned permanently to Shaba. In addition to these changes, the army's strength was reduced by 25 percent. Also, Zaire's allies provided a large influx of military equipment, and Belgian, French, and American advisers assisted in rebuilding and retraining the force.
Despite these improvements, a second invasion by the former Katangan gendarmerie, known as Shaba II in May–June 1978, was only dispersed with the despatch of the French 2e régiment étranger de parachutistes and a battalion of the Belgian Paracommando Regiment. Kamanyola Division units collapsed almost immediately. French units fought the Battle of Kolwezi to recapture the town from the FLNC. The U.S. provided logistical assistance.
In July 1975, according to the IISS Military Balance, the FAZ was made up of 14 infantry battalions, seven "Guard" battalions, and seven other infantry battalions variously designated as "parachute" (or possibly "commando"; probably the units of the new parachute brigade originally formed in 1968). There were also an armored car regiment and a mechanized infantry battalion. Organisationally, the army was made up of seven brigade groups and one parachute division. In addition to these units, a tank battalion was reported to have formed by 1979.
In January 1979 "General de Division" Boyenge Mosambay Singa was named as both military region commander and Region Commissioner for Shaba.
In 1984, a militarised police force, the Guard Civile, was formed. It was eventually commanded by Général d'armée Kpama Baramoto Kata.
Further details of FAZ operations in the 1980s and onwards can be found in John W. Turner's book "A Continent Ablaze."
Thomas Turner wrote in the late 1990s that.. 'ajor acts of violence, such as the killings that followed the 'Kasongo uprising' in Bandundu Region in 1978, the killings of diamond miners in Kasai-Oriental Region in 1979, and, more recently, the massacre of students in Lubumbashi in 1990, continued to intimidate the population.'
The authors of the Library of Congress Country Study on Zaire commented in 1992-93 that: "The maintenance status of equipment in the inventory has traditionally varied, depending on a unit's priority and the presence or absence of foreign advisers and technicians. A considerable portion of military equipment is not operational, primarily as a result of shortages of spare parts, poor maintenance, and theft. For example, the tanks of the 1st Armored Brigade often have a nonoperational rate approaching 70 to 80 percent. After a visit by a Chinese technical team in 1985, most of the tanks operated, but such an improved status generally has not lasted long beyond the departure of the visiting team. Several factors complicate maintenance in Zairian units. Maintenance personnel often lack the training necessary to maintain modern military equipment. Moreover, the wide variety of military equipment and the staggering array of spare parts necessary to maintain it not only clog the logistic network but also are expensive.
The most important factor that negatively affects maintenance is the low and irregular pay that soldiers receive, resulting in the theft and sale of spare parts and even basic equipment to supplement their meager salaries. When not stealing spare parts and equipment, maintenance personnel often spend the better part of their duty day looking for other ways to profit. American maintenance teams working in Zaire found that providing a free lunch to the work force was a good, sometimes the only, technique to motivate personnel to work at least half of the duty day.
The army's logistics corps is to provide logistic support and conduct direct, indirect, and depot-level maintenance for the FAZ. But because of Zaire's lack of emphasis on maintenance and logistics, a lack of funding, and inadequate training, the corps is understaffed, underequipped, and generally unable to accomplish its mission. It is organized into three battalions assigned to Mbandaka, Kisangani, and Kamina, but only the battalion at Kamina is adequately staffed; the others are little more than skeleton" units.
The poor state of discipline of the Congolese forces became apparent again in 1990. Foreign military assistance to Zaire ceased following the end of the Cold War and Mobutu deliberately allowed the military's condition to deteriorate so that it did not threaten his hold on power. Protesting low wages and lack of pay, paratroopers began looting Kinshasa in September 1991 and were only stopped after intervention by French ('Operation Baumier') and Belgian ('Operation Blue Beam') forces.
In 1993, according to the Library of Congress Country Studies, the 25,000-member FAZ ground forces consisted of one infantry division (with three infantry brigades); one airborne brigade (with three parachute battalions and one support battalion); one special forces (commando/counterinsurgency) brigade; the Special Presidential Division; one independent armored brigade; and two independent infantry brigades (each with three infantry battalions, one support battalion). These units were deployed throughout the country, with the main concentrations in Shaba Region (approximately half the force). The Kamanyola Division, consisting of three infantry brigades operated generally in western Shaba Region; the 21st Infantry Brigade was located in Lubumbashi; the 13th Infantry Brigade was deployed throughout eastern Shaba; and at least one battalion of the 31st Airborne Brigade stayed at Kamina. The other main concentration of forces was in and around Kinshasa: the 31st Airborne Brigade was deployed at N'djili Airport on the outskirts of the capital; the Special Presidential Division (DSP) resided adjacent to the presidential compound; and the 1st Armored Brigade was at Mbanza-Ngungu (in Bas-Congo, approximately 120 kilometers southwest of Kinshasa). Finally the 41st Commando Brigade was at Kisangani.
This superficially impressive list of units overstates the actual capability of the armed forces at the time. Apart from privileged formations such as the Presidential Division and the 31st Airborne Brigade, most units were poorly trained, divided and so badly paid that they regularly resorted to looting. What operational abilities the armed forces had were gradually destroyed by politicisation of the forces, tribalisation, and division of the forces, included purges of suspectedly disloyal groups, intended to allow Mobutu to divide and rule. All this occurred against the background of increasing deterioration of state structures under the kleptocratic Mobutu regime.
For a concise general description of the FAZ in the 1990s, see René Lemarchand, "The dynamics of violence in Central Africa", University of Pennsylvania Press, 2009, pages 226-228.
Mobutu's overthrow and after.
Much of the origins of the recent conflict in what is now the Democratic Republic of the Congo stems from the turmoil following the Rwandan Genocide of 1994, which then led to the Great Lakes refugee crisis. Within the largest refugee camps, beginning in Goma in Nord-Kivu, were Rwandan Hutu fighters, which were eventually organised into the Rassemblement Démocratique pour le Rwanda, who launched repeated attacks into Rwanda. Rwanda eventually backed Laurent-Désiré Kabila and his quickly organised Alliance of Democratic Forces for the Liberation of Congo in invading Zaire, aiming to stop the attacks on Rwanda in the process of toppling Mobutu's government. When the militias rebelled, backed by Rwanda, the FAZ, weakened as is noted above, proved incapable of mastering the situation and preventing the overthrow of Mobutu in 1997.
When Kabila took power in 1997, the country was renamed the Democratic Republic of the Congo and so the name of the national army changed once again, to the "Forces armées congolaises" (FAC). Tanzania sent six hundred military advisors to train Kabila's new army in May 1997. Command over the armed forces in the first few months of Kabila's rule was vague. Gérard Prunier writes that 'there was no minister of defence, no known chief of staff, and no ranks; all officers were Cuban-style 'commanders' called 'Ignace', 'Bosco', Jonathan', or 'James', who occupied connecting suites at the Intercontinental Hotel and had presidential list cell-phone numbers. None spoke French or Lingala, but all spoke Kinyarwanda, Swahili, and, quite often, English.' On being asked by Belgian journalist Colette Braeckman what was the actual army command structure apart from himself, Kabila answered 'We are not going to expose ourselves and risk being destroyed by showing ourselves openly... . We are careful so that the true masters of the army are not known. It is strategic. Please, let us drop the matter.' Kabila's new "Forces armées congolaises" were riven with internal tensions. The new FAC had Banyamulenge fighters from South Kivu, "kadogo" child soldiers from various eastern tribes, such as Thierry Nindaga, Safari Rwekoze, etc... he mostl Lunda Katangese Tigers of the former FNLC, and former FAZ personnel. Mixing these disparate and formerly warring elements together led to mutuny. On 23 February 1998, a mostly Banyamulenge unit mutiniued at Bukavu after its officers tried to disperse the soldiers into different units spread all around the Congo. By mid-1998, formations on the outbreak of the Second Congo War included the Tanzanian-supported 50th Brigade, headquartered at Camp Kokolo in Kinshasa, and the 10th Brigade — one of the best and largest units in the army — stationed in Goma, as well as the 12th Brigade in Bukavu. The declaration of the 10th Brigade's commander, former DSP officer Jean-Pierre Ondekane, on 2 August 1998 that he no longer recognised Kabila as the state's president was one of the factors in the beginning of the Second Congo War.
The FAC performed poorly throughout the Second Congo War and "demonstrated little skill or recognisable military doctrine". At the outbreak of the war in 1998 the Army was ineffective and the DRC Government was forced to rely on assistance from Angola, Chad, Namibia and Zimbabwe. As well as providing expeditionary forces, these countries unsuccessfully attempted to retrain the DRC Army. North Korea and Tanzania also provided assistance with training. During the first year of the war the Allied forces defeated the Rwandan force which had landed in Bas-Congo and the rebel forces south-west of Kinshasa and eventually halted the rebel and Rwandan offensive in the east of the DRC. These successes contributed to the Lusaka Ceasefire Agreement which was signed in July 1999. Following the Lusaka Agreement, in mid-August 1999 President Kabila issued a decree dividing the country into eight military regions. The first military region, Congolese state television reported, would consist of the two Kivu provinces, Orientale Province would form the second region, and Maniema and Kasai-Oriental provinces the third. Katanga and Équateur would fall under the fourth and fifth regions, respectively, while Kasai-Occidental and Bandundu would form the sixth region. Kinshasa and Bas-Congo would form the seventh and eighth regions, respectively. In November 1999 the Government attempted to form a 20,000-strong paramilitary force designated the People's Defence Forces. This force was intended to support the FAC and national police but never became effective.
1999-present.
The Lusaka Ceasefire Agreement was not successful in ending the war, and fighting resumed in September 1999. The FAC's performance continued to be poor and both the major offensives the Government launched in 2000 ended in costly defeats. President Kabila's mismanagement was an important factor behind the FAC's poor performance, with soldiers frequently going unpaid and unfed while the Government purchased advanced weaponry which could not be operated or maintained. The defeats in 2000 are believed to have been the cause of President Kabila's assassination in January 2001. Following the assassination, Joseph Kabila assumed the presidency and was eventually successful in negotiating an end to the war in 2002-2003.
The December 2002 Global and All-Inclusive Agreement devoted Chapter VII to the armed forces. It stipulated that the armed forces chief of staff, and the chiefs of the army, air force, and navy were not to come from the same warring faction. The new 'national, restructured and integrated' army would be made up from Kabila's government forces (the FAC), the RCD, and the MLC. Also stipulated in VII(b) was that the RCD-N, RCD-ML, and the Mai-Mai would become part of the new armed forces. An intermediate mechanism for physical identification of the soldiers, and their origin, date of enrolment, and unit was also called for (VII(c)). It also provided for the creation of a Conseil Superieur de la Defense (Superior Defence Council) which would declare states of siege or war and give advice on security sector reform, disarmament/demobilization, and national defence policy.
A decision on which factions were to name chiefs of staff and military regional commanders was announced on 19 August 2003 as the first move in military reform, superimposed on top of the various groups of fighters, government and former rebels. Kabila was able to name the armed forces chief of staff, Lieutenant General Liwanga Mata, who previously served as navy chief of staff under Laurent Kabila. Kabila was able to name the air force commander (John Numbi), the RCD-Goma received the Land Force commander's position (Sylvain Buki) and the MLC the navy (Dieudonne Amuli Bahigwa). Three military regional commanders were nominated by the former Kinshasa government, two commanders each by the RCD-Goma and the MLC, and one region commander each by the RCD-K/ML and RCD-N. However these appointments were announced for Kabila's "Forces armées congolaises" (FAC), not the later FARDC. Another report however says that the military region commanders were only nominated in January 2004, and that the troop deployment on the ground did not change substantially until the year afterward.
On 24 January 2004, a decree created the "Structure Militaire d'Intégration" (SMI, Military Integration Structure). Together with the SMI, CONADER also was designated to manage the combined "tronc commun" DDR element and military reform programme. The first post-Sun City military law appears to have been passed on 12 November 2004, which formally created the new national Forces Armées de la République Démocratique du Congo (FARDC). Included in this law was article 45, which recognized the incorporation of a number of armed groups into the FARDC, including the former government army Forces Armées Congolaises (FAC), ex-FAZ personnel also known as former President Mobutu's 'les tigres', the RCD-Goma, RCD-ML, RCD-N, MLC, the Mai-Mai, as well as other government-determined military and paramilitary groups.
Turner writes that the two most prominent opponents of military integration ("brassage") were Colonel Jules Mutebusi, a Munyamulenge from South Kivu, and Laurent Nkunda, a Rwandaphone Tutsi who Turner says was allegedly from Rutshuru in North Kivu. In May–June 2004 Mutebusi led a revolt against his superiors from Kinshasa in South Kivu. Nkunda began his long series of revolts against central authority by helping Mutebusi in May–June 2004. In November 2004 a Rwandan government force entered North Kivu to attack the FDLR, and, it seems, reinforced and resupplied RCD-Goma (ANC) at the same time. Kabila despatched 10,000 government troops to the east in response, launching an attack which was called 'Operation Bima.' In the midst of this tension, Nkunda's men launched attacks in North Kivu in December 2004.
There was another major personnel reshuffle on 12 June 2007. FARDC chief General Kisempia Sungilanga Lombe was replaced with General Dieudonne Kayembe Mbandankulu. General Gabriel Amisi Kumba retained his post as Land Forces commander. John Numbi, a trusted member of Kabila's inner circle, was shifted from air force commander to Police Inspector General. U.S. diplomats reported that the former Naval Forces Commander Maj. General Amuli Bahigua (ex-MLC) became the FARDC's Chief of Operations; former FARDC Intelligence Chief General Didier Etumba (ex-FAC) was promoted to Vice Admiral and appointed Commander of Naval Forces; Maj. General Rigobert Massamba (ex-FAC), a former commander of the Kitona air base, was appointed as Air Forces Commander; and Brig. General Jean-Claude Kifwa, commander of the Republican Guard, was appointed as a regional military commander.
Much of the east of the country remains insecure, however. In the far northeast this is due primarily to the Ituri conflict. In the area around Lake Kivu, primarily in North Kivu, fighting continues among the Democratic Forces for the Liberation of Rwanda and between the government FARDC and Laurent Nkunda's troops, with all groups greatly exacerbating the issues of internal refugees in the area of Goma, the consequent food shortages, and loss of infrastructure from the years of conflict. In 2009, several United Nations officials stated that the army is a major problem, largely due to corruption that results in food and pay meant for soldiers being diverted and a military structure top-heavy with colonels, many of whom are former warlords. In a 2009 report itemizing FARDC abuses, Human Rights Watch urged the UN to stop supporting government offensives against eastern rebels until the abuses ceased.
In 2010, thirty FARDC officers were given scholarships to study in Russian military academies. This is part of a greater effort by Russia to help improve the FARDC. A new military attaché and other advisers from Russia visited the DRC.
On 22 November 2012, Gabriel Amisi Kumba was suspended from his position in the Forces Terrestres by president Joseph Kabila due to an inquiry into his alleged role in the sale of arms to various rebel groups in the eastern part of the country, which may have implicated the rebel group M23. In December 2012 it was reported that members of Army units in the north east of the country are often not paid due to corruption, and these units rarely made against villages by the Lord's Resistance Army.
The FARDC deployed 850 soldiers and 150 PNC police officers as part of an international force in the Central African Republic, which the DRC borders to the north. The country had been in a state of civil war since 2012, when the president was ousted by rebel groups. The DRC was urged by French president Hollande to keep its troops in CAR.
In July 2014, the Congolese army carried out a joint operation with UN troops in the Masisi and Walikale territories of the North Kivu province. In the process, they liberated over 20 villages and a mine from rebel control, specifically, from the Mai Mai Cheka and the Alliance for the Sovereign and Patriotic Congo rebel groups.
Current organisation.
The President, Major General Joseph Kabila is the Commander-in-Chief of the Armed Forces. The Minister of Defence, formally Ministers of Defence, Disarmament, and Veterans (Ancien Combattants), with the French acronym MDNDAC, is Alexandre Luba Ntambo.
The Colonel Tshatshi Military Camp in the Kinshasa suburb of Ngaliema hosts the defence department and the Chiefs of Staff central command headquarters of the FARDC. Jane's data from 2002 appears inaccurate; there is at least one ammunition plant in Katanga.
Below the Chief of Staff, the current organisation of the FARDC is not fully clear. There is known to be a Military Intelligence branch - Service du Renseignement militaire (SRM), the former DEMIAP. The FARDC is known to be broken up into the Land Forces ("Forces Terrestres"), Navy and Air Force. The Land Forces are distributed around ten military regions, up from the previous eight, following the ten provinces of the country. There is also a training command, the Groupement des Écoles Supérieurs Militaires (GESM) or Group of Higher Military Schools, which, in January 2010, was under the command of Major General Marcellin Lukama. The Navy and Air Forces are composed of various "groupments" (see below). There is also a central logistics base.
It should be made clear also that Joseph Kabila does not trust the military; the Republican Guard is the only component he trusts. Major General John Numbi, former Air Force chief, now inspector general of police, ran a parallel chain of command in the east to direct the 2009 Eastern Congo offensive, Operation Umoja Wetu; the regular chain of command was by-passed. Previously Numbi negotiated the agreement to carry out the "mixage" process with Laurent Nkunda. Commenting on a proposed vote of no confidence in the Minister of Defence in September 2012, Baoudin Amba Wetshi of "lecongolais.cd" described Ntolo as a 'scapegoat'. Wetshi said that all key military and security questions were handled in total secrecy by the President and other civil and military personalities trusted by him, such as John Numbi, Gabriel Amisi Kumba ('Tango Four'), Delphin Kahimbi, and others such as Kalev Mutond and Pierre Lumbi Okongo.
Armed Forces Chiefs of Staff.
The available information on armed forces' Chiefs of Staff is incomplete and sometimes contradictory. In addition to armed forces chiefs of staff, in 1966 Lieutenant Colonel Ferdinand Malila was listed as Army Chief of Staff.
Command structure in January 2005.
Virtually all officers have now changed positions, but this list gives an outline of the present structure. Despite the planned subdivision of the country into more numerous provinces, the actual splitting of the former provinces has not taken place.
Updates to command structure in 2014.
As of 2014, several changes took place among the FARDC command staff:
Land forces.
The land forces are made up of about 14 integrated brigades, of fighters from all the former warring factions which have gone through an "brassage" integration process (see next paragraph), and a not-publicly known number of non-integrated brigades which remain solely made up from single factions (the Congolese Rally for Democracy (RCD)'s "Armée national congolaise", the ex-government former Congolese Armed Forces (FAC), the ex-RCD KML, the ex-Movement for the Liberation of Congo, the armed groups of the Ituri conflict (the Mouvement des Révolutionnaires Congolais (MRC), Forces de Résistance Patriotique d'Ituri (FRPI) and the Front Nationaliste Intégrationniste (FNI)) and the Mai-Mai).
It appears that about the same time that Presidential Decree 03/042 of 18 December 2003 established the National Commission for Demobilisation and Reinsertion (CONADER), '..all ex-combatants were officially declared as FARDC soldiers and the then FARDC brigades ere t rest deployed until the order to leave for "brassage."
The reform plan adopted in 2005 envisaged the formation of eighteen integrated brigades through the "brassage" process as its first of three stages. The process consists firstly of regroupment, where fighters are disarmed. Then they are sent to orientation centres, run by CONADER, where fighters take the choice of either returning to civilian society or remaining in the armed forces. Combatants who choose demobilisation receive an initial cash payment of US $110. Those who choose to stay within the FARDC are then transferred to one of six integration centres for a 45-day training course, which aims to build integrated formations out of factional fighters previously heavily divided along ethnic, political and regional lines. The centres are spread out around the country at Kitona, Kamina, Kisangani, Rumangabo and Nyaleke (within the Virunga National Park) in Nord-Kivu, and Luberizi (on the border with Burundi) in South Kivu. The process has suffered severe difficulties due to construction delays, administration errors, and the amount of travel former combatants have to do, as the three stages' centres are widely separated. Following the first 18 integrated brigades, the second goal is the formation of a ready reaction force of two to three brigades, and finally, by 2010 when MONUC is anticipated to have withdrawn, the creation of a Main Defence Force of three divisions.
In February 2008, the current reform plan was described as:
"The short term, 2008-2010, will see the setting in place of a Rapid Reaction Force; the medium term, 2008 -2015, with a Covering Force; and finally the long term, 2015-2020, with a Principal Defence Force." He added that the reform plan rests on a programme of synergy based on the four pillars of dissuasion, production, reconstruction and excellence. "The Rapid Reaction Force is expected to focus on dissuasion, through a Rapid Reaction Force of 12 battalions, capable of aiding MONUC to secure the east of the country and to realise constitutional missions," Defence Minister Chikez Diemu said.
Amid the other difficulties in building new armed forces for the DRC, in early 2007 the integration and training process was distorted as the DRC government under Kabila attempted to use it to gain more control over the dissident general Laurent Nkunda. A hastily negotiated verbal agreement in Rwanda saw three government FAC brigades integrated with Nkunda's former ANC 81st and 83rd Brigades in what was called "mixage". "Mixage" brought multiple factions into composite brigades, but without the 45-day retraining provided by "brassage", and it seems that actually, the process was limited to exchanging battalions between the FAC and Nkunda brigades in North Kivu, without further integration. Due to Nkunda's troops having greater cohesion, Nkunda effectively gained control of all five brigades - not what the DRC central government had been hoping! However, after Nkunda used the "mixage" brigades to fight the FDLR, strains arose between the FARDC and Nkunda-loyalist troops within the brigades and they fell apart in the last days of August 2007. The International Crisis Group says that 'by 30 August 00 Nkunda's troops had left the mixed brigades and controlled a large part of the Masisi and Rutshuru territories' (of North Kivu).
Both formally integrated brigades and the non-integrated units continue to conduct arbitrary arrests, rapes, robbery, and other crimes and these human rights violations are "regularly" committed by both officers and members of the rank and file. Members of the Army also often strike deals to gain access to resources with the militias they are meant to be fighting.
The various brigades and other formations and units number at least 100,000 troops. The status of these brigades has been described as "pretty chaotic." A 2007 disarmament and repatriation study said "army units that have not yet gone through the process of brassage are usually much smaller than what they ought to be. Some non-integrated brigades have only 500 men (and are thus nothing more than a small battalion) whereas some battalions may not even have the size of a normal company (over a 100 men)."
Known integrated brigades in 2007.
See also U.S. State Department, 07KINSHASA452 Congolese Military Proposes Redeployment, Renaming Of Integrated Brigades, 19 April 2007. Like the Force Publique in the Congo Free State, FARDC brigades have been deploying to their areas of operation with their families in tow. 2nd Commando Battalion of the Belgian Paracommando Brigade trained one of the first integrated brigades from January to June 2004. By 13 September 2006, the Government had established 13 out of the 18 integrated brigades it had planned to create before the elections. (S/2006/759, 21 September 2006, 12) A fourteenth brigade was created by March 2007. (S/2007/156, 20 March 2007, 7)
A number of outside donor countries are also carrying out separate training programmes for various parts of the Forces du Terrestres (Land Forces). The People's Republic of China has trained Congolese troops at Kamina in Katanga from at least 2004 to 2009, and the Belgian government is training at least one 'rapid reaction' battalion. When Kabila visited U.S. President George W. Bush in Washington D.C., he also asked the U.S. Government to train a battalion, and as a result, a private contractor, Protection Strategies Incorporated, started training a FARDC battalion at Camp Base, Kisangani, in February 2010. The company is being supervised by Special Operations Command-Africa Command. The various international training programmes are not well integrated.
Equipment.
Attempting to list the equipment available to the DRC's land forces is difficult; most figures are unreliable estimates based on known items delivered in the past. The IISS's Military Balance 2007 and Orbat.com's Concise World Armies 2005 give only slightly differing figures however (the figures below are from the IISS Military Balance 2007). Much of the Army's equipment is non-operational due to insufficient maintenance—in 2002 only 20 percent of the Army's armoured vehicles were estimated as being serviceable.
In addition to these 2007 figures, In March 2010, it was reported that the DRC's land forces had ordered USD $80 million worth of military equipment from Ukraine which included 20 T-72 main battle tanks, 100 trucks and various small arms. 20 x T-72 have been reported by World Defence Almanac. Tanks have been used in the Kivus in the 2005-9 period.
In February 2014, Ukraine revealed that it had achieved the first export order for the T-64 tank to the DRC Land Forces for 50 T-64BV-1s.
In June 2015 it was reported that Georgia had sold 12 of its Didgori-2 to the DRC for $4 million. The vehicles were specifically designed for reconnaissance and special operations. Two of the vehicles are a recently developed conversion to serve for medical field evacuation.
Republican Guard.
In addition to the other land forces, President Joseph Kabila also has a Republican Guard presidential force ("Garde Républicaine" or GR), formerly known as the Special Presidential Security Group (GSSP). FARDC military officials state that the Garde Républicaine is not the responsibility of FARDC, but of the Head of State. Apart from Article 140 of the Law on the Army and Defence, no legal stipulation on the DRC's Armed Forces makes provision for the GR as a distinct unit within the national army. In February 2005 President Joseph Kabila passed a decree which appointed the GR's commanding officer and "repealed any previous provisions contrary" to that decree. The GR, more than 10,000 strong (the ICG said 10,000 to 15,000 in January 2007), has better working conditions and is paid regularly, but still commits rapes and robberies in the vicinity of its bases.
In an effort to extend his personal control across the country, Joseph Kabila has deployed the GR at key airports, ostensibly in preparation for an impending presidential visit. there were Guards deployed in the central prison of Kinshasa, N'djili Airport, Bukavu, Kisangani, Kindu, Lubumbashi, Matadi, and Moanda, where they appear to answer to no local commander and have caused trouble with MONUC troops there.
The GR is also supposed to undergo the integration process, but in January 2007, only one battalion had been announced as having been integrated. Formed at a brassage centre in the Kinshasa suburb of Kibomango, the battalion included 800 men, half from the former GSSP and half from the MLC and RCD Goma.
Other forces active in the country.
There are currently large numbers of United Nations troops stationed in the DRC. The United Nations Organization Stabilization Mission in the Democratic Republic of the Congo (MONUSCO), on had a strength of over 19,000 peacekeepers (including 16,998 military personnel) and has a mission of assisting Congolese authorities maintain security. The UN and foreign military aid missions, the most prominent being EUSEC RD Congo, are attempting to assist the Congolese in rebuilding the armed forces, with major efforts being made in trying to assure regular payment of salaries to armed forces personnel and also in military justice. Retired Canadian Lieutenant General Marc Caron also served for a time as Security Sector Reform advisor to the head of MONUC.
Groups of anti-Rwandan government rebels like the FDLR, and other foreign fighters remain inside the DRC. The FDLR which is the greatest concern, was some 6,000 strong, in July 2007. By late 2010 the FDLR's strength however was estimated at 2,500. The other groups are smaller: the Ugandan Lord's Resistance Army, the Ugandan rebel group the Allied Democratic Forces in the remote area of Mt Rwenzori, and the Burundian Parti pour la Libération du Peuple Hutu—Forces Nationales de Liberation (PALIPEHUTU-FNL).
Finally there is a government paramilitary force, created in 1997 under President Laurent Kabila. The National Service is tasked with providing the army with food and with training the youth in a range of reconstruction and developmental activities. There is not much further information available, and no internet-accessible source details the relationship of the National Service to other armed forces bodies; it is not listed in the constitution. President Kabila, in one of the few comments available, says National Service will provide a gainful activity for street children. Obligatory civil service administered through the armed forces was also proposed under the Mobutu regime during the 'radicalisation' programme of December 1974-January 1975; the FAZ was opposed to the measure and the plan 'took several months to die.'
Air Force.
All military aircraft in the DRC are operated by the Air Force. Jane's World Air Forces states that the Air Force has an estimated strength of 1,800 personnel and is organised into two Air Groups. These Groups command five wings and nine squadrons, of which not all are operational. 1 Air Group is located at Kinshasa and consists of Liaison Wing, Training Wing and Logistical Wing and has a strength of five squadrons. 2 Tactical Air Group is located at Kaminia and consists of Pursuit and Attack Wing and Tactical Transport Wing and has a strength of four squadrons. Foreign private military companies have reportedly been contracted to provide the DRC's aerial reconnaissance capability using small propeller aircraft fitted with sophisticated equipment. Jane's states that National Air Force of Angola fighter aircraft would be made available to defend Kinshasa if it came under attack.
Like the other services, the Congolese Air Force is not capable of carrying out its responsibilities. Few of the Air Force's aircraft are currently flyable or capable of being restored to service and it is unclear whether the Air Force is capable of maintaining even unsophisticated aircraft. Moreover, Jane's states that the Air Force's Ecole de Pilotage is 'in near total disarray' though Belgium has offered to restart the Air Force's pilot training program.
Navy.
The 2002 edition of "Jane's Sentinel" described the Navy as being "in a state of near total disarray" and stated that it did not conduct any training or have operating procedures. The Navy shares the same discipline problems as the other services. It was initially placed under command of the MLC when the transition began: the current situation is uncertain.
The 2007 edition of "Jane's Fighting Ships" states that the Navy is organised into four commands, based at Matadi, near the coast; the capital Kinshasa, further up the Congo river; Kalemie, on Lake Tanganyika; and Goma, on Lake Kivu.
The IISS, in its 2007 edition of the "Military Balance", confirms the bases listed in "Jane's" and adds a fifth base at Boma, a coastal city near Matadi.
Various sources also refer to numbered Naval Regions. Operations of the 1st Naval Region have been reported in Kalemie, the 4th near the northern city of Mbandaka, and the 5th at Goma.
The IISS lists the Navy at 1,000 personnel and a total of eight patrol craft, of which only one is operational, a Shanghai II Type 062 class gunboat designated "102". There are five other 062s as well as two Swiftships which are not currently operational, though some may be restored to service in the future. According to "Jane's", the Navy also operates barges and small craft armed with machine guns.
Before the downfall of Mobutu, a small navy operated on the Congo river. One of its installations was at the village of N'dangi near the presidential residence in Gbadolite. The port at N'dangi was the base for several patrol boats, helicopters and the presidential yacht.

</doc>
<doc id="8032" url="https://en.wikipedia.org/wiki?curid=8032" title="Geography of Denmark">
Geography of Denmark

Denmark is a Nordic country located in Northern Europe. It consists of the Jutland peninsula and several islands in the Baltic sea, referred to as the Danish Archipelago. Denmark is located southwest of Sweden and due south of Norway and is bordered by the German state (and former possession) Schleswig-Holstein to the south, on Denmark's only land border, 68 kilometres (42 miles) long.
Denmark borders both the Baltic and North Seas along its tidal shoreline. Denmark's general coastline is much shorter, at , as it would not include most of the 1,419 offshore islands (each defined as exceeding 100 square meters in area) and the 180 km long Limfjorden, which separates Denmark's second largest island, North Jutlandic Island, 4,686 km in size, from the rest of Jutland. No location in Denmark is further from the coast than . The size of the land area of Denmark cannot be stated exactly since the ocean constantly erodes and adds material to the coastline, and because of human land reclamation projects (to counter erosion). On the southwest coast of Jutland, the tide is between , and the tideline moves outward and inward on a stretch.
A circle enclosing the same area as Denmark would be 742 km (461 miles) long. Denmark has 443 named islands (1,419 islands above 100 m²), of which 72 are inhabited (, Statistics Denmark). The largest islands are Zealand "(Sjælland)" and Funen "(Fyn)". The island of Bornholm is located east of the rest of the country, in the Baltic Sea. Many of the larger islands are connected by bridges; the Øresund Bridge connects Zealand with Sweden; the Great Belt Bridge connects Funen with Zealand; and the Little Belt Bridge connects Jutland with Funen. Ferries or small aircraft connect to the smaller islands. Main cities are the capital Copenhagen on Zealand; Århus, Aalborg and Esbjerg in Jutland; and Odense on Funen.
Denmark experiences a temperate climate. This means that the winters are mild and windy and the summers are cool. The local terrain is generally flat with a few gently rolling plains. The territory of Denmark includes the island of Bornholm in the Baltic Sea and the rest of metropolitan Denmark, but excludes the Faroe Islands and Greenland. Its position gives Denmark complete control of the Danish Straits (Skagerrak and Kattegat) linking the Baltic and North Seas. The country's natural resources include petroleum, natural gas, fish, salt, limestone, chalk, stone, gravel and sand.
Environment.
Land use.
Irrigated land: 4,354 km² (2007)
Total renewable water resources: 6 km (2011)
Freshwater withdrawal (domestic/industrial/agricultural):
<br>"total:" 0.66 km/yr (58%/5%/36%)
<br> "per capita:" 118.4 m/yr (2009)
Transnational issues.
Population.
 Denmark has a population of 5,543,453. About a quarter of Danes live in the capital Copenhagen.

</doc>
<doc id="8033" url="https://en.wikipedia.org/wiki?curid=8033" title="Demographics of Denmark">
Demographics of Denmark

This article is about the demographic features of the population of Denmark, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.
National demographics.
According to 2012 figures from Statistics Denmark, 89.6% of Denmark’s population of over 5,580,516 was of Danish descent, defined as having at least one parent who was born in Denmark and has Danish citizenship. Many of the remaining 10.4% were immigrants—or descendants of recent immigrants (defined as people born in Denmark from migrant parents, or parents without Danish citizenship) —less than a third of whom are from the neighbouring Scandinavian countries and Germany. Over two-thirds include people from Turkey, Iraq, Somalia, Bosnia and Herzegovina, South Asia, and from Western Asia.
More than 590 000 individuals (10.4%) are migrants and their descendants (142 000 second generation migrants born in Denmark).
Of these 590 000 immigrants and their descendants:
According to Mete Feridun, immigration has implications for the labour market in Denmark. Moreover, according to the figures from Danmarks Statistik, crime rate among refugees and their descendants is 73% higher than for the male population average, even when taking into account their socioeconomic background. A report from Teori- og Metodecentret from 2006 found that seven out of ten young people placed on the secured youth institutions in Denmark are immigrants (with 40 percent of them being refugees).
Ethnic groups.
Non-Scandinavian ethnic minorities include:
Historic minorities.
Ethnic minorities in Denmark include a handful of groups:
Vital statistics since 1900.
Data according to Statistics Denmark, which collects the official statistics for Denmark.
Religion.
The Church of Denmark () is state-supported and, according to statistics from January 2006, accounts for about 80% of Denmark's religious affiliation. Denmark has had religious freedom guaranteed since 1849 by the Constitution, and numerous other religions are officially recognised, including several Christian denominations, Muslim, Jewish, Buddhist, Hindu and other congregations as well as Forn Siðr, a revival of Scandinavian pagan tradition. The Department of Ecclesiastical Affairs recognises roughly a hundred religious congregations for tax and legal purposes such as conducting wedding ceremonies.
Islam is the second largest religion in Denmark.
For historical reasons, there is a formal distinction between 'approved' ("godkendte") and 'recognised' ("anerkendte") congregations of faith. The latter include 11 traditional denominations, such as Roman Catholics, the Reformed Church, the Mosaic Congregation, Methodists and Baptists, some of whose privileges in the country date hundreds of years back. These have the additional rights of having priests appointed by royal resolution and to christen/name children with legal effect.
Demographic statistics.
Population.
Denmark's population from 1769 to 2007.
"CIA World Factbook" demographic statistics.
The following demographic statistics are from the "CIA World Factbook", unless otherwise indicated.
Population:
Age structure:
Median age:
Population growth rate:
Net migration rate:
"Urbanization":
Sex ratio:
Infant mortality rate:
Life expectancy at birth:
Total fertility rate:
HIV/AIDS - adult prevalence rate:
HIV/AIDS - people living with HIV/AIDS:
HIV/AIDS - deaths:
Nationality:
Ethnic groups:
Religions:
Languages:
Literacy:
School life expectancy (primary to tertiary education):
Education expenditures:

</doc>
<doc id="8035" url="https://en.wikipedia.org/wiki?curid=8035" title="Economy of Denmark">
Economy of Denmark

Denmark has a diverse, mixed economy. It relies heavily on human resources, but not exclusively, as there are a few significant and valuable natural resources available, including mature oil and gas wells in the North Sea. Cooperatives form a large part of some sectors, be it in housing, agriculture or retail. Foundations play a large role as owners of private sector companies. Denmark's nominal GDP was estimated to be $333 billion, the 32nd largest in the world. It has the world's lowest level of income inequality, according to the World Bank Gini (%), but no legally stipulated minimum wage. As of January 2015 the unemployment rate is at 6.2%, which is below the Euro Area average of 11.2%. As of 28 February 2014 Denmark is among the countries with the highest credit rating.
Denmark's main exports are: industrial production/manufactured goods 73.3% (of which machinery and instruments were 21.4%, and fuels, chemicals, etc. 26%); agricultural products and others for consumption 18.7% (in 2009 meat and meat products were 5.5% of total export; fish and fish products 2.9%). Denmark is a net exporter of food and energy and has since the 1990s had a balance of payments surplus. The accumulated value of service and merchandise exports in 2013 amounted to 54% of GDP, and imports in 2013 amounted to 49% of GDP. Notable among the service exports are container shipping. There is no net foreign debt as other countries owe more money to Denmark than Denmark owes to them, but because of large deficits due to increased unemployment levels the "central" government has increased its debt level since the end of September 2008, when it stood at 21 percent (gross debt) of GDP, according to the central bank - in accordance with the Eurostat EMU- "gross" debt numbers, which only take liabilities into account. (See below (Budgets)). Taking assets into account as well "net" debt of the "central" government was 11 percent. The public sector as a "whole" had net assets of 108 billion kroner in 2008. Within the European Union, Denmark advocates a liberal trade policy. Its standard of living is average among the Western European countries - and for many years the most equally distributed as shown by the Gini coefficient - in the world, and the Danes devote 0.8% of gross national income (GNI) to foreign aid. It is a society based on consensus (dialogue and compromise) with the Danish Confederation of Trade Unions and the Confederation of Danish Employers in 1899 in "Septemberforliget" (The September Settlement) recognising each other's right to organise, thus, negotiate. The employer's right to hire and fire their employees "whenever" they find it necessary is recognised. There is no official minimum wage () set by the government; the minimum of wages () is determined by negotiations between the organisations of employers and employees. Denmark produces oil, natural gas, wind- and bio-energy. Its principal exports are machinery, instruments and food products. The US is Denmark's largest non-European trading partner, accounting for around 5% of total Danish merchandise trade. Aircraft, computers, machinery, and instruments are among the major US exports to Denmark. Among major Danish exports to the U.S. are industrial machinery, chemical products, furniture, pharmaceuticals, Lego and canned ham and pork.
Overview.
This thoroughly modern market economy features high-tech agriculture, up-to-date small-scale and corporate industry, extensive government welfare measures, comfortable living standards, and high dependence on foreign trade. Denmark is a net exporter of food. The center-left coalition government (1993–2001) concentrated on reducing the unemployment rate and turning the budget deficit into a surplus, as well as following the previous government's policies of maintaining low inflation and a current account surplus. The coalition also committed itself to maintaining a stable currency. The coalition lowered marginal income tax rates while maintaining overall tax revenues; boosted industrial competitiveness through labor market and tax reforms, increased research and development funds. The availability and duration of unemployment benefit has been restricted to four years and because of rapidly rising prices on housing this has led to an increase in poverty from below 4% in 1995 to 5% in 2006 according to the Danish Economic Council . Despite these cuts, the part of the public sector in Denmark which buys goods and services from the private sector and provides the public sector administration and direct service to the public - nursing institutions for the young or old, hospitals, schools, police, and so on. - has risen from 25.5% of GDP during the former government to 26% today and is projected to be at 26.5% in 2015 if current policies continue .
Denmark chose not to join the 11 other European Union members who launched the euro on 1 January 1999. Especially from 2006, economists and political pundits have expressed concern that the lack of skilled labour will result in higher pay increases and an overheating of the economy, which would repeat the boom-and-bust cycle in 1986, when government introduced a tax reform and restricted the private loan market because of a record balance-of-payments deficit. As a consequence, the trade balance showed a surplus in 1987, and the balance-of-payments in 1990 (first surplus since 1963). They have remained in surplus since, except for the balance of payments in 1998.
Welfare state.
Denmark has a broad-reaching welfare system, which ensures that all Danes receive tax-funded health care. Expenses to medicine is only partially funded and some non-vital medical treatments are not funded at all. Denmark has an unemployment insurance system called the A-kasse ("arbejdsløshedskasse"). This system requires a paying membership of a state recognized unemployment fund. Most of these funds are managed by trade unions, and a high percentage of their expenses are financed through the state tax-system. Members of an A-kasse are not obliged to be members of a trade union. Not every Danish citizen or employee qualifies for a membership of an unemployment fund and membership benefits will be terminated after 2 or more years of unemployment. A person that is not member of an A-kasse, can not receive unemployment benefits. Unemployment funds does not pay benefits to sick members, they will be transferred to a municipal social support system instead. Denmark has a countrywide, but municipal administered social support system against poverty, securing that qualified citizens has a minimum income of living. All Danish citizens above 18 years of age can apply for some financial support, if they cannot sustain themselves or their family. Approval is by no means automatic and the extent of this system has generally been diminished over the last 30 years. After a newly implemented reform by 5. January 2015, sick people can receive some financial support throughout the extent of their illness and not just for the maximum of 1 year as previously. Their ability to work will be re-evaluated by the municipality after 5 months of illness. Denmark ranked the first in the European pensions barometer survey for the past two years. The lowest-income group before retirement from the age of 65 receive 120% of their pre-retirement income in pension and miscellaneous subsidies.
The worlds largest public sector (30% of the entire workforce on a full-time basis) is financed by the world's highest taxes. A value added tax of 25% is levied on the sale of most goods and services (including groceries). The income tax in Denmark ranges from 37.4% to 63% progressively, levied on 4 out of 10 full-time employees. Such high rates meant that 1,010,000 Danes before the end of 2008 (44% of all full-time employees) were paying a marginal income tax of 63% and a combined marginal tax of 70.9% resulting in warnings from organisations such as the OECD. "TV2 (Denmark)" reported in April 2008 that abolishing the middle- and top-level income tax brackets would amount to two (2) and one (1) percent of public sector revenue, respectively, which equals one and a half percent of GDP. The public sector as a whole had a budget surplus of 4.4% of GDP in 2007, but the tax cuts would increase private consumption and the labor shortage, thus, resulting in a deficit on the trade balance and pressure to increase wages even further. Proceeds from selling ones home are not taxed, as the marginal tax rate on capital income from housing savings is around 0 percent. A survey by Standard & Poor's found that the total debt secured by mortgages in Danish homes amounts to 89.8% of GDP, which is above the debt level in other EU countries (and the USA at 74.6% of GDP).
Political agendas for increasing the labor supply has resulted in several reforms and financial cuts. Reforms were initiated with the abolishing of the labor market arrangement called "efterløn" (eng.:early retirement pay), at the present (end of 3rd quarter 2008) with more than 130,000 participants (60 years until 64 years of age). Participation in this scheme was also open for self-employed people (farmers, fishermen, lawyers, and so on). Several reforms of the rights of the unemployed has followed up, partially inspired by the Danish Economic Council. Halving the time unemployment benefits can be received from four to two years and making it twice as hard to regain this right, has recently been implemented for example. This particular reform resulted in more than 50,000 unemployed people dropping out of the social benefit system within the first year and the majority were not qualified for the municipal administered social support system either. This situation has caused a lot of debate and political conflict in Denmark in recent years. The Cabinets of Helle Thorning-Schmidt attempted several short term solutions, but there were no political mandate to roll back the reforms. From 2015, a large majority of the population and a new found broad political alliance now suggests a partial roll back, but nothing has been effectuated yet.
Taxation and employment.
Taxation.
With a GDP of 1,642,215 million DKK and revenue from taxes and ownership at 803,693 million DKK (2006), 49.07% of GDP, it is of extreme importance what happens in the tax-financed part of the economy. According to newly revised statistics, Denmark had the world's highest tax level in 2005 and 2006, when tax revenue collected amounted to 50.7% and 49.1% of GDP respectively and also held this position 1970-74 and 1993-95. These figures do not include income from ownership. In 2013, the combined tax revenues collected (Danish:"samlede indtægter fra skatter og afgifter") amounted to 47.9% of GDP.
Budgets.
The overall surpluses after operating and capital expenditure in the "whole" public sector for the years 2004-2008: (million DKK) 27,327; 77,362; 79,937; 75,560 ('07 preliminary); 69,140 ('08 estimate). The public sector debt-liabilities still outstanding 1 January 2008 in accordance with the Eurostat EMU-debt numbers (gross debt) are 440.9 billion DKK (26.0% of GDP). In spite of falling surpluses this debt is expected to fall until 2015. As of 2008 there is no "net" debt in the public sector as a whole, but instead net assets of 43 billion DKK. The central government is determined to pay off the debt as fast as possible, avoiding the temptation to increase spending which might overheat the economy (increase wages and eventually prices drastically) because of a short supply of skilled labor and in the end require financial austerity measures to cool off the economy. Reporting on the record low unemployment numbers of "under" 50,000 persons in April 2008 published 9.30 am 29 May by Statistics Denmark, "TV2" (Denmark), at 10 pm, with comments from Nordea Bank's (Denmark) chief economist Helge Pedersen, and "DR2" (Danish Broadcasting Corporation), at 10.30 pm stressed the danger of overheating the economy and keeping public sector spending in check or otherwise risk economical-political measures. Being surprised at how low unemployment was, the economist said (TV2) that compared with previous periods with such a low unemployment rate, a trade deficit was avoided mainly because of the oil export.
The EMU-"gross" debt was 730 billion DKK at the end of 1993, 80.1% of GDP. During the four-year period 2004-2007 the public sector EMU-gross debt fell from 43.8% (641.9 billion DKK) to 26.0% (440.9 billion DKK) of GDP. The budget surpluses were (in billion DKK) 1.9% (27.2), 5.0% (77.4), 4.8% (79.3), and 4.4% (74.6) of GDP, respectively.
Employment.
Public sector employment (full-time and part-time) has been relatively steady at more than 800,000 a year this first decade, making up around 38% of total full-time (28% of full-time and part-time) employment, whereas private sector employment has risen by over 300,000 since the 1990s to slightly over 2 million in 2007 (full-time and part-time). With the information based partly on payments to the "Arbejdsmarkedets Tillægspension" pension fund of "all" employees and insured but unemployed members of an unemployment fund in Denmark, full-time employment is calculated at over 2.3 million persons in the third quarter of 2007. The increase in the fourth quarter of 2007 from a year ago in the number of employed persons was 1.0% and the amount of hours worked was 2.9% higher.
The share of employees leaving jobs every year (for a new job, retirement or unemployment (unempl.:15% of job leavers)) in the private sector is around 30% (of 1.25 million), at more than 300,000 - a level also observed in the U.K. and U.S.- but much higher than in continental Europe, where the corresponding figure is around 10%, and in Sweden. This attrition can be very costly, with new and old employees requiring half a year to return to old productivity levels, but with attrition bringing the number of people that have to be fired down. Productivity increased at an average of 2.3% a year in 2004, 2005 and 2006, recently being revised upward from an average of just 0.9% and previously with a too high employment level estimated. The upward revision is good, because a high wage economy like Denmark's with very few valuable natural resources needs to be highly productive, or efficient, and innovative to compete with other countries for a market share in the global economy. However, according to OECD, the distortions imposed by a combined marginal tax wedge of 70% (60% income tax plus 25% VAT, not counting elevated excise duties on certain goods) are hurting productivity and in turn the country's competitiveness.
Sectors.
Public sector reform.
To gain synergies through economies of scale (critical mass) (greater professional and financial sustainability) and big item discounts and to offer a wider array of services closer to the public (be a one-stop place of access to the public sector not unlike the unitary councils), it was deemed necessary to merge the municipalities and other administrative entities in the public sector. This would also help alleviate the financial problems of depopulation due to limited job opportunities, high unemployment and aging and make introduction of new information technology more affordable With the tax burden at around half of GDP, a survey July 2008 found that 81% of Danes are of the opinion that the public sector can deliver more service for the same money, harnessing the advantages of the recent reform. Mainly from 1 January 2007, the new center-right government streamlined the public sector extensively by decreasing the number of administrative units drastically in the different tiers of government, that is, in the number of city court circuits (from 82 to 24), police districts (from 54 to 12), tax districts (before 2007 the responsibility of the municipalities;after that part of the central government Ministry of Taxation), reshuffling tasks among the three government levels and abolished the counties in "Kommunalreformen" ("The Municipal Reform" of 2007), thereby reducing the number of local and regional politicians by almost half to 2,522 (municipal councillors) (council elections November 2005;reduced in the 2009 elections to 2,468;in 2013 to 2,444) (1978: 4,735;1998: 4,685; reduced somewhat in council elections November 2001 (Bornholm)) and 205 (regional councillors) (1998: 374) respectively. Before 1970 (a previous reform in effect from 1 April that year) the number of councillors (both categories) was around 11,000 in around 1,000 parish municipalities ("sognekommuner"), being supervised by their county, and market city municipalities ("købstadskommuner"), the latter numbering 86 (including Bornholm whose county as an exception supervised the county's 6 market city municipalities (of 22 in total)) and not being part of a county but being supervised by the Interior Ministry. This distinction (having independent municipalities not being under county supervision) ending (except for Copenhagen, Frederiksberg and Bornholm (2003–06)) with the reform of 1970, the term municipality ("kommune") replaced the previous two terms, which are now never used except for historical purposes. The number of municipalities had been reduced when during the period from April 1962 to 1966 398 municipalities merged to form 118 voluntarily. The number of municipalities was the highest in 1965, at 1345, of which 88 were market city municipalities, including Copenhagen and Frederiksberg, and 1257 were parish municipalities . Many of the 275 municipalities after 1 April 1974 built large city halls to consolidate the administration, thus, changing the cityscape of Denmark. It also consolidated other municipal enterprises and the purchase of goods and services from the private sector, as will some of the present 98 municipalities over time."TV2"(Denmark) reported 24 September 2007, that SKI, a mutual purchasing service company for central government, regions, and municipalities, made purchases of 140 billion DKK (almost 9% of GDP) of goods and services in bulk every year, prompting private sector companies to complain over razorthin profit margins and that for instance innovative (but expensive) products and energy efficiency sometimes were better than a very low price.
Agriculture.
Denmark is home to various types of agricultural production. Within animal husbandry, it includes dairy and beef cattle, pigs, poultry and fur animals – primarily mink, all sectors with a major export. Regarding vegetable production, Denmark is a leading producer of grass-, clover- and horticultural seeds.
The Danish agricultural industry is characterized by freehold and family ownership but due to structural development farms have become fewer and larger. With modern trade patterns the profitability increasingly depends on global market trends. The arable land in Denmark is approximately 2,646,000 hectares, and the number of farms approximately 40,000, out of which approximately one third is owned by full-time farmers.
The agriculture is intensive with 64 per cent of the land area being used for production. This equals production of food for 15 million people. The value of Danish agricultural export, including the agribusiness sector, has risen steadily in recent years and accounted for 16 billion Euros in 2011. The agriculture and food sector as a whole represents 20 per cent of total Danish commodity exports.
Animal production.
The tendency towards fewer and larger farms has been accompanied by an increased animal production, using fewer resources per produced unit.
The number of dairy farmers has reduced to about 3,800 with an average herd size of 150 cows. The milk quota is 1,142 tonnes. Danish dairy farmers are among the largest and most modern producers in Europe. More than half of the cows live in new loose-housing systems. Export of dairy products accounts for more than 20 per cent of the total Danish agricultural export. The total number of cattle in 2011 was approximately 1.5 million. Of these, 565,000 were dairy cows and 99,000 were suckler cows. The yearly number of slaughtering of beef cattle is around 550,000.
For more than 100 years the production of pigs and pig meat has been a major source of income in Denmark. The Danish pig industry is among the world’s leaders in areas such as breeding, quality, food safety, animal welfare and traceability creating the basis for Denmark being among the world’s largest pig meat exporters. Approximately 90 per cent of the production is exported. This accounts for almost half of all agricultural exports and for more than 5 per cent of Denmark’s total exports. About 4,200 farmers produce 28 million pigs annually. Of these, 20.9 million are slaughtered in Denmark.
Fur animal production on an industrial scale started in the 1930s in Denmark. Denmark is now the world’s largest producer of mink furs, with 1,400 mink farmers fostering 17.2 million mink and producing around 14 million furs of the highest quality every year. Approximately 98 per cent of the skins sold at Kopenhagen Fur Auction are exported. Fur ranks as Danish agriculture’s third largest export article, at more than DKK 7 billion annually. The number of farms peaked in the late 1980s at more than 5,000 farms, but the number has declined steadily since, as individual farms grew in size. Danish mink farmers claims their business to be sustainable, feeding the mink food industry waste and using all parts of the dead animal as meat, bone meal and biofuel. Special attention is given to the welfare of the mink, and regular “Open Farm” arrangements are made for the general public. Mink thrive in, but are not a native to Denmark, and it is considered an invasive species. American Mink is now widespread in Denmark and continues to cause problems for the native wildlife, in particular waterfowl. Denmark also has a small production of fox, chinchilla and rabbit furs. 
Two hundred professional producers are responsible for the Danish egg production, which was 66 million kg in 2011. Chickens for slaughter are often produced in units with 40,000 broilers. In 2012, 100 million chickens were slaughtered. In the minor productions of poultry, 13 million ducks, 1.4 million geese and 5.0 million turkeys were slaughtered in 2012.
Organic production.
Organic farming and production has increased dramatically in Denmark in the last 25 years and continues to expand with more than a quadrupling of exports since 2006. In 2012 the export of organic products reached DK 1.2 billion, a 12.3% increase from 2011. This figure should be seen in the context of a DK 360 billion global market for organic products and a total export from the Danish food and agriculture sector at DK 148 billion that same year. The import of organic products has always been higher than the exports though and reached DK 1.5 billion in 2012. 7% of the cultivated land is now categorized as organically farmed and 10% for the dairy industry as of 2008. Denmark has a high consumption of organic products per capita compared to other European countries, only surpassed by Switzerland. In 2011 Denmark surpassed Switzerland with the highest retail consumption share for organic products in the world. In 2012 the share was at 7.8%, accounting for a total of DK 5.5 billion.
Organic farming and production is officially a target and focus area for the Danish government in its ambition to effect a so-called green transition (Danish: "Den Grønne Omstilling"). In this respect it is the official goal of the government to double the area used for organic farming in the country from 2011 to 2020. The rise and increase of organic production has been driven by a plethora of activist groups and NGOs in all levels of production and consumption since the 1970s, a number of governmental institutions and subsidies.
Tourism.
Tourism is a major economical and job contributor in Denmark and it constitutes a growth sector.
Transport.
Significant investment has been made in building road and rail links between Copenhagen and Malmö, Sweden (the Øresund Bridge), and between Zealand and Funen (the Great Belt Fixed Link). The Copenhagen Malmö Port was also formed between the two cities as the common port for the cities of both nations.
The main railway operator is Danske Statsbaner (Danish State Railways) for passenger services and DB Schenker Rail for freight trains. The railway tracks are maintained by Banedanmark. Copenhagen has a small Metro system, the Copenhagen Metro and the greater Copenhagen area has an extensive electrified suburban railway network, the S-train.
Private vehicles are increasingly used as a means of transport. Because of the high registration tax (180%) and VAT (25%), and the world's highest income tax rate, new cars are very expensive. The purpose of the tax is to discourage car ownership. Whether a smaller fleet of aging cars is better than a larger fleet of modern cars is a matter for debate, however as the car fleet has increased by 45% over the last 30 years the effect of high taxation on the fleet size seems small. The motorway network now covers 1,111 km
In 2007, an attempt was made by the government to favour environmentally friendly cars by slightly reducing taxes on high mileage vehicles. However, this has had little effect, and in 2008 Denmark experienced an increase in the import of fuel inefficient old cars (mostly older than 10 years), primarily from Germany as their costs including taxes keeps these cars within the budget of many Danes.
Denmark is in a strong position in terms of integrating fluctuating and unpredictable energy sources such as wind power in the grid. It is this knowledge that Denmark now aims to exploit in the transport sector by focusing on intelligent battery systems (V2G) and plug-in vehicles.
Energy.
Denmark has changed its energy consumption from 99% fossil fuels (92% oil (all imported) and 7% coal) and 1% biofuels in 1972 to 73% fossil fuels (37% oil (all domestic), 18% coal and 18% natural gas (all domestic)) and 27% renewables (largely biofuels) in 2015. The goal is a full independence of fossil fuels by 2050. This drastic change was initially inspired largely by the discovery of Danish oil and gas reserves in the North Sea in 1972 and the 1973 oil crisis. The course took a giant leap forward in 1984, when the Danish North Sea oil and gas fields, developed by native industry in close cooperation with the state, started major productions. In 1997, Denmark became self-sufficient with energy and the overall CO2 emission from the energy sector began to fall by 1996. Wind energy contribution to the total energy consumption has risen from 1% in 1997 to 5% in 2015.
Since 2000, Denmark has increased Gross National Product (GNP) and at the same time decreased energy consumption. Since 1972, the overall energy consumption has dropped by 6%, eventhough the GNP has doubled in the same period. Denmark had the 6th best energy security in the world in 2014. Denmark has had relatively high energy taxation to encourage careful use of energy since the oil crises in the 1970ies, and Danish industry has adapted to this and gained a competitive edge. The so-called "green taxes" have been broadly criticised partly for being higher than in other nations, but also for being more of a tool for gathering government revenue than a method of promoting "greener" behaviour.
Denmark has mediocre electricity costs (including costs for cleaner energy) in EU, but general taxes (11.7 billion DKK in 2015) increase the price to the highest in Europe. , Denmark has no environment tax on electricity.
Denmark is a long time leader in wind energy and a prominent exporter of Vestas and Siemens wind turbines, and Denmark derives 3.1% of its gross domestic product from renewable (clean) energy technology and energy efficiency, or around €6.5 billion ($9.4 billion). It has integrated fluctuating and less predictable energy sources such as wind power into the grid, and wind produced the equivalent of 42% of Denmark's total electricity consumption in 2015. When viewed in the context of overall energy consumption, wind only accounts for 5%.
Energinet.dk is the Danish national transmission system operator for electricity and natural gas. The electricity grids of western Denmark and eastern Denmark were not connected until 2010, when the 600MW Great Belt Power Link went into operation.
Cogeneration plants are the norm in Denmark, usually with district heating which serves 1.6 million households.
Waste-to-energy incinerators produce both electricity and heating. in Glostrup Municipality operates Denmark's largest incinerator, supplying electricity and heating equivalent to the consumption in over 50,000 households. Amager Bakke is an example of a new incinerator being built.
Oil and Natural Gas.
Denmark has considerable sources of oil and natural gas in the North Sea and ranks as number 32 in the world among net exporters of crude oil.
Esbjerg is Denmark's main city for the oil and gas industry, this is because of its ideal location close to the North Sea, which is where most of Denmark's oil and gas deposits are found. Companies like Maersk Oil, Ramboll, Stimwell Services, ABB, Schlumberger, COWI and Atkins all have offshore related activities in the city. Denmark could have large oil and gas reserves near the Faroe Islands and in Greenland.
Greenland and the Faroe Islands.
Greenland suffered negative economic growth in the early 1990s, but since 1993 the economy has improved. A tight fiscal policy by the Greenland Home Rule Government since the late 1980s helped create a low inflation rate and surpluses in the public budget, but at the cost of rising foreign debt in the Home Rule Government's commercial entities. Since 1990, Greenland has registered a foreign trade deficit.
Following the closure of Greenland's last lead and zinc mine in 1989, Greenland's economy is solely dependent on the fishing and tourism and financial transfers from the Danish central government. Despite resumption of several interesting hydrocarbon and mineral exploration activities, it will take several years before production will begin. Greenland's shrimp fishery is by far the largest source of income, since cod catches have dropped to historically low levels. Greenland also has a prominent whaling industry, Greenlandic Inuit whalers catch around 175 whales per year, making them the third largest hunt in the world after Japan and Norway, though their take is small compared to those nations, who annually averaged around 730 and 590 whales respectively in 1998–2007. The IWC treats the west and east coasts of Greenland as two separate population areas and sets separate quotas for each coast. The far more densely populated west coast accounts for over 90 percent of the catch. In a typical year around 150 minke and 10 fin whales are taken from west coast waters and around 10 minkes are from east coast waters. Since the fishing industry is in decline including whaling, tourism is the only sector offering any near-term potential, and even this is limited due to the short season and high costs. The public sector plays a dominant role in Greenland's economy. Grants from mainland Denmark and EU fisheries payments make up about one-half of the home-rule government's revenues. Recently, Greenland has seen interest from other countries due to the possibilities of large amounts of natural resources, which include: coal, iron ore, lead, zinc, molybdenum, diamonds, gold, platinum, niobium, tantalite, uranium, fish, seals, whales, hydropower, possible oil and gas.
The Faroe Islands also depend almost entirely on fisheries, salmon farming, tourism and related exports. Without Danish Government bailouts in 1992 and 1993, the Faroese economy would have gone bankrupt. Since 1995, the Faroese economy has seen a noticeable upturn, but remains extremely vulnerable. Recent off-shore oil finds close to the Faroese area give hope for Faroese deposits, too, which may form the basis for an economic rebound over the longer term. Like Greenland, the Faroe Islands are also known for its whale hunting, around 950 long-finned pilot whales (Globicephala melaena) are caught each year, mainly during the summer. This should be seen in the context of an estimated local pilot whale population of more than 100,000. Other species are not hunted, though occasionally Atlantic white-sided dolphin can be found among the pilot whales. The Faroese whale slaughter has recently been under attack by the media because of the way it is generally perceived. The whale hunting is not a commercial industry, of no significance to the economy other than the fact that the Faroes need to import less meat from other animals because people get meat for free from the pilot whales, and the whales are for local consumption only.
Neither Greenland, nor the Faroe Islands are members of the European Union. Greenland left the European Economic Community in 1986 and the Faroe Islands declined membership in 1973, when Denmark joined.
GDP.
Table showing selected PPP GDPs and growth - 2002 to 2007 est.:
Major companies.
Denmark has fostered and is home to many multi-national companies, among them: 

</doc>
