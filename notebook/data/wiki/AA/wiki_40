<doc id="5717" url="https://en.wikipedia.org/wiki?curid=5717" title="Canary Islands">
Canary Islands

The Canary Islands (; , ), also known as the Canaries (), are a Spanish archipelago located just off the southern coast of Morocco, west of its southern border. The Canaries constitute one of Spain's 17 autonomous communities and are among the outermost regions (OMR) of the European Union proper. The main islands are (from largest to smallest) Tenerife, Fuerteventura, Gran Canaria, Lanzarote, La Palma, La Gomera and El Hierro. The archipelago also includes a number of islands and islets: La Graciosa, Alegranza, Isla de Lobos, Montaña Clara, Roque del Oeste and Roque del Este.
The archipelago's beaches, climate and important natural attractions, especially Maspalomas in Gran Canaria and Teide National Park and Mount Teide (a World Heritage Site) in Tenerife (the third tallest volcano in the world measured from its base on the ocean floor), make it a major tourist destination with over 12 million visitors per year, especially Tenerife, Fuerteventura, Gran Canaria and Lanzarote. The islands have a subtropical climate, with long warm summers and moderately warm winters. The precipitation levels and the level of maritime moderation varies depending on location and elevation. Green areas as well as desert exist on the archipelago. Due to their location above the temperature inversion layer, the high mountains of these islands are ideal for astronomical observation. For this reason, two professional observatories, Teide Observatory on the island of Tenerife and Roque de los Muchachos Observatory on the island of La Palma, have been built on the islands.
The capital of the Autonomous Community is shared by the rival cities of Santa Cruz de Tenerife and Las Palmas de Gran Canaria, which in turn are the capitals of the provinces of Santa Cruz de Tenerife and Province of Las Palmas. Las Palmas de Gran Canaria has been the largest city in the Canaries since 1768, except for a brief period in the 1910s. Between the 1833 territorial division of Spain and 1927 Santa Cruz de Tenerife was the sole capital of the Canary Islands. In 1927 a decree ordered that the capital of the Canary Islands be shared, as it remains at present. The third largest city of the Canary Islands is San Cristóbal de La Laguna (a World Heritage Site) on Tenerife. This city is also home to the "Consejo Consultivo de Canarias", which is the supreme consultative body of the Canary Islands.
During the times of the Spanish Empire the Canaries were the main stopover for Spanish galleons on their way to the Americas, who came South to catch the prevailing Northeast trade winds.
Etymology.
The name "Islas Canarias" is likely derived from the Latin name "Canariae Insulae", meaning "Islands of the Dogs", a name applied originally only to Gran Canaria. According to the historian Pliny the Elder, the Mauretanian king Juba II named the island "Canaria" because it contained "vast multitudes of dogs of very large size".
Another speculation is that the so-called dogs were actually a species of monk seal ("canis marinus" or "sea dog" was a Latin term for "seal"), critically endangered and no longer present in the Canary Islands. The dense population of seals may have been the characteristic that most struck the few ancient Romans who established contact with these islands by sea.
Alternatively, it is said that the original inhabitants of the island, Guanches, used to worship dogs, mummified them and treated dogs generally as holy animals. The ancient Greeks also knew about a people, living far to the west, who are the "dog-headed ones", who worshipped dogs on an island. Some hypothesize that the Canary Islands dog-worship and the ancient Egyptian cult of the dog-headed god, Anubis are closely connected but there is no explanation given as to which one was first.
Other theories speculate that the name comes from a reported Berber tribe living in the Moroccan Atlas, named in Roman sources as "Canarii", though Pliny again mentions the relation of this term with dogs.
The connection to dogs is retained in their depiction on the islands' coat-of-arms (shown above).
What is certain is that the name of the islands does not derive from the canary bird; rather, the birds are named after the islands.
Geography.
Tenerife is the most populous island, and also the largest island of the archipelago. Gran Canaria, with 865,070 inhabitants, is both the Canary Islands' second most populous island, and the third most populous one in Spain after Majorca. The island of Fuerteventura is the second largest in the archipelago and located from the African coast.
The islands form the Macaronesia ecoregion with the Azores, Cape Verde, Madeira, and the Savage Isles. The archipelago consists of seven large and several smaller islands, all of which are volcanic in origin. The Teide volcano on Tenerife is the highest mountain in Spain, and the third tallest volcano on Earth on a volcanic ocean island. All the islands except La Gomera have been active in the last million years; four of them (Lanzarote, Tenerife, La Palma and El Hierro) have historical records of eruptions since European discovery. The islands rise from Jurassic oceanic crust associated with the opening of the Atlantic. Underwater magmatism commenced during the Cretaceous, and reached the ocean's surface during the Miocene. The islands are considered as a distinct physiographic section of the Atlas Mountains province, which in turn is part of the larger African Alpine System division.
In the summer of 2011 a series of low-magnitude earthquakes occurred beneath El Hierro. These had a linear trend of northeast-southwest. In October a submarine eruption occurred about south of Restinga. This eruption produced gases and pumice but no explosive activity was reported.
According to the position of the islands with respect to the north-east trade winds, the climate can be mild and wet or very dry. Several native species form laurisilva forests.
As a consequence, the individual islands in the Canary archipelago tend to have distinct microclimates. Those islands such as El Hierro, La Palma and La Gomera lying to the west of the archipelago have a climate which is influenced by the moist Gulf Stream. They are well vegetated even at low levels and have extensive tracts of sub-tropical laurisilva forest. As one travels east toward the African coast, the influence of the gulf stream diminishes, and the islands become increasingly arid. Fuerteventura and Lanzarote the islands which are closest to the African mainland are effectively desert or semi desert. Gran Canaria is known as a "continent in miniature" for its diverse landscapes like Maspalomas and Roque Nublo. In terms of its climate Tenerife is particularly interesting. The north of the island lies under the influence of the moist Atlantic winds and is well vegetated, while the south of the island around the tourist resorts of Playa de las Americas and Los Cristianos is arid. The island rises to almost above sea level, and at altitude, in the cool relatively wet climate, forests of the endemic pine "Pinus canariensis" thrive. Many of the plant species in the Canary Islands, like the Canary Island pine and the dragon tree, "Dracaena draco" are endemic, as noted by Sabin Berthelot and Philip Barker Webb in their epic work, "L'Histoire Naturelle des Îles Canaries" (1835–50).
Four of Spain's thirteen national parks are located in the Canary Islands, more than any other autonomous community. Teide National Park is the most visited in Spain, and the oldest and largest within the Canary Islands. The parks are:
The following table shows the highest mountains in each of the islands:
Climate.
The climate is subtropical and desertic, moderated by the sea and in summer by the trade winds. There are a number of microclimates and the classifications range mainly from semi-arid to desert. According to the Köppen climate classification, the majority of the Canary Islands have a hot desert climate represented as BWh. There also exists a subtropical humid climate which is very influenced by the ocean in the middle of the islands of La Gomera, Tenerife and La Palma; where the laurisilva forests grow.
Geology.
The originally volcanic islands –seven major islands, one minor island, and several small islets– were formed by the Canary hotspot. The Canary Islands is the only place in Spain where volcanic eruptions have been recorded during the Modern Era, with some volcanoes still active (El Hierro, 2011).
Volcanic islands such as the those in the Canary chain often have steep ocean cliffs caused by catastrophic debris avalanches and landslides.
Political geography.
The Autonomous Community of the Canary Islands consists of two provinces, Las Palmas and Santa Cruz de Tenerife, whose capitals (Las Palmas de Gran Canaria and Santa Cruz de Tenerife) are capitals of the autonomous community. Each of the seven major islands is ruled by an island council named "Cabildo Insular".
The international boundary of the Canaries is the subject of dispute between Spain and Morocco. Morocco's official position is that international laws regarding territorial limits do not authorise Spain to claim seabed boundaries based on the territory of the Canaries, since the Canary Islands enjoy a high degree of autonomy. 
The boundary determines the ownership of seabed oil deposits and other ocean resources. Morocco and Spain have therefore been unable to agree on a compromise regarding the territorial boundary, since neither nation wants to cede its claimed right to the vast resources whose ownership depends upon the boundary. In 2002, for example, Morocco rejected a unilateral Spanish proposal.
The Islands have 13 seats in the Spanish Senate. Of these, 11 seats are directly elected, 3 for Gran Canaria, 3 for Tenerife, 1 for each other island; 2 seats are indirectly elected by the regional Autonomous Government. The local government is presided over by Paulino Rivero, the current President of the Canary Islands.
History.
Ancient and pre-colonial times.
Before the arrival of the aborigines, the Canaries were inhabited by prehistoric animals; for example, the giant lizard ("Gallotia goliath"), or giant rats ("Canariomys bravoi" and "Canariomys tamarani").
The islands were visited by the Phoenicians, the Greeks, and the Carthaginians. According to the 1st century AD Roman author and philosopher Pliny the Elder, the archipelago was found to be uninhabited when visited by the Carthaginians under Hanno the Navigator, but that they saw ruins of great buildings. This story may suggest that the islands were inhabited by other peoples prior to the Guanches. King Juba, Augustus's Numidian protégé, is credited with discovering the islands for the Western world. He dispatched a naval contingent to re-open the dye production facility at Mogador in what is now western Morocco in the early 1st century AD. That same naval force was subsequently sent on an exploration of the Canary Islands, using Mogador as their mission base.
The Romans called each of the islands; "Ninguaria" or "Nivaria" (Tenerife), "Canaria" (Gran Canaria), "Pluvialia" or "Invale" (Lanzarote), "Ombrion" (La Palma), "Planasia" (Fuerteventura), "Iunonia" or "Junonia" (El Hierro) and "Capraria" (La Gomera).
When the Europeans began to explore the islands in the late Middle Ages, they encountered several indigenous populations living at a Neolithic level of technology. Although the prehistory of the settlement of the Canary Islands is still unclear, linguistic and genetic analyses seem to indicate that at least some of these inhabitants shared a common origin with the Berbers of northern Africa. The pre-colonial inhabitants came to be known collectively as the Guanches, although "Guanches" was originally the name for only the indigenous inhabitants of Tenerife. From the 14th century onward, numerous visits were made by sailors from Majorca, Portugal and Genoa. Lancelotto Malocello settled on Lanzarote in 1312. The Majorcans established a mission with a bishop in the islands that lasted from 1350 to 1400.
Castilian conquest.
There may have been a Portuguese expedition that attempted to colonise the islands as early as 1336, but there is not enough hard evidence to support this. In 1402, the Castilian conquest of the islands began, with the expedition of French explorers Jean de Béthencourt and Gadifer de la Salle, nobles and vassals of Henry III of Castile, to Lanzarote. From there, they conquered Fuerteventura (1405) and El Hierro. Béthencourt received the title King of the Canary Islands, but still recognised King Henry III as his overlord.
Béthencourt also established a base on the island of La Gomera, but it would be many years before the island was truly conquered. The natives of La Gomera, and of Gran Canaria, Tenerife, and La Palma, resisted the Castilian invaders for almost a century. In 1448 Maciot de Béthencourt sold the lordship of Lanzarote to Portugal's Prince Henry the Navigator, an action that was not accepted by the natives nor by the Castilians. Despite Pope Nicholas V ruling that the Canary Islands were under Portuguese control, a crisis swelled to a revolt which lasted until 1459 with the final expulsion of the Portuguese. In 1479, Portugal and Castile signed the Treaty of Alcáçovas. The treaty settled disputes between Castile and Portugal over the control of the Atlantic, in which Castilian control of the Canary Islands was recognised but which also confirmed Portuguese possession of the Azores, Madeira, the Cape Verde islands and gave them rights to lands discovered and to be discovered ... and any other island which might be found and conquered from the Canary islands beyond toward Guinea.
The Castilians continued to dominate the islands, but due to the topography and the resistance of the native Guanches, complete pacification was not achieved until 1495, when Tenerife and La Palma were finally subdued by Alonso Fernández de Lugo. After that, the Canaries were incorporated into the Kingdom of Castile.
After the conquest.
After the conquest, the Castilians imposed a new economic model, based on single-crop cultivation: first sugar cane; then wine, an important item of trade with England. In this era, the first institutions of colonial government were founded. Both Gran Canaria, a colony of Castile since March 6, 1480 (from 1556, of Spain), and Tenerife, a Spanish colony since 1495, had separate governors.
The cities of Santa Cruz de Tenerife and Las Palmas de Gran Canaria became a stopping point for the Spanish conquerors, traders, and missionaries on their way to the New World. This trade route brought great prosperity to some of the social sectors of the islands. The islands became quite wealthy and soon were attracting merchants and adventurers from all over Europe. Magnificent palaces and churches were built on La Palma during this busy, prosperous period. The Church of El Salvador survives as one of the island's finest examples of the architecture of the 16th century.
The Canaries' wealth invited attacks by pirates and privateers. Ottoman Turkish admiral and privateer Kemal Reis ventured into the Canaries in 1501, while Murat Reis the Elder captured Lanzarote in 1585.
The most severe attack took place in 1599, during the Dutch War of Independence. A Dutch fleet of 74 ships and 12,000 men, commanded by Pieter van der Does, attacked the capital Las Palmas de Gran Canaria (the city had 3,500 of Gran Canaria's 8,545 inhabitants). The Dutch attacked the Castillo de la Luz, which guarded the harbor. The Canarians evacuated civilians from the city, and the Castillo surrendered (but not the city). The Dutch moved inland, but Canarian cavalry drove them back to Tamaraceite, near the city.
The Dutch then laid siege to the city, demanding the surrender of all its wealth. They received 12 sheep and 3 calves. Furious, the Dutch sent 4,000 soldiers to attack the Council of the Canaries, who were sheltering in the village of Santa Brígida. 300 Canarian soldiers ambushed the Dutch in the village of Monte Lentiscal, killing 150 and forcing the rest to retreat. The Dutch concentrated on Las Palmas de Gran Canaria, attempting to burn it down. The Dutch pillaged Maspalomas, on the southern coast of Gran Canaria, San Sebastian on La Gomera, and Santa Cruz on La Palma, but eventually gave up the siege of Las Palmas de Gran Canaria and withdrew.
In 1618 the Algerian pirates attacked Lanzarote and La Gomera taking 1000 captives to be sold as slaves. Another noteworthy attack occurred in 1797, when Santa Cruz de Tenerife was attacked by a British fleet under the future Lord Nelson on 25 July. The British were repulsed, losing almost 400 men. It was during this battle that Nelson lost his right arm.
18th to 19th century.
The sugar-based economy of the islands faced stiff competition from Spain's American colonies. Low prices in the sugar market in the 19th century caused severe recessions on the islands. A new cash crop, cochineal ("cochinilla"), came into cultivation during this time, saving the islands' economy.
By the end of the 18th century, Canary Islanders had already emigrated to Spanish American territories, such as Havana, Veracruz, Santo Domingo, San Antonio, Texas and St. Bernard Parish, Louisiana. These economic difficulties spurred mass emigration, primarily to the Americas, during the 19th and first half of the 20th century. Between 1840 and 1890 as many as 40,000 Canary Islanders emigrated to Venezuela. Also, thousands of Canarians moved to Puerto Rico where the Spanish monarchy felt that Canarians would adapt to island life better than other immigrants from the mainland of Spain. Deeply entrenched traditions, such as the Mascaras Festival in the town of Hatillo, Puerto Rico, are an example of Canarian culture still preserved in Puerto Rico. Similarly, many thousands of Canarians emigrated to the shores of Cuba. During the Spanish–American War of 1898, the Spanish fortified the islands against possible American attack, but an attack never came.
Romantic period and scientific expeditions.
Sirera and Renn (2004) distinguish two different types of expeditions, or voyages, during the period 1770–1830, which they term "the Romantic period":
First are "expeditions financed by the States, closely related with the official scientific Institutions. characterised by having strict scientific objectives (and inspired by) the spirit of Illustration and progress".
In this type of expedition, Sirera and Renn include the following travellers:
The second type of expedition identified by Sirera and Renn is one that took place starting from more or less private initiatives. Among these, the key exponents were the following:
Sirera and Renn identify the period 1770–1830 as one in which "In a panorama dominated until that moment by France and England enters with strength and brio Germany of the Romantic period whose presence in the islands will increase".
Early 20th century.
At the beginning of the 20th century, the British introduced a new cash-crop, the banana, the export of which was controlled by companies such as Fyffes.
The rivalry between the elites of the cities of Las Palmas de Gran Canaria and Santa Cruz de Tenerife for the capital of the islands led to the division of the archipelago into two provinces in 1927. This has not laid to rest the rivalry between the two cities, which continues to this day.
During the time of the Second Spanish Republic, Marxist and anarchist workers' movements began to develop, led by figures such as Jose Miguel Perez and Guillermo Ascanio. However, outside of a few municipalities, these organisations were a minority and fell easily to Nationalist forces during the Spanish Civil War.
Franco regime.
In 1936, Francisco Franco was appointed General Commandant of the Canaries. He joined the military revolt of July 17 which began the Spanish Civil War. Franco quickly took control of the archipelago, except for a few points of resistance on La Palma and in the town of Vallehermoso, on La Gomera. Though there was never a proper war in the islands, the post-war suppression of political dissent on the Canaries was most severe.
During the Second World War, Winston Churchill prepared plans for the British seizure of the Canary Islands as a naval base, in the event of Gibraltar being invaded from the Spanish mainland.
Opposition to Franco's regime did not begin to organise until the late 1950s, which experienced an upheaval of parties such as the Communist Party of Spain and the formation of various nationalist, leftist parties.
Self-governance.
After the death of Franco, there was a pro-independence armed movement based in Algeria, the Movement for the Independence and Self-determination of the Canaries Archipelago (MAIAC). In 1968, the Organisation of African Unity recognized the MAIAC as a legitimate African independence movement, and declared the Canary Islands as an African territory still under foreign rule. Now there are some pro-independence political parties, like the CNC and the Popular Front of the Canary Islands, but none of them calls for an armed struggle. Their popular support is almost insignificant, with no presence in either the autonomous parliament or the "cabildos insulares".
After the establishment of a democratic constitutional monarchy in Spain, autonomy was granted to the Canaries via a law passed in 1982. In 1983, the first autonomous elections were held. The Spanish Socialist Workers' Party (PSOE) won. In the 2007 elections, the PSOE gained a plurality of seats, but the nationalist Canarian Coalition and the conservative Partido Popular (PP) formed a ruling coalition government.
According to "Centro de Investigaciones Sociológicas" (Sociological Research Center) in 2010, 43.5% of the population of the Canary Islands feels more Canarian than Spanish (37.6%), only Canarian (7.6%), compared to 5.4% that feels more Spanish than Canarian (2.4%) or only Spanish (3%). The most popular choice of those who feel equally Spanish and Canarian, with 49.9%. With these data, one of the Canary recorded levels of identification with higher autonomy from Spain.
Demographics.
The Canary Islands have a population of 2,117,519 inhabitants (2011), making it the eighth most populous of Spain's autonomous communities, with a density of 282.6 inhabitants per km². The total area of the archipelago is .
The Canarian population includes long-tenured residents and new waves of mainland Spanish immigrants, as well as Portuguese, Italians, Flemings and Britons. Of the total Canarian population in 2009 (2,098,593) 1,799,373 were Spanish and 299,220 foreigners. Of these, the majority are Europeans (55%), including Germans (39,505), British (37,937) and Italians (24,177). There are also 86,287 inhabitants from the Americas, mainly Colombians (21,798), Venezuelans (11,958), Cubans (11,098) and Argentines (10,159). There are also 28,136 African residents, mostly Moroccans (16,240).
Population of the individual islands.
The population of the islands according to the 2010 data are:
Religion.
The Catholic religion has been since the Conquest of the Canary Islands the majority religion in the archipelago (for more than five centuries). However, there are other religious communities:
Roman Catholic Church.
The overwhelming majority of native Canarians are Roman Catholic with various smaller foreign-born populations of other Christian beliefs such as Protestants from northern Europe.
The appearance of the Virgin of Candelaria (Patron of Canary Islands) was credited with moving the Canary Islands toward Christianity. In the Canary Islands they were born two important Catholic saints: Peter of Saint Joseph de Betancur and José de Anchieta. Both born on the island of Tenerife, they were respectively missionaries in Guatemala and Brazil.
The Canary Islands are divided into two Catholic dioceses, each governed by a bishop:
Other religions.
Separate from the overwhelming Christian majority are a minority of Muslims, though no official mention is made of them. Other religious faiths represented include The Church of Jesus Christ of Latter-day Saints as well as Hinduism. Minority religions are also present such as the Church of the Guanche People which is classified as a neo-pagan native religion, it also highlights Buddhism, Baha'i, Chinese religions and Afro-American religion.
Statistics.
The distribution of beliefs in 2012 according to the CIS Barometer Autonomy was as follows:
Among the believers 38.7% go to religious services frequently.
Islands.
El Hierro.
El Hierro, the westernmost island, covers , making it the smallest of the major islands, and the least populous with 10,753 inhabitants. The whole island was declared Reserve of the Biosphere in 2000. Its capital is Valverde. Also known as Ferro, it was once believed to be the westernmost land in the world.
Fuerteventura.
Fuerteventura, with a surface of , is the second-most extensive island of the archipelago. It has been declared a Biosphere reserve by Unesco. It has a population of 100,929. Being also the most ancient of the islands, it is the one that is more eroded: its highest point is the Peak of the Bramble, at a height of . Its capital is Puerto del Rosario.
Gran Canaria.
Gran Canaria has 845,676 inhabitants. The capital, Las Palmas de Gran Canaria (377,203 inhabitants), is the most populous city and shares the status of capital of the Canaries with Santa Cruz de Tenerife. Gran Canaria's surface area is . In center of the island lie the Roque Nublo and Pico de las Nieves ("Peak of Snow") . In the south of island are the Maspalomas Dunes (Gran Canaria), these are the biggest tourist attractions.
La Gomera.
La Gomera has an area of and is the second least populous island with 22,622 inhabitants. Geologically it is one of the oldest of the archipelago. The insular capital is San Sebastian de La Gomera. Garajonay's National Park is here.
Lanzarote.
Lanzarote is the easternmost island and one of the most ancient of the archipelago, and it has shown evidence of recent volcanic activity. It has a surface of , and a population of 139,506 inhabitants, including the adjacent islets of the Chinijo Archipelago. The capital is Arrecife, with 56,834 inhabitants.
Chinijo Archipelago.
The Chinijo Archipelago includes the islands La Graciosa, Alegranza, Montaña Clara, Roque del Este and Roque del Oeste. It has a surface of , and a population of 658 inhabitants all of them in the la Graciosa island. With , La Graciosa, is the smallest inhabited island of the Canaries, and the major island of the Chinijo Archipelago.
La Palma.
La Palma, with 86,528 inhabitants covering an area of , is in its entirety a biosphere reserve. It shows no recent signs of volcanic activity, even though the volcano Teneguía entered into eruption last in 1971. In addition, it is the second-highest island of the Canaries, with the Roque de los Muchachos as highest point. Santa Cruz de La Palma (known to those on the island as simply "Santa Cruz") is its capital.
Tenerife.
Tenerife is, with its area of , the most extensive island of the Canary Islands. In addition, with 906,854 inhabitants it is the most populated island of the archipelago and Spain. Two of the islands' principal cities are located on it: The capital, Santa Cruz de Tenerife and San Cristóbal de La Laguna (a World Heritage Site). San Cristóbal de La Laguna, the second city of the island is home to the oldest university in the Canary Islands, the University of La Laguna. The Teide, with its is the highest peak of Spain and also a World Heritage Site. Tenerife is the site of the worst air disaster in the history of aviation, in which 583 people were killed in the collision of two Boeing 747s on March 27, 1977.
Economy.
The economy is based primarily on tourism, which makes up 32% of the GDP. The Canaries receive about 12 million tourists per year. Construction makes up nearly 20% of the GDP and tropical agriculture, primarily bananas and tobacco, are grown for export to Europe and the Americas. Ecologists are concerned that the resources, especially in the more arid islands, are being overexploited but there are still many agricultural resources like tomatoes, potatoes, onions, cochineal, sugarcane, grapes, vines, dates, oranges, lemons, figs, wheat, barley, maize, apricots, peaches and almonds.
The economy is € 25 billion (2001 GDP figures). The islands experienced continuous growth during a 20-year period, up until 2001, at a rate of approximately 5% annually. This growth was fueled mainly by huge amounts of Foreign Direct Investment, mostly to develop tourism real estate (hotels and apartments), and European Funds (near €11 billion euro in the period from 2000 to 2007), since the Canary Islands are labelled Region Objective 1 (eligible for euro structural funds). Additionally, the EU allows the Canary Islands Government to offer special tax concessions for investors who incorporate under the Zona Especial Canaria (ZEC) regime and create more than 5 jobs.
Spain gave permission in August 2014 for Repsol and its partners to explore oil and gas prospects off the Canary Islands, involving an investment of €7.5 billion over four years, commencing at the end of 2016. Repsol at the time said the area could ultimately produce 100,000 barrels of oil a day, which would meet 10 percent of Spain's energy needs.
The Canary Islands have great natural attractions, climate and beaches make the islands a major tourist destination, being visited each year by about 12 million people (11,986,059 in 2007, noting 29% of Britons, 22% of Spanish, not residents of the Canaries, and 21% of Germans). Among the islands, Tenerife has the largest number of tourists received annually, followed by Gran Canaria and Lanzarote. The archipelago's principal tourist attraction is the Teide National Park (in Tenerife) where the highest mountain in Spain and third largest volcano in the world (Mount Teide), receives over 2.8 million visitors annually.
The combination of high mountains, proximity to Europe, and clean air has made the Roque de los Muchachos peak (on La Palma island) a leading location for telescopes like the Grantecan.
The islands are outside the European Union customs territory and VAT area, though politically within the EU. Instead of VAT there is a local Sales Tax (IGIC) which has a general rate of 7%, an increased tax rate of 13.5%, a reduced tax rate of 3% and a zero tax rate for certain basic need products and services. Consequently, some products are subject to import tax and VAT if being exported from the islands into mainland Spain or the rest of the EU.
Canarian time is Western European Time (WET) (or GMT; in summer one hour ahead of GMT). So Canarian time is one hour behind that of mainland Spain and the same as that of the UK, Ireland and Portugal all year round.
Transport.
The Canary Islands have eight airports altogether, two of the main ports of Spain, and an extensive network of autopistas (highways) and other roads. For a road map see multimap.
There are large ferry boats that link islands as well as fast ferries linking most of the islands. Both types can transport large numbers of passengers and cargo (including vehicles). Fast ferries are made of aluminium and powered by modern and efficient diesel engines, while conventional ferries have a steel hull and are powered by heavy oil. Fast ferries travel relatively quickly (in excess of 30 knots) and are a faster method of transportation than the conventional ferry (some 20 knots). A typical ferry ride between La Palma and Tenerife may take up to eight hours or more while a fast ferry takes about 2 and a half hours and between Tenerife and Gran Canaria can be about one hour.
The largest airport is the Gran Canaria airport, with about 10,000,000 passengers. It is also the 5th largest airport in Spain. The biggest port is in Las Palmas de Gran Canaria. It is an important port for commerce with Europe, Africa and the Americas. It is the 4th biggest commercial port in Spain with more than 1,400,000 TEU's. The largest commercial companies of the world, including MSC and Maersk, operate here. In this port there is an international post of the Red Cross, one of only four points like this all around the world. Tenerife has two airports, Tenerife North Airport (4,048,281 passengers) and Tenerife South Airport (6,939,168 passengers).
Canary Islands has an input of 16,874,532 passengers. The two main islands (Tenerife and Gran Canaria) receive the greatest number of passengers; Tenerife 6,204,499 passengers and Gran Canaria 5,011,176 passengers.
The port of Las Palmas is first in freight traffic in the islands, while the port of Santa Cruz de Tenerife is the first fishing port with approximately 7,500 tons of fish caught, according to the Spanish government publication Statistical Yearbook of State Ports. Similarly, it is the second port in Spain as regards ship traffic, only surpassed by the Port of Algeciras Bay. The port's facilities include a border inspection post (BIP) approved by the European Union, which is responsible for inspecting all types of imports from third countries or exports to countries outside the European Economic Area. The port of Los Cristianos (Tenerife) has the greatest number of passengers recorded in the Canary Islands, followed by the port of Santa Cruz de Tenerife. The Port of Las Palmas is the third port in the islands in passengers and first in number of vehicles transported.
Rail transport.
The Tenerife Tram opened in 2007 and the only one in the Canary Islands, travelling between the cities of Santa Cruz de Tenerife and San Cristóbal de La Laguna. It is currently planned to have three lines in the Canary Islands (two in Tenerife and one in Gran Canaria). The planned Gran Canaria tram route will be from Las Palmas de Gran Canaria to Maspalomas (south).
Wildlife.
The official symbols from nature associated with Canary Islands are the bird "Serinus canaria" (canary) and the "Phoenix canariensis" palm.
Terrestrial wildlife.
With a range of habitats, the Canary Islands exhibit diverse plant species. The bird life includes European and African species, such as the black-bellied sandgrouse; and a rich variety of endemic (local) taxa including the:
Terrestrial fauna includes geckos (such as the striped Canary Islands gecko) and wall lizards, and three endemic species of recently rediscovered and critically endangered giant lizard: the El Hierro giant lizard (or Roque Chico de Salmor giant lizard), La Gomera giant lizard, and La Palma giant lizard. Mammals include the Canarian shrew, Canary big-eared bat, the Algerian hedgehog (which may have been introduced) and the more recently introduced mouflon. Some endemic mammals, the lava mouse, Tenerife giant rat and Gran Canaria giant rat, are extinct, as are the Canary Islands quail, long-legged bunting, and the eastern Canary Islands chiffchaff.
Marine life.
The marine life found in the Canary Islands is also varied, being a combination of North Atlantic, Mediterranean and endemic species. In recent years, the increasing popularity of both scuba diving and underwater photography have provided biologists with much new information on the marine life of the islands.
Fish species found in the islands include many species of shark, ray, moray eel, bream, jack, grunt, scorpionfish, triggerfish, grouper, goby, and blenny. In addition, there are many invertebrate species, including sponge, jellyfish, anemone, crab, mollusc, sea urchin, starfish, sea cucumber and coral.
There are a total of 5 different species of marine turtle that are sighted periodically in the islands, the most common of these being the endangered loggerhead sea turtle. The other four are the green sea turtle, hawksbill sea turtle, leatherback sea turtle and Kemp's ridley sea turtle. Currently, there are no signs that any of these species breed in the islands, and so those seen in the water are usually migrating. However, it is believed that some of these species may have bred in the islands in the past, and there are records of several sightings of leatherback sea turtle on beaches in Fuerteventura, adding credibility to the theory.
Marine mammals include the large varieties of cetaceans including rare and not well-known species (see more details in the "Marine life of the Canary Islands"). Hooded seals have also been known to be vagrant in the Canary Islands every now and then. The Canary Islands were also formerly home to a population of the rarest pinniped in the world, the Mediterranean monk seal.
National parks of the Canary Islands.
The Canary Islands officially has four national parks, of which two have been declared World Heritage Site by UNESCO, and the other two declared a World Biosphere Reserve, these national parks are:
Sports.
A unique form of wrestling known as Canarian wrestling ("lucha canaria") has opponents stand in a special area called a "terrero" and try to throw each other to the ground using strength and quick movements.
Another sport is the "game of the sticks" where opponents fence with long sticks. This may have come about from the shepherds of the islands who would challenge each other using their long walking sticks.
Another sport is called the shepherd's jump ("salto del pastor"). This involves using a long stick to vault over an open area. This sport possibly evolved from the shepherd's need to occasionally get over an open area in the hills as they were tending their sheep.
The two main football teams in the archipelago are: the CD Tenerife (founded in 1912) and UD Las Palmas (founded in 1949). Now Tenerife play in Liga Adelante and Las Palmas in La Liga.
Carnival.
The Carnival of Santa Cruz de Tenerife and Carnival of Las Palmas are one of the most famous Carnivals in Spain. It is celebrated on the streets between the months of February and March.

</doc>
<doc id="5718" url="https://en.wikipedia.org/wiki?curid=5718" title="Chuck D">
Chuck D

Carlton Douglas Ridenhour (born August 1, 1960), better known by his stage name Chuck D, is an American emcee, author, and producer. He helped create politically and socially conscious hip hop music in the mid-1980s as the leader of the rap group Public Enemy. About.com ranked him at No. 9 on their list of the Top 50 MCs of Our Time, while "The Source" ranked him at No. 12 on their list of the Top 50 Hip-Hop Lyricists of All Time.
Early life.
Ridenhour was born in Queens, New York. After graduating from Roosevelt Junior-Senior High School, he went to Adelphi University on Long Island to study graphic design, where he met William Drayton (Flavor Flav). He received a B.F.A. from Adelphi in 1984 and later received an honorary doctorate from Adelphi in 2013. He is the son of Lorenzo Ridenhour.
Career.
Upon hearing Ridenhour's demo track "Public Enemy Number One", fledgling producer/upcoming music-mogul Rick Rubin insisted on signing him to his Def Jam label.
Their major label albums were "Yo! Bum Rush the Show" (1987), "It Takes a Nation of Millions to Hold Us Back" (1988), "Fear of a Black Planet" (1990), "Apocalypse 91... The Enemy Strikes Black" (1991), "Greatest Misses" (1992), and "Muse Sick-n-Hour Mess Age" (1994). They also released a full-length album soundtrack for the film "He Got Game" in 1998. Ridenhour also contributed (as Chuck D) to several episodes of the PBS documentary series "The Blues". He has appeared as a featured artist on many other songs and albums, having collaborated with artists such as Janet Jackson, Kool Moe Dee, The Dope Poet Society, Run–D.M.C., Ice Cube, Boom Boom Satellites, Rage Against the Machine, Anthrax, John Mellencamp and many others. In 1990, he appeared on "Kool Thing", a song by the alternative rock band Sonic Youth, and along with Flavor Flav, he sang on George Clinton's song "Tweakin'", which appears on his 1989 album "The Cinderella Theory". In 1993, he executive produced "Got 'Em Running Scared", an album by Ichiban Records group Chief Groovy Loo and the Chosen Tribe.
Later career.
In 1996, Ridenhour released "Autobiography of Mistachuck" on Mercury Records. Chuck D made a rare appearance at the 1998 MTV Video Music Awards, presenting the Video Vanguard Award to the Beastie Boys, whilst commending their musicianship. In November 1998, he settled out of court with Christopher "The Notorious B.I.G." Wallace's estate over the latter's sampling of his voice in the song "Ten Crack Commandments". The specific sampling is Ridenhour counting off the numbers one to nine on the track "Shut 'Em Down".
In September 1999, he launched a multi-format "supersite" on the web site Rapstation.com. A home for the vast global hip hop community, the site boasts a TV and radio station with original programming, many of hip hop's most prominent DJs, celebrity interviews, free MP3 downloads (the first was contributed by multi-platinum rapper Coolio), downloadable ringtones by ToneThis, social commentary, current events, and regular features on turning rap careers into a viable living. Since 2000, he has been one of the most vocal supporters of peer-to-peer file sharing in the music industry.
He loaned his voice to ' as DJ Forth Right MC for the radio station Playback FM. In 2000, he collaborated with Public Enemy's Gary G-Whiz and MC Lyte on the theme music to the television show "Dark Angel". He appeared with Henry Rollins in a cover of Black Flag's "Rise Above" for the album '. He was also featured on Z-Trip's album "Shifting Gears" on a track called "Shock and Awe"; a 12-inch of the track was released featuring artwork by Shepard Fairey. In 2008 he contributed a chapter to "Sound Unbound: Sampling Digital Music and Culture" (The MIT Press, 2008) edited by Paul D. Miller a.k.a. DJ Spooky, and also turned up on The Go! Team's album "Proof of Youth" on the track "Flashlight Fight." He also fulfilled his childhood dreams of being a sports announcer by performing the play-by-play commentary in the video game "NBA Ballers: Chosen One" on Xbox 360 and PlayStation 3.
In 2009, Ridenhour wrote the foreword to the book "The Love Ethic: The Reason Why You Can't Find and Keep Beautiful Black Love" by Kamau and Akilah Butler. He also appeared on Brother Ali's album, "Us".
In March 2011, Chuck D re-recorded vocals with The Dillinger Escape Plan for a cover of "Fight the Power".
Chuck D duetted with Rock singer Meat Loaf on his 2011 album "Hell in a Handbasket" on the song "Mad Mad World/The Good God Is a Woman and She Don't Like Ugly".
Rapping technique and creative process.
Chuck D is known for his powerful rapping voice - "How to Rap" says, “Chuck D of Public Enemy has a powerful, resonant voice that is often acclaimed as one of the most distinct and impressive in hip-hop”. Chuck D says this was based on listening to Melle Mel and sportscasters such as Marv Albert.
Chuck D often comes up with a title for a song first and that he writes on paper, though he sometimes edits using a computer. He also prefers to not punch in vocals, and he prefers to not overdub vocals.
Politics.
Ridenhour is politically active; he co-hosted "Unfiltered" on Air America Radio, testified before Congress in support of peer-to-peer MP3 sharing, and was involved in a 2004 rap political convention. He continues to be an activist, publisher, lecturer, and producer. Addressing the negative views associated with rap music, he co-wrote the essay book "Fight the Power: Rap, Race, and Reality", along with Yusuf Jah. He argues that "music and art and culture is escapism, and escapism sometimes is healthy for people to get away from reality", but sometimes the distinction is blurred and that's when "things could lead a young mind in a direction." He also founded the record company Slam Jamz and acted as narrator in Kareem Adouard's short film "Bling: Consequences and Repercussions", which examines the role of conflict diamonds in bling fashion. Despite Chuck D and Public Enemy's success, Chuck D claims that popularity or public approval was never a driving motivation behind their work. He is admittedly skeptical of celebrity status, revealing in a 1999 interview with BOMB Magazine that, "The key for the record companies is to just keep making more and more stars, and make the ones who actually challenge our way of life irrelevant. The creation of celebrity has clouded the minds of most people in America, Europe and Asia. It gets people off the path they need to be on as individuals." 
In an interview with "Le Monde" published January 29, 2008, Chuck D stated that rap is devolving so much into a commercial enterprise, that the relationship between the rapper and the record label is that of slave to a master. He believes that nothing has changed for African-Americans since the debut of Public Enemy and, although he thinks that an Obama-Clinton alliance is great, he does not feel that the establishment will allow anything of substance to be accomplished. He also stated that French President Sarkozy is like any other European elite: he has profited through the murder, rape, and pillaging of those less fortunate and he refuses to allow equal opportunity for those men and women from Africa. In this article, he also defended a comment made by Professor Griff in the past that he says was taken out of context by the media. The real statement was a critique of the Israeli government and its treatment of the Palestinian people. Chuck D stated that it is Public Enemy's belief that all human beings are equal.
In an interview with the magazine "N'Digo" published in late June 2008, he spoke of today's mainstream urban music seemingly relishing the addictive euphoria of materialism and sexism, perhaps being the primary cause of many people harboring resentment towards the genre and its future. However he has expressed hope for its resurrection, saying "It's only going to be dead if it doesn’t talk about the messages of life as much as the messages of death and non-movement", citing artists such as NYOil, M.I.A. and The Roots as socially conscious artists who push the envelope creatively. "A lot of cats are out there doing it, on the Web and all over. They’re just not placing their career in the hands of some major corporation."
Most recently Chuck D became involved in "Let Freedom Sing: The Music of the Civil Rights", a 3-CD box set from Time Life. He wrote the introduction to the liner notes and is visiting colleges across the nation discussing the significance of the set. He's also set to appear in a follow up movie called "Let Freedom Sing: The Music That Inspired the Civil Rights Movement".
In 2010, Chuck D released a track entitled "Tear Down That Wall". He said, “I talked about the wall not only just dividing the U.S. and Mexico but the states of California, New Mexico and Texas. But Arizona, it's like, come on. Now they're going to enforce a law that talks about basically racial profiling.”
He is on the board of the TransAfrica Forum a Pan African organization that works for the right of Africa, Caribbean and Latin American issues.
Personal life.
Chuck D is married to Gaye Theresa Johnson, an associate professor in the Department of Black Studies at the University of California, Santa Barbara.
He is a pescatarian.
Discography.
Chuck D.
Studio albums
Compilation albums

</doc>
<doc id="5719" url="https://en.wikipedia.org/wiki?curid=5719" title="Cutaway (filmmaking)">
Cutaway (filmmaking)

In film and video, a cutaway shot is the interruption of a continuously filmed action by inserting a view of something else. It is usually, although not always, followed by a cut back to the first shot, when the cutaway avoids a jump cut. The cutaway shot does not necessarily contribute any dramatic content of its own, but is used to help the editor assemble a longer sequence. For this reason, editors choose cutaway shots related to the main action, such as another action or object in the same location. For example, if the main shot is of a man walking down an alley, possible cutaways may include a shot of a cat on a nearby dumpster or a shot of a person watching from a window overhead.
Similarly, a cutaway scene is the interruption of a scene with the insertion of another scene, generally unrelated or only peripherally related to the original scene. The interruption is usually quick, and is usually, although not always, ended by a return to the original scene. The effect is of commentary to the original scene, frequently comic in nature.
Usage.
The most common use of cutaway shots in dramatic films is to adjust the pace of the main action, to conceal the deletion of some unwanted part of the main shot, or to allow the joining of parts of two versions of that shot. For example, a scene may be improved by cutting a few frames out of an actor's pause; a brief view of a listener can help conceal the break. Or the actor may fumble some of his lines in a group shot; rather than discarding a good version of the shot, the director may just have the actor repeat the lines for a new shot, and cut to that alternate view when necessary.
Cutaways are also used often in older horror films in place of special effects. For example, a shot of a zombie getting its head cut off may, for instance, start with a view of an axe being swung through the air, followed by a close-up of the actor swinging it, then followed by a cut back to the now severed head. George A. Romero, creator of the Dead Series, and Tom Savini pioneered effects that removed the need for cutaways in horror films. The animated television show "Family Guy" often uses cutaway gags as humor.
In news broadcasting and documentary work, the cutaway is used much as it would be in fiction. On location, there is usually just one camera to film an interview, and it's usually trained on the interviewee. Often there is also only one microphone. After the interview, the interviewer will usually repeat his questions while he himself is being filmed, with pauses as they act as if to listen to the answers. These shots can be used as cutaways. Cutaways to the interviewer, called noddies, can also be used to cover cuts.

</doc>
<doc id="5721" url="https://en.wikipedia.org/wiki?curid=5721" title="Coma">
Coma

In medicine, coma (from the Greek "koma", meaning "deep sleep") is a state of unconsciousness in which a person: cannot be awakened; fails to respond normally to painful stimuli, light, or sound; lacks a normal wake-sleep cycle; and does not initiate voluntary actions. A person in a state of coma is described as being comatose. Typically, a distinction is made in the medical community between a coma and a medically induced coma, the former is generally understood to be a result of circumstances beyond the control of the medical community, while the latter is generally understood to be a means by which medical professionals may allow a patient's injuries to heal in a controlled environment.
A comatose person exhibits a complete absence of wakefulness and is unable to consciously feel, speak, hear, or move. For a patient to maintain consciousness, two important neurological components must function. The first is the cerebral cortex—the gray matter that forms the outer layer of the brain. The other is a structure located in the brainstem, called reticular activating system (RAS).
Injury to either or both of these components is sufficient to cause a patient to experience a coma. The cerebral cortex is a group of tight, dense, "gray matter" composed of the nuclei of the neurons whose axons then form the "white matter", and is responsible for perception, relay of the sensory input (sensation) via the thalamic pathway, and many other neurological functions, including complex thinking.
RAS, on the other hand, is a more primitive structure in the brainstem that is tightly in connection with reticular formation (RF). The RAS area of the brain has two tracts, the ascending and descending tract. Made up of a system of acetylcholine-producing neurons, the ascending track, or ascending reticular activating system (ARAS), works to arouse and wake up the brain, from the RF, through the thalamus, and then finally to the cerebral cortex. A failure in ARAS functioning may then lead to a coma.
Signs and symptoms.
Generally, a person who is unable to voluntarily open the eyes, does not have a sleep-wake cycle, is unresponsive in spite of strong tactile (painful) or verbal stimuli, and who generally scores between 3 and 8 on the Glasgow Coma Scale is considered in a coma. Coma may have developed in humans as a response to injury to allow the body to pause bodily actions and heal the most immediate injuries - if at all - before waking. It therefore could be a compensatory state in which the body's expenditure of energy is not superfluous. The severity and mode of onset of coma depends on the underlying cause. For instance, severe hypoglycemia (low blood sugar) or hypercapnia (increased carbon dioxide levels in the blood) initially cause mild agitation and confusion, but progress to obtundation, stupor, and finally, complete unconsciousness. In contrast, coma resulting from a severe traumatic brain injury or subarachnoid hemorrhage can be instantaneous. The mode of onset may therefore be indicative of the underlying cause.
Causes of coma.
Coma may result from a variety of conditions, including intoxication (such as drug abuse, overdose or misuse of over the counter medications, prescribed medication, or controlled substances), metabolic abnormalities, central nervous system diseases, acute neurologic injuries such as strokes or herniations, hypoxia, hypothermia, hypoglycemia, Eclampsia or traumatic injuries such as head trauma caused by falls or vehicle collisions. It may also be deliberately induced by pharmaceutical agents during major neurosurgery, to preserve higher brain functions following brain trauma, or to save the patient from extreme pain during healing of injuries or diseases.
Forty percent of comatose states result from drug poisoning. Drugs damage or weaken the synaptic functioning in the ARAS and keep the system from properly functioning to arouse the brain. Secondary effects of drugs, which include abnormal heart rate and blood pressure, as well as abnormal breathing and sweating, may also indirectly harm the functioning of the ARAS and lead to a coma. Seizures and hallucinations have shown to also play a major role in ARAS malfunction. Given that drug poisoning is the cause for a large portion of patients in a coma, hospitals first test all comatose patients by observing pupil size and eye movement, through the vestibular-ocular reflex.
The second most common cause of coma, which makes up about 25% of comatose patients, occurs from lack of oxygen, generally resulting from cardiac arrest. The Central Nervous System (CNS) requires a great deal of oxygen for its neurons. Oxygen deprivation in the brain, also known as hypoxia, causes neuronal extracellular sodium and calcium to decrease and intracellular calcium to increase, which harms neuron communication. Lack of oxygen in the brain also causes ATP exhaustion and cellular breakdown from cytoskeleton damage and nitric oxide production.
Twenty percent of comatose states result from the side effects of a stroke. During a stroke, blood flow to part of the brain is restricted or blocked. An ischemic stroke, brain hemorrhage, or tumor may cause such cessation of blood flow. Lack of blood to cells in the brain prevents oxygen from getting to the neurons, and consequently causes cells to become disrupted and eventually die. As brain cells die, brain tissue continues to deteriorate, which may affect functioning of the ARAS.
The remaining 15% of comatose cases result from trauma, excessive blood loss, malnutrition, hypothermia, hyperthermia, abnormal glucose levels, and many other biological disorders.
Diagnosis.
Diagnosis of coma is simple, but diagnosing the cause of the underlying disease process is often challenging. The first priority in treatment of a comatose patient is stabilization following the basic ABCs (standing for airway, breathing, and circulation). Once a person in a coma is stable, investigations are performed to assess the underlying cause. Investigative methods are divided into physical examination findings and imaging (such as CAT scan, MRI, etc.) and special studies (EEG, etc.)
Diagnostic steps.
When an unconscious patient enters a hospital, the hospital utilizes a series of diagnostic steps to identify the cause of unconsciousness. According to Young, the following steps should be taken when dealing with a patient possibly in a coma:
Initial assessment and evaluation.
In the initial assessment of coma, it is common to gauge the level of consciousness by spontaneously exhibited actions, response to vocal stimuli ("Can you hear me?"), and painful stimuli; this is known as the AVPU (alert, vocal stimuli, painful stimuli, unresponsive) scale. More elaborate scales, such as the Glasgow Coma Scale, quantify an individual's reactions such as eye opening, movement and verbal response on a scale; Glasgow Coma Scale (GCS) is an indication of the extent of brain injury varying from 3 (indicating severe brain injury and death) to a maximum of 15 (indicating mild or no brain injury).
In those with deep unconsciousness, there is a risk of asphyxiation as the control over the muscles in the face and throat is diminished. As a result, those presenting to a hospital with coma are typically assessed for this risk ("airway management"). If the risk of asphyxiation is deemed high, doctors may use various devices (such as an oropharyngeal airway, nasopharyngeal airway or endotracheal tube) to safeguard the airway.
Physical examination findings.
Physical examination is critical after stabilization. It should include vital signs, a general portion dedicated to making observations about the patient's respiration (breathing pattern), body movements (if any), and of the patient's body habitus (physique); it should also include assessment of the brainstem and cortical function through special reflex tests such as the oculocephalic reflex test (doll's eyes test), oculovestibular reflex test (cold caloric test), nasal tickle, corneal reflex, and the gag reflex.
Vital signs in medicine are temperature (rectal is most accurate), blood pressure, heart rate (pulse), respiratory rate, and oxygen saturation. It should be easy to evaluate these vitals quickly to gain insight into a patient's metabolism, fluid status, heart function, vascular integrity, and tissue oxygenation.
Respiratory pattern (breathing rhythm) is significant and should be noted in a comatose patient. Certain stereotypical patterns of breathing have been identified including Cheyne–Stokes, a form of breathing in which the patient's breathing pattern is described as alternating episodes of hyperventilation and apnea. This is a dangerous pattern and is often seen in pending herniations, extensive cortical lesions, or brainstem damage. Another pattern of breathing is apneustic breathing, which is characterized by sudden pauses of inspiration and is due to a lesion of the pons. Ataxic breathing is irregular and is due to a lesion (damage) of the medulla.
Assessment of posture and body habitus is the next step. It involves general observation about the patient's positioning. There are often two stereotypical postures seen in comatose patients. Decorticate posturing is a stereotypical posturing in which the patient has arms flexed at the elbow, and arms adducted toward the body, with both legs extended. Decerebrate posturing is a stereotypical posturing in which the legs are similarly extended (stretched), but the arms are also stretched (extended at the elbow). The posturing is critical since it indicates where the damage is in the central nervous system. A decorticate posturing indicates a lesion (a point of damage) at or above the red nucleus, whereas a decerebrate posturing indicates a lesion at or below the red nucleus. In other words, a decorticate lesion is closer to the cortex, as opposed to a decerebrate cortex that is closer to the brainstem.
Oculocephalic reflex also known as the doll's eye is performed to assess the integrity of the brainstem. Patient's eyelids are gently elevated and the cornea is visualized. The patient's head is then moved to the patient's left, to observe if the eyes stay or deviate toward the patient's right; same maneuver is attempted on the opposite side. If the patient's eyes move in a direction opposite to the direction of the rotation of the head, then the patient is said to have an intact brainstem. However, failure of both eyes to move to one side, can indicate damage or destruction of the affected side. In special cases, where only one eye deviates and the other does not, this often indicates a lesion (or damage) of the medial longitudinal fasciculus (MLF), which is a brainstem nerve tract. Caloric reflex test also evaluates both cortical and brainstem function; cold water is injected into one ear and the patient is observed for eye movement; if the patient's eyes slowly deviate toward the ear where the water was injected, then the brainstem is intact, however failure to deviate toward the injected ear indicates damage of the brainstem on that side. Cortex is responsible for a rapid nystagmus away from this deviated position and is often seen in patients who are conscious or merely lethargic.
An important part of the physical exam is also assessment of the cranial nerves. Due to the unconscious status of the patient, only a limited number of the nerves can be assessed. These include the cranial nerves number 2 (CN II), number 3 (CN III), number 5 (CN V), number 7 (CN VII), and cranial nerves 9 and 10 (CN IX, CN X). Gag reflex helps assess cranial nerves 9 and 10. Pupil reaction to light is important because it shows an intact retina, and cranial nerve number 2 (CN II); if pupils are reactive to light, then that also indicates that the cranial nerve number 3 (CN III) (or at least its parasympathetic fibers) are intact. Corneal reflex assess the integrity of cranial nerve number 7 (CN VII), and cranial nerve number 5 (CN V). Cranial nerve number 5 (CN V), and its ophthalmic branch (V) are responsible for the afferent arm of the reflex, and the cranial nerve number 7 (CN VII) also known a facial nerve, is responsible for the efferent arm, causing contraction of the muscle orbicularis oculi resulting in closing of the eyes.
Pupil assessment is often a critical portion of a comatose examination, as it can give information as to the cause of the coma; the following table is a technical, medical guideline for common pupil findings and their possible interpretations:
Imaging and special tests findings.
Imaging basically encompasses computed tomography (CAT or CT) scan of the brain, or MRI for example, and is performed to identify specific causes of the coma, such as hemorrhage in the brain or herniation of the brain structures. Special tests such as an EEG can also show a lot about the activity level of the cortex such as semantic processing, presence of seizures, and are important available tools not only for the assessment of the cortical activity but also for predicting the likelihood of the patient's awakening. The autonomous responses such as the skin conductance response may also provide further insight on the patient's emotional processing.
History.
When diagnosing any neurological condition, history and examination are fundamental. History is obtained by family, friends or EMS. The Glasgow Coma Scale is a helpful system used to examine and determine the depth of coma, track patients progress and predict outcome as best as possible. In general a correct diagnosis can be achieved by combining findings from physical exam, imaging, and history components and directs the appropriate therapy.
Severity and classification.
A coma can be classified as (1) supratentoral (above Tentorium cerebelli), (2) infratentoral (below Tentorium cerebelli), (3) metabolic or (4) diffused. This classification is merely dependent on the position of the original damage that caused the coma, and does not correlate with severity or the prognosis.
The severity of coma impairment however is categorized into several levels. Patients may or may not progress through these levels. In the first level, the brain responsiveness lessens, normal reflexes are lost, the patient no longer responds to pain and cannot hear.
The Rancho Los Amigos Scale is a complex scale that has eight separate levels, and is often used in the first few weeks or months of coma while the patient is under closer observation, and when shifts between levels are more frequent.
Treatment.
Medical treatment.
The treatment hospitals use on comatose patients depends on both the severity and cause of the comatose state. Although the best treatment for comatose patients remains unknown, hospitals usually place comatose patients in an Intensive Care Unit (ICU) immediately. In the ICU, the hospital monitors a patient’s breathing and brain activity through CT scans. Attention must first be directed to maintaining the patient's respiration and circulation, using intubation and ventilation, administration of intravenous fluids or blood and other supportive care as needed. Once a patient is stable and no longer in immediate danger, the medical staff may concentrate on maintaining the health of patient’s physical state. The concentration is directed to preventing infections such as pneumonias, bedsores (decubitus ulcers), and providing balanced nutrition. Infections may appear from the patient not being able to move around, and being confined to the bed. The nursing staff moves the patient every 2–3 hours from side to side and depending on the state of consciousness sometimes to a chair. The goal is to move the patient as much as possible to try to avoid bedsores, atelectasis and pneumonia. Pneumonia can occur from the person’s inability to swallow leading to aspiration, lack of gag reflex or from feeding tube, (aspiration pneumonia). Physical therapy may also be used to prevent contractures and orthopedic deformities that would limit recovery for those patients who awaken from coma.
A person in a coma may become restless, or seize and need special care to prevent them from hurting themselves. Medicine may be given to calm such individuals. Patients who are restless may also try to pull on tubes or dressings so soft cloth wrist restraints may be put on. Side rails on the bed should be kept up to prevent the patient from falling.
In attempt to wake comatose patients, some hospitals treat their patients by either reversing the cause of the coma (i.e., glucose shock if low sugar), giving medication to stop brain swelling, or inducing hypothermia. Inducing hypothermia on comatose patients provides one of the main treatments for patients after suffering from cardiac arrest. In this treatment, medical personnel expose patients to “external or intravascular cooling” at 32-34 °C for 24 hours; this treatment cools patients down about 2-3 °C less than normal body temperature. In 2002, Baldursdottir and her coworkers found that in the hospital, more comatose patients survived after induced hypothermia than patients that remained at normal body temperature. For this reason, the hospital chose to continue the induced hypothermia technique for all of its comatose patients that suffered from cardiac arrest.
Emotional challenges.
Coma has a wide variety of emotional reactions from the family members of the affected patients, as well as the primary care givers taking care of the patients. Common reactions, such as desperation, anger, frustration, and denial are possible. The focus of the patient care should be on creating an amicable relationship with the family members or dependents of a comatose patient as well as creating a rapport with the medical staff.
Prognosis.
Comas can last from several days to several weeks. In more severe cases a coma may last for over five weeks, while some have lasted as long as several years. After this time, some patients gradually come out of the coma, some progress to a vegetative state, and others die. Some patients who have entered a vegetative state go on to regain a degree of awareness. Others remain in a vegetative state for years or even decades (the longest recorded period being 42 years).
The outcome for coma and vegetative state depends on the cause, location, severity and extent of neurological damage. A deeper coma alone does not necessarily mean a slimmer chance of recovery, because some people in deep coma recover well while others in a so-called milder coma sometimes fail to improve.
People may emerge from a coma with a combination of physical, intellectual, and psychological difficulties that need special attention. Recovery usually occurs gradually—patients acquire more and more ability to respond. Some patients never progress beyond very basic responses, but many recover full awareness. Regaining consciousness is not instant: in the first days, patients are only awake for a few minutes, and duration of time awake gradually increases. This is unlike the situation in many movies where people who awake from comas are instantly able to continue their normal lives. In reality, the coma patient awakes sometimes in a profound state of confusion, not knowing how they got there and sometimes suffering from dysarthria, the inability to articulate any speech, and with many other disabilities.
Predicted chances of recovery are variable owing to different techniques used to measure the extent of neurological damage. All the predictions are based on statistical rates with some level of chance for recovery present: a person with a low chance of recovery may still awaken. Time is the best general predictor of a chance of recovery: after four months of coma caused by brain damage, the chance of partial recovery is less than 15%, and the chance of full recovery is very low.
The most common cause of death for a person in a vegetative state is secondary infection such as pneumonia, which can occur in patients who lie still for extended periods.
There are reports of patients coming out of coma after long periods of time. After 19 years in a minimally conscious state, Terry Wallis spontaneously began speaking and regained awareness of his surroundings. Similarly, Polish railroad worker Jan Grzebski woke up from a 19-year coma in 2007.
A brain-damaged man, trapped in a coma-like state for six years, was brought back to consciousness in 2003 by doctors who planted electrodes deep inside his brain. The method, called deep brain stimulation (DBS) successfully roused communication, complex movement and eating ability in the 38-year-old American man who suffered a traumatic brain injury. His injuries left him in a minimally conscious state (MCS), a condition akin to a coma but characterized by occasional, but brief, evidence of environmental and self-awareness that coma patients lack.
Comas lasting seconds to minutes result in post-traumatic amnesia (PTA) that lasts hours to days; recovery plateau occurs over days to weeks.
Comas that last hours to days result in PTA lasting days to weeks; recovery plateau occurs over months.
Comas lasting weeks result in PTA that lasts months; recovery plateau occurs over months to years.
Society and culture.
Research by Dr. Eelco Wijdicks on the depiction of comas in movies was published in Neurology in May 2006. Dr. Wijdicks studied 30 films (made between 1970 and 2004) that portrayed actors in prolonged comas, and he concluded that only two films accurately depicted the state of a coma victim and the agony of waiting for a patient to awaken: "Reversal of Fortune" (1990) and "The Dreamlife of Angels" (1998). The remaining 28 were criticized for portraying miraculous awakenings with no lasting side effects, unrealistic depictions of treatments and equipment required, and comatose patients remaining muscular and tanned.

</doc>
<doc id="5722" url="https://en.wikipedia.org/wiki?curid=5722" title="Call of Cthulhu (role-playing game)">
Call of Cthulhu (role-playing game)

Call of Cthulhu is a horror fiction role-playing game based on H. P. Lovecraft's story of the same name and the associated Cthulhu Mythos. The game, often abbreviated as "CoC", is published by Chaosium; it was first released in 1981 and is currently in its seventh edition, with many different versions released. It makes use of Chaosium's Basic Role-Playing (BRP) system, with special rules for Sanity.
Gameplay.
The setting of "Call of Cthulhu" is a darker version of our world, based on H. P. Lovecraft's observation (from his essay, "Supernatural Horror in Literature") that "The oldest and strongest emotion of mankind is fear, and the strongest kind of fear is fear of the unknown." The original game, first published in 1981, uses mechanics from Basic Role-Playing, and is set in the 1920s, the setting of many of Lovecraft's stories. Additional settings were developed in the 1890s "Cthulhu by Gaslight" supplement, a blend of occult and Holmesian mystery and mostly set in England, and modern/1980s conspiracy with "Cthulhu Now." More recent additions include 1000 AD ("Cthulhu: Dark Ages"), 23rd century ("Cthulhu Rising") and Ancient Roman times ("Cthulhu Invictus"). The protagonists may also travel to places that are not of this earth, represented in the Dreamlands (which can be accessed through dreams as well as being physically connected to the earth), to other planets, or into the voids of space.
"Call of Cthulhu" uses the Basic Role-Playing system used by other Chaosium games (first seen in "RuneQuest"). For as long as they stay functionally healthy and sane, characters grow and develop. "Call of Cthulhu" does not use levels, but is completely skill-based, with player characters getting better with their skills by succeeding at them. They do not, however, gain "hit points" and do not become significantly harder to kill.
Unlike Dungeons and Dragons, which typically uses a d20 to determine outcomes of particular decisions and events, "Call of Cthulhu" uses percentile dice (with a results ranging from 1 to 100) to determine such events. Every player statistic is intended to be compatible with the notion that there is a probability of success for a particular action given what the player is capable of doing. For example, an artist may have a 75% of being able to draw something (represented by having 75 in Art skill), and thus rolling a number under 75 would yield a success. Rolling 1/5 or less of the skill level (1-15 in our example) would be a "special success" (or an "impale" for combat skills) and would yield some extra bonus to be determined by the keeper. For example, the artist character might draw especially well or especially fast, or catch some unapparent detail in the drawing.
The players take the roles of ordinary people drawn into the realm of the mysterious: detectives, criminals, scholars, artists, war veterans, etc. Often, happenings begin innocently enough, until more and more of the workings behind the scenes are revealed. As the characters learn more of the true horrors of the world and the irrelevance of humanity, their sanity (represented by "Sanity Points", abbreviated SAN) inevitably withers away. The game includes a mechanism for determining how damaged a character's sanity is at any given point; encountering the horrific beings usually triggers a loss of SAN points. To gain the tools they need to defeat the horrors – mystic knowledge and magic – the characters may end up losing some of their sanity, though other means such as pure firepower or simply outsmarting one's opponents also exist. "Call of Cthulhu" has a reputation as a game in which it is quite common for a player character to die in gruesome circumstances or end up in a mental institution. Eventual triumph of the players is not assumed.
History.
The original conception of "Call of Cthulhu" was "Dark Worlds", a game commissioned by the publisher Chaosium but never published. Sandy Petersen, now best known for his work on the "Doom" computer game, contacted them regarding writing a supplement for their popular fantasy game "RuneQuest" set in Lovecraft's Dreamlands. He took over the writing of "Call of Cthulhu", and the game was released in 1981, using a version of the Basic Role-Playing system used in "RuneQuest".
Editions.
Since Petersen's departure from Chaosium, continuing development of "Call of Cthulhu" passed to Lynn Willis, credited as co-author in the fifth and sixth editions, and more recently to Paul Fricker and Mike Mason. The game system underwent only minor rules changes in its first six editions (between 1981 and 2011); the current seventh edition, released 2014, includes more significant rules alterations than in any previous release.
Early releases.
For those grounded in the RPG tradition, the very first release of "Call of Cthulhu" created a brand new framework for table-top gaming. Rather than the traditional format established by "Dungeons & Dragons", which often involved the characters wandering through caves or tunnels and fighting different types of monsters, Sandy Petersen introduced the concept of the "Onion Skin": Interlocking layers of information and nested clues that lead the Player Characters from seemingly minor investigations into a missing person to discovering mind-numbingly awful, global conspiracies to destroy the world. Unlike its predecessor games, "CoC" assumed that most investigators would not survive, alive or sane, and that the only safe way to deal with the vast majority of nasty things described in the rule books was to run away. A well-run "CoC" campaign should engender a sense of foreboding and inevitable doom in its players. The style and setting of the game, in a relatively modern time period, created an emphasis on real-life settings, character research, and thinking one's way around trouble.
The first book of "Call of Cthulhu" adventures was "Shadows of Yog-Sothoth". In this work, the characters come upon a secret society's foul plot to destroy mankind, and pursue it first near to home and then in a series of exotic locations. This template was to be followed in many subsequent campaigns, including "Fungi from Yuggoth" (later known as "Curse of Cthulhu" and "Day of the Beast"), "Spawn of Azathoth", and possibly the most highly acclaimed, "Masks of Nyarlathotep". Many of these seem closer in tone to the pulp adventures of "Indiana Jones" than H. P. Lovecraft, but they are nonetheless beloved by many gamers.
"Shadows of Yog-Sothoth" is important not only because it represents the first published addition to the boxed first edition of "Call of Cthulhu", but because its format defined a new way of approaching a campaign of linked RPG scenarios involving actual clues for the would-be detectives amongst the players to follow and link in order to uncover the dastardly plots afoot. Its format has been used by every other campaign-length "Call of Cthulhu" publication. The standard of "CoC" scenarios was well received by independent reviewers. "The Asylum and Other Tales", a series of stand alone articles released in 1983, rated an overall 9/10 in Issue 47 of "White Dwarf" magazine.
The standard of the included 'clue' material varies from scenario to scenario, but reached its zenith in the original boxed versions of the "Masks of Nyarlathotep" and "Horror on the Orient Express" campaigns. Inside these one could find matchbooks and business cards apparently defaced by non-player characters, newspaper cuttings and (in the case of "Orient Express") period passports to which players could attach their photographs, bringing a Live Action Role Playing feel to a tabletop game. Indeed, during the period that these supplements were produced, third party campaign publishers strove to emulate the quality of the additional materials, often offering separately-priced 'deluxe' clue packages for their campaigns.
Additional milieu were provided by Chaosium with the release of "Dreamlands", a boxed supplement containing additional rules needed for playing within the Lovecraft Dreamlands, a large map and a scenario booklet, and "Cthulhu By Gaslight", another boxed set which moved the action from the 1920s to the 1890s.
"Cthulhu Now".
In 1987, Chaosium issued the supplement titled "Cthulhu Now", a collection of rules, supplemental source materials and scenarios for playing "Call of Cthulhu" in the present day. This proved to be a very popular alternative milieu, so much so that much of the supplemental material is now included in the core rule book.
"Delta Green".
Pagan Publishing has released a series of supplements in a similar vein, by the name "Delta Green", that is set in the 1990s (although later supplements add support for playing closer to the present day).
Lovecraft Country.
"Lovecraft Country" was a line of supplements for "Call of Cthulhu" released in 1990. These supplements were overseen by Keith Herber and provided backgrounds and adventures set in Lovecraft's fictional towns of Arkham, Kingsport, Innsmouth, Dunwich, and their environs. The intent was to give investigators a common base, as well as to center the action on well-drawn characters with clear motivations.
Recent history.
In the years since the collapse of the "Mythos" collectible card game (production ceased in 1997), the release of "CoC" books has been very sporadic with up to a year between releases. Chaosium struggled with near bankruptcy for many years before finally starting their upward climb again.
2005 was Chaosium's busiest year for many years with ten releases for the game. Chaosium took to marketing "monographs"—short books by individual writers with editing and layout provided out-of-house—directly to the consumer, allowing the company to gauge market response to possible new works. The range of times and places in which the horrors of the Mythos can be encountered was also expanded in late 2005 onwards with the addition of "Cthulhu Dark Ages" by Stéphane Gesbert, which gives a framework for playing games set in 11th century Europe, "Secrets of Japan" by Michael Dziesinski for gaming in modern day Japan, and "Secrets of Kenya" by David Conyers for gaming in interwar period Africa.
In July 2011, Chaosium announced it would re-release a 30th anniversary edition of the "CoC" 6th edition role-playing game. This 320-page book features thick (3 mm) leatherette hard-covers with the front cover and spine stamped with gold foil. The interior pages are printed in black ink, on 90 gsm matte art paper. The binding is thread sewn, square backed. Chaosium offered a one-time printing of this Collector's Edition.
In May 28, 2013, a kickstarter for the 7th Edition of Call of Cthulhu was launched, it ended in June 29 of the same year and collected $561,836.
Licenses.
Chaosium has licensed other publishers to create supplements using their rule system, notably including "Delta Green" by Pagan Publishing. Other licensees have included Miskatonic River Press, Theater of the Mind Enterprises, Triad Entertainment, Games Workshop, Fantasy Flight Games, RAFM, Goodman Games, Grenadier Models Inc. and Yog-Sothoth.com. These supplements may be set in different time frames or even different game universes from the original game.
"D20 Call of Cthulhu".
In 2001, a stand-alone version of "Call of Cthulhu" was released by Wizards of the Coast, for the d20 system. Intended to preserve the feeling of the original game, the d20 conversion of the game rules were supposed to make the game more accessible to the large "D&D" player base. The d20 system also made it possible to use "Dungeons & Dragons" characters in "Call of Cthulhu", as well as to introduce the Cthulhu Mythos into "Dungeons & Dragons" games. The d20 version of the game is no longer supported by Wizards as per their contract with Chaosium. Chaosium included d20 stats as an appendix in three releases (see Lovecraft Country), but have since dropped the "dual stat" idea.
"Dark Corners of the Earth".
A licensed first-person shooter adventure game by Headfirst Productions, based on "Call of Cthulhu" campaign "Escape from Innsmouth" and released by Bethesda Softworks in 2005/2006 for the PC and Xbox.
"Trail of Cthulhu".
In February 2008, Pelgrane Press published "Trail of Cthulhu", a stand-alone game created by Kenneth Hite using the GUMSHOE System developed by Robin Laws. "Trail of Cthulhu"s system is more mystery oriented and focuses mostly on interpreting clues.
"Shadows of Cthulhu".
In September 2008, Reality Deviant Publications published "Shadows of Cthulhu", a supplement that brings Lovecraftian gaming to Green Ronin's True20 system.
"Realms of Cthulhu".
In October 2009, Reality Blurs published "Realms of Cthulhu", a supplement for Pinnacle Entertainment's Savage Worlds system.
"The Laundry".
In 2010, Cubicle 7 published an official role-playing game, "The Laundry" (2010, ISBN 1-907204-93-8, Gareth Hanrahan) and a number of supplements, all based on Charles Stross's "Bob Howard – Laundry" series.
"The Wasted Land".
In April 2011, Chaosium and new developer Red Wasp Design announced a joint project to produce a mobile video game based on the "Call of Cthulhu" RPG, entitled "Call of Cthulhu: The Wasted Land". The game was released on 30 January 2012.
Card games.
"Mythos" was a collectible card game (CCG) based on the Cthulhu Mythos that Chaosium produced and marketed during the mid-1990s. While generally praised for its fast gameplay and unique mechanics, it ultimately failed to gain a very large market presence. It bears mention because its eventual failure brought the company to hard times that affected its ability to produce material for "Call of Cthulhu". "Call of Cthulhu: The Card Game" is a second collectible card game, produced by Fantasy Flight Games.
Miniatures.
The first licensed "Call of Cthulhu" 25mm gaming miniatures were sculpted by Andrew Chernack and released by Grenadier Models in boxed sets and blister packs in 1983. The license was later transferred to RAFM. As of 2011, RAFM still produce licensed C"all of Cthulhu" models sculpted by Bob Murch. Both lines include investigator player character models and the iconic monsters of the Cthulhu mythos.
As of July 2015, Reaper Miniatures started its third "Bones Kickstarter", a Kickstarter intended to help the company migrate some miniatures from metal to plastic, and introducing some new ones. Among the stretch goals was the second 50$ expansion, devoted to the Mythos, with miniatures such as Cultists, Deep Ones, Mi'Go, and an extra 15$ Shub-Niggurath "miniature" (it is, at least, 6x4 squares). It is expected for those miniatures to remain in the Reaper Miniatures catalogue after the Kickstarter project finishes.
Reception.
The game won several major awards in the following years:

</doc>
<doc id="5723" url="https://en.wikipedia.org/wiki?curid=5723" title="Constellations (journal)">
Constellations (journal)

Constellations: An International Journal of Critical and Democratic Theory is a quarterly peer-reviewed academic journal of critical and democratic theory and successor of "Praxis International". It is edited by Andrew Arato, Amy Allen, and Andreas Kalyvas. Seyla Benhabib is a co-founding former editor and Nancy Fraser a former co-editor.

</doc>
<doc id="5724" url="https://en.wikipedia.org/wiki?curid=5724" title="Cape Breton Island">
Cape Breton Island

Cape Breton Island (—formerly "Île Royale", Scottish Gaelic: "Ceap Breatainn" or "Eilean Cheap Bhreatainn", Míkmaq: "Únamakika", simply: "Cape Breton") is an island on the Atlantic coast of North America. Its name may derive from Capbreton near Bayonne, or more probably from the word "Breton", the French adjective form of the proper noun "Bretagne", the French historical region.
Cape Breton Island is part of the province of Nova Scotia, Canada. The island accounts for 18.7% of the total area of Nova Scotia. Although physically separated from the Nova Scotia peninsula by the Strait of Canso, it is artificially connected to mainland Nova Scotia by the long rock-fill Canso Causeway. The island is located east-northeast of the mainland with its northern and western coasts fronting on the Gulf of Saint Lawrence; its western coast also forming the eastern limits of the Northumberland Strait. The eastern and southern coasts front the Atlantic Ocean; its eastern coast also forming the western limits of the Cabot Strait. Its landmass slopes upward from south to north, culminating in the highlands of its northern cape. One of the world's larger salt water lakes, Bras d'Or ("Arm of Gold" in French), dominates the centre of the island.
The island is divided into four of Nova Scotia's eighteen counties: Cape Breton, Inverness, Richmond, and Victoria. Their total population at the 2011 census numbered 135,974 "Cape Bretoners"; this is approximately 15% of the provincial population. Cape Breton Island has experienced a decline in population of approximately 4.4% since the previous census in 2006. Approximately 75% of the island's population is located in the Cape Breton Regional Municipality (CBRM) which includes all of Cape Breton County and is often referred to as Industrial Cape Breton, given the history of coal mining and steel manufacturing in this area, which was Nova Scotia's industrial heartland throughout the 20th century.
The island contains five reserves of the Mi'kmaq Nation, these being: Eskasoni, Membertou, Wagmatcook, Waycobah, and Potlotek/Chapel Island. Eskasoni is the largest in both population and land area.
History.
Cape Breton Island's first residents were most likely Archaic maritime natives, ancestors of the Mi'kmaq, the people who were inhabiting the island at the time of European arrival. John Cabot reportedly visited the island in 1497. However, historians are unclear as to whether Cabot first visited Newfoundland or Cape Breton Island. This discovery is commemorated by Cape Breton's Cabot Trail, and by the "Cabot's Landing Historic Site & Provincial Park", located near the village of Dingwall.
In about 1521–22, the Portuguese under João Álvares Fagundes established a fishing colony on the island. As many as two hundred settlers lived in a village, the name of which is not known, located according to some historians at what is now present day Ingonish on the island's northeastern peninsula. The fate of this Portuguese colony is unknown, but it is mentioned as late as 1570.
During the Anglo-French War of 1627 to 1629, under Charles I, by 1629 the Kirkes took Quebec City; Sir James Stewart of Killeith, Lord Ochiltree planted a colony on Cape Breton Island at Baleine, Nova Scotia; and Alexander’s son, William Alexander, 1st Earl of Stirling, established the first incarnation of "New Scotland" at Port Royal. This set of Scottish triumphs which left Cape Sable as the only major French holding in North America was not destined to last. Charles I’s haste to make peace with France on the terms most beneficial to him meant that the new North American gains would be bargained away in the Treaty of Saint-Germain-en-Laye (1632).
The French quickly defeated the Scottish at Baleine, and established the first permanent settlements on Île Royale: present day Englishtown (1629) and St. Peter's (1630). These settlements lasted almost continuously until Nicolas Denys left in 1659. Île Royale then remained vacant for more than fifty years, until the communities along with Louisbourg were established in 1713.
Île Royale.
Known as "Île Royale" to the French, the island also saw active settlement by France. After the French ceded their colonies on Newfoundland and the Acadian mainland to the British by the Treaty of Utrecht in 1713, the French relocated the population of Plaisance, Newfoundland, to Île Royale and the French garrison was established in the central eastern part at Sainte Anne. As the harbour at Sainte Anne experienced icing problems, it was decided to construct a much larger fortification at Louisbourg to improve defences at the entrance to the Gulf of Saint Lawrence and to defend France's fishing fleet on the Grand Banks. The French also built the Louisbourg Lighthouse in 1734, the first lighthouse in Canada and one of the first in North America. In addition to Cape Breton Island, the French colony of Île Royale also included Île Saint-Jean, today called Prince Edward Island.
Louisbourg itself was one of the most important commercial and military centres in New France. Louisbourg was captured by New Englanders with British naval assistance in 1745 and by British forces in 1758. The French population of Île Royale was deported to France after each siege. While French settlers returned to their homes in Île Royale after the Treaty of Aix-la-Chapelle was signed in 1748, the fortress was demolished after the second siege. Île Royale remained formally part of New France until it was ceded to Great Britain by the Treaty of Paris in 1763. It was then merged with the adjacent, British colony of Nova Scotia (present day peninsular Nova Scotia and New Brunswick). Acadians who had been expelled from Nova Scotia and Île Royale were permitted to settle in Cape Breton beginning in 1764, and established communities in north-western Cape Breton, near Cheticamp, and southern Cape Breton, on and near Isle Madame.
Some of the first British-sanctioned settlers on the island following the Seven Years' War were Irish, although upon settlement they merged with local French communities to form a culture rich in music and tradition. From 1763 to 1784, the island was administratively part of the colony of Nova Scotia and was governed from Halifax.
The first permanently settled Scottish community on Cape Breton Island was Judique, settled in 1775 by Michael Mor MacDonald. He spent his first winter using his upside-down boat for shelter, which is reflected in the architecture of the village's Community Centre. He composed a song about the area called "O's alainn an t-aite", or "Fair is the Place."
Colony of Cape Breton.
In 1784, Britain split the colony of Nova Scotia into three separate colonies: New Brunswick, Cape Breton Island, and present-day peninsular Nova Scotia, in addition to the adjacent colonies of St. John's Island (renamed Prince Edward Island in 1798) and Newfoundland. The colony of Cape Breton Island had its capital at Sydney on its namesake harbour fronting on Spanish Bay and the Cabot Strait. Its first Lieutenant-Governor was Joseph Frederick Wallet DesBarres (1784–1787) and his successor was William Macarmick (1787).
A number of United Empire Loyalists emigrated to the Canadian colonies, including Cape Breton. David Mathews, the former Mayor of New York City during the American Revolution, emigrated with his family to Cape Breton in 1783. He succeeded Macarmick as head of the colony and served from 1795 to 1798.
From 1799 to 1807, the military commandant was John Despard, brother of Edward.
An order forbidding the granting of land in Cape Breton, issued in 1763, was removed in 1784. The mineral rights to the island were given over to the Duke of York by an order-in-council. The British government had intended that the Crown take over the operation of the mines when Cape Breton was made a colony, but this was never done, probably because of the rehabilitation cost of the mines. The mines were in a neglected state, caused by careless operations dating back at least to the time of the final fall of Louisbourg.
Large-scale shipbuilding began in the 1790s, beginning with schooners for local trade moving in the 1820s to larger brigs and brigantines, mostly built for British shipowners. Shipbuilding peaked in the 1850s, marked in 1851 by the full rigged ship "Lord Clarendon", the largest wooden ship ever built in Cape Breton.
Merger with Nova Scotia.
In 1820, the colony of Cape Breton Island was merged for the second time with Nova Scotia. This development is one of the factors which led to large-scale industrial development in the Sydney Coal Field of eastern Cape Breton County. By the late 19th century, as a result of the faster shipping, expanding fishery and industrialization of the island, exchanges of people between the island of Newfoundland and Cape Breton increased, beginning a cultural exchange that continues to this day.
During the first half of the 19th century, Cape Breton Island experienced an influx of Highland Scots numbering approximately 50,000 as a result of the Highland Clearances. Today, the descendants of the Highland Scots dominate Cape Breton Island's culture, particularly in rural communities. To this day, Gaelic is still the first language of a number of elderly Cape Bretoners. The growing influence of English-dominated media from outside the Scottish communities saw the use of this language erode quickly during the 20th century. Many of the Scots who immigrated there were either Roman Catholics or Presbyterians, which can be seen in a number of island landmarks and place names.
The 1920s were some of the most violent times in Cape Breton. They were marked by several severe labour disputes. The famous murder of William Davis by strike breakers, and the seizing of the New Waterford power plant by striking miners led to a major union sentiment that persists to this day in some circles. William Davis Miners' Memorial Day is celebrated in coal mining towns to commemorate the deaths of miners at the hands of the coal companies.
20th century.
The turn of the 20th century saw Cape Breton Island at the forefront of scientific achievement with the now-famous activities launched by inventors Alexander Graham Bell and Guglielmo Marconi.
Following his successful invention of the telephone and being relatively wealthy, Bell acquired land near Baddeck in 1885, largely due to surroundings reminiscent of his early years in Scotland. He established a summer estate complete with research laboratories, working with deaf people—including Helen Keller—and continued to invent. Baddeck would be the site of his experiments with hydrofoil technologies as well as the Aerial Experiment Association, financed by his wife, which saw the first powered flight in the British Empire when the AEA "Silver Dart" took off from the ice-covered waters of Bras d'Or Lake. Bell also built the forerunner to the iron lung and experimented with breeding sheep.
Marconi's contributions to Cape Breton Island were also quite significant, as he used the island's geography to his advantage in transmitting the first North American trans-Atlantic radio message from a station constructed at Table Head in Glace Bay to a receiving station at Poldhu in Cornwall, England. Marconi's pioneering work in Cape Breton marked the beginning of modern radio technology. Marconi's station at Marconi Towers, on the outskirts of Glace Bay, became the chief communication centre for the Royal Canadian Navy in World War I through to the early years of World War II.
Promotions for tourism beginning in the 1950s recognized the importance of the Scottish culture to the province, and the provincial government started encouraging the use of Gaelic once again. The establishment of funding for the Gaelic College of Celtic Arts and Crafts and formal Gaelic language courses in public schools are intended to address the near-loss of this culture to English assimilation.
In the 1960s, the Fortress of Louisbourg was partially reconstructed by Parks Canada. Since 2009, this National Historic Site of Canada has attracted an average of 90 000 visitors per year.
Environment.
Geography.
The island measures in area, making it the 77th largest island in the world and Canada's 18th largest island. Cape Breton Island is composed mainly of rocky shores, rolling farmland, glacial valleys, barren headlands, mountains, woods and plateaus. Geological evidence suggests that at least part of the island was originally joined with present-day Scotland and Norway, now separated by millions of years of continental drift.
The northern portion of Cape Breton Island is dominated by the Cape Breton Highlands, commonly shortened to simply the "Highlands", which are an extension of the Appalachian mountain chain. The Highlands comprise the northern portions of Inverness and Victoria counties. In 1936 the federal government established the Cape Breton Highlands National Park covering across the northern third of the Highlands. The Cabot Trail scenic highway also encircles the coastal perimeter of the plateau.
Cape Breton Island's hydrological features include the Bras d'Or Lake system, a salt-water fjord at the heart of the island, and freshwater features including Lake Ainslie, the Margaree River system, and the Mira River. Innumerable smaller rivers and streams drain into the Bras d'Or Lake estuary and onto the Gulf of St. Lawrence and Atlantic coasts.
Cape Breton Island is joined to the mainland by the Canso Causeway, which was completed in 1955, enabling direct road and rail traffic to and from the island, but requiring marine traffic to pass through the Canso Canal at the eastern end of the causeway.
Cape Breton Island is divided into four counties: Cape Breton, Inverness, Richmond, and Victoria.
Demographics.
The island's residents can be grouped into five main cultures; Scottish, Mi'kmaq, Acadian, Irish, and English, with respective languages Gaelic (Scottish and Irish), Mi'kmaq, French, and English. English is now the primary spoken language, though Mi'kmaq, Gaelic and French are still heard.
Later migrations of Black Loyalists, Italians, and Eastern Europeans mostly settled in the eastern part of the island around the Industrial Cape Breton region. The population of Cape Breton Island has been in decline for almost two decades with an increasing population exodus in recent years due to economic conditions.
According to the Census of Canada, the population of Cape Breton Island in 2011 was 135,974, a 4.4% decline from 142,298 in 2006, and a 14.1% decline from 158,260 in 1996.
Religious groups
Statistics Canada in 2001 reported a "religion" total of 145,525 for Cape Breton, including 5,245 with "no religious affiliation." Major categories included:
A Synagogue in Sydney serves a small historic Jewish community which was once one of the largest ones in eastern Canada with four shuls: one in Glace Bay, one in New Waterford, one in Whitney Pier, and the one in Sydney. More recent Muslim immigrants hold Friday prayers at Cape Breton University and the former Holy Redeemer Hall in Whitney Pier. Buddhists are a small minority (105 in 2001, according to Statistics Canada), although Gampo Abbey in Pleasant Bay has been operational since 1984.
Economy.
Much of the recent economic history of Cape Breton Island can be tied to the coal industry.
The island has two major coal deposits:
Sydney has traditionally been the main port, with various facilities in a large, sheltered, natural harbour. It is the island's largest commercial centre and home to the "Cape Breton Post" daily newspaper, as well as one television station, CJCB-TV (CTV), and several radio stations. The Marine Atlantic terminal at North Sydney is the terminal for large ferries traveling to Channel-Port aux Basques and seasonally to Argentia, both on the island of Newfoundland.
Point Edward on the west side of Sydney Harbour is the location of Sydport, a former navy base () now converted to commercial use. The Canadian Coast Guard College is located nearby at Westmount. Petroleum, bulk coal, and cruise ship facilities are also located in Sydney Harbour.
Glace Bay is the second largest urban community in population and was the island's main coal mining centre until its last mine ceased operation in the 1980s. Glace Bay served as the hub of the Sydney & Louisburg Railway and also as a major fishing port. At one time, Glace Bay was known as the largest town in Nova Scotia, based on population.
Port Hawkesbury has risen to prominence since the completion of the Canso Causeway and Canso Canal created an artificial deep-water port, allowing extensive petrochemical, pulp and paper, and gypsum handling facilities to be established. The Strait of Canso is completely navigable to Seawaymax vessels, and Port Hawkesbury is open to the deepest-draught vessels on the world's oceans. Large marine vessels may also enter Bras d'Or Lake through the Great Bras d'Or channel, whereas small craft have the additional use of the Little Bras d'Or channel or St. Peters Canal. The St. Peters Canal is no longer used by commercial shipping on Cape Breton Island, but is an important waterway for recreational vessels.
The industrial Cape Breton area faced several challenges with the closure of the Cape Breton Development Corporation's (DEVCO) coal mines and the Sydney Steel Corporation's (SYSCO) steel mill. In recent years, the Island's residents have been attempting to diversify the area economy by investing in tourism developments, call centres, and small businesses, as well as manufacturing ventures in such fields as auto parts, pharmaceuticals, and window glazings.
While the Cape Breton Regional Municipality is in transition from an industrial to a service-based economy, the rest of Cape Breton Island outside the industrial area surrounding Sydney-Glace Bay has been more stable, with a mixture of fishing, forestry, small-scale agriculture, and tourism.
Tourism in particular has grown throughout the post-Second World War era, especially the growth in vehicle-based touring, which was furthered by the creation of the Cabot Trail scenic drive. The scenery of the island is rivalled in northeastern North America by only Newfoundland; and Cape Breton Island tourism marketing places a heavy emphasis on its Scottish Gaelic heritage through events such as the Celtic Colours Festival, held each October, as well as promotions through the Gaelic College of Celtic Arts and Crafts.
Whale-watching is a popular attraction for tourists. Whale-watching cruises are operated by numerous vendors from Baddeck to Cheticamp. The most popular species of whale found in Cape Breton's waters is the Pilot whale.
The primary east-west road on the island is Highway 105, the Trans-Canada Highway, although Trunk 4 is also heavily used. Highway 125 is an important arterial route around Sydney Harbour in the Cape Breton Regional Municipality. The Cabot Trail, circling the Cape Breton Highlands, and Trunk 19, along the western coast of the island, are important secondary roads. Railway connections between the port of Sydney to Canadian National Railway in Truro are maintained by the Cape Breton and Central Nova Scotia Railway.
The Cabot Trail is a scenic road circuit around and over the Cape Breton Highlands with spectacular coastal vistas; over 400,000 visitors drive the Cabot Trail each summer and fall. Coupled with the Fortress of Louisbourg, it has driven the growth of the tourism industry on the island in recent decades. The "Condé Nast" travel guide has rated Cape Breton Island as one of the best island destinations in the world.
Traditional music.
Cape Breton is well known for its traditional fiddle music, which was brought to North America by Scottish immigrants during the Highland Clearances. The traditional style has been well preserved in Cape Breton, and céilidhs have become a popular attraction for summer tourists. Inverness County in particular has a heavy concentration of musical activity, with regular performances in communities such as Mabou and Judique. Judique is recognized as 'Baile nam Fonn', (literally: Village of Tunes) or the 'Home of Celtic Music', featuring the Celtic Music Interpretive Centre. Performers who have received significant recognition outside of Cape Breton include Bruce Guthro, Buddy MacMaster, Natalie MacMaster, Ashley MacIsaac, The Rankin Family, Aselin Debison, Lee Cremo, and the Barra MacNeils.
The Men of the Deeps are a male choral group of current and former miners from the industrial Cape Breton area.
Notable people.
Cape Breton artists who have been recognized with major national or international awards include actor Harold Russell of North Sydney, who won an Academy Award in 1946 for his portrayal of Homer Parrish in "The Best Years of Our Lives", and Lynn Coady and Linden MacIntyre of Inverness County, who are both past winners of the Giller Prize for Canadian literature. The Rankin Family and Rita MacNeil have both recorded multiple albums certified as Double Platinum by Music Canada.
People from Cape Breton have also achieved a number of firsts in Canadian politics and governance. These include Mayann Francis of Whitney Pier, the first Black Lieutenant Governor of Nova Scotia, and Elizabeth May of Margaree Harbour, the first member of the Green Party of Canada elected to the Canadian House of Commons.

</doc>
<doc id="5725" url="https://en.wikipedia.org/wiki?curid=5725" title="Cthulhu Mythos">
Cthulhu Mythos

The Cthulhu Mythos is a shared fictional universe, based on the work of American horror writer H. P. Lovecraft.
The term was first coined by August Derleth, a contemporary correspondent of Lovecraft, who used the name of the creature "Cthulhu"—a central figure in Lovecraft literature and the focus of Lovecraft's short story "The Call of Cthulhu" (first published in pulp magazine "Weird Tales" in 1928)—to identify the system of lore employed by Lovecraft and his literary successors. The writer Richard L. Tierney later applied the term "Derleth Mythos" to distinguish between Lovecraft's works and Derleth's later stories.
Authors of Lovecraftian horror use elements of the Mythos in an ongoing expansion of the fictional universe.
History.
Robert M. Price described, in his essay "H. P. Lovecraft and the Cthulhu Mythos," two stages in the development of the Cthulhu Mythos. The first stage, termed the "Cthulhu Mythos proper" by Price, was formulated during Lovecraft's lifetime and was subject to his guidance. The second stage was guided by August Derleth who, in addition to publishing Lovecraft's stories after his death, attempted to categorize and expand the Mythos.
First stage.
An ongoing theme in Lovecraft's work is the complete irrelevance of mankind in the face of the cosmic horrors that apparently exist in the universe. Lovecraft made frequent reference to the "Great Old Ones": a loose pantheon of ancient, powerful deities from space who once ruled the Earth and who have since fallen into a deathlike sleep. This was first established in "The Call of Cthulhu", in which the minds of the human characters deteriorated when afforded a glimpse of what exists outside their perceived reality. Lovecraft emphasized the point by stating in the opening sentence of the story that "The most merciful thing in the world, I think, is the inability of the human mind to correlate all its contents."
Writer Dirk W. Mosig notes that Lovecraft was a "mechanistic materialist" who embraced the philosophy of "cosmic indifferentism". Lovecraft believed in a purposeless, mechanical, and uncaring universe that human beings, with their limited faculties, could never fully understand, and the cognitive dissonance caused by this leads to insanity. Lovecraft's viewpoint made no allowance for religious belief which could not be supported scientifically, with the incomprehensible, cosmic forces of his tales having as little regard for humanity as humans have for insects.
There have been attempts at categorizing this fictional group of beings, and Phillip A. Schreffler argues that by carefully scrutinizing Lovecraft's writings a workable framework emerges that outlines the entire "pantheon" – from the unreachable "Outer Ones" (e.g. Azathoth, who apparently occupies the centre of the universe) and "Great Old Ones" (e.g. Cthulhu, imprisoned on Earth in the sunken city of R'lyeh) to the lesser castes (the lowly slave shoggoths and the Mi-go).
David E. Schultz, however, believes Lovecraft never meant to create a canonical Mythos but rather intended his imaginary pantheon to merely serve as a background element. Lovecraft himself humorously referred to his mythos as "Yog Sothothery" (Mosig coincidentally suggested the term "Yog-Sothoth Cycle of Myth" be substituted for "Cthulhu Mythos") and at times had to remind readers his mythos creations were entirely fictional.
The view that there was no rigid structure is reinforced by S. T. Joshi, who stated "Lovecraft's imaginary cosmogony was never a static system but rather a sort of aesthetic construct that remained ever adaptable to its creator's developing personality and altering interests... here was never a rigid system that might be posthumously appropriated... he essence of the mythos lies not in a pantheon of imaginary deities nor in a cobwebby collection of forgotten tomes, but rather in a certain convincing cosmic attitude."
Price, however, believed that Lovecraft's writings could at least be divided into categories and identified three distinct themes: the "Dunsanian" (written in the vein of Lord Dunsany), "Arkham" (occurring in Lovecraft's fictionalized New England setting), and "Cthulhu" (the cosmic tales) cycles. Writer Will Murray noted that while Lovecraft often used his fictional pantheon in the stories he ghostwrote for other authors, he reserved Arkham and its environs exclusively for those tales he wrote under his own name.
Although not formalized and acknowledged as a mythos per se, Lovecraft did correspond with contemporary writers Clark Ashton Smith, Robert E. Howard, Robert Bloch, Frank Belknap Long, Henry Kuttner, and Fritz Leiber – a group referred to as the "Lovecraft Circle" – and shared story elements: Robert E. Howard's character Friedrich Von Junzt reads Lovecraft's "Necronomicon" in the short story "The Children of the Night" (1931), and in turn Lovecraft mentions Howard's "Unaussprechlichen Kulten" in the stories "Out of the Aeons" (1935) and "The Shadow Out of Time" (1936). Many of Howard's original unedited Conan stories also form part of the Cthulhu Mythos.
Second stage.
Price's dichotomy dictates the second stage commenced with August Derleth, the principal difference between Lovecraft and Derleth being the latter's use of hope and that the Cthulhu mythos essentially represented a struggle between good and evil. Derleth is credited with creating the "Elder Gods", and stated:
As Lovecraft conceived the deities or forces of his mythos, there were, initially, the Elder Gods... hese Elder Gods were benign deities, representing the forces of good, and existed peacefully...very rarely stirring forth to intervene in the unceasing struggle between the powers of evil and the races of Earth. These powers of evil were variously known as the Great Old Ones or the Ancient Ones...
—August Derleth, "The Cthulhu Mythos"
Price suggests that the basis of Derleth's systematization is found in Lovecraft, stating: "Was Derleth's use of the rubric 'Elder Gods' so alien to Lovecraft's in "At the Mountains of Madness"? Perhaps not. In fact, this very story, along with some hints from 'The Shadow over Innsmouth', provides the key to the origin of the 'Derleth Mythos'. For in "At the Mountains of Madness" we find the history of a conflict between two interstellar races (among others): the Elder Ones and the Cthulhu-spawn." Derleth himself believed that Lovecraft wished for other authors to actively write about the myth-cycle as opposed to it being a discrete plot device. Derleth expanded the boundaries of the Mythos by including any passing reference to another author's story elements by Lovecraft as part of the genre: just as Lovecraft made passing reference to Clark Ashton Smith's "Book of Eibon", Derleth in turn added Smith's Ubbo-Sathla to the Mythos.
Derleth also attempted to connect the deities of the Mythos to the four elements ("air", "earth", "fire", and "water"), but was forced to adopt artistic license and create beings to represent certain elements ("air" and "fire") to legitimize his system of classification. In applying the elemental theory to beings that function on a cosmic scale (e.g. Yog-Sothoth) some authors created a separate category termed "aethyr".
"Lovecraft" mythos.
A lesser known term employed by the scholar S. T. Joshi to describe the works of Lovecraft. Joshi identified four key elements in Lovecraft's mythos (that Price would later condense to three themes), being the fundamental principle of cosmicism (which once again highlighted the irrelevance of mankind), the imaginary New England setting, a pantheon of recurring "pseudomythological" entities and a collection of arcane books that supposedly yield insights into the mythology.

</doc>
<doc id="5726" url="https://en.wikipedia.org/wiki?curid=5726" title="Crane shot">
Crane shot

In filmmaking and video production, a crane shot is a shot taken by a camera on a crane or jib. The most obvious uses are to view the actors from above or to move up and away from them, a common way of ending a movie. Camera cranes go back to the dawn of movie-making, and were frequently used in silent films to enhance the epic nature of large sets and massive crowds.
The major supplier of cranes in Hollywood throughout the 1940s, 1950s, and 1960s was the Chapman Company (later Chapman-Leonard of North Hollywood), supplanted by dozens of similar manufacturers around the world. The typical design provided seats for both the director and the camera operator, and sometimes a third seat for the cinematographer as well. Large weights on the back of the crane provided a perfect balance to compensate for the weight of the people riding the crane. The weights had to be adjusted carefully to avoid the possibility of accidents. During the 1960s, the tallest Hollywood crane was the Chapman Titan crane, a massive design over 20' high that won an Academy Scientific & Engineering award. Most cranes like this were manually operated, requiring an experienced boom operator who knew how to raise, lower, and "crab" the camera alongside actors while the crane platform rolled on separate tracks. The crane operator and camera operator had to precisely coordinate their moves so that focus, pan, and camera position all started and stopped at the same time, requiring great skill and rehearsal.
Some filmmakers like to have the camera on a boom arm just to make it easier to move around between ordinary set-ups. Most cranes accommodate both the camera and an operator, but some can be operated by remote control. They are usually, but not always, found in what are supposed to be emotional or suspenseful scenes. One example of this technique is the shots taken by remote cranes in the car-chase sequence of "To Live and Die in L.A."
During the last few years, camera cranes have been miniaturized and costs have dropped so dramatically that most aspiring film makers have access to these tools. What was once a "Hollywood" effect is now available for under $400.
Crane-truck operator and kranmeysterom
Types.
Camera crane is traditionally divided into small, medium and large, depending on the load capacity and loading arm . Historically, the first was a kind of camera crane, which provides for lifting the chamber together with the operator, and sometimes with an assistant. It imposes certain restrictions on the range of motion of the boom because of the high load capacity and the need to ensure operator safety. In recent years more and more popular acquires another kind of camera crane with remote control - crane boom tripod . This crane carries the boom only movie camera or a television camera without operator and allows you to shoot from difficult points, as a small load capacity makes it possible to ensure the long reach of the crane boom and the relative freedom of movement. Availability of motorized panoramic head, remote control and video surveillance allows the operator to control the camera from the ground, watching the image on the monitor . A separate category consists of telescopic camera cranes. These devices allow you to set an arbitrary trajectory of the camera and get rid of characteristic jib cranes radial displacement during "spans" over shoot the scene.
Variations.
Crane-truck operator and kranmeysterom
Camera crane is traditionally divided into small, medium and large, depending on the load capacity and loading arm . Historically, the first was a kind of camera crane, which provides for lifting the chamber together with the operator, and sometimes with an assistant. It imposes certain restrictions on the range of motion of the boom because of the high load capacity and the need to ensure operator safety. In recent years more and more popular acquires another kind of camera crane with remote control - crane boom tripod . This crane carries the boom only movie camera or a television camera without operator and allows you to shoot from difficult points, as a small load capacity makes it possible to ensure the long reach of the crane boom and the relative freedom of movement. Availability of motorized panoramic head, remote control and video surveillance allows the operator to control the camera from the ground, watching the image on the monitor . A separate category consists of telescopic camera cranes. These devices allow you to set an arbitrary trajectory of the camera and get rid of characteristic jib cranes radial displacement during "spans" over shoot the scene.
Device.
On your device large camera cranes are almost indistinguishable from the usual boom-type cranes, with the exception of special requirements for the smooth movement of the boom and noise mechanisms. Small camera cranes and crane-trucks have lightweight construction, and often do not have a mechanical drive. These valves are controlled manually by balancing the load special counterweight, facilitating manipulation. Camera crane controls kranmeyster that mounts it on the set, establishes a movie camera or a television camera, is responsible for security in accordance with the plan of the operator-director provides movement of the crane boom. To improve usability and repeatability of movement of the crane in different takes, the axis of rotation arrows provided with limbs, and the pointer. In some cases, the camera crane mounted on the dolly to give even greater mobility of the camera. Such devices are called crane trolleys. Also in the modern cinema there robotic cranes that allow using multiple actuators with high accuracy repeated movement of the camera to perform the trick photography. These devices are called tap-robots and some sources are marked Motion control.
Manufacturers.
The main producers of cranes companies considered ABC-Products, Cambo, Filmotechnic, Polecam, Panther and Matthews Studio Equipment.
References.
Cinema Technologies Group http://cinetechno.com/

</doc>
<doc id="5729" url="https://en.wikipedia.org/wiki?curid=5729" title="Chariots of Fire">
Chariots of Fire

Chariots of Fire is a 1981 British historical drama film. It tells the fact-based story of two athletes in the 1924 Olympics: Eric Liddell, a devout Scottish Christian who runs for the glory of God, and Harold Abrahams, an English Jew who runs to overcome prejudice.
The film was conceived and produced by David Puttnam, written by Colin Welland, and directed by Hugh Hudson. It was nominated for seven Academy Awards and won four, including Best Picture and Best Screenplay. It is ranked 19th in the British Film Institute's list of Top 100 British films. The film is also notable for its memorable instrumental theme tune by Vangelis, who won the Academy Award for Best Original Score.
The film's title was inspired by the line, "Bring me my chariot of fire," from the William Blake poem adapted into the popular British hymn "Jerusalem"; the hymn is heard at the end of the film. The original phrase "chariot(s) of fire" is from 2 Kings and in the Bible.
Plot.
In 1919, Harold Abrahams (Ben Cross) enters the University of Cambridge, where he experiences anti-Semitism from the staff, but enjoys participating in the Gilbert and Sullivan club. He becomes the first person to ever complete the Trinity Great Court Run – running around the college courtyard in the time it takes for the clock to strike 12. Abrahams achieves an undefeated string of victories in various national running competitions. Although focused on his running, he falls in love with a leading Gilbert and Sullivan soprano, Sybil (Alice Krige).
Eric Liddell (Ian Charleson), born in China of Scottish missionary parents, is in Scotland. His devout sister Jennie (Cheryl Campbell) disapproves of Liddell's plans to pursue competitive running. But Liddell sees running as a way of glorifying God before returning to China to work as a missionary.
When they first race against each other, Liddell beats Abrahams. Abrahams takes it poorly, but Sam Mussabini (Ian Holm), a professional trainer whom he had approached earlier, offers to take him on to improve his technique. This attracts criticism from the Cambridge college masters (John Gielgud and Lindsay Anderson), who allege it is not gentlemanly for an amateur to "play the tradesman" by employing a professional coach. Abrahams dismisses this concern, interpreting it as cover for anti-Semitic and class-based prejudice.
When Eric Liddell accidentally misses a church prayer meeting because of his running, his sister Jennie upbraids him and accuses him of no longer caring about God. Eric tells her that though he intends to eventually return to the China mission, he feels divinely inspired when running, and that not to run would be to dishonour God, saying, "I believe that God made me for a purpose. But He also made me fast, and when I run, I feel His pleasure."
The two athletes, after years of training and racing, are accepted to represent Great Britain in the 1924 Olympics in Paris. Also accepted are Abrahams' Cambridge friends, Lord Andrew Lindsay (Nigel Havers), Aubrey Montague (Nicholas Farrell), and Henry Stallard (Daniel Gerroll). While boarding the boat to Paris for the Olympics, Liddell learns the news that the heat for his 100-metre race will be on a Sunday. He refuses to run the race – despite strong pressure from the Prince of Wales and the British Olympic committee – because his Christian convictions prevent him from running on the Sabbath.
Hope appears when Liddell's teammate Lindsay, having already won a silver medal in the 400 metres hurdles, proposes to yield his place in the 400-metre race on the following Thursday to Liddell, who gratefully agrees. His religious convictions in the face of national athletic pride make headlines around the world.
Liddell delivers a sermon at the Paris Church of Scotland that Sunday, and quotes from , ending with:
Abrahams is badly beaten by the heavily favoured United States runners in the 200 metre race. He knows his last chance for a medal will be the 100 metres. He competes in the race, and wins. His coach Sam Mussabini is overcome that the years of dedication and training have paid off with an Olympic gold medal. Now Abrahams can get on with his life and reunite with his girlfriend Sybil, whom he had neglected for the sake of running. Before Liddell's race, the American coach remarks dismissively to his runners that Liddell has little chance of doing well in his now far longer 400 metre race. But one of the American runners, Jackson Scholz, hands Liddell a note of support for his convictions. Liddell defeats the American favourites and wins the gold medal.
The British team returns home triumphant. As the film ends, onscreen text explains that Abrahams married Sybil, and became the elder statesman of British athletics. Liddell went on to missionary work in China. All of Scotland mourned his death in 1945 in Japanese-occupied China.
Historical accuracy.
Characters.
The film depicts Abrahams as attending Gonville and Caius College, Cambridge with three other Olympic athletes: Henry Stallard, Aubrey Montague, and Lord Andrew Lindsay. Abrahams and Stallard were in fact students there and competed in the 1924 Olympics. Montague also competed in the Olympics as depicted, but he attended Oxford, not Cambridge. Aubrey Montague sent daily letters to his mother about his time at Oxford and the Olympics; these letters were the basis of Montague's narration in the film.
The character of Lindsay was based partially on Lord Burghley, a significant figure in the history of British athletics. Although Burghley did attend Cambridge, he was not a contemporary of Harold Abrahams, as Abrahams was an undergraduate from 1919 to 1923 and Burghley was at Cambridge from 1923 to 1927. One scene in the film depicts the Burghley-based "Lindsay" as practising hurdles on his estate with full champagne glasses placed on each hurdle – this was something the wealthy Burghley did, although he used matchboxes instead of champagne glasses. The fictional character of Lindsay was created when Douglas Lowe, who was Britain's third athletics gold medallist in the 1924 Olympics, was not willing to be involved with the film.
Another scene in the film recreates the Great Court Run, in which the runners attempt to run around the perimeter of the Great Court at Trinity College, Cambridge in the time it takes the clock to strike 12 at midday. The film shows Abrahams performing the feat for the first time in history. In fact, Abrahams never attempted this race, and at the time of filming the only person on record known to have succeeded was Lord Burghley, in 1927. In "Chariots of Fire", Lindsay, who is based on Lord Burghley, runs the Great Court Run with Abrahams in order to spur him on, and crosses the finish line just a moment too late. Since the film's release, the Great Court Run has also been successfully run by Trinity undergraduate Sam Dobin, in October 2007.
In the film, Eric Liddell is tripped up by a Frenchman in the 400-metre event of a Scotland–France international athletic meeting. He recovers, makes up a 20-metre deficit, and wins. This was based on fact; the actual race was the 440 yards at a Triangular Contest meet between Scotland, England, and Ireland at Stoke-on-Trent in England in July 1923. His achievement was remarkable as he had already won the 100- and 220-yard events that day. Also unmentioned with regard to Liddell is that it was he who introduced Abrahams to Sam Mussabini. This is alluded to: In the film Abrahams first encounters Mussabini while he is watching Liddell race. The film, however, suggests that Abrahams himself sought Mussabini's assistance.
Abrahams and Liddell did race against each other once, but not quite as depicted in the film, which shows Liddell winning the final of the 100 yards against a shattered Abrahams at the 1923 AAA Championship at Stamford Bridge. In fact, they raced only in a heat of the 220 yards, which Liddell won, five yards ahead of Abrahams who did not progress to the final. In the 100 yards, Abrahams was eliminated in the heats and never raced against Liddell, who won the finals of both races the next day.
Abrahams' fiancée is misidentified as Sybil Gordon, a soprano at the D'Oyly Carte Opera Company. In fact, in 1936, Abrahams married Sybil Evers, who sang at the D'Oyly Carte, but they did not meet until 1934. Also, in the film, Sybil is depicted as singing the role of Yum-Yum in "The Mikado", but neither Sybil Gordon nor Sybil Evers ever sang that role with D'Oyly Carte, although Evers was known for her charm in singing Peep-Bo, one of the two other "little maids from school". Harold Abrahams' love of and heavy involvement with Gilbert and Sullivan, as depicted in the film, is factual.
Liddell's sister was several years younger than she was portrayed in the film. Her disapproval of Liddell's track career was creative licence; she actually fully supported his sporting work. Jenny Liddell Somerville cooperated fully with the making of the film and has a brief cameo in the Paris Church of Scotland during Liddell's sermon.
At the memorial service for Harold Abrahams, which opens the film, Lord Lindsay mentions that he and Aubrey Montague are the only members of the 1924 Olympic team still alive. However, Montague died in 1948, 30 years before Abrahams' death.
1924 Olympics.
The film takes some liberties with the events at the 1924 Olympics, including the events surrounding Liddell's refusal to race on a Sunday. In the film, he doesn't learn that the 100-metre heat is to be held on the Christian Sabbath until he is boarding the boat to Paris. In fact, the schedule was made public several months in advance. Liddell did however face immense pressure to run on that Sunday and to compete in the 100 metres, getting called before a grilling by the British Olympic Committee, the Prince of Wales, and other grandees; and his refusal to run made headlines around the world. The decision to change races was, even so, made well before embarking to Paris, and Liddell spent the intervening months training for the 400 metres, an event in which he had previously excelled. It is true, nonetheless, that Liddell's success in the Olympic 400m was largely unexpected.
The film depicts Lindsay, having already won a medal in the 400-metre hurdles, giving up his place in the 400-metre race for Liddell. In fact Burghley, on whom Lindsay is loosely based, was eliminated in the heats of the 110 hurdles (he would go on to win a gold medal in the 400 hurdles at the 1928 Olympics), and was not entered for the 400 metres.
The film reverses the order of Abrahams' 100m and 200m races at the Olympics. In reality, after winning the 100 metres race, Abrahams ran the 200 metres but finished last, Jackson Scholz taking the gold medal. In the film, before his triumph in the 100m, Abrahams is shown losing the 200m and being scolded by Mussabini. And during the following scene in which Abrahams speaks with his friend Montague while receiving a massage from Mussabini, there is a French newspaper clipping showing Scholz and Charlie Paddock with a headline which states that the 200 metres was a triumph for the United States. In the same conversation, Abrahams laments getting "beaten out of sight" in the 200. The film thus has Abrahams overcoming the disappointment of losing the 200 by going on to win the 100, a reversal of the real order.
Eric Liddell actually also ran in the 200m race, and finished third, behind Paddock and Scholz. This was the only time in reality that Liddell and Abrahams competed in the same race. Their meeting in the 1923 AAA Championship in the film was fictitious, though Liddell's record win in that race did spur Abrahams to train even harder.
Abrahams also won a silver medal as an opening runner for the 4 x 100 metres relay team, not shown in the film. Aubrey Montague placed sixth in the steeplechase, as depicted.
Personal inaccuracies at the Olympics.
In the film, the 100m bronze medallist is a character called "Tom Watson"; the real medallist was Arthur Porritt of New Zealand, who refused permission for his name to be used in the film, allegedly out of modesty. His wish was accepted by the film's producers, even though his permission was not necessary. However, the brief back-story given for Watson, who is called up to the New Zealand team from the University of Oxford, substantially matches Porritt's history. With the exception of Porritt, all the runners in the 100m final are identified correctly when they line up for inspection by the Prince of Wales.
Jackson Scholz is depicted as handing Liddell an inspirational Bible-quotation message before the 400 metres final: "It says in the good Book, 'He that honors me, I will honor.' Good luck." In reality, the note was from members of the British team, and was handed to Liddell before the race by his attending masseur at the team's Paris hotel. For dramatic purposes, screenwriter Welland asked Scholz if he could be depicted handing the note, and Scholz readily agreed, saying "Yes, great, as long as it makes me look good."
Production.
Script and direction.
Producer David Puttnam was looking for a story in the mold of "A Man for All Seasons" (1966), regarding someone who follows his conscience, and felt sports provided clear situations in this sense. He discovered Eric Liddell's story by accident in 1977, when he happened upon a reference book on the Olympics while housebound from the flu in a rented house in Los Angeles.
Screenwriter Colin Welland, commissioned by Puttnam, did an enormous amount of research for his Academy Award-winning script. Among other things, he took out advertisements in London newspapers seeking memories of the 1924 Olympics, went to the National Film Archives for pictures and footage of the 1924 Olympics, and interviewed everyone involved who was still alive. Welland just missed Abrahams, who died 14 January 1978, but he did attend Abrahams' February 1978 memorial service, which inspired the present-day framing device of the film. Aubrey Montague's son saw Welland's newspaper ad and sent him copies of the letters his father had sent home – which gave Welland something to use as a narrative bridge in the film. Except for changes in the greetings of the letters from "Darling Mummy" to "Dear Mum" and the change from Oxford to Cambridge, all of the readings from Montague's letters are from the originals.
Welland's original script also featured, in addition to Eric Liddell and Harold Abrahams, a third protagonist, 1924 Olympic gold medallist Douglas Lowe, who was presented as a privileged aristocratic athlete. However, Lowe refused to have anything to do with the film, and his character was written out and replaced by the fictional character of Lord Andrew Lindsay.
Ian Charleson himself wrote Eric Liddell's speech to the post-race workingmen's crowd at the Scotland v. Ireland races. Charleson, who had studied the Bible intensively in preparation for the role, told director Hugh Hudson that he didn't feel the portentous and sanctimonious scripted speech was either authentic or inspiring. Hudson and Welland allowed him to write words he personally found inspirational instead.
The film was slightly altered for the U.S. audience. A brief scene depicting a pre-Olympics cricket game between Abrahams, Liddell, Montague, and the rest of the British track team appears shortly after the beginning of the original film. For the American audience, this brief scene was deleted. In the U.S., to avoid the initial G rating, which had been strongly associated with children's films and might have hindered box office sales, a different scene was used – one depicting Abrahams and Montague arriving at a Cambridge railway station and encountering two World War I veterans who use an obscenity – in order to be given a PG rating.
Puttnam chose Hugh Hudson, a multiple award-winning advertising and documentary filmmaker who had never helmed a feature film, to direct "Chariots of Fire". Hudson and Puttnam had known each other since the 1960s, when Puttnam was an advertising executive and Hudson was making films for ad agencies. In 1977, Hudson had also been second-unit director on the Puttnam-produced film "Midnight Express".
Casting.
Director Hugh Hudson was determined to cast young, unknown actors in all the major roles of the film, and to back them up by using veterans like John Gielgud, Lindsay Anderson, and Ian Holm as their supporting cast. Hudson and producer David Puttnam did months of fruitless searching for the perfect actor to play Eric Liddell. They then saw Scottish stage actor Ian Charleson performing the role of Pierre in the Royal Shakespeare Company's production of "Piaf", and knew immediately they had found their man. Unbeknownst to them, Charleson had heard about the film from his father, and desperately wanted to play the part, feeling it would "fit like a kid glove".
Ben Cross, who plays Harold Abrahams, was discovered while playing Billy Flynn in "Chicago". In addition to having a natural pugnaciousness, he had the desired ability to sing and play the piano. Cross was thrilled to be cast, and said he was moved to tears by the film's script.
20th Century Fox, which put up half of the production budget in exchange for distribution rights outside of North America, insisted on having a couple of notable American names in the cast. Thus the small parts of the two American champion runners, Jackson Scholz and Charlie Paddock, were cast with recent headliners: Brad Davis had recently starred in "Midnight Express" (also produced by Puttnam), and Dennis Christopher had recently starred, as a young bicycle racer, in the popular indie film "Breaking Away".
All of the actors portraying runners underwent a gruelling three-month training intensive, with renowned running coach Tom McNab. This training and isolation of the actors also created a strong bond and sense of camaraderie among them.
Music.
Although the film is a period piece, set in the 1920s, the Academy Award-winning original soundtrack composed by Vangelis uses a modern 1980s electronic sound, with a strong use of synthesizer and piano among other instruments. This was a bold and significant departure from earlier period films, which employed sweeping orchestral instrumentals. The title theme of the film has become iconic, and has been used in subsequent films and television shows during slow-motion segments.
Vangelis, a Greek-born electronic composer who moved to Paris in the late 1960s, had been living in London since 1974. Director Hugh Hudson had collaborated with him on documentaries and commercials, and was also particularly impressed with his 1979 albums "Opera Sauvage" and "China". David Puttnam also greatly admired Vangelis's body of work, having originally selected his compositions for his previous film "Midnight Express". Hudson made the choice for Vangelis and for a modern score: "I knew we needed a piece which was anachronistic to the period to give it a feel of modernity. It was a risky idea but we went with it rather than have a period symphonic score." The soundtrack had a personal significance to Vangelis: After composing the iconic theme tune he told Puttnam, "My father is a runner, and this is an anthem to him."
Hudson originally wanted Vangelis's 1977 tune "L'Enfant", from his "Opera Sauvage" album, to be the title theme of the film, and the beach running sequence was actually filmed with "L'Enfant" playing on loudspeakers for the runners to pace to. Vangelis finally convinced Hudson he could create a new and better piece for the film's main theme – and when he played the now-iconic "Chariots of Fire" theme for Hudson, it was agreed the new tune was unquestionably better. The "L'Enfant" melody still made it into the film: When the athletes reach Paris and enter the stadium, a brass band marches through the field, and first plays a modified, acoustic performance of the piece. Vangelis's electronic "L'Enfant" track eventually was used prominently in the 1982 film "The Year of Living Dangerously".
Some pieces of Vangelis's music in the film did not end up on the film's soundtrack album. One of them is the background music to the race Eric Liddell runs in the Scottish highlands. This piece is a version of "Hymn", the original version of which appears on Vangelis's 1979 album, "Opéra sauvage". Various versions are also included on Vangelis's compilation albums "Themes", "Portraits", and "", though none of these include the version used in the film.
Five lively Gilbert and Sullivan tunes also appear in the soundtrack, and serve as jaunty period music which nicely counterpoints Vangelis's modern electronic score. These are: "He is an Englishman" from "H.M.S. Pinafore", "Three Little Maids from School Are We" from "The Mikado", "With Catlike Tread" from "The Pirates of Penzance", "The Soldiers of Our Queen" from "Patience", and "There Lived a King" from "The Gondoliers".
The film also incorporates a major traditional work: "Jerusalem", sung by a British choir at the 1978 funeral of Harold Abrahams. The words, written by William Blake in 1804-8, were set to music by Parry in 1916 as a celebration of England. This hymn has been described as "England's unofficial national anthem", concludes the film and inspired its title. A handful of other traditional anthems and hymns and period-appropriate instrumental ballroom-dance music round out the film's soundtrack.
Filming locations.
The beach scenes associated with the theme tune were filmed at West Sands, St. Andrews. A plaque commemorating the filming can be found there today. The very last scene of the opening titles crosses the 1st and 18th holes of the Old Course at St Andrews Links.
All of the Cambridge scenes were actually filmed at Hugh Hudson's alma mater Eton College, because Cambridge refused filming rights, fearing depictions of anti-Semitism. The Cambridge administration greatly regretted the decision after the film's enormous success.
Liverpool Town Hall was the setting for the scenes depicting the British Embassy in Paris. The Colombes Olympic Stadium in Paris was represented by the Oval Sports Centre, Bebington, Merseyside. The nearby Woodside ferry terminal was used to represent the embarkation scenes set in Dover. The railway station scenes were filmed at the National Railway Museum in York. The scene depicting a performance of "The Mikado" was filmed in the Royal Court Theatre, Liverpool with members of the D'Oyly Carte Opera Company who were on tour.
Revival for the 2012 Olympics.
"Chariots of Fire" became a recurring theme in promotions for the 2012 Summer Olympics in London. The film's theme tune was featured at the opening of the 2012 London New Years fireworks celebrating the Olympics, and the film's iconic beach-running scene and theme tune were used in "The Sun"'s "Let's Make It Great, Britain" Olympic ads. The runners who first tested the new Olympic Park were spurred on by the "Chariots of Fire" theme tune, and the iconic music was also used to fanfare the carriers of the Olympic flame on parts of its route through the UK. The film's theme was also performed by the London Symphony Orchestra, conducted by Simon Rattle, during the Opening Ceremony of the games; the performance was accompanied by a comedy skit by Rowan Atkinson (in persona as Mr. Bean) which included the opening beach-running footage from the film. The film's theme tune was also played during each medal ceremony of the 2012 Olympics.
Stage adaptation.
A stage adaptation of "Chariots of Fire" was mounted in honour of the 2012 Olympics. The play, "Chariots of Fire", which was adapted by playwright Mike Bartlett and included the iconic Vangelis score, ran from 9 May to 16 June 2012 at London's Hampstead Theatre, and transferred to the Gielgud Theatre in the West End on 23 June, where it ran until 5 January 2013. It starred Jack Lowden as Eric Liddell and James McArdle as Harold Abrahams, and Edward Hall directed. Stage designer Miriam Buether transformed each theatre into an Olympic stadium, and composer Jason Carr wrote additional music. Vangelis also created several new pieces of music for the production. The stage version for the London Olympic year was the idea of the film's director, Hugh Hudson, who co-produced the play; he stated, "Issues of faith, of refusal to compromise, standing up for one's beliefs, achieving something for the sake of it, with passion, and not just for fame or financial gain, are even more vital today."
Another play, "Running for Glory", written by Philip Dart, based on the 1924 Olympics, and focusing on Abrahams and Liddell, toured parts of Britain from 25 February to 1 April 2012. It starred Nicholas Jacobs as Harold Abrahams, and Tom Micklem as Eric Liddell.
UK cinematic re-release, Blu-ray.
As an official part of the London 2012 Festival celebrations, a new digitally re-mastered version of the film screened in 150 cinemas throughout the UK. The re-release began 13 July 2012, two weeks before the opening ceremony of the London Olympics.
A Blu-ray of the film was released on 10 July 2012 in North America, and was released 16 July 2012 in the UK. The release includes nearly an hour of special features, a CD sampler, and a 32-page "digibook".
Accolades.
"Chariots of Fire" was very successful at the 54th Academy Awards, winning four of seven nominations. When accepting his Oscar for Best Original Screenplay, Colin Welland famously announced "The British are coming". At the 1981 Cannes Film Festival the film won two awards and competed for the Palme d'Or.
American Film Institute recognition

</doc>
<doc id="5734" url="https://en.wikipedia.org/wiki?curid=5734" title="Consequentialism">
Consequentialism

Consequentialism is the class of normative ethical theories holding that the consequences of one's conduct are the ultimate basis for any judgment about the rightness or wrongness of that conduct. Thus, from a consequentialist standpoint, a morally right act (or omission from acting) is one that will produce a good outcome, or consequence. In an extreme form, the idea of consequentialism is commonly encapsulated in the English saying, "the end justifies the means", meaning that if a goal is morally important enough, any method of achieving it is acceptable.
Consequentialism is usually contrasted with deontological ethics (or "deontology"), in that deontology, in which rules and moral duty are central, derives the rightness or wrongness of one's conduct from the character of the behaviour itself rather than the outcomes of the conduct. It is also contrasted with virtue ethics, which focuses on the character of the agent rather than on the nature or consequences of the act (or omission) itself, and pragmatic ethics which treats morality like science: advancing socially over the course of many lifetimes, such that any moral criterion is subject to revision. Consequentialist theories differ in how they define moral goods.
Some argue that consequentialist and deontological theories are not necessarily mutually exclusive. For example, T. M. Scanlon advances the idea that human rights, which are commonly considered a "deontological" concept, can only be justified with reference to the consequences of having those rights. Similarly, Robert Nozick argues for a theory that is mostly consequentialist, but incorporates inviolable "side-constraints" which restrict the sort of actions agents are permitted to do.
Philosophies.
State consequentialism.
Mohist consequentialism, also known as state consequentialism, is an ethical theory which evaluates the moral worth of an action based on how much it contributes to the welfare of a state. According to the "Stanford Encyclopedia of Philosophy", Mohist consequentialism, dating back to the 5th century BCE, is the "world's earliest form of consequentialism, a remarkably sophisticated version based on a plurality of intrinsic goods taken as constitutive of human welfare." Unlike utilitarianism, which views utility as the sole moral good, "the basic goods in Mohist consequentialist thinking are... order, material wealth, and increase in population". During Mozi's era, war and famines were common, and population growth was seen as a moral necessity for a harmonious society. The "material wealth" of Mohist consequentialism refers to basic needs like shelter and clothing, and the "order" of Mohist consequentialism refers to Mozi's stance against warfare and violence, which he viewed as pointless and a threat to social stability. Stanford sinologist David Shepherd Nivison, in the "The Cambridge History of Ancient China", writes that the moral goods of Mohism "are interrelated: more basic wealth, then more reproduction; more people, then more production and wealth... if people have plenty, they would be good, filial, kind, and so on unproblematically." The Mohists believed that morality is based on "promoting the benefit of all under heaven and eliminating harm to all under heaven." In contrast to Jeremy Bentham's views, state consequentialism is not utilitarian because it is not hedonistic or individualistic. The importance of outcomes that are good for the community outweigh the importance of individual pleasure and pain. The term state consequentialism has also been applied to the political philosophy of the Confucian philosopher Xunzi.
Utilitarianism.
In summary, Jeremy Bentham states that people are driven by their interests and their fears, but their interests take precedence over their fears, and their interests are carried out in accordance with how people view the consequences that might be involved with their interests. "Happiness" on this account is defined as the maximization of pleasure and the minimization of pain.
Historically, hedonistic utilitarianism is the paradigmatic example of a consequentialist moral theory. This form of utilitarianism holds that what matters is the aggregate happiness; the happiness of everyone and not the happiness of any particular person. John Stuart Mill, in his exposition of hedonistic utilitarianism, proposed a hierarchy of pleasures, meaning that the pursuit of certain kinds of pleasure is more highly valued than the pursuit of other pleasures. However, some contemporary utilitarians, such as Peter Singer, are concerned with maximizing the satisfaction of preferences, hence "preference utilitarianism". Other contemporary forms of utilitarianism mirror the forms of consequentialism outlined below.
Ethical egoism.
Ethical egoism can be understood as a consequentialist theory according to which the consequences for the individual agent are taken to matter more than any other result. Thus, egoism will prescribe actions that may be beneficial, detrimental, or neutral to the welfare of others. Some, like Henry Sidgwick, argue that a certain degree of egoism "promotes" the general welfare of society for two reasons: because individuals know how to please themselves best, and because if everyone were an austere altruist then general welfare would inevitably decrease.
Ethical altruism.
Ethical altruism can be seen as a consequentialist ethic which prescribes that an individual take actions that have the best consequences for everyone except for himself. This was advocated by Auguste Comte, who coined the term "altruism," and whose ethics can be summed up in the phrase "Live for others".
Rule consequentialism.
In general, consequentialist theories focus on actions. However, this need not be the case. Rule consequentialism is a theory that is sometimes seen as an attempt to reconcile deontology and consequentialism—and in some cases, this is stated as a criticism of rule consequentialism. Like deontology, rule consequentialism holds that moral behavior involves following certain rules. However, rule consequentialism chooses rules based on the consequences that the selection of those rules have. Rule consequentialism exists in the forms of rule utilitarianism and rule egoism.
Various theorists are split as to whether the rules are the only determinant of moral behavior or not. For example, Robert Nozick holds that a certain set of minimal rules, which he calls "side-constraints", are necessary to ensure appropriate actions. There are also differences as to how absolute these moral rules are. Thus, while Nozick's side-constraints are absolute restrictions on behavior, Amartya Sen proposes a theory that recognizes the importance of certain rules, but these rules are not absolute. That is, they may be violated if strict adherence to the rule would lead to much more undesirable consequences.
One of the most common objections to rule-consequentialism is that it is incoherent, because it is based on the consequentialist principle that what we should be concerned with is maximizing the good, but then it tells us not to act to maximize the good, but to follow rules (even in cases where we know that breaking the rule could produce better results).
Brad Hooker avoided this objection by not basing his form of rule-consequentialism on the ideal of maximizing the good. He writes:
…the best argument for rule-consequentialism is not that it derives from an overarching commitment to maximise the good. The best argument for rule-consequentialism is that it does a better job than its rivals of matching and tying together our moral convictions, as well as offering us help with our moral disagreements and uncertainties
Derek Parfit described Brad Hooker's book on rule-consequentialism "Ideal Code, Real World" as the "best statement and defence, so far, of one of the most important moral theories."
Two-level consequentialism.
The two-level approach involves engaging in critical reasoning and considering all the possible ramifications of one's actions before making an ethical decision, but reverting to generally reliable moral rules when one is not in a position to stand back and examine the dilemma as a whole. In practice, this equates to adhering to rule consequentialism when one can only reason on an intuitive level, and to act consequentialism when in a position to stand back and reason on a more critical level.
This position can be described as a reconciliation between act consequentialism - in which the morality of an action is determined by that action's effects - and rule consequentialism - in which moral behavior is derived from following rules that lead to positive outcomes.
The two-level approach to consequentialism is most often associated with R. M. Hare and Peter Singer.
Motive consequentialism.
Another consequentialist version is motive consequentialism which looks if the state of affairs that results from the motive to choose an action is better or at least as good as each of the alternative state of affairs that would have resulted from alternative actions. This version gives relevance to the motive of an act and links it to its consequences. An act can therefore not be wrong if the decision to act was based on a right motive. A possible inference is, that one can not be blamed for mistaken judgements if the motivation was to do good.
Negative consequentialism.
Most consequentialist theories focus on "promoting" some sort of good consequences. However, Negative utilitarianism lays out a consequentialist theory that focuses solely on minimizing bad consequences.
One major difference between these two approaches is the agent's responsibility. Positive consequentialism demands that we bring about good states of affairs, whereas negative consequentialism requires that we avoid bad ones. Stronger versions of negative consequentialism will require active intervention to prevent bad and ameliorate existing harm. In weaker versions, simple forbearance from acts tending to harm others is sufficient.
Often "negative" consequentialist theories assert that reducing suffering is more important than increasing pleasure. Karl Popper, for example, claimed "…from the moral point of view, pain cannot be outweighed by pleasure...". (While Popper is not a consequentialist per se, this is taken as a classic statement of negative utilitarianism.) When considering a theory of justice, negative consequentialists may use a statewide or global-reaching principle: the reduction of suffering (for the disadvantaged) is more valuable than increased pleasure (for the affluent or luxurious).
Teleological ethics.
Teleological ethics (Greek telos, "end"; logos, "science") is an ethical theory that holds that the ends or consequences of an act determine whether an act is good or evil. Teleological theories are often discussed in opposition to deontological ethical theories, which hold that acts themselves are "inherently" good or evil, regardless of the consequences of acts.
Teleological theories differ on the nature of the end that actions ought to promote. Eudaemonist theories (Greek eudaimonia, "happiness") hold that the goal of ethics consists in some function or activity appropriate to man as a human being, and thus tend to emphasize the cultivation of virtue or excellence in the agent as the end of all action. These could be the classical virtues—courage, temperance, justice, and wisdom—that promoted the Greek ideal of man as the "rational animal", or the theological virtues—faith, hope, and love—that distinguished the Christian ideal of man as a being created in the image of God.
Utilitarian-type theories hold that the end consists in an experience or feeling produced by the action. Hedonism, for example, teaches that this feeling is pleasure—either one's own, as in egoism (the 17th-century English philosopher Thomas Hobbes), or everyone's, as in universalistic hedonism, or utilitarianism (the 19th-century English philosophers Jeremy Bentham, John Stuart Mill, and Henry Sidgwick), with its formula of the "greatest pleasure of the greatest number."
Other utilitarian-type views include the claims that the end of action is survival and growth, as in evolutionary ethics (the 19th-century English philosopher Herbert Spencer); the experience of power, as in despotism; satisfaction and adjustment, as in pragmatism (20th-century American philosophers Ralph Barton Perry and John Dewey); and freedom, as in existentialism (the 20th-century French philosopher Jean-Paul Sartre).
The chief problem for eudaemonist theories is to show that leading a life of virtue will also be attended by happiness—by the winning of the goods regarded as the chief end of action. That Job should suffer and Socrates and Jesus die while the wicked prosper, then seems unjust. Eudaemonists generally reply that the universe is moral and that, in Socrates' words, "No evil can happen to a good man, either in life or after death," or, in Jesus' words, "But he who endures to the end will be saved." (Matt 10:22).
Utilitarian theories, on the other hand, must answer the charge that ends do not justify the means. The problem arises in these theories because they tend to separate the achieved ends from the action by which these ends were produced. One implication of utilitarianism is that one's intention in performing an act may include all of its foreseen consequences. The goodness of the intention then reflects the balance of the good and evil of these consequences, with no limits imposed upon it by the nature of the act itself—even if it be, say, the breaking of a promise or the execution of an innocent man. Utilitarianism, in answering this charge, must show either that what is apparently immoral is not really so or that, if it really is so, then closer examination of the consequences will bring this fact to light. Ideal utilitarianism (G.E. Moore and Hastings Rashdall) tries to meet the difficulty by advocating a plurality of ends and including among them the attainment of virtue itself, which, as John Stuart Mill affirmed, "may be felt a good in itself, and desired as such with as great intensity as any other good."
Acts and omissions, and the "act and omissions doctrine".
Since pure consequentialism holds that an action is to be judged solely by its result, most consequentialist theories hold that a deliberate action is no different from a deliberate decision not to act. This contrasts with the "acts and omissions doctrine", which is upheld by some medical ethicists and some religions: it asserts there is a significant moral distinction between acts and deliberate non-actions which lead to the same outcome. This contrast is brought out in issues such as voluntary euthanasia.
Issues.
Action guidance.
One important characteristic of many normative moral theories such as consequentialism is the ability to produce practical moral judgements. At the very least, any moral theory needs to define the standpoint from which the goodness of the consequences are to be determined. What is primarily at stake here is the "responsibility" of the agent.
The ideal observer.
One common tactic among consequentialists, particularly those committed to an altruistic (selfless) account of consequentialism, is to employ an ideal, neutral observer from which moral judgements can be made. John Rawls, a critic of utilitarianism, argues that utilitarianism, in common with other forms of consequentialism, relies on the perspective of such an ideal observer. The particular characteristics of this ideal observer can vary from an omniscient observer, who would grasp all the consequences of any action, to an ideally informed observer, who knows as much as could reasonably be expected, but not necessarily all the circumstances or all the possible consequences. Consequentialist theories that adopt this paradigm hold that right action is the action that will bring about the best consequences from this ideal observer's perspective.
The real observer.
In practice, it is very difficult, and at times arguably impossible, to adopt the point of view of an ideal observer. Individual moral agents do not know everything about their particular situations, and thus do not know all the possible consequences of their potential actions. For this reason, some theorists have argued that consequentialist theories can only require agents to choose the best action in line with what they know about the situation. However, if this approach is naïvely adopted, then moral agents who, for example, recklessly fail to reflect on their situation, and act in a way that brings about terrible results, could be said to be acting in a morally justifiable way. Acting in a situation without first informing oneself of the circumstances of the situation can lead to even the most well-intended actions yielding miserable consequences. As a result, it could be argued that there is a moral imperative for an agent to inform himself as much as possible about a situation before judging the appropriate course of action. This imperative, of course, is derived from consequential thinking: a better-informed agent is able to bring about better consequences.
Consequences for whom.
Moral action always has consequences for certain people or things. Varieties of consequentialism can be differentiated by the beneficiary of the good consequences. That is, one might ask "Consequences for whom?"
Agent-focused or agent-neutral.
A fundamental distinction can be drawn between theories which require that agents act for ends perhaps disconnected from their own interests and drives and theories which permit that agents act for ends in which they have some personal interest or motivation. These are called "agent-neutral" and "agent-focused" theories respectively.
Agent-neutral consequentialism ignores the specific value a state of affairs has for any particular agent. Thus, in an agent-neutral theory, an actor's personal goals do not count any more than anyone else's goals in evaluating what action the actor should take. Agent-focused consequentialism, on the other hand, focuses on the particular needs of the moral agent. Thus, in an agent-focused account, such as one that Peter Railton outlines, the agent might be concerned with the general welfare, but the agent is "more" concerned with the immediate welfare of herself and her friends and family.
These two approaches could be reconciled by acknowledging the tension between an agent's interests as an individual and as a member of various groups, and seeking to somehow optimize among all of these interests. For example, it may be meaningful to speak of an action as being good for someone as an individual but bad for them as a citizen of their town.
Human-centered?
Many consequentialist theories may seem primarily concerned with human beings and their relationships with other human beings. However, some philosophers argue that we should not limit our ethical consideration to the interests of human beings alone. Jeremy Bentham, who is regarded as the founder of utilitarianism, argues that animals can experience pleasure and pain, thus demanding that 'non-human animals' should be a serious object of moral concern. More recently, Peter Singer has argued that it is unreasonable that we do not give equal consideration to the interests of animals as to those of human beings when we choose the way we are to treat them. Such equal consideration does not necessarily imply identical treatment of humans and non-humans, any more than it necessarily implies identical treatment of all humans.
Kinds of consequences.
One way to divide various consequentialisms is by the types of consequences that are taken to matter most, that is, which consequences count as good states of affairs. According to utilitarianism, a good action is one that results in an increase in pleasure, and the best action is one that results in the most pleasure for the greatest number. Closely related is eudaimonic consequentialism, according to which a full, flourishing life, which may or may not be the same as enjoying a great deal of pleasure, is the ultimate aim. Similarly, one might adopt an aesthetic consequentialism, in which the ultimate aim is to produce beauty. However, one might fix on non-psychological goods as the relevant effect. Thus, one might pursue an increase in material equality or political liberty instead of something like the more ephemeral "pleasure". Other theories adopt a package of several goods, all to be promoted equally.
Virtue ethics.
Consequentialism can also be contrasted with aretaic moral theories such as virtue ethics. Whereas consequentialist theories posit that consequences of action should be the primary focus of our thinking about ethics, virtue ethics insists that it is the character rather than the consequences of actions that should be the focal point. Some virtue ethicists hold that consequentialist theories totally disregard the development and importance of moral character. For example, Philippa Foot argues that consequences in themselves have no ethical content, unless it has been provided by a virtue such as benevolence.
However, consequentialism and virtue ethics need not be entirely antagonistic. Philosopher Iain King has developed an approach which reconciles the two schools. Other consequentialists consider effects on the character of people involved in an action when assessing consequence. Similarly, a consequentialist theory may aim at the maximization of a particular virtue or set of virtues. Finally, following Foot's lead, one might adopt a sort of consequentialism that argues that virtuous activity ultimately produces the best consequences.
Ultimate end.
The "ultimate end" is a concept in the moral philosophy of Max Weber, in which individuals act in a faithful, rather than rational, manner.
Etymology.
The term "consequentialism" was coined by G. E. M. Anscombe in her essay "Modern Moral Philosophy" in 1958, to describe what she saw as the central error of certain moral theories, such as those propounded by Mill and Sidgwick.
Criticisms.
G. E. M. Anscombe objects to consequentialism on the grounds that it does not provide guidance in what one ought to do because there is no distinction between consequences that are foreseen and those that are intended (see Principle of double effect).
Bernard Williams has argued that consequentialism is alienating because it requires moral agents to put too much distance between themselves and their own projects and commitments. Williams argues that consequentialism requires moral agents to take a strictly impersonal view of all actions, since it is only the consequences, and not who produces them, that is said to matter. Williams argues that this demands too much of moral agents—since (he claims) consequentialism demands that they be willing to sacrifice any and all personal projects and commitments in any given circumstance in order to pursue the most beneficent course of action possible. He argues further that consequentialism fails to make sense of intuitions that it can matter whether or not someone is personally the author of a particular consequence. For example, that participating in a crime can matter, even if the crime would have been committed anyway, or would even have been worse, without the agent's participation.
Some consequentialists—most notably Peter Railton—have attempted to develop a form of consequentialism that acknowledges and avoids the objections raised by Williams. Railton argues that Williams's criticisms can be avoided by adopting a form of consequentialism in which moral decisions are to be determined by the sort of life that they express. On his account, the agent should choose the sort of life that will, on the whole, produce the best overall effects.

</doc>
<doc id="5735" url="https://en.wikipedia.org/wiki?curid=5735" title="Conscription">
Conscription

Conscription, or drafting, is the compulsory enlistment of people in a national service, most often a military service. Conscription dates back to antiquity and continues in some countries to the present day under various names. The modern system of near-universal national conscription for young men dates to the French Revolution in the 1790s, where it became the basis of a very large and powerful military. Most European nations later copied the system in peacetime, so that men at a certain age would serve 1–8 years on active duty and then transfer to the reserve force.
In China, the State of Qin instituted universal military service following the registration of every household. This allowed huge armies to be raised, and was instrumental in the creation of the Qin Empire that conquered a large area of what is now China in 221 BC.
Conscription is controversial for a range of reasons, including conscientious objection to military engagements on religious or philosophical grounds; political objection, for example to service for a disliked government or unpopular war; and ideological objection, for example, to a perceived violation of individual rights. Those conscripted may evade service, sometimes by leaving the country. Some selection systems accommodate these attitudes by providing alternative service outside combat-operations roles or even outside the military, such as Zivildienst (civil service) in Austria and Switzerland. Most post-Soviet countries conscript soldiers not only for Armed Forces but also for paramilitary organizations which are dedicated to police-like "domestic only" service (Internal Troops) or "non-combat" rescue duties (Civil Defence Troops) – none of which is considered alternative to the military conscription.
As of the early 21st century, many states no longer conscript soldiers, relying instead upon professional militaries with volunteers enlisted to meet the demand for troops. The ability to rely on such an arrangement, however, presupposes some degree of predictability with regard to both war-fighting requirements and the scope of hostilities. Many states that have abolished conscription therefore still reserve the power to resume it during wartime or times of crisis.
History.
Conscription in pre-modern times.
Ilkum.
Around the reign of Hammurabi (1791–1750 BC), the Babylonian Empire used a system of conscription called "Ilkum". Under that system those eligible were required to serve in the royal army in time of war. During times of peace they were instead required to provide labour for other activities of the state. In return for this service, people subject to it gained the right to hold land. It is possible that this right was not to hold land "per se" but specific land supplied by the state.
Various forms of avoiding military service are recorded. While it was outlawed by the Code of Hammurabi, the hiring of substitutes appears to have been practiced both before and after the creation of the code. Later records show that Ilkum commitments could become regularly traded. In other places, people simply left their towns to avoid their Ilkum service. Another option was to sell Ilkum lands and the commitments along with them. With the exception of a few exempted classes, this was forbidden by the Code of Hammurabi.
Medieval levies.
Under the feudal conditions for holding land in the medieval period, most peasants and freemen were liable to provide one man of suitable age per family for military duty when required by either the king or the local lord. The levies raised in this way fought as infantry under local superiors. Although the exact laws varied greatly depending on the country and the period, generally these levies were only obliged to fight for one to three months. Most were subsistence farmers, and it was in everyone's interest to send the men home for harvest-time.
In medieval Scandinavia the "leiðangr" (Old Norse), "leidang" (Norwegian), "leding", (Danish), "ledung" (Swedish), "lichting" (Dutch), "expeditio" (Latin) or sometimes "leþing" (Old English), was a levy of free farmers conscripted into coastal fleets for seasonal excursions and in defence of the realm.
The bulk of the Anglo-Saxon English army, called the "fyrd", was composed of part-time English soldiers drawn from the landowning minor nobility. These thegns were the land-holding aristocracy of the time and were required to serve with their own armour and weapons for a certain number of days each year. The historian David Sturdy has cautioned about regarding the "fyrd" as a precursor to a modern national army composed of all ranks of society, describing it as a "ridiculous fantasy":The persistent old belief that peasants and small farmers gathered to form a national army or "fyrd" is a strange delusion dreamt up by antiquarians in the late eighteenth or early nineteenth centuries to justify universal military conscription.
Medieval levy in Poland was known as the "pospolite ruszenie".
Military slavery.
The system of military slaves was widely used in the Middle East, beginning with the creation of the corps of Turkish slave-soldiers ("ghulams" or "mamluks") by the Abbasid caliph al-Mu'tasim in the 820s and 830s. The Turkish troops soon came to dominate the government, establishing a pattern throughout the Islamic world of a ruling military class, often separated by ethnicity, culture and even religion by the mass of the population, a paradigm that found its apogee in the Mamluks of Egypt and the Janissary corps of the Ottoman Empire, institutions that survived until the early 19th century.
In the middle of the 14th century, Ottoman Sultan Murad I developed personal troops to be loyal to him, with a slave army called the "Kapıkulu". The new force was built by taking Christian children from newly conquered lands, especially from the far areas of his empire, in a system known as the "devşirme" (translated "gathering" or "converting"). The captive children were forced to convert to Islam. The Sultans had the young boys trained over several years. Those who showed special promise in fighting skills were trained in advanced warrior skills, put into the sultan's personal service, and turned into the Janissaries, the elite branch of the "Kapıkulu". A number of distinguished military commanders of the Ottomans, and most of the imperial administrators and upper-level officials of the Empire, such as Pargalı İbrahim Pasha and Sokollu Mehmet Paşa, were recruited in this way. By 1609, the Sultan's "Kapıkulu" forces increased to about 100,000.
In later years, Sultans turned to the Barbary Pirates to supply their Jannissaries corps. Their attacks on ships off the coast of Africa or in the Mediterranean, and subsequent capture of able-bodied men for ransom or sale provided some captives for the Sultan's system. Starting in the 17th century, Christian families living under the Ottoman rule began to submit their sons into the Kapikulu system willingly, as they saw this as a potentially invaluable career opportunity for their children. Eventually the Sultan turned to foreign volunteers from the warrior clans of Circassians in southern Russia to fill his Janissary armies. As a whole the system began to break down, the loyalty of the Jannissaries became increasingly suspect. Mahmud II forcibly disbanded the Janissary corps in 1826.
Similar to the Janissaries in origin and means of development were the Mamluks of Egypt in the Middle Ages. The Mamluks were usually captive non-Muslim Iranian and Turkish children who had been kidnapped or bought as slaves from the Barbary coasts. The Egyptians assimilated and trained the boys and young men to become Islamic soldiers who served the Muslim caliphs and the Ayyubid sultans during the Middle Ages. The first mamluks served the Abbasid caliphs in 9th century Baghdad. Over time they became a powerful military caste. On more than one occasion, they seized power, for example, ruling Egypt from 1250–1517.
From 1250 Egypt had been ruled by the Bahri dynasty of Kipchak origin. Slaves from the Caucasus served in the army and formed an elite corp of troops. They eventually revolted in Egypt to form the Burgi dynasty. The Mamluks' excellent fighting abilities, massed Islamic armies, and overwhelming numbers succeeded in overcoming the Christian Crusader fortresses in the Holy Land. The Mamluks were the most successful defense against the Mongol Ilkhanate of Persia and Iraq from entering Egypt.
On the western coast of Africa, Berber Muslims captured non-Muslims to put to work as laborers. They generally converted the younger people to Islam and many became quite assimilated. In Morocco, the Berber looked south rather than north. The Moroccan Sultan Moulay Ismail, called "the Bloodthirsty" (1672–1727), employed a corps of 150,000 black slaves, called his Black Guard. He used them to coerce the country into submission.
Conscription in modern times.
Modern conscription, the massed military enlistment of national citizens, was devised during the French Revolution, to enable the Republic to defend itself from the attacks of European monarchies. Deputy Jean-Baptiste Jourdan gave its name to the 5 September 1798 Act, whose first article stated: "Any Frenchman is a soldier and owes himself to the defense of the nation." It enabled the creation of the "Grande Armée", what Napoleon Bonaparte called "the nation in arms," which overwhelmed European professional armies that often numbered only into the low tens of thousands. More than 2.6 million men were inducted into the French military in this way between the years 1800 and 1813.
The defeat of the Prussian Army in particular shocked the Prussian establishment, which had believed it was invincible after the victories of Frederick the Great. The Prussians were used to relying on superior organization and tactical factors such as order of battle to focus superior troops against inferior ones. Given approximately equivalent forces, as was generally the case with professional armies, these factors showed considerable importance. However, they became considerably less important when the Prussian armies faced forces that outnumbered their own in some cases by more than ten to one. Scharnhorst advocated adopting the "levée en masse", the military conscription used by France. The "Krümpersystem" was the beginning of short-term compulsory service in Prussia, as opposed to the long-term conscription previously used.
In the Russian Empire, the military service time "owed" by serfs was 25 years at the beginning of the 19th century. In 1834 it was decreased to 20 years. The recruits were to be not younger than 17 and not older than 35. In 1874 Russia introduced universal conscription in the modern pattern, an innovation only made possible by the abolition of serfdom in 1861. New military law decreed that all male Russian subjects, when they reached the age of 20, were eligible to serve in the military for six years.
In the decades prior to World War I universal conscription along broadly Prussian lines became the norm for European armies, and those modeled on them. By 1914 the only substantial armies still completely dependent on voluntary enlistment were those of Britain and the United States. Some colonial powers such as France reserved their conscript armies for home service while maintaining professional units for overseas duties.
World Wars.
The range of eligible ages for conscripting was expanded to meet national demand during the World Wars.
In the United States, the Selective Service System drafted men for World War I initially in an age range from 21 to 30 but expanded its eligibility in 1918 to an age range of 18 to 45. In the case of a widespread mobilization of forces where service includes homefront defense, ages of conscripts may range much higher, with the oldest conscripts serving in roles requiring lesser mobility. Expanded-age conscription was common during the Second World War: in Britain, it was commonly known as "call-up" and extended to age 51. Nazi Germany termed it Volkssturm ("People's Storm") and included men as young as 16 and as old as 60. During the Second World War, both Britain and the Soviet Union conscripted women. The United States was on the verge of drafting women into the Nurse Corps because it anticipated it would need the extra personnel for its planned invasion of Japan. However, the Japanese surrendered and the idea was abandoned.
Arguments against conscription.
Gender-based.
Conscription has been criticized as sexist. Historically, only men have been subjected to conscription, and only in the late 20th century has this begun to change, though most countries still require only men to serve in the military. The integration of women into militaries, and especially into combat forces, did not begin on a large scale until late in the 20th century. Men who opt out of military service must often perform alternative service, such as Zivildienst in Austria and Switzerland, whereas women do not have even these obligations.
Involuntary servitude.
American libertarians oppose conscription and call for the abolition of the Selective Service System, believing that impressment of individuals into the armed forces is "involuntary servitude." Ron Paul, a former presidential nominee of the U.S. Libertarian Party has said that conscription "is wrongly associated with patriotism, when it really represents slavery and involuntary servitude." The philosopher Ayn Rand opposed conscription, suggesting that "of all the statist violations of individual rights in a mixed economy, the military draft is the worst. It is an abrogation of rights. It negates man's fundamental right—the right to life—and establishes the fundamental principle of statism: that a man's life belongs to the state, and the state may claim it by compelling him to sacrifice it in battle."
In 1917, a number of radicals and anarchists, including Emma Goldman, challenged the new draft law in federal court arguing that it was a direct violation of the Thirteenth Amendment's prohibition against slavery and involuntary servitude. However, the Supreme Court unanimously upheld the constitutionality of the draft act in the case of "Arver v. United States" on 7 January 1918. The decision said the Constitution gave Congress the power to declare war and to raise and support armies. The Court emphasized the principle of the reciprocal rights and duties of citizens:
Economic.
It can be argued that in a cost-to-benefit ratio, conscription during peace time is not worthwhile. Months or years of service amongst the most fit and capable subtracts from the productivity of the economy; add to this the cost of training them, and in some countries paying them. Compared to these extensive costs, some would argue there is very little benefit; if there ever was a war then conscription and basic training could be completed quickly, and in any case there is little threat of a war in most countries with conscription. In the United States, every male resident theoretically must register with the Selective Service System within 30 days following his 18th birthday and is available for a draft.
The cost of conscription can be related to the parable of the broken window. The cost of the work, military service, does not disappear even if no salary is paid. The work effort of the conscripts is effectively wasted, as an unwilling workforce is extremely inefficient. The impact is especially severe in wartime, when civilian professionals are forced to fight as amateur soldiers. Not only is the work effort of the conscripts wasted and productivity lost, but professionally skilled conscripts are also difficult to replace in the civilian workforce. Every soldier conscripted in the army is taken away from his civilian work, and away from contributing to the economy which funds the military. This is not a problem in an agrarian or pre-industrialized state where the level of education is universally low, and where a worker is easily replaced by another. However, this proves extremely problematic in a post-industrial society where educational levels are high and where the workforce is highly sophisticated and a replacement for a conscripted specialist is difficult to find. Even direr economic consequences result if the professional conscripted as an amateur soldier is killed or maimed for life; his work effort and productivity is irrevocably lost.
Arguments in favor of conscription.
Political and moral motives.
Jean Jacques Rousseau argued vehemently against professional armies, feeling it was the right and privilege of every citizen to participate to the defense of the whole society and a mark of moral decline to leave this business to professionals. He based this view on the development of the Roman republic, which came to an end at the same time as the Roman army changed from a conscript to professional force. Similarly, Aristotle linked the division of armed service among the populace intimately with the political order of the state. Niccolò Machiavelli argued strongly for conscription, seeing the professional armies as the cause of the failure of societal unity in Italy.
Other proponents, such as William James, consider both mandatory military and national service as ways of instilling maturity in young adults. Some proponents, such as Jonathan Alter and Mickey Kaus, support a draft in order to reinforce social equality, create social consciousness, break down class divisions and for young adults to immerse themselves in public enterprise.
Economic and resource efficiency.
It is estimated by the British military that in a professional military, a company deployed for active duty in peacekeeping corresponds to three inactive companies at home. Salaries for each are paid from the military budget. In contrast, volunteers from a trained reserve are in their civilian jobs when they are not deployed.
Drafting of women.
Traditionally conscription has been limited to the male population of a given body. Women and handicapped males have been exempt from conscription. Many societies have traditionally considered military service as a test of manhood and a rite of passage from boyhood into manhood.
, countries that were drafting women into military service included
Bolivia,
Chad, Eritrea,
Israel,
Mozambique 
and North Korea. Israel has universal female conscription, although in practice women can avoid service by claiming a religious exemption and over a third of Israeli women do so.
Sudanese law allows for conscription of women, but this is not implemented in practice.
In the United Kingdom during World War II, beginning in 1941, women were brought into the scope of conscription but, as all women with dependent children were exempt and many women were informally left in occupations such as nursing or teaching, the number conscripted was relatively few.
Sweden has also considered female conscription because excluding women was thought to go against the ideology of equality.
In June 2013, the parliament of Norway made a principal resolution to introduce female conscription, being the first country in NATO and Europe to do so. In October 2014 laws were passed by parliament, declaring female conscription, effective from January 2015.
In the USSR, there was no systematic conscription of women for the armed forces, but the severe disruption of normal life and the high proportion of civilians affected by World War II after the German invasion attracted many volunteers for what was termed "The Great Patriotic War". Medical doctors of both sexes could and would be conscripted (as officers). Also, the free Soviet university education system required Department of Chemistry students of both sexes to complete an ROTC course in NBC defense, and such female reservist officers could be conscripted in times of war. The United States came close to drafting women into the Nurse Corps in preparation for a planned invasion of Japan.
In 1981 in the United States, several men filed lawsuit in the case "Rostker v. Goldberg", alleging that the Selective Service Act of 1948 violates the Due Process Clause of the Fifth Amendment by requiring that only men register with the Selective Service System (SSS). The Supreme Court eventually upheld the Act, stating that "the argument for registering women was based on considerations of equity, but Congress was entitled, in the exercise of its constitutional powers, to focus on the question of military need, rather than 'equity.'"
On October 1, 1999 in the Taiwan Area, the Judicial Yuan of the Republic of China in its Interpretation 490 considered that the physical differences between males and females and the derived role differentiation in their respective social functions and lives would not make drafting only males a violation of the Constitution of the Republic of China. Though women are conscripted in Taiwan, transsexual persons are exempt.
Conscientious objection.
A conscientious objector is an individual whose personal beliefs are incompatible with military service, or, more often, with any role in the armed forces." In some countries, conscientious objectors have special legal status, which augments their conscription duties. For example, Sweden used to allow conscientious objectors to choose a service in the "weapons-free" branch, such as an airport fireman, nurse or telecommunications technician.
Most refuse such service, as they feel that such roles are a part of the military complex. The reasons for refusing to serve are varied. Some conscientious objectors are so for religious reasons — notably, the members of the historic peace churches, pacifist by doctrine; Jehovah's Witnesses, while not strictly pacifists, refuse to participate in the armed forces on the ground that they believe Christians should be neutral in worldly conflicts.
Conscription by country.
China.
Universal conscription in China dates back to the State of Qin, which eventually became the Qin Empire of 221 BC. Following unification, historical records show that a total of 300,000 conscript soldiers and 500,000 conscript labourers constructed the Great Wall of China.
In the following dynasties, universal conscription was abolished and reintroduced on numerous occasions.
, universal military conscription is theoretically mandatory in the People's Republic of China, and reinforced by law. However, due to the large population of China and large pool of candidates available for recruitment, the People's Liberation Army has always had sufficient volunteers, so conscription has not been required in practice at all.
Europe.
Denmark.
Conscription is known in Denmark since the Viking Age, where one physical man of every 10th court had to serve the king. Frederick IV of Denmark changed the law in 1710 to every 4th court. The men were chosen by the landowner and it was seen as a penalty.
Since 12 February 1849, every physical fit man must absolve conscription. According to §81 in the Constitution of Denmark, which was founded in 1849:Every male person able to carry arms shall be liable with his person to contribute to the defence of his country under such rules as are laid down by Statute. — Constitution of DenmarkThe legislation about the compulsory military service is articulated in the Danish Law of Conscription. The national service takes 4–12 month. It is possible to postpone the duty when one is under education. Every male turning 18 will be draft to the 'Day of Defence', where they will be introduced to Danish military and their health will be tested. Physically unfit persons are not required to fullfill the conscription. It's only a duty for men, while women are free to choose to join the Danish army.
After some sort of lottery, one can become a conscientious objector.
Netherlands.
Conscription, which was called "Service Duty" () in the Netherlands, was first employed in 1810 by French occupying forces. Napoleon's brother Louis Bonaparte, who was King of Holland from 1806 to 1810, had tried to introduce conscription a few years earlier, unsuccessfully. Every man aged 20 years or older had to enlist. By means of drawing lots it was decided who had to undertake service in the French army. It was possible to arrange a substitute against payment.
Later on, conscription was used for all men over the age of 18. Postponement was possible, due to study, for example. Conscientious objectors could perform an alternative civilian service instead of military service. For various reasons, this forced military service was criticized at the end of the twentieth century. Since the Cold War was over, so was the direct threat of a war. Instead, the Dutch army was employed in more and more peacekeeping operations. The complexity and danger of these missions made the use of conscripts controversial. Furthermore, the conscription system was thought to be unfair as only men were drafted.
In the European part of Netherlands, compulsory attendance has been officially suspended since 1 May 1997. Between 1991 and 1996, the Dutch armed forces phased out their conscript personnel and converted to an all-volunteer force. The last conscript troops were inducted in 1995, and demobilized in 1996. The suspension means that citizens are no longer forced to serve in the armed forces, as long as it is not required for the safety of the country. Since then, the Dutch army is an all-volunteer force. However, to this day, every male citizen aged 17 gets a letter in which he is told that he has been registered but does not have to present himself for service. The Dutch army allowed its male soldiers to have long hair from the early 1970s to the end of conscription in the mid-1990s.
Even though it is generally thought that conscription has been abolished in the Netherlands, it is compulsory attendance that was abolished, not conscription. The laws and systems which provide for the conscription of armed forces personnel still remain in place.
Norway.
As of now, Norway employs a weak form of mandatory military service for men and women. In practice recruits are not forced to serve, instead only those who are motivated are selected. About 60,000 Norwegians are available for conscription every year, but only 8,000 to 10,000 are conscripted. On 14 June 2013 the Norwegian Parliament voted to extend conscription to women making Norway the first NATO member and first European country to make national service compulsory for both men and women. There is a right of conscientious objection. In earlier times, up to at least the early 2000s, all men aged 19–44 was subject to a mandatory service and good reasons were required to avoid getting drafted. From 1985 women could enlist to a voluntary service as a regular recruit.
United Kingdom.
The United Kingdom introduced conscription to full-time military service for the first time in January 1916 (the eighteenth month of World War I) and abolished it in 1920. Ireland, then part of the United Kingdom, was excepted from the original 1916 military service legislation, and although further legislation in 1918 gave power for an extension of conscription to Ireland, the power was never put into effect.
Conscription was reintroduced in 1939, in the lead up to World War II, and continued in force until 1963. Northern Ireland was excepted from conscription legislation throughout the whole period.
In all, 8,000,000 men were conscripted in the Second World War, as well as several hundred thousand younger single women. The introduction of conscription in May 1939, before the war began, was partly due to pressure from the French, who emphasized the need for a large British army to oppose the Germans. From early 1942 unmarried women age 19–30 were conscripted. Most were sent to the factories, but they could volunteer for the Auxiliary Territorial Service (ATS) and other women's services. None was assigned to combat roles unless she volunteered. By 1943 women were liable to some form of directed labour up to age 51. During the Second World War, 1.4 million British men volunteered for service and 3.2 million were conscripted. Conscripts comprised 50% of the Royal Air Force, 60% of the Royal Navy and 80% of the British Army.
The abolition of conscription in Britain was announced on 4 April 1957, by new prime minister Harold Macmillan, with the last conscripts being recruited three years later.
Israel.
In Israel, the Muslim and Christian Arab minority are exempt from mandatory service, as are permanent residents such as the Druze of the Golan Heights. Male Ultra-Orthodox Jews may apply for a deferment of draft to study in Yeshiva, and the deferment tends to become an exemption, while female religious Jews can be exempted after presenting "religious declaration" to the IDF authorities, and some (primarily National Religious or Modern Orthodox) choose to volunteer for national service instead. Male Druze and Circassian Israeli citizens are liable, by agreement with their community leaders (Female Druze and Circassian are exempt from service). Members of the exempted groups can still volunteer, including Bedouin males, who serve in relatively large numbers.
United States.
In the United States, conscription, also called "the draft", ended in 1973, but males between 18 and 25 are required to register with the Selective Service System to enable a reintroduction of conscription if necessary. President Gerald Ford suspended mandatory draft registration in 1975, but President Jimmy Carter reinstated that requirement when the Soviet Union intervened in Afghanistan. Selective Service registration is still required of all young men, although the draft has not been used since 1973.
Colonial and Early National.
In America before 1862, combat duty was always voluntary, but white men aged 18 to 45 were usually required to join local militia units. Colonial militia laws—and after 1776 those of the states—required able-bodied white men to enroll in the militia and to undergo a minimum of military training, all without pay. Colonial Pennsylvania (controlled by Quakers) did not have such laws. Members of pacifist religious denominations were exempt. When combat troops were needed, some of the militiamen volunteered for short terms of service, for which they were paid. Following this system in its essentials, the Continental Congress in 1778 recommended that the states draft men from their militias for one year's service in the Continental army; this first national conscription was irregularly applied and failed to fill the Continental ranks.
In 1814, President James Madison proposed conscription of 40,000 men for the army, but the War of 1812 ended before Congress took any action. An 1840 proposal for a standing army of 200,000 men included conscription, but it never passed and military service was voluntary before 1862.
American Civil War.
Although both North and South resorted to conscription during the Civil War, in neither region did the system work effectively. The Confederate Congress on April 16, 1862, passed an act requiring military service for three years from all males aged eighteen to thirty-five not legally exempt, and it later extended the obligation so that all soldiers were required to serve for the duration of the conflict. The U.S. Congress followed on July 17, 1862, with an act authorizing a militia draft within a state when it could not meet its quota with volunteers. However, this failed to produce adequate enlistees, and with few men still volunteering by late 1862, it became necessary for the first time to impose national conscription. This met with considerable outcry among states rights advocates who wrote numerous letters to President Lincoln pleading against the unconstitutionality of such an action. In the end however, their complaints were ignored as Congress approved the first national conscription act on March 1, 1863, which made all white males between 20 and 44 liable for military service.
Quotas were assigned in each state, the deficiencies in volunteers to be met by conscription. But men drafted could provide substitutes or, until mid-1864, avoid service by paying commutation money. Many eligibles pooled their money to cover the cost of anyone drafted. Families used the substitute provision to select which man should go into the army and which should stay home. There was much evasion and overt resistance to the draft, especially in Catholic areas. The great draft riot in New York City in July 1863 involved Irish immigrants who had been signed up as citizens to swell the machine vote, not realizing it made them liable for the draft. Of the 168,649 men procured for the Union through the draft, 117,986 were substitutes, leaving only 50,663 who had their personal services conscripted.
In the end, conscription was largely a failure. The draft failed to bring in high-quality soldiers to the Union armies and instead most draftees were lazy, unmotivated men, men with physical or mental disabilities, and even criminals. They frequently met with contempt from the volunteer soldiers and required extra amounts of discipline and surveillance to prevent them from committing desertion and petty crimes.
The problem of Confederate desertion was aggravated by the inequitable inclinations of conscription officers and local judges. The three conscription acts of the Confederacy exempted certain categories, most notably the planter class, and enrolling officers and local judges often practiced favoritism, sometimes accepting bribes. Attempts to effectively deal with the issue were frustrated by conflict between state and local governments on the one hand and the national government of the Confederacy.
World War I.
In 1917, the administration of Woodrow Wilson decided to rely primarily on conscription, rather than voluntary enlistment, to raise military manpower for World War I. The Selective Service Act of 1917 was carefully drawn to remedy the defects in the Civil War system and—by allowing exemptions for dependency, essential occupations, and religious scruples—to place each man in his proper niche in a national war effort. The act established a "liability for military service of all male citizens"; authorized a selective draft of all those between twenty-one and thirty-one years of age (later from eighteen to forty-five); and prohibited all forms of bounties, substitutions, or purchase of exemptions. Administration was entrusted to local boards composed of leading civilians in each community. These boards issued draft calls in order of numbers drawn in a national lottery and determined exemptions. In 1917 and 1918 some 24 million men were registered and nearly 3 million inducted into the military services, with little of the resistance that characterized the Civil War.
World War II.
In 1940 Congress passed the first peacetime draft legislation, which was led by Grenville Clark. It was renewed (by one vote) in summer 1941. It involved questions as to who should control the draft, the size of the army, and the need for deferments. The system worked through local draft boards comprising community leaders who were given quotas and then decided how to fill them. There was very little draft resistance.
The nation went from a surplus manpower pool with high unemployment and relief in 1940 to a severe manpower shortage by 1943. Industry realized that the Army urgently desired production of essential war materials and foodstuffs more than soldiers. (Large numbers of soldiers were not used until the invasion of Europe in summer 1944.) In 1940 to 1943, the Army often transferred soldiers to civilian status in the Enlisted Reserve Corps in order to increase production. Those transferred would return to work in essential industry, although they could be called back to active duty if the Army needed them. Others were discharged if their civilian work was deemed absolutely essential. There were instances of mass releases of men to increase production in various industries. Blacks and Asians were drafted under the same terms as whites. Over ten million men were drafted for combat in World War II, more than twice the amount drafted for World War One, the Korean War, and the Vietnam War combined.
One contentious issue involved the drafting of fathers, which was avoided as much as possible. Farmers demanded and were generally given occupational deferments (many volunteered anyway, and those who stayed at home were not eligible for postwar veteran's benefits). The draft law as established in 1940 exempted males under 21 from mandatory service due to public opposition against the idea of drafting 18-year-olds.
Later in the war, as the need for manpower grew more and more pressing, many earlier deferment categories became draft eligible.
Draft evasion.
The New York Draft Riots (July 11 to July 16, 1863; known at the time as "Draft Week"), were violent disturbances in New York City that were the culmination of discontent with new laws passed by Congress to draft men to fight in the ongoing American Civil War.
In the United States and some other countries, the Vietnam War saw new levels of opposition to conscription and the Selective Service System. Many people opposed to and facing conscription chose to either apply for classification and assignment to civilian alternative service or noncombatant service within the military as conscientious objectors, or to evade the draft by fleeing to a neutral country. A small proportion, like Muhammad Ali, chose to resist the draft by publicly and politically fighting conscription. Some people resisted at the point of registration for the draft. In the United States around 1970, for example, the draft resistance movement focused on mandatory draft registration. Others resisted at the point of induction, when they were ordered to put on a uniform, when they were ordered to carry or use a weapon, or when they were ordered into combat.
In the United States, especially during the Vietnam War, some used political connections to ensure that they were placed well away from any potential harm, serving in what was termed a Champagne unit. Many would avoid military service altogether through college deferments, by becoming fathers, or serving in various exempt jobs (teaching was one possibility). Others used educational exemptions, became conscientious objectors or pretended to be conscientious objectors, although they might then be drafted for non-combat work, such as serving as a combat medic. It was also possible they could be asked to do similar civilian work, such as being a hospital orderly.
It was, in fact, quite easy for those with some knowledge of the system to avoid being drafted. A simple route, widely publicized, was to get a medical rejection. While a person could claim to have symptoms (or feign homosexuality) if enough physicians sent letters that a person had a problem, he might well be rejected. It often wasn't worth the Army's time to dispute this claim. Such an approach worked best in a larger city where there was no stigma to not serving, and the potential draftee was not known to those reviewing him.
For others, the most common method of avoiding the draft was to cross the border into another country. People who have been "called up" for military service and who attempted to avoid it in some way were known as "draft-dodgers". Particularly during the Vietnam War, U.S. draft-dodgers usually made their way to Canada, Mexico, or Sweden.
Many people looked upon draft-dodgers with scorn as being "cowards", but some supported them in their efforts. In the late years of the Vietnam War, objections against it and support for draft-dodgers was much more outspoken, because of the casualties suffered by American troops, and the actual cause and purpose of the war being heavily questioned.
Toward the end of the U.S. draft, an attempt was made to make the system somewhat fairer by turning it into a lottery, with each of the year's calendar dates randomly assigned a number. Men born on lower-numbered dates were called up for review. For the reasons given above, this did not make the system any fairer, and the entire system ended in 1973. By 1975, the draft was no longer mandatory. Today, American men aged 18–25 are required to register with the Selective Service, but there has not been a call-up since the Vietnam War.

</doc>
<doc id="5736" url="https://en.wikipedia.org/wiki?curid=5736" title="Catherine Coleman">
Catherine Coleman

Catherine Grace "Cady" Coleman (born December 14, 1960) is an American chemist, a former United States Air Force officer, and a current NASA astronaut. She is a veteran of two Space Shuttle missions, and departed the International Space Station on May 23, 2011, as a crew member of Expedition 27 after logging 159 days in space.
Education.
Coleman graduated from Wilbert Tucker Woodson High School, Fairfax, Virginia, in 1978; in 1978–1979 she was an exchange student at Røyken upper secondary school in Norway with the AFS Intercultural Programs. She received a bachelor of science degree in chemistry from the Massachusetts Institute of Technology in 1983, and a doctorate in polymer science and engineering from the University of Massachusetts Amherst in 1991 as a member of the Air Force ROTC. She was a member of the intercollegiate crew and was a resident of Baker House.
Military career.
After completing her regular education, Coleman joined the U.S. Air Force as a Second Lieutenant while continuing her graduate work for a PhD at the University of Massachusetts Amherst. In 1988 she entered active duty at Wright-Patterson Air Force Base as a research chemist. During her work she participated as a surface analysis consultant on the NASA Long Duration Exposure Facility experiment. In 1991 she received her doctorate in polymer science and engineering. She retired from the Air Force in November 2009.
NASA career.
Coleman was selected by NASA in 1992 to join the NASA Astronaut Corps. In 1995 she was a member of the STS-73 crew on the scientific mission USML-1 with experiments including biotechnology, combustion science and the physics of fluids. During the flight, she reported to Houston Mission Control that she had spotted an unidentified flying object. She also trained for the mission STS-83 to be the backup for Donald A. Thomas; however, as he recovered on time, she did not fly that mission. STS-93 was Coleman's second space flight in 1999. She was mission specialist in charge of deploying the Chandra X-ray Observatory and its Inertial Upper Stage out of the shuttle's cargo bay.
Coleman served as Chief of Robotics for the Astronaut Office, to include robotic arm operations and training for all Space Shuttle and International Space Station missions. In October 2004, Coleman served as an aquanaut during the mission aboard the Aquarius underwater laboratory, living and working underwater for eleven days.
Coleman was assigned as a backup U.S. crew member for Expeditions 19, 20 and 21 and served as a backup crew member for Expeditions 24 and 25 as part of her training for Expedition 26.
Coleman launched on December 15, 2010 (December 16 Baikonur time), aboard Soyuz TMA-20 to join the Expedition 26 mission aboard the International Space Station.
Spaceflight experience.
STS-73 on Space Shuttle "Columbia" (October 20 to November 5, 1995) was the second United States Microgravity Laboratory mission. The mission focused on materials science, biotechnology, combustion science, the physics of fluids, and numerous scientific experiments housed in the pressurized Spacelab module. In completing her first space flight, Coleman orbited the Earth 256 times, traveled over 6 million miles, and logged a total of 15 days, 21 hours, 52 minutes and 21 seconds in space.
STS-93 on "Columbia" (July 22 to 27, 1999) was a five-day mission during which Coleman was the lead mission specialist for the deployment of the Chandra X-ray Observatory. Designed to conduct comprehensive studies of the universe, the telescope will enable scientists to study exotic phenomena such as exploding stars, quasars, and black holes. Mission duration was 118 hours and 50 minutes.
Soyuz TMA-20 / Expedition 26/27 (December 15, 2010, to May 23, 2011) was an extended duration mission to the International Space Station.
Personal.
Coleman is married to glass artist Josh Simpson who lives in Massachusetts. They have one son. She is part of the band Bandella, which also includes fellow NASA astronaut Steven Robinson, Canadian astronaut Chris Hadfield, and Micki Pettit (astronaut Don Pettit's wife). Coleman is a flute player and has taken several flutes with her to the ISS, including a pennywhistle from Paddy Moloney of the Chieftains, an old Irish flute from Matt Molloy of the Chieftains, and a flute from Ian Anderson of Jethro Tull. On February 15, 2011, she played one of the instruments live from orbit on National Public Radio. On April 12, 2011, she played live through video link for the audience of Jethro Tull's show in Russia in honour of the 50th anniversary of Yuri Gagarin's flight. She played the duet from orbit while Anderson played on the ground in Russia. On May 13 of that year, Coleman delivered a taped commencement address to the class of 2011 at the University of Massachusetts Amherst.
As do many other astronauts, Coleman holds an amateur radio license (callsign: KC5ZTH).
As of 2015, she is also a member of the board of directors for the Hollywood Sci-Fi Museum.
As of 2015 she is also known to be working as a guest speaker in Baylor College of Medicine as a speaker for the kids program 'Saturday Morning Science'.

</doc>
<doc id="5738" url="https://en.wikipedia.org/wiki?curid=5738" title="Cervix">
Cervix

The cervix or cervix uteri ("") is the lower part of the uterus in the human female reproductive system. In a non-pregnant woman, the cervix is usually between 2 and 3 cm long and roughly cylindrical in shape. The narrow, central cervical canal runs along its entire length, connecting the uterine cavity and the lumen of the vagina. The opening into the uterus is called the internal os and the opening into the vagina is called the external os. The lower part of the cervix, known as the vaginal portion of the cervix (or ectocervix), bulges into the top of the vagina. The cervix has been documented anatomically since at least the time of Hippocrates, over 2,000 years ago.
The cervical canal is a passage through which sperm must travel to fertilize an egg cell after sexual intercourse. Several methods of contraception, including cervical caps and cervical diaphragms aim to block or prevent the passage of sperm through the cervical canal. Cervical mucus is used in several methods of fertility awareness, such as the Creighton model and Billings method, due to its changes in consistency throughout the menstrual period. During vaginal childbirth, the cervix must flatten and dilate to allow the fetus to progress along the birth canal. Midwives and doctors use the extent of the dilation of the cervix to assist decision-making during childbirth.
The endocervical canal is lined with a single layer of column-shaped cells, while the ectocervix is covered with multiple layers of cells topped with flat cells. The two types of epithelia meet the squamocolumnar junction. Infection with the human papillomavirus (HPV) can cause changes in the epithelium, which can lead to cancer of the cervix. Cervical cytology tests can often detect cervical cancer and its precursors, and enable early successful treatment. Ways to avoid HPV include avoiding sex, using condoms, and HPV vaccination. HPV vaccines, developed in the early 21st century, reduce the risk of cervical cancer by preventing infections from the main cancer-causing strains of HPV.
Structure.
The cervix is part of the female reproductive system. Around in length, it is the lower narrower part of the uterus continuous above with the broader upper part—or body—of the uterus. The lower end of the cervix bulges through the anterior wall of the vagina, and is referred to as the vaginal portion of cervix (or ectocervix) while the rest of the cervix above the vagina is called the supravaginal portion of cervix. A central canal, known as the cervical canal, runs along its length and connects the cavity of the body of the uterus with the lumen of the vagina. The openings are known as the internal os and external orifice of the uterus (or external os) respectively. The mucosa lining the cervical canal is known as the endocervix and the mucosa covering the ectocervix is known as the exocervix. The cervix has an inner mucosal layer, a thick layer of smooth muscle, and posteriorly the supravaginal portion has a serosal covering consisting of connective tissue and overlying peritoneum.
In front of the upper part of the cervix lies the bladder, separated from it by cellular connective tissue known as parametrium, which also extends over the sides of the cervix. To the rear, the supravaginal cervix is covered by peritoneum, which runs onto the back of the vaginal wall and then turns upwards and onto the rectum forming the recto-uterine pouch. The cervix is more tightly connected to surrounding structures than the rest of the uterus.
The cervical canal varies greatly in length and width between women and over the course of a woman's life, and can measure 8 mm (0.3 in) at its widest diameter in premenopausal adults. It is wider in the middle and narrower at each end. The anterior and posterior walls of the canal each have a vertical fold, from which ridges run diagonally upwards and laterally. These are known as palmate folds due to their resemblance to a palm leaf. The anterior and posterior ridges are arranged in such a way that they interlock with each other and close the canal. They are often effaced after pregnancy.
The ectocervix (also known as the vaginal portion of the cervix) has a convex, elliptical shape and projects into the cervix between the anterior and posterior vaginal fornices. On the rounded part of the ectocervix is a small, depressed external opening, connecting the cervix with the vagina. The size and shape of the ectocervix and the external opening (external os) can vary according to age, hormonal state, and whether natural or normal childbirth has taken place. In women who have not had a vaginal delivery, the external opening is small and circular, and in women who have had a vaginal delivery, it is slit-like. On average, the ectocervix is long and wide.
Blood is supplied to the cervix by the descending branch of the uterine artery and drains into the uterine vein. The pelvic splanchnic nerves, emerging as S2–S3, transmit the sensation of pain from the cervix to the brain. These nerves travel along the uterosacral ligaments, which pass from the uterus to the anterior sacrum.
Three channels facilitate lymphatic drainage from the cervix. The anterior and lateral cervix drains to nodes along the uterine arteries, travelling along the cardinal ligaments at the base of the broad ligament to the external iliac lymph nodes and ultimately the paraaortic lymph nodes. The posterior and lateral cervix drains along the uterine arteries to the internal iliac lymph nodes and ultimately the paraaortic lymph nodes, and the posterior section of the cervix drains to the obturator and presacral lymph nodes. However, there are variations as lymphatic drainage from the cervix travels to different sets of pelvic nodes in some people. This has implications in scanning nodes for involvement in cervical cancer.
After menstruation and directly under the influence of estrogen, the cervix undergoes a series of changes in position and texture. During most of the menstrual cycle, the cervix remains firm, and is positioned low and closed. However, as ovulation approaches, the cervix becomes softer and rises to open in response to the higher levels of estrogen present. These changes are also accompanied by changes in cervical mucus, described below.
Development.
As a component of the female reproductive system, the cervix is derived from the two paramesonephric ducts (also called Müllerian ducts), which develop around the sixth week of embryogenesis. During development, the outer parts of the two ducts fuse, forming a single urogenital canal that will become the vagina, cervix and uterus. The cervix grows in size at a smaller rate than the body of the uterus, so the relative size of the cervix over time decreases, decreasing from being much larger than the body of the uterus in fetal life, twice as large during childhood, and decreasing to its adult size, smaller than the uterus, after puberty. Previously it was thought that during fetal development, the original squamous epithelium of the cervix is derived from the urogenital sinus and the original columnar epithelium is derived from the paramesonephric duct. The point at which these two original epithelia meet is called the original squamocolumnar junction. New studies show, however, that all the cervical as well as large part of the vaginal epithelium are derived from Müllerian duct tissue and that phenotypic differences might be due to other causes.
Histology.
The endocervical mucosa is about 3 mm thick, lined with a single layer of columnar mucous cells, and contains numerous tubular mucous glands which empty viscous alkaline mucus into the lumen. In contrast, the ectocervix is covered with nonkeratinized stratified squamous epithelium, which resembles the squamous epithelium lining the vaginal. The junction between these two types of epithelia is called the squamocolumnar junction. Underlying both types of epithelium is a tough layer of collagen. The mucosa of the endocervix is not shed during menstruation. The cervix has more fibrous tissue, including collagen and elastin, than the rest of the uterus.
In prepubertal girls, the functional squamocolumnar junction is present just within the endocervical canal. Upon entering puberty, due to hormonal influence, and during pregnancy, the columnar epithelium extends outwards over the ectocervix as the cervix everts. Hence, this also causes the squamocolumnar junction to move outwards onto the vaginal portion of the cervix, where it is exposed to the acidic vaginal environment. The exposed columnar epithelium can undergo physiological metaplasia and change to tougher metaplastic squamous epithelium in days or weeks, which when mature is very similar to the original squamous epithelium. The new squamocolumnar junction is therefore internal to the original squamocolumnar junction, and the zone of unstable epithelium between the two junctions is called the transformation zone of the cervix. After menopause, the uterine structures involute and the functional squamocolumnar junction moves into the endocervical canal.
Nabothian cysts (or Nabothian follicles) form in the transformation zone where the lining of metaplastic epithelium has replaced mucous epithelium and caused a strangulation of the outlet of some of the mucous glands. A build up of mucus in the glands forms Nabothian cysts, usually less than about 5 mm in diameter, which are considered physiological rather than pathological. Both gland openings and Nabothian cysts are helpful to identify the transformation zone.
Function.
Fertility.
The cervical canal is a pathway through which sperm enter the uterus after sexual intercourse, and some forms of artificial insemination. Some sperm remains in cervical crypts, infoldings of the endocervix, which act as a reservoir, releasing sperm over several hours and maximising the chances of fertilisation. A theory states the cervical and uterine contractions during orgasm draw semen into the uterus. Although the "upsuck theory" has been generally accepted for some years, it has been disputed due to lack of evidence, small sample size, and methodological errors.
Some methods of fertility awareness, such as the Creighton model and the Billings method involve estimating a woman's periods of fertility and infertility by observing physiological changes in her body. Among these changes are several involving the quality of her cervical mucus: the sensation it causes at the vulva, its elasticity ("Spinnbarkeit"), its transparency, and the presence of ferning.
Cervical mucus.
Several hundred glands in the endocervix produce 20–60 mg of cervical mucus a day, increasing to 600 mg around the time of ovulation. It is viscous as it contains large proteins known as mucins. The viscosity and water content varies during the menstrual cycle; mucus is composed of around 93% water, reaching 98% at midcycle. These changes allow it to function either as a barrier or a transport medium to spermatozoa. It contains electrolytes such as calcium, sodium, and potassium; organic components such as glucose, amino acids, and soluble proteins; trace elements including zinc, copper, iron, manganese, and selenium; free fatty acids; enzymes such as amylase; and prostaglandins. Its consistency is determined by the influence of the hormones estrogen and progesterone. At midcycle around the time of ovulation—a period of high estrogen levels— the mucus is thin and serous to allow sperm to enter the uterus, and is more alkaline and hence more hospitable to sperm. It is also higher in electrolytes, which results in the "ferning" pattern that can be observed in drying mucus under low magnification; as the mucus dries, the salts crystallize, resembling the leaves of a fern. The mucus has stretchy character described as "Spinnbarkeit" most prominent around the time of ovulation.
At other times in the cycle, the mucus is thick and more acidic due to the effects of progesterone. This "infertile" mucus acts as a barrier to sperm from entering the uterus. Women taking an oral contraceptive pill also have thick mucus from the effects of progesterone. Thick mucus also prevents pathogens from interfering with a nascent pregnancy.
A cervical mucus plug, called the operculum, forms inside the cervical canal during pregnancy. This provides a protective seal for the uterus against the entry of pathogens and against leakage of uterine fluids. The mucus plug is also known to have antibacterial properties. This plug is released as the cervix dilates, either during the first stage of childbirth or shortly before. It is visible as a blood-tinged mucous discharge.
Childbirth.
The cervix plays a major role in childbirth. As the fetus descends within the uterus in preparation for birth, the presenting part, usually the head, rests on and is supported by the cervix. As labour progresses, the cervix becomes softer and shorter, begins to dilate, and rotates to face anteriorly. The support the cervix provides to the fetal head starts to give way when the uterus begins its contractions. During childbirth, the cervix must dilate to a diameter of more than to accommodate the head of the fetus as it descends from the uterus to the vagina. In becoming wider, the cervix also becomes shorter, a phenomenon known as effacement.
Along with other factors, midwives and doctors use the extent of cervical dilation to assist decision making during childbirth. Generally, the active first stage of labour, when the uterine contractions become strong and regular, begins when the cervical dilation is more than . The second phase of labor begins when the cervix has dilated to , which is regarded as its fullest dilation, and is when active pushing and contractions push the baby along the birth canal leading to the birth of the baby. The number of past vaginal deliveries is a strong factor in influencing how rapidly the cervix is able to dilate in labour. The time taken for the cervix to dilate and efface is one factor used in reporting systems such as the Bishop score, used to recommend whether interventions such as a forceps delivery, induction, or Caesarean section should be used in childbirth.
Cervical incompetence is a condition in which shortening of the cervix due to dilation and thinning occurs, before term pregnancy. Short cervical length is the strongest predictor of preterm birth.
Contraception.
Several methods of contraception involve the cervix. Cervical diaphragms are reusable, firm-rimmed plastic devices inserted by a woman prior to intercourse that cover the cervix. Pressure against the walls of the vagina maintain the position of the diaphragm, and it acts as a physical barrier to prevent the entry of sperm into the uterus, preventing fertilisation. Cervical caps are a similar method, although they are smaller and adhere to the cervix by suction. Diaphragms and caps are often used in conjunction with spermicides. In one year, 12% of women using the diaphragm will undergo an unintended pregnancy, and with optimal use this falls to 6%. Efficacy rates are lower for the cap, with 18% of women undergoing an unintended pregnancy, and 10–13% with optimal use. Most types of progestogen-only pills are effective as a contraceptive because they thicken cervical mucus making it difficult for sperm to pass along the endocervical canal. In addition, they may also sometimes prevent ovulation. In contrast, contraceptive pills that contain both oestrogen and progesterone, the combined oral contraceptive pills, work mainly by preventing ovulation. They also thicken cervical mucus and thin the lining of the uterus enhancing their effectiveness.
Clinical significance.
Cancer.
In 2008, cervical cancer was the third-most common cancer in women worldwide, with rates varying geographically from less than one to more than 50 cases per 100,000 women. It is a leading cause of cancer-related death in poor countries, where delayed diagnosis leading to poor outcomes is common. The introduction of routine screening has resulted in fewer cases of (and deaths from) cervical cancer, however this has mainly taken place in developed countries. Most developing countries have limited or no screening, and 85% of the global burden occurring there.
Cervical cancer nearly always involves human papillomavirus (HPV) infection. HPV is a virus with numerous strains, several of which predispose to precancerous changes in the cervical epithelium, particularly in the transformation zone, which is the most common area for cervical cancer to start. HPV vaccines, such as Gardasil and Cervarix, reduce the incidence of cervical cancer, by inoculating against the viral strains involved in cancer development.
Potentially precancerous changes in the cervix can be detected by cervical screening, using methods including a Pap smear (also called a cervical smear), in which epithelial cells are scraped from the surface of the cervix and examined under a microscope. The colposcope, an instrument used to see a magnified view of the cervix, was invented in 1925. The Pap smear was developed by Georgios Papanikolaou in 1928. A LEEP procedure using a heated loop of platinum to excise a patch of cervical tissue was developed by Aurel Babes in 1927. In some parts of the developed world including the UK, the Pap test has been superseded with liquid-based cytology.
A cheap, cost-effective and practical alternative in poorer countries is visual inspection with acetic acid (VIA). Instituting and sustaining cytology-based programs in these regions can be difficult, due to the need for trained personnel, equipment and facilities and difficulties in follow-up. With VIA, results and treatment can be available on the same day. As a screening test, VIA is comparable to cervical cytology in accurately identifying precancerous lesions.
A result of dysplasia is usually further investigated, such as by taking a cone biopsy, which may also remove the cancerous lesion. Cervical intraepithelial neoplasia is a possible result of the biopsy, and represents dysplastic changes that may eventually progress to invasive cancer. Most cases of cervical cancer are detected in this way, without having caused any symptoms. When symptoms occur, they may include vaginal bleeding, discharge, or discomfort.
Inflammation.
Inflammation of the cervix is referred to as cervicitis. This inflammation may be of the endocervix or ectocervix. When associated with the endocervix, it is associated with a mucous vaginal discharge and the sexually transmitted infections such as chlamydia and gonorrhoea. As many as half of pregnant women having a gonorrheal infection of the cervix are asymptomatic. Other causes include overgrowth of the commensal flora of the vagina. When associated with the ectocervix, inflammation may be caused by the herpes simplex virus. Inflammation is often investigated through directly visualising the cervix using a speculum, which may appear whiteish due to exudate, and by taking a Pap smear and examining for causal bacteria. Special tests may be used to identify particular bacteria. If the inflammation is due to a bacterium, then antibiotics may be given as treatment.
Anatomical abnormalities.
Cervical stenosis refers to an abnormally narrow cervical canal, typically associated with trauma caused by removal of tissue for investigation or treatment of cancer, or cervical cancer itself. Diethylstilbestrol, used from 1938 to 1971 to prevent preterm labour and miscarriage, is also strongly associated with the development of cervical stenosis and other abnormalities in the daughters of the exposed women. Other abnormalities include vaginal adenosis, in which the squamous epithelium of the ectocervix becomes columnar, cancers such as clear cell adenocarcinomas, cervical ridges and hoods, and development of a "cockscomb" cervical appearance.
Cervical agenesis is a rare congenital condition in which the cervix completely fails to develop, often associated with the concurrent failure of the vagina to develop. Other congenital cervical abnormalities exist, often associated with abnormalities of the vagina and uterus. The cervix may be duplicated in situations such as bicornuate uterus and uterine didelphys.
Other.
Cervical polyps, which are benign overgrowths of endocervical tissue, if present, may cause bleeding, or a benign overgrowth may be present in the endocervical canal. Cervical ectropion refers to the horizontal overgrowth of the endocervical columnar lining in a one-cell-thick layer over the ectocervix.
History.
The name of the cervix comes from (neck) from the Proto-Indo-European root "ker-", referring to a "structure that projects". Thus, the word cervix is linguistically related to the English word "horn", the Persian word for "head" (sar), the Greek word for "head" (), and the Welsh word for "deer" ().
The cervix was documented in anatomical literature in at least the time of Hippocrates, although there was some variation in early writers, who used the term to refer to both the cervix and the internal uterine orifice. The first attested use of the word to refer to the cervix of the uterus was in 1702.
Cervical cancer has been described for over 2,000 years, with descriptions provided by both Hippocrates and Aretaeus, although the causal role played by HPV for cervical cancer was only elucidated in the late 20th century by Harald zur Hausen, who published a hypothesis in 1976, and whose hypothesis was confirmed in 1983 and 1984. Based on work done by Jian Zhou and Ian Fraser, a vaccine for four strains of HPV was released in 2006.
Other mammals.
Female marsupials have paired uteri and cervices. Most eutherian (placental) mammal species have a single cervix and single, bipartite or bicornuate uterus. Lagomorphs, rodents, aardvarks and hyraxes have a duplex uterus and two cervices. Lagomorphs and rodents share many morphological characteristics and are grouped together in the clade Glires. Anteaters of the family myrmecophagidae are unusual in that they lack a defined cervix; they are thought to have lost the characteristic rather than other mammals developing a cervix on more than one lineage.

</doc>
<doc id="5739" url="https://en.wikipedia.org/wiki?curid=5739" title="Compiler">
Compiler

A compiler is a computer program (or a set of programs) that transforms source code written in a programming language (the source language) into another computer language (the target language), with the latter often having a binary form known as object code. The most common reason for converting source code is to create an executable program.
The name "compiler" is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g., assembly language or machine code). If the compiled program can run on a computer whose CPU or operating system is different from the one on which the compiler runs, the compiler is known as a cross-compiler. More generally, compilers are a specific type of translator.
While all programs that take a set of programming specifications and translate them, i.e. create a means to execute those specifications, are technically "compilers", the term generally means a program that produces a separate executable from the compiler (that may require a run time library or subsystem to operate), a compiler that merely executes the original specifications is usually referred to as an "interpreter", although because of differing methods of analyzing what represents compilation and what represents interpretation, there is some overlap between the two terms.
A program that translates from a low level language to a higher level one is a decompiler. A program that translates between high-level languages is usually called a source-to-source compiler or transpiler. A language rewriter is usually a program that translates the form of expressions without a change of language. The term compiler-compiler is sometimes used to refer to a parser generator, a tool often used to help create the lexer and parser.
A compiler is likely to perform many or all of the following operations: lexical analysis, preprocessing, parsing, semantic analysis (syntax-directed translation), code generation, and code optimization. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementors invest significant effort to ensure compiler correctness.
History.
Software for early computers was primarily written in assembly language. Although the first high level language is nearly as old as the first computer, the limited memory capacity of early computers led to substantial technical challenges when the first compilers were designed.
The first high-level programming language (Plankalkül) was proposed by Konrad Zuse in 1943. The first compiler was written by Grace Hopper, in 1952, for the A-0 programming language; the A-0 functioned more as a loader or linker than the modern notion of a compiler. The first autocode and its compiler were developed by Alick Glennie in 1952 for the Mark 1 computer at the University of Manchester and is considered by some to be the first compiled programming language. The FORTRAN team led by John Backus at IBM is generally credited as having introduced the first complete compiler in 1957. COBOL was an early language to be compiled on multiple architectures, in 1960.
In many application domains the idea of using a higher level language quickly caught on. Because of the expanding functionality supported by newer programming languages and the increasing complexity of computer architectures, compilers have become more complex.
Early compilers were written in assembly language. The first "self-hosting" compiler – capable of compiling its own source code in a high-level language – was created in 1962 for Lisp by Tim Hart and Mike Levin at MIT. Since the 1970s it has become common practice to implement a compiler in the language it compiles, although both Pascal and C have been popular choices for implementation language. Building a self-hosting compiler is a bootstrapping problem—the first such compiler for a language must be compiled either by hand or by a compiler written in a different language, or (as in Hart and Levin's Lisp compiler) compiled by running the compiler in an interpreter.
Compilers in education.
Compiler construction and compiler optimization are taught at universities and schools as part of a computer science curriculum. Such courses are usually supplemented with the implementation of a compiler for an educational programming language. A well-documented example is Niklaus Wirth's PL/0 compiler, which Wirth used to teach compiler construction in the 1970s. In spite of its simplicity, the PL/0 compiler introduced several influential concepts to the field:
Compilation.
Compilers enabled the development of programs that are machine-independent. Before the development of FORTRAN, the first higher-level language, in the 1950s, machine-dependent assembly language was widely used. While assembly language produces more abstraction than machine code on the same architecture, just as with machine code, it has to be modified or rewritten if the program is to be executed on different computer hardware architecture.
With the advent of high-level programming languages that followed FORTRAN, such as COBOL, C, and BASIC, programmers could write machine-independent source programs. A compiler translates the high-level source programs into target programs in machine languages for the specific hardware. Once the target program is generated, the user can execute the program.
Structure of a compiler.
Compilers bridge source programs in high-level languages with the underlying hardware. A compiler verifies code syntax, generates efficient object code, performs run-time organization, and formats the output according to assembler and linker conventions. A compiler consists of:
Compiler output.
One classification of compilers is by the platform on which their generated code executes. This is known as the "target platform."
A "native" or "hosted" compiler is one which output is intended to directly run on the same type of computer and operating system that the compiler itself runs on. The output of a cross compiler is designed to run on a different platform. Cross compilers are often used when developing software for embedded systems that are not intended to support a software development environment.
The output of a compiler that produces code for a virtual machine (VM) may or may not be executed on the same platform as the compiler that produced it. For this reason such compilers are not usually classified as native or cross compilers.
The lower level language that is the target of a compiler may itself be a high-level programming language. C, often viewed as some sort of portable assembler, can also be the target language of a compiler. E.g.: Cfront, the original compiler for C++ used C as target language. The C created by such a compiler is usually not intended to be read and maintained by humans. So indent style and pretty C intermediate code are irrelevant. Some features of C turn it into a good target language. E.g.: C code with codice_1 directives can be generated to support debugging of the original source.
Compiled versus interpreted languages.
Higher-level programming languages usually appear with a type of translation in mind: either designed as compiled language or interpreted language. However, in practice there is rarely anything about a language that "requires" it to be exclusively compiled or exclusively interpreted, although it is possible to design languages that rely on re-interpretation at run time. The categorization usually reflects the most popular or widespread implementations of a language — for instance, BASIC is sometimes called an interpreted language, and C a compiled one, despite the existence of BASIC compilers and C interpreters.
Interpretation does not replace compilation completely. It only hides it from the user and makes it gradual. Even though an interpreter can itself be interpreted, a directly executed program is needed somewhere at the bottom of the stack (see machine language). Modern trends toward just-in-time compilation and bytecode interpretation at times blur the traditional categorizations of compilers and interpreters.
Some language specifications spell out that implementations "must" include a compilation facility; for example, Common Lisp. However, there is nothing inherent in the definition of Common Lisp that stops it from being interpreted. Other languages have features that are very easy to implement in an interpreter, but make writing a compiler much harder; for example, APL, SNOBOL4, and many scripting languages allow programs to construct arbitrary source code at runtime with regular string operations, and then execute that code by passing it to a special evaluation function. To implement these features in a compiled language, programs must usually be shipped with a runtime library that includes a version of the compiler itself.
Hardware compilation.
The output of some compilers may target computer hardware at a very low level, for example a Field Programmable Gate Array (FPGA) or structured Application-specific integrated circuit (ASIC). Such compilers are said to be "hardware compilers" or synthesis tools because the source code they compile effectively controls the final configuration of the hardware and how it operates; the output of the compilation is not instructions that are executed in sequence - only an interconnection of transistors or lookup tables.
For example, XST is the Xilinx Synthesis Tool used for configuring FPGAs. Similar tools are available from Altera, Synplicity, Synopsys and other vendors.
Compiler construction.
In the early days, the approach taken to compiler design used to be directly affected by the complexity of the processing, the experience of the person(s) designing it, and the resources available.
A compiler for a relatively simple language written by one person might be a single, monolithic piece of software. When the source language is large and complex, and high quality output is required, the design may be split into a number of relatively independent phases. Having separate phases means development can be parceled up into small parts and given to different people. It also becomes much easier to replace a single phase by an improved one, or to insert new phases later (e.g., additional optimizations).
The division of the compilation processes into phases was championed by the Production Quality Compiler-Compiler Project (PQCC) at Carnegie Mellon University. This project introduced the terms "front end", "middle end", and "back end".
All but the smallest of compilers have more than two phases. However, these phases are usually regarded as being part of the front end or the back end. The point at which these two "ends" meet is open to debate. The front end is generally considered to be where syntactic and semantic processing takes place, along with translation to a lower level of representation (than source code).
The middle end is usually designed to perform optimizations on a form other than the source code or machine code. This source code/machine code independence is intended to enable generic optimizations to be shared between versions of the compiler supporting different languages and target processors.
The back end takes the output from the middle. It may perform more analysis, transformations and optimizations that are for a particular computer. Then, it generates code for a particular processor and OS.
This front-end/middle/back-end approach makes it possible to combine front ends for different languages with back ends for different CPUs. Practical examples of this approach are the GNU Compiler Collection, LLVM, and the Amsterdam Compiler Kit, which have multiple front-ends, shared analysis and multiple back-ends.
One-pass versus multi-pass compilers.
Classifying compilers by number of passes has its background in the hardware resource limitations of computers. Compiling involves performing lots of work and early computers did not have enough memory to contain one program that did all of this work. So compilers were split up into smaller programs which each made a pass over the source (or some representation of it) performing some of the required analysis and translations.
The ability to compile in a single pass has classically been seen as a benefit because it simplifies the job of writing a compiler and one-pass compilers generally perform compilations faster than multi-pass compilers. Thus, partly driven by the resource limitations of early systems, many early languages were specifically designed so that they could be compiled in a single pass (e.g., Pascal).
In some cases the design of a language feature may require a compiler to perform more than one pass over the source. For instance, consider a declaration appearing on line 20 of the source which affects the translation of a statement appearing on line 10. In this case, the first pass needs to gather information about declarations appearing after statements that they affect, with the actual translation happening during a subsequent pass.
The disadvantage of compiling in a single pass is that it is not possible to perform many of the sophisticated optimizations needed to generate high quality code. It can be difficult to count exactly how many passes an optimizing compiler makes. For instance, different phases of optimization may analyse one expression many times but only analyse another expression once.
Splitting a compiler up into small programs is a technique used by researchers interested in producing provably correct compilers. Proving the correctness of a set of small programs often requires less effort than proving the correctness of a larger, single, equivalent program.
While the typical multi-pass compiler outputs machine code from its final pass, there are several other types:
Front end.
The "compiler frontend" analyzes the source code to build an internal representation of the program, called the intermediate representation or "IR". It also manages the symbol table, a data structure mapping each symbol in the source code to associated information such as location, type and scope.
While the frontend can be a single monolithic function or program, as in a scannerless parser, it is more commonly implemented and analyzed as several phases, which may execute sequentially or concurrently. This is particularly done for good engineering: modularity and separation of concerns. Most commonly today this is done as three phases: lexing, parsing, and semantic analysis. Lexing and parsing comprise the syntactic analysis (word syntax and phrase syntax, respectively), and in simple cases these modules (the lexer and parser) can be automatically generated from a grammar for the language, though in more complex cases these require manual modification or writing by hand. The lexical grammar and phrase grammar are usually context-free grammars, which simplifies analysis significantly, with context-sensitivity handled at the semantic analysis phase. The semantic analysis phase is generally more complex and written by hand, but can be partially or fully automated using attribute grammars. These phases themselves can be further broken down – lexing as scanning and evaluating, parsing as first building a concrete syntax tree (CST, parse tree), and then transforming it into an abstract syntax tree (AST, syntax tree).
In some cases additional phases are used, notably "line reconstruction" and "preprocessing," but these are rare. A detailed list of possible phases includes:
Back end.
The term "back end" is sometimes confused with "code generator" because of the overlapped functionality of generating assembly code. Some literature uses "middle end" to distinguish the generic analysis and optimization phases in the back end from the machine-dependent code generators.
The main phases of the back end include the following:
Compiler analysis is the prerequisite for any compiler optimization, and they tightly work together. For example, dependence analysis is crucial for loop transformation.
In addition, the scope of compiler analysis and optimizations vary greatly, from as small as a basic block to the procedure/function level, or even over the whole program (interprocedural optimization). Obviously, a compiler can potentially do a better job using a broader view. But that broad view is not free: large scope analysis and optimizations are very costly in terms of compilation time and memory space; this is especially true for interprocedural analysis and optimizations.
Interprocedural analysis and optimizations are common in modern commercial compilers from HP, IBM, SGI, Intel, Microsoft, and Sun Microsystems. The open source GCC was criticized for a long time for lacking powerful interprocedural optimizations, but it is changing in this respect. Another open source compiler with full analysis and optimization infrastructure is Open64, which is used by many organizations for research and commercial purposes.
Due to the extra time and space needed for compiler analysis and optimizations, some compilers skip them by default. Users have to use compilation options to explicitly tell the compiler which optimizations should be enabled.
Compiler correctness.
Compiler correctness is the branch of software engineering that deals with trying to show that a compiler behaves according to its language specification. Techniques include developing the compiler using formal methods and using rigorous testing (often called compiler validation) on an existing compiler.
Conferences and organizations.
A number of conferences in the field of programming languages present advances in compiler construction as one of their main topics.
ACM SIGPLAN supports a number of conferences, including:
The European Joint Conferences on Theory and Practice of Software (ETAPS) sponsors the International Conference on Compiler Construction, with papers from both the academic and industrial sectors.
Asian Symposium on Programming Languages and Systems (APLAS) is organized by the Asian Association for Foundation of Software (AAFS).
Related techniques.
Assembly language is a type of low-level language and a program that compiles it is more commonly known as an "assembler", with the inverse program known as a disassembler.
A program that translates from a low level language to a higher level one is a decompiler.
A program that translates between high-level languages is usually called a language translator, source to source translator, language converter, or language rewriter. The last term is usually applied to translations that do not involve a change of language.
A program that translates into an object code format that is not supported on the compilation machine is called a cross compiler and is commonly used to prepare code for embedded applications.

</doc>
<doc id="5742" url="https://en.wikipedia.org/wiki?curid=5742" title="Castrato">
Castrato

A castrato (Italian, plural: "castrati") is a type of classical male singing voice equivalent to that of a soprano, mezzo-soprano, or contralto. The voice is produced by castration of the singer before puberty, or it occurs in one who, due to an endocrinological condition, never reaches sexual maturity.
Castration before puberty (or in its early stages) prevents a boy's larynx from being transformed by the normal physiological events of puberty. As a result, the vocal range of prepubescence (shared by both sexes) is largely retained, and the voice develops into adulthood in a unique way. Prepubescent castration for this purpose diminished greatly in the late 18th century and was made illegal in Italy in 1870.
As the castrato's body grew, his lack of testosterone meant that his epiphyses (bone-joints) did not harden in the normal manner. Thus the limbs of the castrati often grew unusually long, as did the bones of their ribs. This, combined with intensive training, gave them unrivalled lung-power and breath capacity. Operating through small, child-sized vocal cords, their voices were also extraordinarily flexible, and quite different from the equivalent adult female voice. Their vocal range was higher than that of the uncastrated adult male (see soprano, mezzo-soprano, alto, sopranist, countertenor and contralto). Listening to the only surviving recordings of a castrato (see below), one can hear that the lower part of the voice sounds like a "super-high" tenor, with a more falsetto-like upper register above that.
Castrati were rarely referred to as such: in the 18th century, the euphemism "musico" (pl "musici") was much more generally used, although it usually carried derogatory implications; another synonym was "evirato," literally meaning "emasculated". Eunuch is a more general term, since historically many eunuchs were castrated after puberty and thus the castration had no impact on their voices.
History.
Castration as a means of subjugation, enslavement or other punishment has a very long pedigree, dating back to ancient Sumer. In a Western context, eunuch singers are known to have existed from the early Byzantine Empire. In Constantinople around AD 400 the empress Aelia Eudoxia had a eunuch choir-master, Brison, who may have established the use of castrati in Byzantine choirs, though whether Brison himself was a singer and whether he had colleagues who were eunuch singers is not certain. By the 9th century, eunuch singers were well-known (not least in the choir of Hagia Sophia) and remained so until the sack of Constantinople by the Western forces of the Fourth Crusade in 1204. Their fate from then until their reappearance in Italy more than three hundred years later is not clear. It seems likely that the Spanish tradition of soprano falsettists may have hidden castrati. Much of Spain was under Muslim rulers during the Middle Ages, and castration had a history going back to the ancient Near East. Stereotypically, eunuchs served as harem guards, but they were also valued as high-level political appointees since they could not start a dynasty which would threaten the ruler.
European classical tradition.
Castrati first appeared in Italy in the mid-16th century, though at first the terms describing them were not always clear. The phrase "soprano maschio" (male soprano), which could also mean falsettist, occurs in the "Due Dialoghi della Musica" of Luigi Dentice, an Oratorian priest, published in Rome in 1553. On 9 November 1555 Cardinal Ippolito II d'Este (famed as the builder of the Villa d'Este at Tivoli), wrote to Guglielmo Gonzaga, Duke of Mantua (1538–1587), that he has heard that His Grace is interested in his "cantoretti" and offered to send him two, so that he could choose one for his own service. This is a rare term but probably does equate to "castrato". The Cardinal's brother, Alfonso II d'Este, Duke of Ferrara, was another early enthusiast, enquiring about castrati in 1556. There were certainly castrati in the Sistine Chapel choir in 1558, although not described as such: on 27 April of that year, Hernando Bustamante, a Spaniard from Palencia, was admitted (the first castrati so termed who joined the Sistine choir were Pietro Paolo Folignato and Girolamo Rossini, admitted in 1599). Surprisingly, considering the later French distaste for castrati they certainly existed in France at this time also, being known of in Paris, Orléans, Picardy and Normandy, though they were not abundant: the King of France himself had difficulty in obtaining them. By 1574 there were castrati in the Ducal court chapel at Munich, where the Kapellmeister (music director) was the famous Orlando di Lasso. In 1589, by the bull "Cum pro nostro pastorali munere", Pope Sixtus V re-organised the choir of St Peter's, Rome specifically to include castrati. Thus the castrati came to supplant both boys (whose voices broke after only a few years) and falsettists (whose voices were weaker and less reliable) from the top line in such choirs. Women were banned by the Pauline dictum "mulieres in ecclesiis taceant" ("let women keep silent in church"; see I Corinthians, ch 14, v 34).
Opera.
Although the castrato (or musico) predates opera, there is some evidence that castrati had parts in the earliest operas. In the first performance of Monteverdi's "Orfeo" (1607), for example, they played subsidiary roles, including Speranza and (possibly) that of Euridice. Although female roles were performed by castrati in some of the papal states, this was increasingly rare; by 1680, they had supplanted "normal" male voices in lead roles, and retained their position as "primo uomo" for about a hundred years; an Italian opera not featuring at least one renowned castrato in a lead part would be doomed to fail. Because of the popularity of Italian opera throughout 18th-century Europe (except France), singers such as Ferri, Farinelli, Senesino and Pacchierotti became the first operatic superstars, earning enormous fees and hysterical public adulation. The strictly hierarchical organisation of "opera seria" favoured their high voices as symbols of heroic virtue, though they were frequently mocked for their strange appearance and bad acting. In his 1755 "Reflections upon theatrical expression in tragedy", Roger Pickering wrote: 
Farinelli drew every Body to the Haymarket. What a Pipe! What Modulation! What Extasy to the Ear! But, Heavens! What Clumsiness! What Stupidity! What Offence to the Eye! Reader, if of the City, thou mayest probably have seen in the Fields of Islington or Mile-End or, If thou art in the environs of St James', thou must have observed in the Park with what Ease and Agility a cow, heavy with calf, has rose up at the command of the milkwoman's foot: thus from the mossy bank sprang the DIVINE FARINELLI.
More modern objections to the existence of castrati in Europe might centre on the means by which the preparation of future singers could lead to premature death. To prevent the child from experiencing the intense pain of castration, many were inadvertently administered lethal doses of opium or some other narcotic, or were killed by overlong compression of the carotid artery in the neck (intended to render them unconscious during the castration procedure).
During the 18th century itself, the music historian Charles Burney was sent from pillar to post in search of places where "the operation" was carried out: 
I enquired throughout Italy at what place boys were chiefly qualified for singing by castration, but could get no certain intelligence. I was told at Milan that it was at Venice; at Venice that it was at Bologna; but at Bologna the fact was denied, and I was referred to Florence; from Florence to Rome, and from Rome I was sent to Naples ... it is said that there are shops in Naples with this inscription: 'QUI SI CASTRANO RAGAZZI' ("Here boys are castrated"); but I was utterly unable to see or hear of any such shops during my residence in that city.
The training of the boys was rigorous. The regimen of one singing school in Rome (c. 1700) consisted of one hour of singing difficult and awkward pieces, one hour practising trills, one hour practising ornamented passaggi, one hour of singing exercises in their teacher's presence and in front of a mirror so as to avoid unnecessary movement of the body or facial grimaces, and one hour of literary study; all this, moreover, before lunch. After, half-an-hour would be devoted to musical theory, another to writing counterpoint, an hour copying down the same from dictation, and another hour of literary study. During the remainder of the day, the young castrati had to find time to practice their harpsichord playing, and to compose vocal music, either sacred or secular depending on their inclination. This demanding schedule meant that, if sufficiently talented, they were able to make a debut in their mid-teens with a perfect technique and a voice of a flexibility and power no woman or ordinary male singer could match.
In the 1720s and 1730s, at the height of the craze for these voices, it has been estimated that upwards of 4,000 boys were castrated annually in the service of art. Many came from poor homes and were castrated by their parents in the hope that their child might be successful and lift them from poverty (this was the case with Senesino). There are, though, records of some young boys asking to be operated on to preserve their voices (e.g. Caffarelli, who was from a wealthy family: his grandmother gave him the income from two vineyards to pay for his studies). Caffarelli was also typical of many castrati in being famous for tantrums on and off-stage, and for amorous adventures with noble ladies. Some, as described by Casanova, preferred gentlemen (noble or otherwise). Modern endocrinology would suggest that the castrati's much-vaunted sexual prowess was more the stuff of legend than reality – in addition to lacking a hormonal (but not a socio-psychological) sex drive, a castrato's remaining genitalia will not develop in size. Only a small percentage of boys castrated to preserve their voices had successful careers on the operatic stage; the better "also-rans" sang in cathedral or church choirs, but because of their marked appearance and the ban on their marrying, there was little room for them in society outside a musical context.
The castrati came in for a great amount of scurrilous and unkind abuse, and as their fame increased, so did the hatred of them. They were often castigated as malign creatures who lured men into homosexuality. There were homosexual castrati, as Casanova's accounts of 18th-century Italy bear witness. He mentions meeting an abbé whom he took for a girl in disguise, only later discovering that "she" was a famous castrato. In Rome in 1762 he attended a performance at which the prima donna was a castrato, "the favourite pathic" of Cardinal Borghese, who dined every evening with his protector. From his behaviour on stage "it was obvious that he hoped to inspire the love of those who liked him as a man, and probably would not have done so as a woman".
Decline.
By the late 18th century, changes in operatic taste and social attitudes spelled the end for castrati. They lingered on past the end of the "ancien régime" (which their style of opera parallels), and two of their number, Pacchierotti and Crescentini, even entranced the iconoclastic Napoleon. The last great operatic castrato was Giovanni Battista Velluti (1781–1861), who performed the last operatic castrato role ever written: Armando in "Il crociato in Egitto" by Meyerbeer (Venice, 1824). Soon after this they were replaced definitively as the first men of the operatic stage by a new breed of heroic tenor, as first incarnated by the Frenchman Gilbert-Louis Duprez, the earliest so-called "king of the high Cs". His successors have included such singers as Enrico Tamberlik, Jean de Reszke, Francesco Tamagno, Enrico Caruso, Giovanni Martinelli, Beniamino Gigli, Jussi Björling, Franco Corelli and Luciano Pavarotti, among others.
After the unification of Italy in 1861, castration for musical purposes was officially made illegal (the new Italian state had adopted a French legal code which expressly forbade the practice). In 1878, Pope Leo XIII prohibited the hiring of new castrati by the church: only in the Sistine Chapel and in other papal basilicas in Rome did a few castrati linger. A group photo of the Sistine Choir taken in 1898 shows that by then only six remained (plus the "Direttore Perpetuo", the fine soprano castrato Domenico Mustafà), and in 1902 a ruling was extracted from Pope Leo that no further castrati should be admitted. The official end to the castrati came on St. Cecilia's Day, 22 November 1903, when the new pope, Pius X, issued his "motu proprio", "Tra le Sollecitudini" ('Amongst the Cares'), which contained this instruction: "Whenever ... it is desirable to employ the high voices of sopranos and contraltos, these parts must be taken by boys, according to the most ancient usage of the Church."
The last Sistine castrato to survive was Alessandro Moreschi, the only castrato to have made solo recordings. While an interesting historical record, these discs of his give us only a glimpse of the castrato voice – although he had been renowned as "The Angel of Rome" at the beginning of his career, some would say he was past his prime when the recordings were made in 1902 and 1904 and he never attempted to sing opera. He retired officially in March 1913, and died in 1922.
The Catholic Church's involvement in the castrato phenomenon has long been controversial, and there have recently been calls for it to issue an official apology for its role. As early as 1748, Pope Benedict XIV tried to ban castrati from churches, but such was their popularity at the time that he realised that doing so might result in a drastic decline in church attendance.
The rumours of another castrato sequestered in the Vatican for the personal delectation of the Pontiff until as recently as 1959 have been proven false. The singer in question was a pupil of Moreschi's, Domenico Mancini, such a successful imitator of his teacher's voice that even Lorenzo Perosi, Direttore Perpetuo of the Sistine Choir from 1898 to 1956 and a lifelong opponent of castrati, thought he was a castrato. Mancini was in fact a moderately skilful falsettist and professional double bass player.
Modern castrati and similar voices.
So-called "natural" or "endocrinological castrati" are born with hormonal anomalies, such as Klinefelter's syndrome and Kallmann's syndrome, or have undergone unusual physical or medical events during their early lives that reproduce the vocal effects of castration without being castrated. Jimmy Scott and Radu Marian are examples of this type of high male voice. Michael Maniaci is somewhat different, in that he has no hormonal or other anomalies, but for some unknown reason, his voice did not "break" in the usual manner, leaving him still able to sing in the soprano register. Other uncastrated male adults sing soprano, generally using some form of falsetto but in a much higher range than most countertenors. Examples are Aris Christofellis, Jörg Waschinski, and Ghio Nannini. However, it is believed the castrati possessed more of a tenorial chest register (the aria "Navigante che non spera" in Leonardo Vinci's opera "Il Medo", written for Farinelli, requires notes down to C3). Similar low-voiced singing can be heard from the jazz vocalist Jimmy Scott, whose range matches approximately that used by female blues singers, while the Turkish popular singer Cem Adrian has the ability to sing from bass to soprano, his vocal folds having been reported to be three times the average length. Actor Chris Colfer has a similar range. Colfer has stated in interviews that when his voice began to change at puberty he sang in a high voice "constantly" in an effort to retain his range. The late musician Jeff Buckley had a four-octave range, which allowed him to cover women's songs in a natural voice and reach notes from bass to soprano.

</doc>
<doc id="5743" url="https://en.wikipedia.org/wiki?curid=5743" title="Counting-out game">
Counting-out game

A counting-out game is a simple game intended to select a person to be "it", often for the purpose of playing another game. These games usually require no materials, and are played with spoken words or hand gestures.
Many such games involve one person pointing at each participant in a circle of players while reciting a rhyme. A new person is pointed at as each word is said. The player who is selected at the conclusion of the rhyme is "it" or "out". In an alternate version, the circle of players may each put two feet in and at the conclusion of the rhyme, that player removes one foot and the rhyme starts over with the next person. In this case, the first player that has both feet removed is "it" or "out". These are often accepted as random selections because the number of words has not been calculated beforehand, so the result is unknown right up until someone is selected.
A variant of counting-out game, known as Josephus problem, represents a famous theoretical problem in mathematics and computer science.
Counting-out games.
Several simple games can be played to select one person from a group, either as a straightforward winner, or as someone who is eliminated. Rock, Paper, Scissors, Odd or Even and Blue Shoe require no materials and are played using hand gestures, although with the former it is possible for a player to win or lose through skill rather than luck. Coin flipping and drawing straws are fair methods of randomly determining a player. Bizz Buzz is a spoken word game where if a player slips up and speaks a word out of sequence, they are eliminated.
In popular culture.
A scene in the Marx Brothers movie "Duck Soup" plays on the fact that counting-out games are not really random. Faced with selecting someone to go on a dangerous mission, the character Chicolini (Chico Marx) chants:
only to stop as he realizes he is about to select himself. He then says, "I did it wrong. Wait, wait, I start here", and repeats the chant—with the same result. After that, he says, "That's no good too. I got it!" and reduces the chant to
And with this version he finally manages to "randomly" select someone else.

</doc>
<doc id="5749" url="https://en.wikipedia.org/wiki?curid=5749" title="Key size">
Key size

In cryptography, key size or key length is the size measured in bits of the key used in a cryptographic algorithm (such as a cipher). An algorithm's key length is distinct from its original cryptographic security, which is a logarithmic measure of the fastest known computational attack on the algorithm, also measured in bits. The security of an algorithm cannot exceed its key length (since any algorithm can be cracked by brute force), but it can be smaller. For example, Triple DES has a key size of 168 bits but provides at most 112 bits of security, since an attack of complexity 2 is known. This property of Triple DES is not a weakness provided 112 bits of security is sufficient for an application. Most symmetric-key algorithms in common use are designed to have security equal to their key length. No asymmetric-key algorithms with this property are known; elliptic curve cryptography comes the closest with an effective security of roughly half its key length.
Significance.
Keys are used to control the operation of a cipher so that only the correct key can convert encrypted text (ciphertext) to plaintext. Many ciphers are actually based on publicly known algorithms or are open source and so it is only the difficulty of obtaining the key that determines security of the system, provided that there is no analytic attack (i.e., a 'structural weakness' in the algorithms or protocols used), and assuming that the key is not otherwise available (such as via theft, extortion, or compromise of computer systems). The widely accepted notion that the security of the system should depend on the key alone has been explicitly formulated by Auguste Kerckhoffs (in the 1880s) and Claude Shannon (in the 1940s); the statements are known as Kerckhoffs' principle and Shannon's Maxim respectively.
A key should therefore be large enough that a brute force attack (possible against any encryption algorithm) is infeasible – i.e., would take too long to execute. Shannon's work on information theory showed that to achieve so called "perfect secrecy", the key length must be at least as large as the message and only used once (this algorithm is called the One-time pad). In light of this, and the practical difficulty of managing such long keys, modern cryptographic practice has discarded the notion of perfect secrecy as a requirement for encryption, and instead focuses on "computational security", under which the computational requirements of breaking an encrypted text must be infeasible for an attacker.
Key size and encryption system.
Encryption systems are often grouped into families. Common families include symmetric systems (e.g. AES) and asymmetric systems (e.g. RSA); they may alternatively be grouped according to the central algorithm used (e.g. elliptic curve cryptography).
As each of these is of a different level of cryptographic complexity, it is usual to have different key sizes for the same level of security, depending upon the algorithm used. For example, the security available with a 1024-bit key using asymmetric RSA is considered approximately equal in security to an 80-bit key in a symmetric algorithm (Source: RSA Security).
The actual degree of security achieved over time varies, as more computational power and more powerful mathematical analytic methods become available. For this reason cryptologists tend to look at indicators that an algorithm or key length shows signs of potential vulnerability, to move to longer key sizes or more difficult algorithms. For example , a 1039 bit integer was factored with the special number field sieve using 400 computers over 11 months. The factored number was of a special form; the special number field sieve cannot be used on RSA keys. The computation is roughly equivalent to breaking a 700 bit RSA key. However, this might be an advance warning that 1024 bit RSA used in secure online commerce should be deprecated, since they may become breakable in the near future. Cryptography professor Arjen Lenstra observed that "Last time, it took nine years for us to generalize from a special to a nonspecial, hard-to-factor number" and when asked whether 1024-bit RSA keys are dead, said: "The answer to that question is an unqualified yes."
The 2015 Logjam attack revealed additional dangers in using Diffie-Helman key exchange when only one or a few common 1024-bit or smaller prime moduli are in use. This common practice allows large amounts of communications to be compromised at the expense of attacking a small number of primes.
Brute force attack.
Even if a symmetric cipher is currently unbreakable by exploiting structural weaknesses in its algorithm, it is possible to run through the entire space of keys in what is known as a "brute force attack". Since longer symmetric keys require exponentially more work to brute force search, a sufficiently long symmetric key makes this line of attack impractical.
With a key of length "n" bits, there are 2 possible keys. This number grows very rapidly as "n" increases. The large number of operations (2) required to try all possible 128-bit keys is widely considered out of reach for conventional digital computing techniques for the foreseeable future. However, experts anticipate alternative computing technologies that may have processing power superior to current computer technology. If a suitably sized quantum computer capable of running Grover's algorithm reliably becomes available, it would reduce a 128-bit key down to 64-bit security, roughly a DES equivalent. This is one of the reasons why AES supports a 256-bit key length. See the discussion on the relationship between key lengths and quantum computing attacks at the bottom of this page for more information.
Symmetric algorithm key lengths.
US Government export policy has long restricted the 'strength' of cryptography that can be sent out of the country. For many years the limit was 40 bits. Today, a key length of 40 bits offers little protection against even a casual attacker with a single PC, a predictable and inevitable consequence of governmental restrictions limiting key length. In response, by the year 2000, most of the major US restrictions on the use of strong encryption were relaxed. However, not all regulations have been removed, and encryption registration with the U.S. Bureau of Industry and Security is still required to export "mass market encryption commodities, software and components with encryption exceeding 64 bits" ().
IBM's Lucifer cipher was selected in 1974 as the base for what would become the Data Encryption Standard. Lucifer's key length was reduced from 128-bits to 56 bits, which the NSA and NIST argued was sufficient. The NSA has major computing resources and a large budget; some cryptographers including Whitfield Diffie and Martin Hellman complained that this made the cipher so weak that NSA computers would be able to break a DES key in a day through brute force parallel computing. The NSA disputed this, claiming brute forcing DES would take them something like 91 years. However, by the late 90s, it became clear that DES could be cracked in a few days' time-frame with custom-built hardware such as could be purchased by a large corporation or government. The book "Cracking DES" (O'Reilly and Associates) tells of the successful attempt in 1998 to break 56-bit DES by a brute force attack mounted by a cyber civil rights group with limited resources; see EFF DES cracker. Even before that demonstration, 56 bits was considered insufficient length for symmetric algorithm keys. In 2002, Distributed.net and its volunteers broke a 64-bit RC5 key after several years effort, using about seventy thousand (mostly home) computers.
The NSA's Skipjack algorithm used in its Fortezza program employs 80-bit keys.
DES has been replaced in many applications by Triple DES, which has 112 bits of security when used 168-bit keys (triple key)
The Advanced Encryption Standard published in 2001 uses a key sizes of 128 bits, 192 or 256 bits. Many observers consider 128 bits sufficient for the foreseeable future for symmetric algorithms of AES's quality until quantum computers become available. However, as of 2015, the U.S. National Security Agency has issued guidance that it plans to switch to quantum computing resistant algorithms and now requires 256-bit AES keys for data classified up to Top Secret.
In 2003, the U.S. National Institute for Standards and Technology, NIST proposed phasing out 80-bit keys by 2015. As of 2005, 80-bit keys were allowed only until 2010.
As of 2015, NIST guidance says that "the use of keys that provide less than 112 bits of security strength for key agreement is now disallowed." NIST approved symmetric encryption algorithms include three-key Triple DES, and AES. Approvals for two-key Triple DES and Skipjack have been withdrawn as of 2015.
Asymmetric algorithm key lengths.
The effectiveness of public key cryptosystems depends on the intractability (computational and theoretical) of certain mathematical problems such as integer factorization. These problems are time consuming to solve, but usually faster than trying all possible keys by brute force. Thus, asymmetric algorithm keys must be longer for equivalent resistance to attack than symmetric algorithm keys. As of 2002, an asymmetric key length of 1024 bits was generally considered the minimum necessary for the RSA encryption algorithm.
The Finite Field Diffie-Hellman algorithm has roughly the same key strength as RSA for the same key sizes. The work factor for breaking Diffie-Hellman is based on the discrete logarithm problem, which is related to the integer factorization problem on which RSA's strength is based. Thus, a 3072-bit Diffie-Hellman key has about the same strength as a 3072-bit RSA key.
One of the asymmetric algorithm types, elliptic curve cryptography, or ECC, appears to be secure with shorter keys than other asymmetric key algorithms require. NIST guidelines state that ECC keys should be twice the length of equivalent strength symmetric key algorithms. So, for example, a 224-bit ECC key would have roughly the same strength as a 112-bit symmetric key. These estimates assume no major breakthroughs in solving the underlying mathematical problems that ECC is based on. A message encrypted with an elliptic key algorithm using a 109-bit long key has been broken by brute force.
The NSA previously specified that "Elliptic Curve Public Key Cryptography using the 256-bit prime modulus elliptic curve as specified in FIPS-186-2 and SHA-256 are appropriate for protecting classified information up to the SECRET level. Use of the 384-bit prime modulus elliptic curve and SHA-384 are necessary for the protection of TOP SECRET information." In 2015 the NSA announced that it plans to transition from Elliptic Curve Cryptography to new algorithms that are resistant to attack by future quantum computers. In the interim it recommends the larger 384-bit curve for all classified information.
Effect of quantum computing attacks on key strength.
The two best known quantum computing attacks are based on Shor's algorithm and Grover's algorithm. Of the two, Shor's offers the greater risk to current security systems.
Derivatives of Shor's algorithm are widely conjectured to be effective against all mainstream public-key algorithms including RSA, Diffie-Hellman and elliptic curve cryptography. According to Professor Gilles Brassard, an expert in quantum computing: "The time needed to factor an RSA integer is the same order as the time needed to use that same integer as modulus for a single RSA encryption. In other words, it takes no more time to break RSA on a quantum computer (up to a multiplicative constant) than to use it legitimately on a classical computer." The general consensus is that these public key algorithms are insecure at any key size if sufficiently large quantum computers capable of running Shor's algorithm become available. The implication of this attack is that all data encrypted using current standards based security systems such as the ubiquitous SSL used to protect e-commerce and Internet banking and SSH used to protect access to sensitive computing systems is at risk. Encrypted data protected using public-key algorithms can be archived and may be broken at a later time.
Mainstream symmetric ciphers (such as AES or Twofish) and collision resistant hash functions (such as SHA) are widely conjectured to offer greater security against known quantum computing attacks. They are widely thought most vulnerable to Grover's algorithm. Bennett, Bernstein, Brassard, and Vazirani proved in 1996 that a brute-force key search on a quantum computer cannot be faster than roughly 2 invocations of the underlying cryptographic algorithm, compared with roughly 2 in the classical case. Thus in the presence of large quantum computers an "n"-bit key can provide at least "n"/2 bits of security. Quantum brute force is easily defeated by doubling the key length, which has little extra computational cost in ordinary use. This implies that at least a 256-bit symmetric key is required to achieve 128-bit security rating against a quantum computer. As mentioned above, the NSA announced in 2015 that it plans to transition to quantum-resistant algorithms.
According to NSA "A sufficiently large quantum computer, if built, would be capable of undermining all widely-deployed public key algorithms used for key establishment and digital signatures. ... It is generally accepted that quantum computing techniques are much less effective against symmetric algorithms than against current widely used public key algorithms. While public key cryptography requires changes in the fundamental design to protect against a potential future quantum computer, symmetric key algorithms are believed to be secure provided a sufficiently large key size is used. ... In the longer term, NSA looks to NIST to identify a broadly accepted, standardized suite of commercial public key algorithms that are not vulnerable to quantum attacks.
, NSA's The Commercial National Security Algorithm Suite includes:

</doc>
<doc id="5750" url="https://en.wikipedia.org/wiki?curid=5750" title="Cognitive behavioral therapy">
Cognitive behavioral therapy

Cognitive behavioral therapy (CBT) is a form of psychotherapy. It was originally designed to treat depression, but is now used for a number of mental disorders.
It works to solve current problems and change unhelpful thinking and behavior. The name refers to behavior therapy, cognitive therapy, and therapy based upon a combination of basic behavioral and cognitive principles. Most therapists working with patients dealing with anxiety and depression use a blend of cognitive and behavioral therapy. This technique acknowledges that there may be behaviors that cannot be controlled through rational thought, but rather emerge based on prior conditioning from the environment and other external and/or internal stimuli. CBT is "problem focused" (undertaken for specific problems) and "action oriented" (therapist tries to assist the client in selecting specific strategies to help address those problems), or directive in its therapeutic approach. It is different from the more traditional, psychoanalytical approach, where therapists look for the unconscious meaning behind the behaviors and then diagnose the patient. Instead, behaviorists believe that disorders, such as depression, have to do with the relationship between a feared stimulus and an avoidance response, resulting in a conditioned fear, much like Ivan Pavlov. Cognitive therapists believed that conscious thoughts could influence a person’s behavior all on its own. Ultimately, the two theories were combined to create what is now known as cognitive behavioral therapy.
CBT is effective for a variety of conditions, including mood, anxiety, personality, eating, addiction, dependence, tic, and psychotic disorders. Many CBT treatment programs have been evaluated for symptom-based diagnoses and been favored over approaches such as psychodynamic treatments. However, other researchers have questioned the validity of such claims to superiority over other treatments.
Description.
Mainstream cognitive behavioral therapy assumes that changing maladaptive thinking leads to change in affect and behavior, but recent variants emphasize changes in one's relationship to maladaptive thinking rather than changes in thinking itself. The goal of Cognitive Behavioral Therapy is not to diagnose a person with a particular disease, but to look at them as a whole and decide what needs to be fixed. The basic steps in a Cognitive-Behavioral Assessment include
These steps are based on a system created by Kanfer and Saslow. After identifying the behaviors that need changing, whether they be in excess or deficit, and treatment has occurred, the psychologist must identify whether or not the intervention succeeded. For example, "If the goal was to decrease the behavior, then there should be a decrease relative to the baseline. If the critical behavior remains at or above the baseline, then the intervention has failed."
Therapists or computer-based programs use CBT techniques to help individuals challenge their patterns and beliefs and replace "errors in thinking such as overgeneralizing, magnifying negatives, minimizing positives and catastrophizing" with "more realistic and effective thoughts, thus decreasing emotional distress and self-defeating behavior." These errors in thinking are known as cognitive distortions. Cognitive distortions can be either a pseudo- discrimination belief or an over-generalization of something. CBT techniques may also be used to help individuals take a more open, mindful, and aware posture toward them so as to diminish their impact. Mainstream CBT helps individuals replace "maladaptive... coping skills, cognitions, emotions and behaviors with more adaptive ones", by challenging an individual's way of thinking and the way that they react to certain habits or behaviors, but there is still controversy about the degree to which these traditional cognitive elements account for the effects seen with CBT over and above the earlier behavioral elements such as exposure and skills training.
Modern forms of CBT include a number of diverse but related techniques such as exposure therapy, stress inoculation training, cognitive processing therapy, cognitive therapy, relaxation training, dialectical behavior therapy, and acceptance and commitment therapy. Some practitioners promote a form of mindful cognitive therapy which includes a greater emphasis on self-awareness as part of the therapeutic process.
CBT has six phases:
The reconceptualization phase makes up much of the "cognitive" portion of CBT. A summary of modern CBT approaches is given by Hofmann.
There are different protocols for delivering cognitive behavioral therapy, with important similarities among them. Use of the term "CBT" may refer to different interventions, including "self-instructions (e.g. distraction, imagery, motivational self-talk), relaxation and/or biofeedback, development of adaptive coping strategies (e.g. minimizing negative or self-defeating thoughts), changing maladaptive beliefs about pain, and goal setting". Treatment is sometimes manualized, with brief, direct, and time-limited treatments for individual psychological disorders that are specific technique-driven. CBT is used in both individual and group settings, and the techniques are often adapted for self-help applications. Some clinicians and researchers are cognitively oriented (e.g. cognitive restructuring), while others are more behaviorally oriented (e.g. "in vivo" exposure therapy). Interventions such as imaginal exposure therapy combine both approaches.
Medical uses.
In adults, CBT has been shown to have effectiveness and a role in the treatment plans for anxiety disorders, depression, eating disorders, chronic low back pain, personality disorders, psychosis, schizophrenia, substance use disorders, in the adjustment, depression, and anxiety associated with fibromyalgia, and with post-spinal cord injuries. Evidence has shown CBT is effective in helping treat schizophrenia, and it is now offered in most treatment guidelines.
In children or adolescents, CBT is an effective part of treatment plans for anxiety disorders, body dysmorphic disorder, depression and suicidality, eating disorders and obesity, obsessive–compulsive disorder, and posttraumatic stress disorder, as well as tic disorders, trichotillomania, and other repetitive behavior disorders. CBT-SP, an adaptation of CBT for suicide prevention (SP), was specifically designed for treating youth who are severely depressed and who have recently attempted suicide within the past 90 days, and was found to be effective, feasible, and acceptable. "Sparx" is a video game to help young persons, using the CBT method to teach them how to resolve their own issues. CBT has also been shown to be effective for posttraumatic stress disorder in very young children (3 to 6 years of age).
Cognitive Behavior Therapy has also been applied to a variety of childhood disorders, including depressive disorders and various anxiety disorders.
Cochrane reviews have found no evidence that CBT is effective for tinnitus, although there appears to be an effect on management of associated depression and quality of life in this condition. Other recent Cochrane Reviews found no convincing evidence that CBT training helps foster care providers manage difficult behaviors in the youth under their care, nor was it helpful in treating men who abuse their intimate partners.
According to a 2004 review by INSERM of three methods, cognitive behavioral therapy was either "proven" or "presumed" to be an effective therapy on several specific mental disorders. According to the study, CBT was effective at treating schizophrenia, depression, bipolar disorder, panic disorder, post-traumatic stress, anxiety disorders, bulimia, anorexia, personality disorders and alcohol dependency.
Some meta-analyses find CBT more effective than psychodynamic therapy and equal to other therapies in treating anxiety and depression. However, psychodynamic therapy may provide better long-term outcomes.
Computerized CBT (CCBT) has been proven to be effective by randomized controlled and other trials in treating depression and anxiety disorders, including children, as well as insomnia. Some research has found similar effectiveness to an intervention of informational websites and weekly telephone calls. CCBT was found to be equally effective as face-to-face CBT in adolescent anxiety and insomnia.
Criticism of CBT sometimes focuses on implementations (such as the UK IAPT) which may result initially in low quality therapy being offered by poorly trained practitioners. However evidence supports the effectiveness of CBT for anxiety and depression.
Mounting evidence suggests that the addition of hypnotherapy as an adjunct to CBT improves treatment efficacy for a variety of clinical issues.
CBT has been applied in both clinical and non-clinical environments to treat disorders such as personality conditions and behavioral problems. A systematic review of CBT in depression and anxiety disorders concluded that "CBT delivered in primary care, especially including computer- or Internet-based self-help programs, is potentially more effective than usual care and could be delivered effectively by primary care therapists."
Emerging evidence suggests a possible role for CBT in the treatment of attention deficit hyperactivity disorder (ADHD); hypochondriasis; coping with the impact of multiple sclerosis; sleep disturbances related to aging; dysmenorrhea; and bipolar disorder, but more study is needed and results should be interpreted with caution. CBT can have a therapeutic effects on easing symptoms of anxiety and depression in people with Alzheimer's disease. CBT has been studied as an aid in the treatment of anxiety associated with stuttering. Initial studies have shown CBT to be effective in reducing social anxiety in adults who stutter, but not in reducing stuttering frequency.
Martinez-Devesa "et al." (2010) found no evidence that CBT is effective for tinnitus, although there appears to be an effect on management of associated depression and quality of life in this condition. Turner "et al." (2007) found no convincing evidence that CBT training helps foster care providers manage difficult behaviors in the youth under their care, and Smedslund "et al." (2007) found that it was not helpful in treating men who abuse their intimate partners.
In the case of metastatic breast cancer, Edwards "et al." (2008) maintained that the current body of evidence is not sufficient to rule out the possibility that psychological interventions may cause harm to women with this advanced neoplasm.
There is some evidence that CBT is superior in the long-term to benzodiazepines and the nonbenzodiazepines in the treatment and management of insomnia. CBT has been shown to be moderately effective for treating chronic fatigue syndrome.
In the United Kingdom, the National Institute for Health and Care Excellence (NICE) recommends CBT in the treatment plans for a number of mental health difficulties, including posttraumatic stress disorder, obsessive–compulsive disorder (OCD), bulimia nervosa, and clinical depression.
Anxiety disorders.
CBT has been shown to be effective in the treatment of adult anxiety disorders.
A basic concept in some CBT treatments used in anxiety disorders is "in vivo" exposure. The term refers to the direct confrontation of feared objects, activities, or situations by a patient. For example, a woman with PTSD who fears the location where she was assaulted may be assisted by her therapist in going to that location and directly confronting those fears. Likewise, a person with social anxiety disorder who fears public speaking may be instructed to directly confront those fears by giving a speech. This "two-factor" model is often credited to O. Hobart Mowrer. Through exposure to the stimulus, this harmful conditioning can be "unlearned" (referred to as extinction and habituation). Studies have provided evidence that when examining animals and humans that glucocorticoids may possibly lead to a more successful extinction learning during exposure therapy. For instance, glucocorticoids can prevent aversive learning episodes from being retrieved and heighten reinforcement of memory traces creating a non-fearful reaction in feared situations. A combination of glucocorticoids and exposure therapy may be a better improved treatment for treating patients with anxiety disorders.
Schizophrenia, psychosis and mood disorders.
Cognitive behavioral therapy has been shown as an effective treatment for clinical depression. The American Psychiatric Association Practice Guidelines (April 2000) indicated that, among psychotherapeutic approaches, cognitive behavioral therapy and interpersonal psychotherapy had the best-documented efficacy for treatment of major depressive disorder. One etiological theory of depression is Aaron T. Beck's cognitive theory of depression. His theory states that depressed people think the way they do because their thinking is biased towards negative interpretations. According to this theory, depressed people acquire a negative schema of the world in childhood and adolescence as an effect of stressful life events, and the negative schema is activated later in life when the person encounters similar situations.
Beck also described a negative cognitive triad, made up of the negative schemata and cognitive biases of the person, theorizing that depressed individuals make negative evaluations of themselves, the world, and the future. According to this theory, depressed people have views such as "I never do a good job", "It is impossible to have a good day", and "things will never get better." A negative schema helps give rise to the cognitive bias, and the cognitive bias helps fuel the negative schema. This is the negative triad. Beck further proposed that depressed people often have the following cognitive biases: arbitrary inference, selective abstraction, over-generalization, magnification, and minimization. These cognitive biases are quick to make negative, generalized, and personal inferences of the self, thus fueling the negative schema.
In long-term psychoses, CBT is used to complement medication and is adapted to meet individual needs. Interventions particularly related to these conditions include exploring reality testing, changing delusions and hallucinations, examining factors which precipitate relapse, and managing relapses. Several meta-analyses suggested that CBT is effective in schizophrenia, and the American Psychiatric Association includes CBT in its schizophrenia guideline as an evidence-based treatment. There is also limited evidence of effectiveness for CBT in bipolar disorder and severe depression.
A 2010 meta-analysis found that no trial employing both blinding and psychological placebo has shown CBT to be effective in either schizophrenia or bipolar disorder, and that the effect size of CBT was small in major depressive disorder. They also found a lack of evidence to conclude that CBT was effective in preventing relapses in bipolar disorder. Evidence that severe depression is mitigated by CBT is also lacking, with anti-depressant medications still viewed as significantly more effective than CBT, although success with CBT for depression was observed beginning in the 1990s.
According to Cox, Abramson, Devine, and Hollon (2012), cognitive behavioral therapy can also be used to reduce prejudice towards others. This other-directed prejudice can cause depression in the "others," or in the self when a person becomes part of a group he or she previously had prejudice towards (i.e. deprejudice). "Devine and colleagues (2012) developed a successful Prejudice Perpetrator intervention with many conceptual parallels to CBT. Like CBT, their intervention taught Sources to be aware of their automative thoughts and to intentionally deploy a variety of cognitive techniques against automatic stereotyping."
With older adults.
CBT is used to help people of all ages, but the therapy should be adjusted based on the age of the patient with whom the therapist is dealing. Older individuals in particular have certain characteristics that need to be acknowledged and the therapy altered to account for these differences thanks to age. Some of the challenges to CBT because of age include the following:
Prevention of mental illness.
For anxiety disorders, use of CBT with people at risk has significantly reduced the number of episodes of generalized anxiety disorder and other anxiety symptoms, and also given significant improvements in explanatory style, hopelessness, and dysfunctional attitudes. In another study, 3% of the group receiving the CBT intervention developed generalized anxiety disorder by 12 months postintervention compared with 14% in the control group. Subthreshold panic disorder sufferers were found to significantly benefit from use of CBT. Use of CBT was found to significantly reduce social anxiety prevalence.
For depressive disorders, a stepped-care intervention (watchful waiting, CBT and medication if appropriate) achieved a 50% lower incidence rate in a patient group aged 75 or older. Another depression study found a neutral effect compared to personal, social, and health education, and usual school provision, and included a comment on potential for increased depression scores from people who have received CBT due to greater self recognition and acknowledgement of existing symptoms of depression and negative thinking styles. A further study also saw a neutral result. A meta-study of the Coping with Depression course, a cognitive behavioural intervention delivered by a psychoeducational method, saw a 38% reduction in risk of major depression.
For schizophrenia, one study of preventative CBT showed a positive effect and another showed neutral effect.
History.
Philosophical roots.
Precursors of certain fundamental aspects of CBT have been identified in various ancient philosophical traditions, particularly Stoicism. Stoic philosophers, particularly Epictetus, believed logic could be used to identify and discard false beliefs that lead to destructive emotions, which has influenced the way modern cognitive-behavioral therapists identify cognitive distortions that contribute to depression and anxiety. For example, Aaron T. Beck's original treatment manual for depression states, "The philosophical origins of cognitive therapy can be traced back to the Stoic philosophers". Another example of Stoic influence on cognitive theorists is Epictetus on Albert Ellis. A key philosophical figure who also influenced the development of CBT was John Stuart Mill.
Behavior therapy roots.
The modern roots of CBT can be traced to the development of behavior therapy in the early 20th century, the development of cognitive therapy in the 1960s, and the subsequent merging of the two. Groundbreaking work of behaviorism began with Watson's and Rayner's studies of conditioning in 1920. Behaviorally-centered therapeutic approaches appeared as early as 1924 with Mary Cover Jones' work dedicated to the unlearning of fears in children. These were the antecedents of the development of Joseph Wolpe's behavioral therapy in the 1950s. It was the work of Wolpe and Watson, which was based on Ivan Pavlov's work on learning and conditioning, that influenced Hans Eysenck and Arnold Lazarus to develop new behavioral therapy techniques based on classical conditioning. One of Eysenck's colleagues, Glenn Wilson showed that classical fear conditioning in humans could be controlled by verbally induced cognitive expectations, thus opening a field of research that supports the rationale of cognitive behaviorial therapy.
During the 1950s and 1960s, behavioral therapy became widely utilized by researchers in the United States, the United Kingdom, and South Africa, who were inspired by the behaviorist learning theory of Ivan Pavlov, John B. Watson, and Clark L. Hull. In Britain, Joseph Wolpe, who applied the findings of animal experiments to his method of systematic desensitization, applied behavioral research to the treatment of neurotic disorders. Wolpe's therapeutic efforts were precursors to today's fear reduction techniques. British psychologist Hans Eysenck presented behavior therapy as a constructive alternative.
At the same time of Eysenck's work, B.F. Skinner and his associates were beginning to have an impact with their work on operant conditioning. Skinner's work was referred to as radical behaviorism and avoided anything related to cognition. However, Julian Rotter, in 1954, and Albert Bandura, in 1969, contributed behavior therapy with their respective work on social learning theory, by demonstrating the effects of cognition on learning and behavior modification.
The emphasis on behavioral factors constituted the "first wave" of CBT.
Cognitive therapy roots.
One of the first therapists to address cognition in psychotherapy was Alfred Adler with his notion of basic mistakes and how they contributed to creation of unhealthy or useless behavioral and life goals. Adler's work influenced the work of Albert Ellis, who developed one of the earliest cognitive-based psychotherapies, known today as Rational emotive behavior therapy, or REBT.
Around the same time that rational emotive therapy, as it was known then, was being developed, Aaron T. Beck was conducting free association sessions in his psychoanalytic practice. During these sessions, Beck noticed that thoughts were not as unconscious as Freud had previously theorized, and that certain types of thinking may be the culprits of emotional distress. It was from this hypothesis that Beck developed cognitive therapy, and called these thoughts "automatic thoughts".
It was these two therapies, rational emotive therapy and cognitive therapy, that started the "second wave" of CBT, which was the emphasis on cognitive factors.
Behavior and cognitive therapies merge.
Although the early behavioral approaches were successful in many of the neurotic disorders, they had little success in treating depression. Behaviorism was also losing in popularity due to the so-called "cognitive revolution". The therapeutic approaches of Albert Ellis and Aaron T. Beck gained popularity among behavior therapists, despite the earlier behaviorist rejection of "mentalistic" concepts like thoughts and cognitions. Both of these systems included behavioral elements and interventions and primarily concentrated on problems in the present.
In initial studies, cognitive therapy was often contrasted with behavioral treatments to see which was most effective. During the 1980s and 1990s, cognitive and behavioral techniques were merged into cognitive behavioral therapy. Pivotal to this merging was the successful development of treatments for panic disorder by David M. Clark in the UK and David H. Barlow in the US.
Over time, cognitive behavior therapy became to be known not only as a therapy, but as an umbrella term for all cognitive-based psychotherapies. These therapies include, but are not limited to, rational emotive therapy, cognitive therapy, acceptance and commitment therapy, dialectical behavior therapy, reality therapy/choice theory, cognitive processing therapy, EMDR, and multimodal therapy. All of these therapies are a blending of cognitive- and behavior-based elements.
This blending of theoretical and technical foundations from both behavior and cognitive therapies constitute the "third wave" of CBT, which is the current wave. The most prominent therapies of this third wave are dialectical behavior therapy and acceptance and commitment therapy.
Methods of access.
Therapist.
A typical CBT programme would consist of face-to-face sessions between patient and therapist, made up of 6-18 sessions of around an hour each with a gap of a 1–3 weeks between sessions. This initial programme might be followed by some booster sessions, for instance after one month and three months. CBT has also been found to be effective if patient and therapist type in real time to each other over computer links.
Cognitive behavioral therapy is most closely allied with the scientist–practitioner model in which clinical practice and research is informed by a scientific perspective, clear operationalization of the problem, and an emphasis on measurement, including measuring changes in cognition and behavior and in the attainment of goals. These are often met through "homework" assignments in which the patient and the therapist work together to craft an assignment to complete before the next session. The completion of these assignments – which can be as simple as a person suffering from depression attending some kind of social event – indicates a dedication to treatment compliance and a desire to change. The therapists can then logically gauge the next step of treatment based on how thoroughly the patient completes the assignment. Effective cognitive behavioral therapy is dependent on a therapeutic alliance between the healthcare practitioner and the person seeking assistance. Unlike many other forms of psychotherapy, the patient is very involved in CBT. For example, an anxious patient may be asked to talk to a stranger as a homework assignment, but if that is too difficult, he or she can work out an easier assignment first. The therapist needs to be flexible and willing to listen to the patient rather than acting as an authority figure.
Computerized or internet-delivered.
Computerized cognitive behavioral therapy (CCBT) has been described by NICE as a "generic term for delivering CBT via an interactive computer interface delivered by a personal computer, internet, or interactive voice response system", instead of face-to-face with a human therapist. It is also known as internet-delivered cognitive behavioral therapy or ICBT. CCBT has potential to improve access to evidence-based therapies, and to overcome the prohibitive costs and lack of availability sometimes associated with retaining a human therapist. In this context, it is important not to confuse CBT with 'computer-based training', which nowadays is more commonly referred to as e-Learning.
CCBT has been found in meta-studies to be cost-effective and often cheaper than usual care, including for anxiety. Studies have shown that individuals with social anxiety and depression experienced improvement with online CBT-based methods. A review of current CCBT research in the treatment of OCD in children found this interface to hold great potential for future treatment of OCD in youth and adolescent populations. CCBT is also predisposed to treating mood disorders amongst non-heterosexual populations, who may avoid face-to-face therapy from fear of stigma. However presently CCBT programs seldom cater to these populations.
A key issue in CCBT use is low uptake and completion rates, even when it has been clearly made available and explained. CCBT completion rates and treatment efficacy have been found in some studies to be higher when use of CCBT is supported personally, with supporters not limited only to therapists, than when use is in a self-help form alone.
In February 2006 NICE recommended that CCBT be made available for use within the NHS across England and Wales for patients presenting with mild-to-moderate depression, rather than immediately opting for antidepressant medication, and CCBT is made available by some health systems. The 2009 NICE guideline recognized that there are likely to be a number of computerized CBT products that are useful to patients, but removed endorsement of any specific product.
A relatively new avenue of research is the combination of artificial intelligence and CCBT. It has been proposed to use modern technology to create CCBT that simulates face-to-face therapy. This might be achieved in cognitive behaviour therapy for a specific disorder using the comprehensive domain knowledge of CBT. One area where this has been attempted is the specific domain area of social anxiety in those who stutter.
Reading self-help materials.
Enabling patients to read self-help CBT guides has been shown to be effective by some studies. However one study found a negative effect in patients who tended to ruminate, and another meta-analysis found that the benefit was only significant when the self-help was guided (e.g. by a medical professional).
Group educational course.
Patient participation in group courses has been shown to be effective. In a meta-analysis reviewing evidence-based treatment of OCD in children, individual CBT was found to be more efficacious than group CBT.
Types.
Brief CBT.
Brief cognitive behavioral therapy (BCBT) is a form of CBT which has been developed for situations in which there are time constraints on the therapy sessions. BCBT takes place over a couple of sessions that can last up to 12 accumulated hours by design. This technique was first implemented and developed on soldiers overseas in active duty by David M. Rudd to prevent suicide.
Breakdown of treatment
Cognitive emotional behavioral therapy.
Cognitive emotional behavioral therapy (CEBT) is a form of (CBT) developed initially for individuals with eating disorders but now used with a range of problems including anxiety, depression, obsessive compulsive disorder (OCD), post traumatic stress disorder (PTSD) and anger problems. It combines aspects of CBT and Dialectical Behavioural Therapy and aims to improve understanding and tolerance of emotions in order to facilitate the therapeutic process. It is frequently used as a 'pretreatment' to prepare and better equip individuals for longer term therapy.
Structured cognitive behavioral training.
Structured cognitive behavioral training (SCBT) is a cognitive-based process with core philosophies that draw heavily from CBT. Like CBT, SCBT asserts that behavior is inextricably related to beliefs, thoughts and emotions. SCBT also builds on core CBT philosophy by incorporating other well-known modalities in the fields of behavioral health and psychology: most notably, Albert Ellis's Rational Emotive Behavior Therapy. SCBT differs from CBT in two distinct ways. Firstly, SCBT is delivered in a highly regimented format. Secondly, SCBT is a predetermined and finite training process that becomes personalized by the input of the participant. SCBT is designed with the intention to bring a participant to a specific result in a specific period of time. SCBT has been used to challenge addictive behavior, particularly with substances such as tobacco, alcohol and food; and to manage diabetes and subdue stress and anxiety. SCBT has also been used in the field of criminal psychology in the effort to reduce recidivism.
Moral reconation therapy.
Moral reconation therapy, a type of CBT used to help felons overcome Anti-Social Personality Disorder, slightly decreases the risk of further offending. It is generally implemented in a group format because of the risk of offenders with ASPD being given one on one therapy reinforces narcissistic behavioral characteristics, and can be used in correctional or outpatient settings. Groups meet weekly for two to six months.
Stress Inoculation Training.
This type of therapy uses a blend of cognitive, behavioral and a some humanistic training techniques to target the stressors of the client. This usually is used to help clients better cope with their stress or anxiety after stressful events. This is a three phase process that trains the client to use skills that they already have to better adapt to their current stressors. The first phase is an interview phase that includes psychological testing, client self-monitoring, and a variety of reading materials. This allows the therapist to individually tailor the training process to the client. Clients learn how to categorize problems into emotion- focused or problem focused, so that they can better treat their negative situations. This phase ultimately prepares the client to eventually confront and reflect upon their current reactions to stressors, before looking at ways to change their reactions and emotions in relation to their stressors. The focus is conceptualization.
The second phase emphasizes the aspect of skills acquisition and rehearsal that continues from the earlier phase of conceptualization. The client is taught skill that help them cope with their stressors. These skills are then practised in the space of therapy. These skills involve self-regulation, problem solving, interpersonal
communication skills, etc.
The third and final phase is the application and following through of the skills learned in the training process. This gives the client opportunities to apply their learned skills to a wide range of stressors. Activities include role-playing, imagery, modeling, etc.
In the end, the client will have been trained on a preventative basis to inoculate personal, chronic, and future stressors by breaking down their stressors into problems they will address in long-term, short-term, and intermediate coping goals.
Criticisms.
The research conducted for CBT has been a topic of sustained controversy. While some researchers write that CBT is more effective than other treatments, many other researchers and practitioners have questioned the validity of such claims. For example, one study determined CBT to be superior to other treatments in treating anxiety and depression. However, researchers responding directly to that study conducted a re-analysis and found no evidence of CBT being superior to other bona fide treatments, and conducted an analysis of thirteen other CBT clinical trials and determined that they failed to provide evidence of CBT superiority.
Additionally, a recent meta-analysis revealed that the positive effects of CBT on depression have been declining since 1977. The overall results showed two different declines in effect sizes: 1) an overall decline between 1977 and 2014, and 2) a steeper decline between 1995 and 2014. Additional sub-analysis revealed that CBT studies where therapists in the test group were instructed to adhere to the Beck CBT manual had a steeper decline in effect sizes since 1977 than studies where therapists in the test group were instructed to use CBT without a manual. The authors reported that they were unsure why the effects were declining but did list inadequate therapist training, failure to adhere to a manual, lack of therapist experience, and patients' hope and faith in its efficacy waning as potential reasons. The authors did mention that the current study was limited to depression disorders only.
Furthermore, other researchers write that CBT studies have high drop-out rates compared to other treatments. At times, the CBT drop-out rates can be more than five times higher than other treatments groups. For example, the researchers provided statistics of 28 participants in a group receiving CBT therapy dropping out, compared to 5 participants in a group receiving problem-solving therapy dropping out, or 11 participants in a group receiving psychodynamic therapy dropping out. This high drop-out rate is also evident in the treatment of several disorders particularly anorexia nervosa, an eating disorder commonly treated by CBT. People with anorexia nervosa who are treated with CBT have a high percent chance of dropping out of therapy before completion and reverting to their anorexia behaviors.
Other researchers conducting an analysis of treatments for youth who self-injure found similar drop-out rates in CBT and DBT groups. In this study, the researchers analyzed several clinical trials that measured the efficacy of CBT administered to youth who self-injure. The researchers concluded that none of them were found to be efficacious. These conclusions were made using the APA Division 12 Task Force on the Promotion and Dissemination of Psychological Procedures to determine intervention potency.
However, the research methods employed in CBT research have not been the only criticisms identified. Others have called CBT theory and therapy into question. For example, Fancher writes the CBT has failed to provide a framework for clear and correct thinking. He states that it is strange for CBT theorists to develop a framework for determining distorted thinking without ever developing a framework for "cognitive clarity" or what would count as "healthy, normal thinking." Additionally, he writes that irrational thinking cannot be a source of mental and emotional distress when there is no evidence of rational thinking causing psychological well-being. Or, that social psychology has proven the normal cognitive processes of the average person to be irrational, even those who are psychologically well. Fancher also says that the theory of CBT is inconsistent with basic principles and research of rationality, and even ignores many rules of logic. He argues that CBT makes something of thinking that is far less exciting and true than thinking probably is. Among his other arguments are the maintaining of the status quo promoted in CBT, the self-deception encouraged within clients and patients engaged in CBT, how poorly the research is conducted, and some of its basic tenets and norms: "The basic norm of cognitive therapy is this: except for how the patient thinks, everything is ok".
Meanwhile, Slife and Williams write that one of the hidden assumptions in CBT is that of determinism, or the absence of free will. They argue that CBT invokes a type of cause-and-effect relationship with cognition. They state that CBT holds that external stimuli from the environment enter the mind, causing different thoughts that cause emotional states. Nowhere in CBT theory is agency, or free will, accounted for. At its most basic foundational assumptions, CBT holds that human beings have no free will and are just determined by the cognitive processes invoked by external stimuli.
Another criticism of CBT theory, especially as applied to Major Depressive Disorder (MDD), is that it confounds the symptoms of the disorder with its causes.
A major criticism has been that clinical studies of CBT efficacy (or any psychotherapy) are not double-blind (i.e., neither subjects nor therapists in psychotherapy studies are blind to the type of treatment). They may be single-blinded, i.e. the rater may not know the treatment the patient received, but neither the patients nor the therapists are blinded to the type of therapy given (two out of three of the persons involved in the trial, i.e., all of the persons involved in the treatment, are unblinded). The patient is an active participant in correcting negative distorted thoughts, thus quite aware of the treatment group they are in.
The importance of double-blinding was shown in a meta-analysis that examined the effectiveness of CBT when placebo control and blindedness were factored in. Pooled data from published trials of CBT in schizophrenia, MDD, and bipolar disorder that used controls for non-specific effects of intervention were analyzed. This study concluded that CBT is no better than non-specific control interventions in the treatment of schizophrenia and does not reduce relapse rates, treatment effects are small in treatment studies of MDD, and it is not an effective treatment strategy for prevention of relapse in bipolar disorder. For MDD, the authors note that the pooled effect size was very low. Nevertheless, the methodological processes used to select the studies in the previously mentioned meta-analysis and the worth of its findings have been called into question.
Society and culture.
The UK's National Health Service announced in 2008 that more therapists would be trained to provide CBT at government expense as part of an initiative called Improving Access to Psychological Therapies (IAPT). NICE said that CBT would become the mainstay of treatment for non-severe depression, with medication used only in cases where CBT had failed. Therapists complained that the data does not fully support the attention and funding CBT receives. Psychotherapist and professor Andrew Samuels stated that this constitutes "a coup, a power play by a community that has suddenly found itself on the brink of corralling an enormous amount of money ... Everyone has been seduced by CBT's apparent cheapness." The UK Council for Psychotherapy issued a press release in 2012 saying that the IAPT's policies were undermining traditional psychotherapy and criticized proposals that would limit some approved therapies to CBT, claiming that they restricted patients to "a watered down version of cognitive behavioural therapy (CBT), often delivered by very lightly trained staff".
NICE also recommends offering CBT to people suffering from schizophrenia, as well as those at risk of suffering from a psychotic episode.

</doc>
<doc id="5751" url="https://en.wikipedia.org/wiki?curid=5751" title="Chinese language">
Chinese language

Chinese (; "Hànyǔ" or ; "Zhōngwén") is a group of related but in many cases mutually unintelligible language varieties, forming a branch of the Sino-Tibetan language family. Chinese is spoken by the Han majority and many other ethnic groups in China. Nearly 1.2 billion people (around 16% of the world's population) speak some form of Chinese as their first language.
The varieties of Chinese are usually described by native speakers as dialects of a single Chinese language, but linguists note that they are as diverse as a language family.
The internal diversity of Chinese has been likened to that of the Romance languages, but may be even more varied. There are between 7 and 13 main regional groups of Chinese (depending on classification scheme), of which the most spoken by far is Mandarin (about 960 million), followed by Wu (80 million), Yue (60 million) and Min (70 million). Most of these groups are mutually unintelligible, although some, like Xiang and the Southwest Mandarin dialects, may share common terms and some degree of intelligibility. All varieties of Chinese are tonal and analytic.
Standard Chinese "(Putonghua/Guoyu/Huayu)" is a standardized form of spoken Chinese based on the Beijing dialect of Mandarin. It is the official language of China and Taiwan, as well as one of four official languages of Singapore. It is one of the six official languages of the United Nations. The written form of the standard language (; "Zhōngwén"), based on the logograms known as Chinese characters (; "Hànzì"), is shared by literate speakers of otherwise unintelligible dialects.
Of the other varieties of Chinese, Cantonese (the prestige variety of Yue) is influential in Guangdong province and in Hong Kong and Macau, and is widely spoken among overseas communities. Southern Min, part of the Min group, is widely spoken in southern Fujian, in neighboring Taiwan (Taiwanese/Hoklo) and in Southeast Asia (Hokkien). Hakka also has a sizeable diaspora in Taiwan and southeast Asia. Shanghainese and other Wu varieties are prominent in the lower Yangtze region of eastern China.
History.
Chinese can be traced back to a hypothetical Sino-Tibetan proto-language. The first written records appeared over 3,000 years ago. As the language evolved over this period, the various local varieties became mutually unintelligible. In reaction, central governments have repeatedly sought to promulgate a unified standard.
Origins.
Most linguists classify all varieties of Chinese as part of the Sino-Tibetan language family, together with Burmese, Tibetan and many other languages spoken in the Himalayas and the Southeast Asian Massif.
Although the relationship was first proposed in the early 19th century and is now broadly accepted, reconstruction of Sino-Tibetan is much less developed than for families such as Indo-European or Austroasiatic.
Difficulties have included the great diversity of the languages, the lack of inflection in many of them, and the effects of language contact.
In addition, many of the smaller languages are spoken in mountainous areas that are difficult to reach, and are often also sensitive border zones.
Without a secure reconstruction of proto-Sino-Tibetan, the higher-level structure of the family remains unclear.
A top-level branching into Chinese and Tibeto-Burman languages is often assumed, but has not been convincingly demonstrated.
Old and Middle Chinese.
The earliest examples of Chinese are divinatory inscriptions on oracle bones from around 1250 BCE in the late Shang dynasty.
Old Chinese was the language of the Western Zhou period (1046–771 BCE), recorded in inscriptions on bronze artifacts, the "Classic of Poetry" and portions of the "Book of Documents" and "I Ching".
Scholars have attempted to reconstruct the phonology of Old Chinese by comparing later varieties of Chinese with the rhyming practice of the "Classic of Poetry" and the phonetic elements found in the majority of Chinese characters.
Although many of the finer details remain unclear, most scholars agree that Old Chinese differed from Middle Chinese in lacking retroflex and palatal obstruents but having initial consonant clusters of some sort, and in having voiceless nasals and liquids.
Most recent reconstructions also describe an atonal language with consonant clusters at the end of the syllable, developing into tone distinctions in Middle Chinese.
Several derivational affixes have also been identified, but the language lacked inflection, and indicated grammatical relationships using word order and grammatical particles.
Middle Chinese was the language used during Northern and Southern dynasties and the Sui, Tang, and Song dynasties (6th through 10th centuries CE).
It can be divided into an early period, reflected by the "Qieyun" rime book (601 CE), and a late period in the 10th century, reflected by rhyme tables such as the "Yunjing" constructed by ancient Chinese philologists as a guide to the "Qieyun" system.
These works define phonological categories, but with little hint of what sounds they represent.
Linguists have identified these sounds by comparing the categories with pronunciations in modern varieties of Chinese, borrowed Chinese words in Japanese, Vietnamese, and Korean, and transcription evidence.
The resulting system is very complex, with a large number of consonants and vowels, but they were probably not all distinguished in any single dialect. Most linguists now believe it represents a diasystem encompassing 6th-century northern and southern standards for reading the classics.
Rise of northern dialects.
After the fall of the Northern Song dynasty, and during the reign of the Jin (Jurchen) and Yuan (Mongol) dynasties in northern China, a common speech (now called Old Mandarin) developed based on the dialects of the North China Plain around the capital.
The "Zhongyuan Yinyun" (1324) was a dictionary that codified the rhyming conventions of new "sanqu" verse form in this language.
Together with the slightly later "Menggu Ziyun", this dictionary describes a language with many of the features characteristic of modern Mandarin dialects.
Until the mid-20th century, most of the Chinese people living in many parts of southern China spoke only their local language.
As a practical measure, officials of the Ming and Qing dynasties carried out the administration of the empire using a common language based on Mandarin varieties, known as "Guānhuà" (官話, literally "language of officials").
For most of this period, this language was a koiné based on dialects spoken in the Nanjing area, though not identical to any single dialect.
By the middle of the 19th century, the Beijing dialect had become dominant and was essential for any business with the imperial court.
In the 1930s a standard national language "Guóyǔ" (国语/國語 "national language") was adopted.
After much dispute between proponents of northern and southern dialects and an abortive attempt at an artificial pronunciation, the National Language Unification Commission finally settled on the Beijing dialect in 1932.
The People's Republic founded in 1949 retained this standard, calling it "pǔtōnghuà" (普通话/普通話 "common speech").
The national language is now used in education, the media, and formal situations in both Mainland China and Taiwan.
In Hong Kong and Macau, because of their colonial and linguistic history, the language of education, the media, formal speech, and everyday life remains the local Cantonese, although the standard language is now very influential and taught in schools.
Influence.
The Chinese language has spread to neighbouring countries through a variety of means.
Northern Vietnam was incorporated into the Han empire in 111 BCE, beginning a period of Chinese control that ran almost continuously for a millennium.
The Four Commanderies were established in northern Korea in the first century BCE, but disintegrated in the following centuries.
Chinese Buddhism spread over East Asia between the 2nd and 5th centuries CE, and with it the study of scriptures and literature in Literary Chinese.
Later Korea, Japan, and Vietnam developed strong central governments modeled on Chinese institutions, with Literary Chinese as the language of administration and scholarship, a position it would retain until the late 19th century in Korea and (to a lesser extent) Japan, and the early 20th century in Vietnam.
Scholars from different lands could communicate, albeit only in writing, using Literary Chinese.
Although they used Chinese solely for written communication, each country had its own tradition of reading texts aloud, the so-called Sino-Xenic pronunciations.
Chinese words with these pronunciations were also borrowed extensively into the Korean, Japanese and Vietnamese languages, and today comprise over half their vocabularies.
This massive influx led to changes in the phonological structure of the languages, contributing to the development of moraic structure in Japanese and the disruption of vowel harmony in Korean.
Borrowed Chinese morphemes have been used extensively in all these languages to coin compound words for new concepts, in a similar way to the use of Latin and Ancient Greek roots in European languages.
Many new compounds, or new meanings for old phrases, were created in the late 19th and early 20th centuries to name Western concepts and artifacts.
These coinages, written in shared Chinese characters, have then been borrowed freely between languages.
They have even been accepted into Chinese, a language usually resistant to loanwords, because their foreign origin was hidden by their written form.
Often different compounds for the same concept were in circulation for some time before a winner emerged, and sometimes the final choice differed between countries.
The proportion of vocabulary of Chinese origin thus tends to be greater in technical, abstract, or formal language.
For example, Sino-Japanese words account for about 35% of the words in entertainment magazines, over half the words in newspapers, and 60% of the words in science magazines.
Vietnam, Korea, and Japan each developed writing systems for their own languages, initially based on Chinese characters, but later replaced with the "Hangul" alphabet for Korean and supplemented with "kana" syllabaries for Japanese, while Vietnamese continued to be written with the complex "Chữ nôm" script.
However, these were limited to popular literature until the late 19th century.
Today Japanese is written with a composite script using both Chinese characters ("Kanji") and kana, but Korean is written exclusively with Hangul in North Korea, and supplementary Chinese characters ("Hanja") are increasingly rarely used in the South.
Vietnamese is written with a Latin-based alphabet.
Examples of loan words in English include "tea", from Hokkien (Min Nan) "tê" (茶) and "kumquat", from Cantonese "gamgwat" (金橘).
Varieties.
Jerry Norman estimated that there are hundreds of mutually unintelligible varieties of Chinese. These varieties form a dialect continuum, in which differences in speech generally become more pronounced as distances increase, though the rate of change varies immensely. Generally, mountainous South China exhibits more linguistic diversity than the North China Plain. In parts of South China, a major city's dialect may only be marginally intelligible to close neighbors. For instance, Wuzhou is about upstream from Guangzhou, but the Yue variety spoken there is more like that of Guangzhou than is that of Taishan, southwest of Guangzhou and separated from it by several rivers. In parts of Fujian the speech of neighboring counties or even villages may be mutually unintelligible.
Until the late 20th century, Chinese emigrants to Southeast Asia and North America came from southeast coastal areas, where Min, Hakka, and Yue dialects are spoken.
The vast majority of Chinese immigrants to North America spoke the Taishan dialect, from a small coastal area southwest of Guangzhou.
Classification.
Local varieties of Chinese are conventionally classified into seven dialect groups, largely on the basis of the different evolution of Middle Chinese voiced initials:
The classification of Li Rong, which is used in the "Language Atlas of China" (1987), distinguishes three further groups:
Numbers of first-language speakers (all countries):
Some varieties remain unclassified, including Danzhou dialect (spoken in Danzhou, on Hainan Island), Waxianghua (spoken in western Hunan) and Shaozhou Tuhua (spoken in northern Guangdong).
Standard Chinese and diglossia.
Putonghua/Guoyu, often called "Mandarin", is the official standard language used by the People's Republic of China, the Republic of China (Taiwan), and Singapore (where it is called "Huayu" or simply Chinese). It is based on the Beijing dialect, which is the dialect of Mandarin as spoken in Beijing. The governments of both countries intend for speakers of all Chinese speech varieties to use it as a common language of communication. Therefore, it is used in government agencies, in the media, and as a language of instruction in schools.
In mainland China and Taiwan, diglossia has been a common feature: For example, in addition to "putonghua", a resident of Shanghai might speak Shanghainese; and, if he or she grew up elsewhere, then he or she may also be likely to be fluent in the particular dialect of that local area. A native of Guangzhou may speak both Cantonese and "putonghua". In addition to Mandarin, most Taiwanese also speak Minnan, Hakka or an Austronesian language. A Taiwanese may commonly mix pronunciations, phrases, and words from Mandarin and other Taiwanese languages, and this mixture is considered normal in daily or informal speech.
Nomenclature.
The official Chinese designation for the major branches of Chinese is "fāngyán" (, literally "regional speech"), whereas the more closely related varieties within these are called "dìdiǎn fāngyán" (/ "local speech"). Conventional English-language usage in Chinese linguistics is to use "dialect" for the speech of a particular place (regardless of status) while regional groupings like Mandarin and Wu are called "dialect groups". Because varieties from different groups are not mutually intelligible, some scholars prefer to describe Wu and so on as languages. Jerry Norman called this practice misleading, pointing out that Wu, which itself contains many mutually unintelligible varieties, could not be properly called a single language under the same criterion, and that the same is true for each of the other groups.
Mutual intelligibility is considered by some linguists to be the main criterion for determining whether varieties are separate languages or dialects of a single language, although others do not regard it as decisive, particularly when cultural factors interfere as they do with Chinese. As explains, linguists often ignore mutual intelligibility when varieties share intelligibility with a central variety (i.e. prestige variety, such as Standard Mandarin), as the issue requires some careful handling when mutual intelligibility is inconsistent with language identity. John DeFrancis considers the mutual unintelligibility too great for the term "dialects" to be used to refer to the different varieties, but also objects to considering them as separate languages, as it incorrectly implies a set of disruptive "religious, economic, political, and other differences" between speakers that exist, for example, between French Catholics and English Protestants in Canada, but not between speakers of Cantonese and Mandarin in China, owing to China's near-uninterrupted history of centralized government.
Because of the difficulties involved in determining the difference between language and dialect, other terms have been proposed: ISO 639-3 follows "Ethnologue" in assigning individual language codes to the 13 main subdivisions, while Chinese as whole is classified as a 'macrolanguage'. Other options include "vernacular", "lect" "regionalect", "topolect", and "variety".
Chinese itself has while ()—this term could be translated to either "language" or "languages" since Chinese lacks grammatical number. For centuries in China, owing to the widespread use of a written standard in Classical Chinese, , as indicated by the employment of two separate morphemes "yǔ" / and "wén" . The characters used in written Chinese are logographs rather than their phonemes, although most logographs are compounds of similar-sounding characters and semantic disambiguation (the "radical"). 
Most Chinese people consider the spoken varieties as one single language because speakers share a common culture and history, as well a shared national identity and a common written form. To Chinese nationalists, the idea of Chinese as a language family may suggest that the Chinese identity is much more fragmented and disunified than it actually is and as such is often looked upon as culturally and politically provocative. Additionally, some of whose supporters promote the local Taiwanese Hokkien variety.
Writing.
The relationship between the Chinese spoken and written language is rather complex. Its spoken varieties evolved at different rates, while written Chinese itself has changed much less. Classical Chinese literature began in the Spring and Autumn period, although written records have been discovered as far back as the 14th to 11th centuries BCE Shang dynasty oracle bones using the oracle bone scripts.
The Chinese orthography centers on Chinese characters, "Hànzì" (汉字/漢字), which are written within imaginary rectangular blocks, traditionally arranged in vertical columns, read from top to bottom down a column, and right to left across columns. Chinese characters are morphemes independent of phonetic change. Thus the character 一 ("one") is uttered "yī" in Standard Chinese, "jat1" in Cantonese and "it" in Hokkien (form of Min). Vocabularies from different major Chinese variants have diverged, and colloquial nonstandard written Chinese often makes use of unique "dialectal characters", such as 冇 and 係 for Cantonese and Hakka, which are considered archaic or unused in standard written Chinese.
Written colloquial Cantonese has become quite popular in online chat rooms and instant messaging amongst Hong-Kongers and Cantonese-speakers elsewhere. Use of it is considered highly informal, and does not extend to many formal occasions.
In Hunan, women in certain areas write their local language in Nü Shu, a syllabary derived from Chinese characters. The Dungan language, considered by many a dialect of Mandarin, is nowadays written in Cyrillic, and was previously written in the Arabic script. The Dungan people are primarily Muslim and live mainly in Kazakhstan, Kyrgyzstan, and Russia; some of the related Hui people also speak the language and live mainly in China.
Chinese characters.
Each Chinese character represents a monosyllabic Chinese word or morpheme. In 100 CE, the famed Han dynasty scholar Xu Shen classified characters into six categories, namely pictographs, simple ideographs, compound ideographs, phonetic loans, phonetic compounds and derivative characters. Of these, only 4% were categorized as pictographs, including many of the simplest characters, such as "rén" 人 (human), "rì" 日 (sun), "shān" 山 (mountain; hill), "shuǐ" 水 (water). Between 80% and 90% were classified as phonetic compounds such as "chōng" 沖 (pour), combining a phonetic component "zhōng" 中 (middle) with a semantic radical 氵 (water). Almost all characters created since have been of this type. The 18th-century Kangxi Dictionary recognized 214 radicals.
Modern characters are styled after the regular script. Various other written styles are also used in Chinese calligraphy, including seal script, cursive script and clerical script. Calligraphy artists can write in traditional and simplified characters, but they tend to use traditional characters for traditional art.
There are currently two systems for Chinese characters. The traditional system, still used in Hong Kong, Taiwan, Macau and Chinese speaking communities (except Singapore and Malaysia) outside mainland China, takes its form from standardized character forms dating back to the late Han dynasty. The Simplified Chinese character system, introduced by the People's Republic of China in 1954 to promote mass literacy, simplifies most complex traditional glyphs to fewer strokes, many to common cursive shorthand variants.
Singapore, which has a large Chinese community, is the first—and at present the only—foreign nation to officially adopt simplified characters, although it has also become the "de facto" standard for younger ethnic Chinese in Malaysia. The Internet provides the platform to practice reading the alternative system, be it traditional or simplified.
A well-educated Chinese reader today recognizes approximately 4,000 to 6,000 characters; approximately 3,000 characters are required to read a Mainland newspaper. The PRC government defines literacy amongst workers as a knowledge of 2,000 characters, though this would be only functional literacy. A large unabridged dictionary, like the Kangxi Dictionary, contains over 40,000 characters, including obscure, variant, rare, and archaic characters; fewer than a quarter of these characters are now commonly used.
Homophones.
Standard Chinese has fewer than 1,700 distinct syllables but 4,000 common written characters, so there are many homophones. For example, the following characters (not necessarily words) are all pronounced "jī": 鸡／雞 "chicken", 机／機 "machine", 基 "basic", 击／擊 "to hit", 饥／饑 "hunger", and 积／積 "accumulate". In speech, the meaning of a syllable is determined by context (for example, in English, "some" as the opposite of "none" as opposed to "sum" in arithmetic) or by the word it is found in ("some" or "sum" vs. "summer"). Speakers may clarify which written character they mean by giving a word or phrase it is found in: 名字叫嘉英，嘉陵江的嘉，英國的英 "Míngzi jiào Jiāyīng, Jiālíng Jiāng de jiā, Yīngguó de yīng" – "My name is "Jiāyīng", 'Jia' as in 'Jialing River' and 'ying' as in 'England'."
Southern Chinese varieties like Cantonese and Hakka preserved more of the rimes of Middle Chinese and also have more tones. Several of the examples of Mandarin "jī" above have distinct pronunciations in Cantonese (romanized using jyutping): "gai1", "gei1", "gei1", "gik1", "gei1", and "zik1" respectively. For this reason, southern varieties tend to need to employ fewer multi-syllabic words.
Phonology.
The phonological structure of each syllable consists of a nucleus consisting of a vowel (which can be a monophthong, diphthong, or even a triphthong in certain varieties), preceded by an onset (a single consonant, or consonant+glide; zero onset is also possible), and followed (optionally) by a coda consonant; a syllable also carries a tone. There are some instances where a vowel is not used as a nucleus. An example of this is in Cantonese, where the nasal sonorant consonants and can stand alone as their own syllable.
Across all the spoken varieties, most syllables tend to be open syllables, meaning they have no coda (assuming that a final glide is not analyzed as a coda), but syllables that do have codas are restricted to nasals , , , the retroflex approximant , and voiceless stops , , , or . Some varieties allow most of these codas, whereas others, such as Standard Chinese, are limited to only , and .
The number of sounds in the different spoken dialects varies, but in general there has been a tendency to a reduction in sounds from Middle Chinese. The Mandarin dialects in particular have experienced a dramatic decrease in sounds and so have far more multisyllabic words than most other spoken varieties. The total number of syllables in some varieties is therefore only about a thousand, including tonal variation, which is only about an eighth as many as English.
Tones.
All varieties of spoken Chinese use tones to distinguish words. A few dialects of north China may have as few as three tones, while some dialects in south China have up to 6 or 12 tones, depending on how one counts. One exception from this is Shanghainese which has reduced the set of tones to a two-toned pitch accent system much like modern Japanese.
A very common example used to illustrate the use of tones in Chinese are the four tones of Standard Chinese (along with the neutral tone) applied to the syllable "ma". The tones are exemplified by the following five Chinese words:
Standard Cantonese, by contrast, has six tones in open syllables and three tones in syllables ending with stops:
Phonetic transcriptions.
The Chinese had no uniform phonetic transcription system until the mid-20th century, although enunciation patterns were recorded in early rime books and dictionaries. Early Indian translators, working in Sanskrit and Pali, were the first to attempt to describe the sounds and enunciation patterns of Chinese in a foreign language. After the 15th century, the efforts of Jesuits and Western court missionaries resulted in some rudimentary Latin transcription systems, based on the Nanjing Mandarin dialect.
Romanization.
Romanization is the process of transcribing a language into the Latin script. There are many systems of romanization for the Chinese varieties, due to the lack of a native phonetic transcription until modern times. Chinese is first known to have been written in Latin characters by Western Christian missionaries in the 16th century.
Today the most common romanization standard for Standard Chinese is "Hanyu Pinyin", often known simply as pinyin, introduced in 1956 by the People's Republic of China, and later adopted by Singapore and Taiwan. Pinyin is almost universally employed now for teaching standard spoken Chinese in schools and universities across America, Australia and Europe. Chinese parents also use Pinyin to teach their children the sounds and tones of new words. In school books that teach Chinese, the Pinyin romanization is often shown below a picture of the thing the word represents, with the Chinese character alongside.
The second-most common romanization system, the Wade–Giles, was invented by Thomas Wade in 1859 and modified by Herbert Giles in 1892. As this system approximates the phonology of Mandarin Chinese into English consonants and vowels, i.e. it is an Anglicization, it may be particularly helpful for beginner Chinese speakers of an English-speaking background. Wade–Giles was found in academic use in the United States, particularly before the 1980s, and until recently was widely used in Taiwan.
When used within European texts, the tone transcriptions in both pinyin and Wade–Giles are often left out for simplicity; Wade–Giles' extensive use of apostrophes is also usually omitted. Thus, most Western readers will be much more familiar with "Beijing" than they will be with "Běijīng" (pinyin), and with "Taipei" than "T'ai²-pei³" (Wade–Giles). This simplification presents syllables as homophones which really are none, and therefore exaggerates the number of homophones almost by a factor of four.
Here are a few examples of "Hanyu Pinyin" and Wade–Giles, for comparison:
Other systems of romanization for Chinese include Gwoyeu Romatzyh, the French EFEO, the Yale (invented during WWII for U.S. troops), as well as separate systems for Cantonese, Min Nan, Hakka, and other Chinese varieties.
Other phonetic transcriptions.
Chinese varieties have been phonetically transcribed into many other writing systems over the centuries. The 'Phags-pa script, for example, has been very helpful in reconstructing the pronunciations of premodern forms of Chinese.
Zhuyin (also called "bopomofo"), a semi-syllabary is still widely used in Taiwan's elementary schools to aid standard pronunciation. Although bopomofo characters are reminiscent of katakana script, there is no source to substantiate the claim that Katakana was the basis for the zhuyin system. A comparison table of zhuyin to pinyin exists in the zhuyin article. Syllables based on pinyin and zhuyin can also be compared by looking at the following articles:
There are also at least two systems of cyrillization for Chinese. The most widespread is the Palladius system.
Grammar and morphology.
Chinese is often described as a "monosyllabic" language. However, this is only partially correct. It is largely accurate when describing Classical Chinese and Middle Chinese; in Classical Chinese, for example, perhaps 90% of words correspond to a single syllable and a single character. In the modern varieties, it is still usually the case that a morpheme (unit of meaning) is a single syllable; contrast English, with plenty of multi-syllable morphemes, both bound and free, such as "seven", "elephant", "para-" and "-able". Some of the conservative southern varieties of modern Chinese still have largely monosyllabic words, especially among the more basic vocabulary.
In modern Mandarin, however, most nouns, adjectives and verbs are largely disyllabic. A significant cause of this is phonological attrition. Sound change over time has steadily reduced the number of possible syllables. In modern Mandarin, there are now only about 1,200 possible syllables, including tonal distinctions, compared with about 5,000 in Vietnamese (still largely monosyllabic) and over 8,000 in English.
This phonological collapse has led to a corresponding increase in the number of homophones. As an example, the small Langenscheidt Pocket Chinese Dictionary lists six common words pronounced "shí" (tone 2): 十 "ten"; 实 "real, actual"; 识 "know (a person), recognize"; 石 "stone"; 时 "time"; 食 "food". These were all pronounced differently in Early Middle Chinese; in William H. Baxter's transcription they were "dzyip", "zyit", "syik", "dzyek", "dzyi" and "zyik" respectively. They are still pronounced differently in today's Cantonese; in Jyutping they are "sap9", "sat9", "sik7", "sek9", "si4", "sik9". In modern spoken Mandarin, however, tremendous ambiguity would result if all of these words could be used as-is; Yuen Ren Chao's modern poem Lion-Eating Poet in the Stone Den exploits this, consisting of 92 characters all pronounced "shi". As such, most of these words have been replaced (in speech, if not in writing) with a longer, less-ambiguous compound. Only the first one, 十 "ten", normally appears as such when spoken; the rest are normally replaced with, respectively, 实际 "shíjì" (lit. "actual-connection"); 认识 "rènshi" (lit. "recognize-know"); 石头 "shítou" (lit. "stone-head"); 时间 "shíjiān" (lit. "time-interval"); 食物 "shíwù" (lit. "food-thing"). In each case, the homophone was disambiguated by adding another morpheme, typically either a synonym or a generic word of some sort (for example, "head", "thing"), whose purpose is simply to indicate which of the possible meanings of the other, homophonic syllable should be selected.
However, when one of the above words forms part of a compound, the disambiguating syllable is generally dropped and the resulting word is still disyllabic. For example, 石 "shí" alone, not 石头 "shítou", appears in compounds meaning "stone-", for example, 石膏 "shígāo" "plaster" (lit. "stone cream"), 石灰 "shíhuī" "lime" (lit. "stone dust"), 石窟 "shíkū" "grotto" (lit. "stone cave"), 石英 "shíyīng" "quartz" (lit. "stone flower"), 石油 "shíyóu" "petroleum" (lit. "stone oil").
Most modern varieties of Chinese have the tendency to form new words through disyllabic, trisyllabic and tetra-character compounds. In some cases, monosyllabic words have become disyllabic without compounding, as in 窟窿 "kūlong" from 孔 "kǒng"; this is especially common in Jin.
Chinese morphology is strictly bound to a set number of syllables with a fairly rigid construction. Although many of these single-syllable morphemes (字, "zì") can stand alone as individual words, they more often than not form multi-syllabic compounds, known as "cí" (词／詞), which more closely resembles the traditional Western notion of a word. A Chinese "cí" (“word”) can consist of more than one character-morpheme, usually two, but there can be three or more.
For example:
All varieties of modern Chinese are analytic languages, in that they depend on syntax (word order and sentence structure) rather than morphology—i.e., changes in form of a word—to indicate the word's function in a sentence. In other words, Chinese has very few grammatical inflections—it possesses no tenses, no voices, no numbers (singular, plural; though there are plural markers, for example for personal pronouns), and only a few articles (i.e., equivalents to "the, a, an" in English).
They make heavy use of grammatical particles to indicate aspect and mood. In Mandarin Chinese, this involves the use of particles like "le" 了 (perfective), "hái" 还／還 ("still"), "yǐjīng" 已经／已經 ("already"), and so on.
Chinese has a subject–verb–object word order, and like many other languages in East Asia, makes frequent use of the topic–comment construction to form sentences. Chinese also has an extensive system of classifiers and measure words, another trait shared with neighboring languages like Japanese and Korean. Other notable grammatical features common to all the spoken varieties of Chinese include the use of serial verb construction, pronoun dropping and the related subject dropping.
Although the grammars of the spoken varieties share many traits, they do possess differences.
Vocabulary.
The entire Chinese character corpus since antiquity comprises well over 20,000 characters, of which only roughly 10,000 are now commonly in use. However Chinese characters should not be confused with Chinese words. Because most Chinese words are made up of two or more characters, there are many times more Chinese words than there are characters. A better term for a Chinese character would be morpheme, as characters represent the smallest grammatical units, individual meanings, and/or syllables in the Chinese language.
Estimates of the total number of Chinese words and phrases vary greatly. The "Hanyu Da Zidian", a compendium of Chinese characters, includes 54,678 head entries for characters, including bone oracle versions. The "Zhonghua Zihai" (1994) contains 85,568 head entries for character definitions, and is the largest reference work based purely on character and its literary variants. The CC-CEDICT project (2010) contains 97,404 contemporary entries including idioms, technology terms and names of political figures, businesses and products. The 2009 version of the Webster's Digital Chinese Dictionary (WDCD), based on CC-CEDICT, contains over 84,000 entries.
The most comprehensive pure linguistic Chinese-language dictionary, the 12-volumed "Hanyu Da Cidian", records more than 23,000 head Chinese characters and gives over 370,000 definitions. The 1999 revised "Cihai", a multi-volume encyclopedic dictionary reference work, gives 122,836 vocabulary entry definitions under 19,485 Chinese characters, including proper names, phrases and common zoological, geographical, sociological, scientific and technical terms.
The latest 2012 6th edition of "Xiandai Hanyu Cidian", an authoritative one-volume dictionary on modern standard Chinese language as used in mainland China, has 69,000 entries and defines 13,000 head characters.
Loanwords.
Like any other language, Chinese has absorbed a sizable number of loanwords from other cultures. Most Chinese words are formed out of native Chinese morphemes, including words describing imported objects and ideas. However, direct phonetic borrowing of foreign words has gone on since ancient times.
Some early Indo-European loanwords in Chinese have been proposed, notably "mì" "honey", "shī" "lion," and perhaps also "mǎ" "horse", "zhū" "pig", 犬 "quǎn" "dog", and "é" "goose".
Ancient words borrowed from along the Silk Road since Old Chinese include 葡萄 "pútáo" "grape", 石榴 "shíliu"/"shíliú" "pomegranate" and 狮子／獅子 "shīzi" "lion". Some words were borrowed from Buddhist scriptures, including 佛 "Fó" "Buddha" and 菩萨／菩薩 "Púsà" "bodhisattva." Other words came from nomadic peoples to the north, such as 胡同 "hútòng" "hutong". Words borrowed from the peoples along the Silk Road, such as 葡萄 "grape," generally have Persian etymologies. Buddhist terminology is generally derived from Sanskrit or Pāli, the liturgical languages of North India. Words borrowed from the nomadic tribes of the Gobi, Mongolian or northeast regions generally have Altaic etymologies, such as 琵琶 "pípá", the Chinese lute, or 酪 "lào"/"luò" "cheese" or "yoghurt", but from exactly which source is not always clear.
Modern borrowings and loanwords.
Modern neologisms are primarily translated into Chinese in one of three ways: free translation ("calque", or by meaning), phonetic translation (by sound), or a combination of the two. Today, it is much more common to use existing Chinese morphemes to coin new words in order to represent imported concepts, such as technical expressions and international scientific vocabulary. Any Latin or Greek etymologies are dropped and converted into the corresponding Chinese characters (for example, "anti-" typically becomes "反", literally "opposite"), making them more comprehensible for Chinese but introducing more difficulties in understanding foreign texts. For example, the word "telephone" was loaned phonetically as 德律风／德律風 (Shanghainese: "télífon" , Mandarin: "délǜfēng") during the 1920s and widely used in Shanghai, but later 电话／電話 "diànhuà" (lit. "electric speech"), built out of native Chinese morphemes, became prevalent (電話 is in fact from the Japanese "denwa"; see below for more Japanese loans). Other examples include 电视／電視 "diànshì" (lit. "electric vision") for television, 电脑／電腦 "diànnǎo" (lit. "electric brain") for computer; 手机／手機 "shǒujī" (lit. "hand machine") for mobile phone, 蓝牙／藍牙 "lányá" (lit. "blue tooth") for Bluetooth, and 网志/網誌 "wǎngzhì" (lit. "internet logbook") for blog in Hong Kong and Macau Cantonese. Occasionally half-transliteration, half-translation compromises (phono-semantic matching) are accepted, such as 汉堡包／漢堡包 "hànbǎobāo" (漢堡 "hànbǎo" "Hamburg" + 包 "bāo" "bun") for "hamburger". Sometimes translations are designed so that they sound like the original while incorporating Chinese morphemes, such as 拖拉机／拖拉機 "tuōlājī" "tractor" (lit. "dragging-pulling machine"), or 马利奥／馬利奧 Mǎlì'ào for the video game character Mario. This is often done for commercial purposes, for example 奔腾／奔騰 "bēnténg" (lit. "dashing-leaping") for Pentium and 赛百味／賽百味 "Sàibǎiwèi" (lit. "better-than hundred tastes") for Subway restaurants.
Foreign words, mainly proper nouns, continue to enter the Chinese language by transcription according to their pronunciations. This is done by employing Chinese characters with similar pronunciations. For example, "Israel" becomes 以色列 "Yǐsèliè", "Paris" becomes 巴黎 "Bālí". A rather small number of direct transliterations have survived as common words, including 沙发／沙發 "shāfā" "sofa", 马达／馬達 "mǎdá" "motor", 幽默 "yōumò" "humor", 逻辑／邏輯 "luóji"/"luójí" "logic", 时髦／時髦 "shímáo" "smart, fashionable", and 歇斯底里 "xiēsīdǐlǐ" "hysterics". The bulk of these words were originally coined in the Shanghai dialect during the early 20th century and were later loaned into Mandarin, hence their pronunciations in Mandarin may be quite off from the English. For example, 沙发／沙發 "sofa" and 马达／馬達 "motor" in Shanghainese sound more like their English counterparts. Cantonese differs from Mandarin with some transliterations, such as 梳化 "so faa" "sofa" and 摩打 "mo daa" "motor".
Western foreign words representing Western concepts have influenced Chinese since the 20th century through transcription. From French came 芭蕾 "bāléi" "ballet" and 香槟 "xiāngbīn", "champagne"; from Italian, 咖啡 "kāfēi" "caffè". English influence is particularly pronounced. From early 20th century Shanghainese, many English words are borrowed, such as 高尔夫／高爾夫 "gāoěrfū" "golf" and the above-mentioned 沙发／沙發 "shāfā" "sofa". Later, the United States soft influences gave rise to 迪斯科 "dísikē"/"dísīkē" "disco", 可乐／可樂 "kělè" "cola", and 迷你 "mínǐ" "mini kir". Contemporary colloquial Cantonese has distinct loanwords from English, such as 卡通 "kaa tung" "cartoon", 基佬 "gei lou" "gay people", 的士 "dik si" "taxi", and 巴士 "baa si" "bus". With the rising popularity of the Internet, there is a current vogue in China for coining English transliterations, for example, 粉丝／粉絲 "fěnsī" "fans", 黑客 "hēikè" "hacker" (lit. "black guest"), and 博客 "bókè" "blog". In Taiwan, some of these transliterations are different, such as 駭客 "hàikè" for "hacker" and 部落格 "bùluògé" for "blog" (lit. "interconnected tribes").
Another result of the English influence on Chinese is the appearance in Modern Chinese texts of so-called 字母词／字母詞 "zìmǔcí" (lit. "lettered words") spelled with letters from the English alphabet. This has appeared in magazines, newspapers, on web sites, and on TV: 三G手机／三G手機 "3rd generation cell phones" (三 "sān" "three" + G "generation" + 手机／手機 "shǒujī" "mobile phones"), IT界 "IT circles" (IT "information technology" + 界 "jiè" "industry"), HSK ("Hànyǔ Shuǐpíng Kǎoshì", 汉语水平考试／漢語水平考試), GB ("Guóbiāo", 国标／國標), CIF价／CIF價 (CIF "Cost, Insurance, Freight" + 价／價 "jià" "price"), e家庭 "e-home" (e "electronic" + 家庭 "jiātíng" "home"), W时代／W時代 "wireless era" (W "wireless" + 时代／時代 "shídài" "era"), TV族 "TV watchers" (TV "television" + 族 "zú" "social group; clan"), 后РС时代／後PC時代 "post-PC era" (后／後 "hòu" "after/post-" + PC "personal computer" + 时代／時代), and so on.
Since the 20th century, another source of words has been Japanese using existing kanji (Chinese characters used in Japanese). Japanese re-molded European concepts and inventions into , and many of these words have been re-loaned into modern Chinese. Other terms were coined by the Japanese by giving new senses to existing Chinese terms or by referring to expressions used in classical Chinese literature. For example, "jīngjì" (经济／經濟; "keizai" in Japanese), which in the original Chinese meant "the workings of the state", was narrowed to "economy" in Japanese; this narrowed definition was then re-imported into Chinese. As a result, these terms are virtually indistinguishable from native Chinese words: indeed, there is some dispute over some of these terms as to whether the Japanese or Chinese coined them first. As a result of this loaning, Chinese, Korean, Japanese, and Vietnamese share a corpus of linguistic terms describing modern terminology, paralleling the similar corpus of terms built from Greco-Latin and shared among European languages.
Education.
With the growing importance and influence of China's economy globally, Mandarin instruction is gaining popularity in schools in the USA, and has become an increasingly popular subject of study amongst the young in the Western world, as in the UK.
In 1991 there were 2,000 foreign learners taking China's official Chinese Proficiency Test (also known as HSK, comparable to the English Cambridge Certificate), while in 2005, the number of candidates had risen sharply to 117,660. By 2010, 750,000 people had taken the Chinese Proficiency Test.

</doc>
<doc id="5759" url="https://en.wikipedia.org/wiki?curid=5759" title="Complex analysis">
Complex analysis

Complex analysis, traditionally known as the theory of functions of a complex variable, is the branch of mathematical analysis that investigates functions of complex numbers. It is useful in many branches of mathematics, including algebraic geometry, number theory, applied mathematics; as well as in physics, including hydrodynamics and thermodynamics and also in engineering fields such as nuclear, aerospace, mechanical and electrical engineering.
Murray R. Spiegel described complex analysis as "one of the most beautiful as well as useful branches of Mathematics".
Complex analysis is particularly concerned with analytic functions of complex variables (or, more generally, meromorphic functions). Because the separate real and imaginary parts of any analytic function must satisfy Laplace's equation, complex analysis is widely applicable to two-dimensional problems in physics.
History.
Complex analysis is one of the classical branches in mathematics with roots in the 19th century and just prior. Important mathematicians associated with complex analysis include Euler, Gauss, Riemann, Cauchy, Weierstrass, and many more in the 20th century. Complex analysis, in particular the theory of conformal mappings, has many physical applications and is also used throughout analytic number theory. In modern times, it has become very popular through a new boost from complex dynamics and the pictures of fractals produced by iterating holomorphic functions. Another important application of complex analysis is in string theory which studies conformal invariants in quantum field theory.
Complex functions.
A complex function is one in which the independent variable and the dependent variable are both complex numbers. More precisely, a complex function is a function whose domain and range are subsets of the complex plane.
For any complex function, both the independent variable and the dependent variable may be separated into real and imaginary parts:
In other words, the components of the function ,
can be interpreted as real-valued functions of the two real variables, and .
The basic concepts of complex analysis are often introduced by extending the elementary real functions (e.g., exponential functions, logarithmic functions, and trigonometric functions) into the complex domain.
Holomorphic functions.
Holomorphic functions are complex functions, defined on an open subset of the complex plane, that are differentiable. Complex differentiability has much stronger consequences than usual (real) differentiability. For instance, holomorphic functions are infinitely differentiable, whereas most real differentiable functions are not. Most elementary functions, including the exponential function, the trigonometric functions, and all polynomial functions, are holomorphic.
"See also": analytic function, holomorphic sheaf and vector bundles.
Major results.
One of the central tools in complex analysis is the line integral. The line integral around a closed path of a function that is holomorphic everywhere inside the area bounded by the closed path is always zero, which is what the Cauchy integral theorem states. The values of such a holomorphic function inside a disk can be computed by a path integral on the disk's boundary, as shown in (Cauchy's integral formula). Path integrals in the complex plane are often used to determine complicated real integrals, and here the theory of residues among others is applicable (see methods of contour integration). A "pole" (or isolated singularity) of a function is a point where the function's value becomes unbounded, or "blows up". If a function has such a pole, then one can compute the function's residue there, which can be used to compute path integrals involving the function; this is the content of the powerful residue theorem. The remarkable behavior of holomorphic functions near essential singularities is described by Picard's Theorem. Functions that have only poles but no essential singularities are called meromorphic. Laurent series are the complex-valued equivalent to Taylor series, but can be used to study the behavior of functions near singularities through infinite sums of more well understood functions, such as polynomials.
A bounded function that is holomorphic in the entire complex plane must be constant; this is Liouville's theorem. It can be used to provide a natural and short proof for the fundamental theorem of algebra which states that the field of complex numbers is algebraically closed.
If a function is holomorphic throughout a connected domain then its values are fully determined by its values on any smaller subdomain. The function on the larger domain is said to be analytically continued from its values on the smaller domain. This allows the extension of the definition of functions, such as the Riemann zeta function, which are initially defined in terms of infinite sums that converge only on limited domains to almost the entire complex plane. Sometimes, as in the case of the natural logarithm, it is impossible to analytically continue a holomorphic function to a non-simply connected domain in the complex plane but it is possible to extend it to a holomorphic function on a closely related surface known as a Riemann surface.
All this refers to complex analysis in one variable. There is also a very rich theory of complex analysis in more than one complex dimension in which the analytic properties such as power series expansion carry over whereas most of the geometric properties of holomorphic functions in one complex dimension (such as conformality) do not carry over. The Riemann mapping theorem about the conformal relationship of certain domains in the complex plane, which may be the most important result in the one-dimensional theory, fails dramatically in higher dimensions.

</doc>
<doc id="5760" url="https://en.wikipedia.org/wiki?curid=5760" title="History of China">
History of China

Written records of the history of China can be found from as early as 1500 BC under the Shang dynasty (c. 1600–1046 BC). Ancient historical texts such as the "Records of the Grand Historian" (ca. 100 BC) and the "Bamboo Annals" describe a Xia dynasty (c. 2070–1600 BC), which had no system of writing on a durable medium, before the Shang. The Yellow River is said to be the cradle of Chinese civilization, although cultures originated at various regional centers along both the Yellow River and the Yangtze River valleys millennia ago in the Neolithic era. With thousands of years of continuous history, China is one of the world's oldest civilizations, and is regarded as one of the cradles of civilization.
Much of Chinese culture, literature and philosophy further developed during the Zhou dynasty (1046–256 BC). The Zhou dynasty began to bow to external and internal pressures in the 8th century BC, and the kingdom eventually broke apart into smaller states, beginning in the Spring and Autumn period and reaching full expression in the Warring States period. This is one of multiple periods of failed statehood in Chinese history, the most recent being the Chinese Civil War that started in 1927.
Between eras of multiple kingdoms and warlordism, Chinese dynasties have ruled parts or all of China; in some eras control stretched as far as Xinjiang and Tibet, as at present. In 221 BC Qin Shi Huang united the various warring kingdoms and created for himself the title of "emperor" ("huangdi") of the Qin dynasty, marking the beginning of imperial China. Successive dynasties developed bureaucratic systems that enabled the emperor to control vast territories directly. China's last dynasty was the Qing (1644–1912), which was replaced by the Republic of China in 1912, and in the mainland by the People's Republic of China in 1949.
The conventional view of Chinese history is that of alternating periods of political unity and disunity, with China occasionally being dominated by steppe peoples, most of whom were in turn assimilated into the Han Chinese population. Cultural and political influences from other parts of Asia and the Western world, carried by successive waves of immigration, cultural assimilation, expansion and foreign contact, form the basis of the modern culture of China.
Prehistory.
Paleolithic.
What is now China was inhabited by "Homo erectus" more than a million years ago. Recent study shows that the stone tools found at Xiaochangliang site are magnetostratigraphically dated to 1.36 million years ago. The archaeological site of Xihoudu in Shanxi Province is the earliest recorded use of fire by "Homo erectus", which is dated 1.27 million years ago.
The excavations at Yuanmou and later Lantian show early habitation. Perhaps the most famous specimen of "Homo erectus" found in China is the so-called Peking Man discovered in 1923–27. Fossilised teeth of "Homo sapiens" dating to 125,000–80,000 BCE have been discovered in Fuyan Cave in Dao County in Hunan.
Neolithic.
The Neolithic age in China can be traced back to about 10,000 BC.
Early evidence for proto-Chinese millet agriculture is radiocarbon-dated to about 7000 BC. The earliest evidence of cultivated rice, found by the Yangtze River, is carbon-dated to 8,000 years ago. Farming gave rise to the Jiahu culture (7000 to 5800 BC). At Damaidi in Ningxia, 3,172 cliff carvings dating to 6000–5000 BC have been discovered, "featuring 8,453 individual characters such as the sun, moon, stars, gods and scenes of hunting or grazing." These pictographs are reputed to be similar to the earliest characters confirmed to be written Chinese. Chinese proto-writing existed in Jiahu around 7000 BC, Dadiwan from 5800 BC to 5400 BC, Damaidi around 6000 BC and Banpo dating from the 5th millennium BC. Some scholars have suggested that Jiahu symbols (7th millennium BC) were the earliest Chinese writing system. Excavation of a Peiligang culture site in Xinzheng county, Henan, found a community that flourished in 5,500 to 4,900 BC, with evidence of agriculture, constructed buildings, pottery, and burial of the dead. With agriculture came increased population, the ability to store and redistribute crops, and the potential to support specialist craftsmen and administrators. In late Neolithic times, the Yellow River valley began to establish itself as a center of Yangshao culture (5000 BC to 3000 BC), and the first villages were founded; the most archaeologically significant of these was found at Banpo, Xi'an. Later, Yangshao culture was superseded by the Longshan culture, which was also centered on the Yellow River from about 3000 BC to 2000 BC.
Bronze Age.
Bronze artifacts have been found at the Majiayao culture site (between 3100 and 2700 BC),The Bronze Age is also represented at the Lower Xiajiadian culture (2200–1600 BC) site in northeast China.
Ancient China.
Xia dynasty (c. 2100 – c. 1600 BC).
The Xia dynasty of China (from c. 2100 to c. 1600 BC) is the first dynasty to be described in ancient historical records such as Sima Qian's "Records of the Grand Historian" and "Bamboo Annals".
Although there is disagreement as to whether the dynasty actually existed, there is some archaeological evidence pointing to its possible existence. Writing in the late 2nd century BC, Sima Qian dated the founding of the Xia dynasty to around 2200 BC, but this date has not been corroborated. Most archaeologists now connect the Xia to excavations at Erlitou in central Henan province, where a bronze smelter from around 2000 BC was unearthed. Early markings from this period found on pottery and shells are thought to be ancestral to modern Chinese characters. With few clear records matching the Shang oracle bones or the Zhou bronze vessel writings, the Xia era remains poorly understood.
According to mythology, the dynasty ended around 1600 BC as a consequence of the Battle of Mingtiao.
Shang dynasty (c. 1600–1046 BC).
Archaeological findings providing evidence for the existence of the Shang dynasty, c. 1600–1046 BC, are divided into two sets. The first set, from the earlier Shang period, comes from sources at Erligang, Zhengzhou, and Shangcheng. The second set, from the later Shang or Yin (殷) period, is at Anyang, in modern-day Henan, which has been confirmed as the last of the Shang's nine capitals (c. 1300–1046 BC). The findings at Anyang include the earliest written record of Chinese past so far discovered: inscriptions of divination records in ancient Chinese writing on the bones or shells of animals — the so-called "oracle bones", dating from around 1500 BC. 
31 kings reigned over the Shang dynasty. During their reign, according to the Records of the Grand Historian, the capital city was moved six times. The final (and most important) move was to Yin in 1350 BC which led to the dynasty's golden age. The term Yin dynasty has been synonymous with the Shang dynasty in history, although it has lately been used to refer specifically to the latter half of the Shang dynasty.
Chinese historians living in later periods were accustomed to the notion of one dynasty succeeding another, but the actual political situation in early China is known to have been much more complicated. Hence, as some scholars of China suggest, the Xia and the Shang can possibly refer to political entities that existed concurrently, just as the early Zhou is known to have existed at the same time as the Shang.
Although written records found at Anyang confirm the existence of the Shang dynasty, Western scholars are often hesitant to associate settlements that are contemporaneous with the Anyang settlement with the Shang dynasty. For example, archaeological findings at Sanxingdui suggest a technologically advanced civilization culturally unlike Anyang. The evidence is inconclusive in proving how far the Shang realm extended from Anyang. The leading hypothesis is that Anyang, ruled by the same Shang in the official history, coexisted and traded with numerous other culturally diverse settlements in the area that is now referred to as China proper.
Zhou dynasty (1046–256 BC).
The Zhou dynasty (1046 BC to approximately 256 BC) was the longest-lasting dynasty in Chinese history. By the end of the 2nd millennium BC, the Zhou dynasty began to emerge in the Yellow River valley, overrunning the territory of the Shang. The Zhou appeared to have begun their rule under a semi-feudal system. The Zhou lived west of the Shang, and the Zhou leader had been appointed Western Protector by the Shang. The ruler of the Zhou, King Wu, with the assistance of his brother, the Duke of Zhou, as regent, managed to defeat the Shang at the Battle of Muye.
The king of Zhou at this time invoked the concept of the Mandate of Heaven to legitimize his rule, a concept that would be influential for almost every succeeding dynasty. Like Shangdi, Heaven ("tian") ruled over all the other gods, and it decided who would rule China. It was believed that a ruler had lost the Mandate of Heaven when natural disasters occurred in great number, and when, more realistically, the sovereign had apparently lost his concern for the people. In response, the royal house would be overthrown, and a new house would rule, having been granted the Mandate of Heaven.
The Zhou initially moved their capital west to an area near modern Xi'an, on the Wei River, a tributary of the Yellow River, but they would preside over a series of expansions into the Yangtze River valley. This would be the first of many population migrations from north to south in Chinese history.
Spring and Autumn period (722–476 BC).
In the 8th century BC, power became decentralized during the Spring and Autumn period, named after the influential Spring and Autumn Annals. In this period, local military leaders used by the Zhou began to assert their power and vie for hegemony. The situation was aggravated by the invasion of other peoples from the northwest, such as the Qin, forcing the Zhou to move their capital east to Luoyang. This marks the second major phase of the Zhou dynasty: the Eastern Zhou. The Spring and Autumn period is marked by a falling apart of the central Zhou power. In each of the hundreds of states that eventually arose, local strongmen held most of the political power and continued their subservience to the Zhou kings in name only. Some local leaders even started using royal titles for themselves. China now consisted of hundreds of states, some of them only as large as a village with a fort.
The Hundred Schools of Thought of Chinese philosophy blossomed during this period, and such influential intellectual movements as Confucianism, Taoism, Legalism and Mohism were founded, partly in response to the changing political world.
Warring States period (476–221 BC).
After further political consolidation, seven prominent states remained by the end of 5th century BC, and the years in which these few states battled each other are known as the Warring States period. Though there remained a nominal Zhou king until 256 BC, he was largely a figurehead and held little real power.
As neighboring territories of these warring states, including areas of modern Sichuan and Liaoning, were annexed, they were governed under the new local administrative system of commandery and prefecture (郡縣/郡县). This system had been in use since the Spring and Autumn period, and parts can still be seen in the modern system of Sheng & Xian (province and county, 省縣/省县).
The final expansion in this period began during the reign of Ying Zheng, the king of Qin. His unification of the other six powers, and further annexations in the modern regions of Zhejiang, Fujian, Guangdong and Guangxi in 214 BC, enabled him to proclaim himself the First Emperor (Qin Shi Huang).
Imperial China.
Qin dynasty (221–206 BC).
Historians often refer to the period from Qin dynasty to the end of Qing dynasty as Imperial China. Though the unified reign of the First Qin Emperor lasted only 12 years, he managed to subdue great parts of what constitutes the core of the Han Chinese homeland and to unite them under a tightly centralized Legalist government seated at Xianyang (close to modern Xi'an). The doctrine of Legalism that guided the Qin emphasized strict adherence to a legal code and the absolute power of the emperor. This philosophy, while effective for expanding the empire in a military fashion, proved unworkable for governing it in peacetime. The Qin Emperor presided over the brutal silencing of political opposition, including the event known as the burning of books and burying of scholars. This would be the impetus behind the later Han synthesis incorporating the more moderate schools of political governance.
Construction of the Great Wall of China, still extant and now a UNESCO World Heritage Site, started during the Qin dynasty; it was later augmented and improved during the Ming dynasty. The other major contributions of the Qin include the concept of a centralized government, the unification of the legal code, development of the written language, measurement, and currency of China after the tribulations of the Spring and Autumn and Warring States periods. Even something as basic as the length of axles for carts—which need to match ruts in the roads—had to be made uniform to ensure a viable trading system throughout the empire.
Han dynasty (202 BC–AD 220).
Western Han.
The Han dynasty was founded by Liu Bang, who emerged victorious in the civil war that followed the collapse of the unified but short-lived Qin dynasty. A golden age in Chinese history, the Han dynasty's long period of stability and prosperity consolidated the foundation of China as a unified state under a central imperial bureaucracy, which was to last intermittently for most of the next two millennium. During the Han dynasty, territory of China was extended to most of the China proper and to areas far west. Confucianism was officially elevated to orthodox status and was to shape the subsequent Chinese Civilization. Art, culture and science all advanced to unprecedented heights. With the profound and lasting impacts of this period of Chinese history, the dynasty name "Han" had been taken as the name of the Chinese people, now the dominant ethnic group in modern China, and had been commonly used to refer to Chinese language and written characters.
After the initial Laissez-faire policies of Emperors Wen and Jing, the ambitious Emperor Wu brought the empire to its zenith. To consolidate his power, Confucianism, which emphasizes stability and order in a well-structured society, was given exclusive patronage to be the guiding philosophical thoughts and moral principles of the empire. Imperial Universities were established to support its study and further development, while other schools of thoughts were discouraged.
Major military campaigns were launched to weaken the nomadic Xiongnu Empire, limiting their influence north of the Great Wall. Along with the diplomatic efforts led by Zhang Qian, the sphere of influence of the Han Empire extended to the states in the Tarim Basin, opened up the Silk Road that connected China to the west, stimulating bilateral trade and cultural exchange. To the south, various small kingdoms far beyond the Yangtze River Valley were formally incorporated into the empire.
Emperor Wu also dispatched a series of military campaigns against the Baiyue tribes. The Han annexed Minyue in 135 BC and 111 BC, Nanyue in 111 BC, and Dian in 109 BC. Migration and military expeditions led to the cultural assimilation of the south. It also brought the Han into contact with kingdoms in Southeast Asia, introducing diplomacy and trade.
After Emperor Wu, the empire slipped into gradual stagnation and decline. Economically, the state treasury was strained by excessive campaigns and projects, while land acquisitions by elite families gradually drained the tax base. Various consort clans exerted increasing control over strings of incompetent emperors and eventually the dynasty was briefly interrupted by the usurpation of Wang Mang.
Xin dynasty.
In AD 9, the usurper Wang Mang claimed that the Mandate of Heaven called for the end of the Han dynasty and the rise of his own, and he founded the short-lived Xin ("New") dynasty. Wang Mang started an extensive program of land and other economic reforms, including the outlawing of slavery and land nationalization and redistribution. These programs, however, were never supported by the landholding families, because they favored the peasants. The instability of power brought about chaos, uprisings, and loss of territories. This was compounded by mass flooding of the Yellow River; silt buildup caused it to split into two channels and displaced large numbers of farmers. Wang Mang was eventually killed in Weiyang Palace by an enraged peasant mob in AD 23.
Eastern Han.
Emperor Guangwu reinstated the Han dynasty with the support of landholding and merchant families at Luoyang, "east" of the former capital Xi'an. Thus, this new era is termed the Eastern Han dynasty. With the capable administrations of Emperors Ming and Zhang, former glories of the dynasty was reclaimed, with brilliant military and cultural achievements. The Xiongnu Empire was decisively defeated. The diplomat and general Ban Chao further expanded the conquests across the Pamirs to the shores of the Caspian Sea, thus reopening the Silk Road, and bringing trade, foreign cultures, along with the arrival of Buddhism. With extensive connections with the west, the first of several Roman embassies to China were recorded in Chinese sources, coming from the sea route in AD 166, and a second one in AD 284.
The Eastern Han dynasty was one of the most prolific era of science and technology in ancient China, notably the historic invention of papermaking by Cai Lun, and the numerous contributions by the polymath Zhang Heng.
Three Kingdoms and Western Jin (AD 265–316).
By the 2nd century, the empire declined amidst land acquisitions, invasions, and feuding between consort clans and eunuchs. The Yellow Turban Rebellion broke out in AD 184, ushering in an era of warlords. In the ensuing turmoil, three states tried to gain predominance in the period of the Three Kingdoms. This time period has been greatly romanticized in works such as "Romance of the Three Kingdoms".
After Cao Cao reunified the north in 208, his son proclaimed the Wei dynasty in 220. Soon, Wei's rivals Shu and Wu proclaimed their independence, leading China into the Three Kingdoms period. This period was characterized by a gradual decentralization of the state that had existed during the Qin and Han dynasties, and an increase in the power of great families.
In 280, the Jin dynasty reunified the country, but this union was short-lived.
Sixteen Kingdoms and Eastern Jin (AD 304–439).
The Jin Dynasty was severely weakened by interceine fighting among imperial princes and lost control of northern China after non-Han Chinese settlers rebelled and captured Luoyang and Chang’an. In 317, a Jin prince in modern-day Nanjing became emperor and continued the dynasty, now known as the Eastern Jin, which held southern China for another century. Prior to this move, historians refer to the Jin dynasty as the Western Jin.
Northern China fragmented into a series of independent kingdoms, most of which were founded by Xiongnu, Xianbei, Jie, Di and Qiang rulers. These non-Han peoples were ancestors of the Turks, Mongols, and Tibetans. Many had, to some extent, been "sinicized" long before their ascent to power. In fact, some of them, notably the Qiang and the Xiongnu, had already been allowed to live in the frontier regions within the Great Wall since late Han times. During the period of the Sixteen Kingdoms, warfare ravaged the north and prompted large-scale Han Chinese migration south to the Yangtze Basin and Delta.
Northern and Southern dynasties (AD 420–589).
In the early 5th century, China entered a period known as the Northern and Southern dynasties, in which parallel regimes ruled the northern and southern halves of the country. In the south, the Eastern Jin gave way to the Liu Song, Southern Qi, Liang and finally Chen. Each of these Southern Dynasties were led by Han Chinese ruling families and used Jiankang (modern Nanjing) as the capital. They held off attacks from the north and preserved many aspects of Chinese civilization, while northern barbarian regimes began to sinify.
In the north, the last of the Sixteen Kingdoms was extinguished in 439 by the Northern Wei, a kingdom founded by the Xianbei, a nomadic people who unified northern China. The Northern Wei eventually split into the Eastern and Western Wei, which then became the Northern Qi and Northern Zhou. These regimes were dominated by Xianbei or Han Chinese who had married into Xianbei families.
Despite the division of the country, Buddhism spread throughout the land. In southern China, fierce debates about whether Buddhism should be allowed were held frequently by the royal court and nobles. Finally, towards the end of the Southern and Northern Dynasties era, Buddhists and Taoists reached a compromise and became more tolerant of each other.
In 589, the Sui dynasty united China once again, ending a prolonged period of division in Chinese history. In the nearly four centuries between the Han and Sui dynasties, the country was united for only 24 years during the Western Jin.
Sui dynasty (AD 589–618).
The Sui dynasty, which lasted 29 years, played a role more important than its length of existence would suggest. The Sui brought China together again and set up many institutions that were to be adopted by their successors, the Tang. These included the government system of Three Departments and Six Ministries, standard coinage, improved defense and expansion of the Great Wall, and official support for Buddhism. Like the Qin, however, the Sui overused their resources and collapsed.
Tang dynasty (AD 618–907).
The Tang dynasty was founded by Emperor Gaozu on 18 June 618. It was a golden age of Chinese civilization with significant developments in art, literature, particularly poetry, and technology. Buddhism became the predominant religion for common people. Chang'an (modern Xi'an), the national capital, was the largest city in the world of its time.
The second emperor, Taizong, started military campaigns to eliminate threats from nomadic tribes, extend the border, and submit neighboring states into a tributary system. Military victories in the Tarim Basin kept the Silk Road open, connecting Chang'an to Central Asia and areas far to the west. In the south, lucrative maritime trade routes began from port cities such as Guangzhou. There was extensive trade with distant foreign countries, and many foreign merchants settled in China, encouraging a cosmopolitan culture. The Tang culture and social systems were observed and imitated by neighboring countries such as Japan. Internally the Grand Canal linked the political heartland in Chang'an to the economic and agricultural centers in the eastern and southern parts of the empire.
Underlying the prosperity of the early Tang dynasty was a strong centralized bureaucracy with efficient policies. The government was organized as "Three Departments and Six Ministries" to separately draft, review, and implement policies. These departments were run by royal family members as well as scholar officials who were selected by imperial examinations. These practices, which matured in the Tang dynasty, were continued by the later dynasties, with some modifications.
Under the Tang "equal-field system" all land was owned by the Emperor and granted to people according to household size. Men granted land were conscripted for military service for a fixed period each year, a military policy known as the "Fubing system". These policies stimulated a rapid growth in productivity and a significant army without much burden on the state treasury. By the dynasty's midpoint, however, standing armies had replaced conscription, and land was continuously falling into the hands of private owners.
The dynasty continued to flourish under Empress Wu Zetian, the only empress regnant in Chinese history, and reached its zenith during the reign of Emperor Xuanzong, who oversaw an empire that stretched from the Pacific to the Aral Sea with at least 50 million people.
At the zenith of prosperity of the empire, the An Lushan Rebellion from 755 to 763 was a watershed event that devastated the population and drastically weakened the central imperial government. Regional military governors, known as Jiedushi, gained increasingly autonomous status while formerly submissive states raided the empire. Nevertheless, after the An Lushan Rebellion, the Tang civil society recovered and thrived amidst the weakened imperial bureaucracy.
From about 860, the Tang dynasty declined due to a series of rebellions within China itself and in the former subject Kingdom of Nanzhao to the south. One warlord, Huang Chao, captured Guangzhou in 879, killing most of the 200,000 inhabitants, including most of the large colony of foreign merchant families there. In late 880, Luoyang surrendered to Huang Chao, and on 5 January 881 he conquered Chang'an. The emperor Xizong fled to Chengdu, and Huang established a new temporary regime which was eventually destroyed by Tang forces. Another time of political chaos followed.
Five Dynasties and Ten Kingdoms (AD 907–960).
The period of political disunity between the Tang and the Song, known as the Five Dynasties and Ten Kingdoms period, lasted from 907 to 960. During this half-century, when China was in all respects a multi-state system, five regimes rapidly succeeded one another in control of the old Imperial heartland in northern China. During this same time, sections of southern and western China were occupied by ten, more stable, regimes so the period is also referred to as the Ten Kingdoms.
Song, Liao, Jin, and Western Xia dynasties (AD 960–1234).
In 960, the Song dynasty gained power over most of China and established its capital in Kaifeng (later known as Bianjing), starting a period of economic prosperity, while the Khitan Liao dynasty ruled over Manchuria, present-day Mongolia, and parts of Northern China. In 1115, the Jurchen Jin dynasty emerged to prominence, annihilating the Liao dynasty in 10 years. Meanwhile, in what are now the northwestern Chinese provinces of Gansu, Shaanxi, and Ningxia, a Western Xia dynasty emerged from 1032 to 1227, established by Tangut tribes.
The Jin dynasty took power and conquered northern China in the Jin–Song Wars, capturing Kaifeng from the Song dynasty, which moved its capital to Hangzhou (杭州). The Southern Song dynasty had to acknowledge the Jin dynasty as formal overlords. In the ensuing years, China was divided between the Song dynasty, the Jin dynasty and the Tangut Western Xia. Southern Song experienced a period of great technological development which can be explained in part by the military pressure that it felt from the north. This included the use of gunpowder weapons, which played a large role in the Song dynasty naval victories against the Jin in the Battle of Tangdao and Battle of Caishi on the Yangtze River in 1161. China's first permanent standing navy was assembled and provided an admiral's office at Dinghai in 1132, under the reign of Emperor Renzong of Song.
The Song dynasty is considered by many to be classical China's high point in science and technology, with innovative scholar-officials such as Su Song (1020–1101) and Shen Kuo (1031–1095). There was court intrigue between the political rivals of the Reformers and Conservatives, led by the chancellors Wang Anshi and Sima Guang, respectively. By the mid-to-late 13th century, the Chinese had adopted the dogma of Neo-Confucian philosophy formulated by Zhu Xi. Enormous literary works were compiled during the Song dynasty, such as the historical work of the "Zizhi Tongjian" ("Comprehensive Mirror to Aid in Government"). Culture and the arts flourished, with grandiose artworks such as "Along the River During the Qingming Festival" and "Eighteen Songs of a Nomad Flute", along with great Buddhist painters such as the prolific Lin Tinggui.
Yuan dynasty (AD 1271–1368).
The Jurchen-founded Jin dynasty was defeated by the Mongols, who then proceeded to defeat the Southern Song in a long and bloody war, the first war in which firearms played an important role. During the era after the war, later called the "Pax Mongolica", adventurous Westerners such as Marco Polo travelled all the way to China and brought the first reports of its wonders to Europe. In the Yuan dynasty, the Mongols were divided between those who wanted to remain based in the steppes and those who wished to adopt the customs of the Chinese.
Kublai Khan, grandson of Genghis Khan, wanting to adopt the customs of China, established the Yuan dynasty. This was the first dynasty to rule the whole of China from Beijing as the capital. Beijing had been ceded to Liao in AD 938 with the Sixteen Prefectures of Yan Yun. Before that, it had been the capital of the Jin, who did not rule all of China.
Before the Mongol invasion, Chinese dynasties reported approximately 120 million inhabitants; after the conquest had been completed in 1279, the 1300 census reported roughly 60 million people. This major decline is not necessarily due only to Mongol killings. Scholars such as Frederick W. Mote argue that the wide drop in numbers reflects an administrative failure to record rather than an actual decrease; others such as Timothy Brook argue that the Mongols created a system of enserfment among a huge portion of the Chinese populace, causing many to disappear from the census altogether; other historians including William McNeill and David Morgan consider that plague was the main factor behind the demographic decline during this period.
In the 14th century China suffered additional depredations from epidemics of plague, estimated to have killed 25 million people, 30% of the population of China.
Ming dynasty (AD 1368–1644).
Throughout the Yuan dynasty, which lasted less than a century, there was relatively strong sentiment among the populace against Mongol rule. The frequent natural disasters since the 1340s finally led to peasant revolts. The Yuan dynasty was eventually overthrown by the Ming dynasty in 1368.
Urbanization increased as the population grew and as the division of labor grew more complex. Large urban centers, such as Nanjing and Beijing, also contributed to the growth of private industry. In particular, small-scale industries grew up, often specializing in paper, silk, cotton, and porcelain goods. For the most part, however, relatively small urban centers with markets proliferated around the country. Town markets mainly traded food, with some necessary manufactures such as pins or oil.
Despite the xenophobia and intellectual introspection characteristic of the increasingly popular new school of neo-Confucianism, China under the early Ming dynasty was not isolated. Foreign trade and other contacts with the outside world, particularly Japan, increased considerably. Chinese merchants explored all of the Indian Ocean, reaching East Africa with the voyages of Zheng He.
Zhu Yuanzhang (the Hongwu Emperor), the founder of the dynasty, laid the foundations for a state interested less in commerce and more in extracting revenues from the agricultural sector. Perhaps because of Zhu's background as a peasant, the Ming economic system emphasized agriculture, unlike that of the Song and the Mongolian dynasties, which relied on traders and merchants for revenue. Neo-feudal landholdings of the Song and Mongol periods were expropriated by the Ming rulers. Land estates were confiscated by the government, fragmented, and rented out. Private slavery was forbidden. Consequently, after the death of the Yongle Emperor, independent peasant landholders predominated in Chinese agriculture. These laws might have paved the way to removing the worst of the poverty during the previous regimes.
The dynasty had a strong and complex central government that unified and controlled the empire. The emperor's role became more autocratic, although Zhu Yuanzhang necessarily continued to use what he called the "Grand Secretaries" (内阁) to assist with the immense paperwork of the bureaucracy, including memorials (petitions and recommendations to the throne), imperial edicts in reply, reports of various kinds, and tax records. It was this same bureaucracy that later prevented the Ming government from being able to adapt to changes in society, and eventually led to its decline.
The Yongle Emperor strenuously tried to extend China's influence beyond its borders by demanding other rulers send ambassadors to China to present tribute. A large navy was built, including four-masted ships displacing 1,500 tons. A standing army of 1 million troops (some estimate as many as 1.9 million ) was created. The Chinese armies conquered Vietnam for around 20 years, while the Chinese fleet sailed the China seas and the Indian Ocean, cruising as far as the east coast of Africa. The Chinese gained influence in eastern Moghulistan. Several maritime Asian nations sent envoys with tribute for the Chinese emperor. Domestically, the Grand Canal was expanded and became a stimulus to domestic trade. Over 100,000 tons of iron per year were produced. Many books were printed using movable type. The imperial palace in Beijing's Forbidden City reached its current splendor. It was also during these centuries that the potential of south China came to be fully exploited. New crops were widely cultivated and industries such as those producing porcelain and textiles flourished.
In 1449 Esen Tayisi led an Oirat Mongol invasion of northern China which culminated in the capture of the Zhengtong Emperor at Tumu.
In 1521, Ming dynasty naval forces fought and repulsed Portuguese ships at Tuen Mun and again fought off the Portuguese in 1522.
In 1542 the Mongol leader, Altan Khan, began to harass China along the northern border, reaching the outskirts of Beijing in 1550. The empire also had to deal with wokou pirates attacking the southeastern coastline; General Qi Jiguang was instrumental in their defeat. In 1556, during the rule of the Jiajing Emperor, the Shaanxi earthquake killed about 830,000 people, the deadliest earthquake of all time.
During the Ming dynasty, the last construction on the Great Wall was undertaken to protect China from foreign invasions. Most of what remains of the Wall in modern times was either built or repaired by the Ming. The brick and granite work was enlarged, the watch towers were redesigned, and cannons were placed along its length.
China defeated the Dutch in the Sino–Dutch conflicts in 1622–1624 over the Penghu islands and again defeated the Dutch at the Battle of Liaoluo Bay in 1633. The Ming loyalist Koxinga (Zheng Chenggong) defeated the Dutch in the Siege of Fort Zeelandia in Taiwan in 1662.
Qing dynasty (AD 1644–1911).
The Qing dynasty (1644–1911) was the last imperial dynasty in China. Founded by the Manchus, it was the second non-Han Chinese dynasty to rule all over Chinese territory. The Manchus were formerly known as Jurchens, residing in the northeastern part of the Ming territory outside the Great Wall. They emerged as the major threat to the late Ming dynasty after Nurhaci united all Jurchen tribes and established an independent state. However, the Ming dynasty would be overthrown by Li Zicheng's peasants rebellion, with Beijing captured in 1644 and the Chongzhen Emperor, the last Ming emperor, committing suicide. The Manchus allied with the former Ming general Wu Sangui to seize Beijing, which was made the capital of the Qing dynasty, and then proceeded to subdue the Ming remnants in the south. The decades of Manchu conquest caused enormous loss of lives and the economic scale of China shrank drastically. In total, the Qing conquest of the Ming (1618–1683) cost as many as 25 million lives. Nevertheless, the Manchus adopted the Confucian norms of traditional Chinese government in their rule and were considered a Chinese dynasty.
The Manchus enforced a 'queue order,' forcing the Han Chinese to adopt the Manchu queue hairstyle. Officials were required to wear Manchu-style clothing "Changshan" (bannermen dress and "Tangzhuang"), but ordinary Han civilians were allowed to wear traditional Han clothing, or "Hanfu". Most Han then voluntarily shifted to wearing Qipao anyway. The Kangxi Emperor ordered the creation of the "Kangxi Dictionary", the most complete dictionary of Chinese characters that had been compiled. The Qing dynasty set up the Eight Banners system that provided the basic framework for the Qing military organization. Bannermen could not undertake trade or manual labor; they had to petition to be removed from banner status. They were considered a form of nobility and were given preferential treatment in terms of annual pensions, land, and allotments of cloth.
Over the next half-century, all areas previously under the Ming dynasty were consolidated under the Qing. Xinjiang, Tibet, and Mongolia were also formally incorporated into Chinese territory. Between 1673 and 1681, the Kangxi Emperor suppressed the Revolt of the Three Feudatories, an uprising of three generals in Southern China who had been denied hereditary rule of large fiefdoms granted by the previous emperor. In 1683, the Qing staged an amphibious assault on southern Taiwan, bringing down the rebel Kingdom of Tungning, which was founded by the Ming loyalist Koxinga (Zheng Chenggong) in 1662 after the fall of the Southern Ming, and had served as a base for continued Ming resistance in Southern China. The Qing defeated the Russians at Albazin, resulting in the Treaty of Nerchinsk.
By the end of Qianlong Emperor's long reign, the Qing Empire was at its zenith. China ruled more than one-third of the world's population, and had the largest economy in the world. By area it was one of the largest empires ever.
In the 19th century the empire was internally stagnant and externally threatened by western powers. The defeat by the British Empire in the First Opium War (1840) led to the Treaty of Nanking (1842), under which Hong Kong was ceded to Britain and importation of opium (produced by British Empire territories) was allowed. Subsequent military defeats and unequal treaties with other western powers continued even after the fall of the Qing dynasty.
Internally the Taiping Rebellion (1851–1864), a quasi-Christian religious movement led by the "Heavenly King" Hong Xiuquan, raided roughly a third of Chinese territory for over a decade until they were finally crushed in the Third Battle of Nanking in 1864. This was one of the largest wars in the 19th century in terms of troop involvement; there was massive loss of life, with a death toll of about 20 million. A string of civil disturbances followed, including the Punti–Hakka Clan Wars, Nian Rebellion, Dungan Revolt, and Panthay Rebellion. All rebellions were ultimately put down, but at enormous cost and with many casualties, seriously weakening the central imperial authority. The Banner system that the Manchus had relied upon for so long failed: Banner forces were unable to suppress the rebels, and the government called upon local officials in the provinces, who raised "New Armies", which successfully crushed the challenges to Qing authority. China never rebuilt a strong central army, and many local officials became warlords who used military power to effectively rule independently in their provinces.
In response to calamities within the empire and threats from imperialism, the Self-Strengthening Movement was an institutional reform in the second half of the 1800s. The aim was to modernize the empire, with prime emphasis on strengthening the military. However, the reform was undermined by corrupt officials, cynicism, and quarrels within the imperial family. As a result, the "Beiyang Fleet" were soundly defeated in the First Sino-Japanese War (1894–1895). The Guangxu Emperor and the reformists then launched a more comprehensive reform effort, the Hundred Days' Reform (1898), but it was soon overturned by the conservatives under Empress Dowager Cixi in a military coup.
At the turn of the 20th century, the violent Boxer Rebellion opposed foreign influence in Northern China, and attacked Chinese Christians and missionaries. When Boxers entered Beijing, the Qing government ordered all foreigners to leave. But instead the foreigners and many Chinese were besieged in the foreign legations quarter. The Eight-Nation Alliance sent the Seymour Expedition of Japanese, Russian, Italian, German, French, American, and Austrian troops to relieve the siege. The Expedition was stopped by the Boxers at the Battle of Langfang and forced to retreat. Due to the Alliance's attack on the Dagu Forts, the Qing government in response sided with the Boxers and declared war on the Alliance. There was fierce fighting at Tientsin. The Alliance formed the second, much larger Gaselee Expedition and finally reached Beijing; the Qing government evacuated to Xi'an. The Boxer Protocol ended the war.
Republican China.
Republic of China (1912–1949).
Frustrated by the Qing court's resistance to reform and by China's weakness, young officials, military officers, and students began to advocate the overthrow of the Qing dynasty and the creation of a republic. They were inspired by the revolutionary ideas of Sun Yat-sen. A revolutionary military uprising, the Wuchang Uprising, began on 10 October 1911, in Wuhan. The provisional government of the Republic of China was formed in Nanjing on 12 March 1912. The Xinhai Revolution ended 2,000 years of dynastic rule in China.
After the success of the overthrow of the Qing Dynasty, Sun Yat-sen was declared President, but Sun was forced to turn power over to Yuan Shikai, who commanded the New Army and was Prime Minister under the Qing government, as part of the agreement to let the last Qing monarch abdicate (a decision Sun would later regret). Over the next few years, Yuan proceeded to abolish the national and provincial assemblies, and declared himself emperor in late 1915. Yuan's imperial ambitions were fiercely opposed by his subordinates; faced with the prospect of rebellion, he abdicated in March 1916, and died in June of that year.
Yuan's death in 1916 left a power vacuum in China; the republican government was all but shattered. This ushered in the Warlord Era, during which much of the country was ruled by shifting coalitions of competing provincial military leaders.
In 1919, the May Fourth Movement began as a response to the terms imposed on China by the Treaty of Versailles ending World War I, but quickly became a nationwide protest movement about the domestic situation in China. The protests were a moral success as the cabinet fell and China refused to sign the Treaty of Versailles, which had awarded German holdings to Japan. The New Culture Movement stimulated by the May Fourth Movement waxed strong throughout the 1920s and 1930s. According to Ebrey:
The discrediting of liberal Western philosophy amongst leftist Chinese intellectuals led to more radical lines of thought inspired by the Russian Revolution, and supported by agents of the Comintern sent to China by Moscow. This created the seeds for the irreconcilable conflict between the left and right in China that would dominate Chinese history for the rest of the century.
In the 1920s, Sun Yat-sen established a revolutionary base in south China, and set out to unite the fragmented nation. With assistance from the Soviet Union (themselves fresh from a socialist uprising), he entered into an alliance with the fledgling Communist Party of China. After Sun's death from cancer in 1925, one of his protégés, Chiang Kai-shek, seized control of the "Kuomintang" (Nationalist Party or KMT) and succeeded in bringing most of south and central China under its rule in a military campaign known as the Northern Expedition (1926–1927). Having defeated the warlords in south and central China by military force, Chiang was able to secure the nominal allegiance of the warlords in the North. In 1927, Chiang turned on the CPC and relentlessly chased the CPC armies and its leaders from their bases in southern and eastern China. In 1934, driven from their mountain bases such as the Chinese Soviet Republic, the CPC forces embarked on the Long March across China's most desolate terrain to the northwest, where they established a guerrilla base at Yan'an in Shaanxi Province. During the Long March, the communists reorganized under a new leader, Mao Zedong (Mao Tse-tung).
The bitter struggle between the KMT and the CPC continued, openly or clandestinely, through the 14-year-long Japanese occupation of various parts of the country (1931–1945). The two Chinese parties nominally formed a united front to oppose the Japanese in 1937, during the Sino-Japanese War (1937–1945), which became a part of World War II. Japanese forces committed numerous war atrocities against the civilian population, including biological warfare (see Unit 731) and the Three Alls Policy ("Sankō Sakusen"), the three alls being: "Kill All, Burn All and Loot All".
Following the defeat of Japan in 1945, the war between the Nationalist government forces and the CPC resumed, after failed attempts at reconciliation and a negotiated settlement. By 1949, the CPC had established control over most of the country "(see Chinese Civil War)". Westad says the Communists won the Civil War because they made fewer military mistakes than Chiang, and because in his search for a powerful centralized government, Chiang antagonized too many interest groups in China. Furthermore, his party was weakened in the war against the Japanese. Meanwhile, the Communists told different groups, such as peasants, exactly what they wanted to hear, and cloaked themselves in the cover of Chinese Nationalism. During the civil war both the Nationalists and Communists carried out mass atrocities, with millions of non-combatants killed by both sides. These included deaths from forced conscription and massacres. When the Nationalist government forces were defeated by CPC forces in mainland China in 1949, the Nationalist government retreated to Taiwan with its forces, along with Chiang and most of the KMT leadership and a large number of their supporters; the Nationalist government had taken effective control of Taiwan at the end of WWII as part of the overall Japanese surrender, when Japanese troops in Taiwan surrendered to Republic of China troops.
People's Republic of China (since 1949).
Major combat in the Chinese Civil War ended in 1949 with Kuomintang (KMT) pulling out of the mainland, with the government relocating to Taipei and maintaining control only over a few islands. The Communist Party of China was left in control of mainland China. On 1 October 1949, Mao Zedong proclaimed the People's Republic of China. "Communist China" and "Red China" were two common names for the PRC.
The PRC was shaped by a series of campaigns and five-year plans. The economic and social plan known as the Great Leap Forward caused an estimated 45 million deaths. Mao's government carried out mass executions of landowners, instituted collectivisation and implemented the Laogai camp system. Execution, deaths from forced labor and other atrocities resulted in millions of deaths under Mao. In 1966 Mao and his allies launched the Cultural Revolution, which continued until Mao's death a decade later. The Cultural Revolution, motivated by power struggles within the Party and a fear of the Soviet Union, led to a major upheaval in Chinese society.
In 1972, at the peak of the Sino-Soviet split, Mao and Zhou Enlai met US president Richard Nixon in Beijing to establish relations with the United States. In the same year, the PRC was admitted to the United Nations in place of the Republic of China, with permanent membership of the Security Council.
A power struggle followed Mao's death in 1976. The Gang of Four were arrested and blamed for the excesses of the Cultural Revolution, marking the end of a turbulent political era in China. Deng Xiaoping outmaneuvered Mao's anointed successor chairman Hua Guofeng, and gradually emerged as the "de facto" leader over the next few years.
Deng Xiaoping was the Paramount Leader of China from 1978 to 1992, although he never became the head of the party or state, and his influence within the Party led the country to significant economic reforms. The Communist Party subsequently loosened governmental control over citizens' personal lives and the communes were disbanded with many peasants receiving multiple land leases, which greatly increased incentives and agricultural production. This turn of events marked China's transition from a planned economy to a mixed economy with an increasingly open market environment, a system termed by some as "market socialism", and officially by the Communist Party of China as "Socialism with Chinese characteristics". The PRC adopted its current constitution on 4 December 1982.
In 1989 the death of former general secretary Hu Yaobang helped to spark the Tiananmen Square protests of that year, during which students and others campaigned for several months, speaking out against corruption and in favour of greater political reform, including democratic rights and freedom of speech. However, they were eventually put down on 4 June when PLA troops and vehicles entered and forcibly cleared the square, with many fatalities. This event was widely reported, and brought worldwide condemnation and sanctions against the government. A filmed incident involving the "tank man" was seen worldwide.
CPC general secretary and PRC President Jiang Zemin and PRC Premier Zhu Rongji, both former mayors of Shanghai, led post-Tiananmen PRC in the 1990s. Under Jiang and Zhu's ten years of administration, the PRC's economic performance pulled an estimated 150 million peasants out of poverty and sustained an average annual gross domestic product growth rate of 11.2%. The country formally joined the World Trade Organization in 2001.
Although the PRC needs economic growth to spur its development, the government began to worry that rapid economic growth was degrading the country's resources and environment. Another concern is that certain sectors of society are not sufficiently benefiting from the PRC's economic development; one example of this is the wide gap between urban and rural areas. As a result, under former CPC general secretary and President Hu Jintao and Premier Wen Jiabao, the PRC initiated policies to address issues of equitable distribution of resources, but the outcome was not known . More than 40 million farmers were displaced from their land, usually for economic development, contributing to 87,000 demonstrations and riots across China in 2005. For much of the PRC's population, living standards improved very substantially and freedom increased, but political controls remained tight and rural areas poor.

</doc>
<doc id="5762" url="https://en.wikipedia.org/wiki?curid=5762" title="Civil engineering">
Civil engineering

Civil engineering is a professional engineering discipline that deals with the design, construction, and maintenance of the physical and naturally built environment, including works like roads, bridges, canals, dams, and buildings. Civil engineering is the second-oldest engineering discipline after military engineering, and it is defined to distinguish non-military engineering from military engineering. It is traditionally broken into several sub-disciplines including architectural engineering, environmental engineering, geotechnical engineering, control engineering, structural engineering, earthquake engineering, transportation engineering, forensic engineering, municipal or urban engineering, water resources engineering, materials engineering, wastewater engineering, offshore engineering, facade engineering, quantity surveying, coastal engineering, construction surveying, and construction engineering. Civil engineering takes place in the public sector from municipal through to national governments, and in the private sector from individual homeowners through to international companies.
History of the civil engineering profession.
Engineering has been an aspect of life since the beginnings of human existence. The earliest practice of civil engineering may have commenced between 4000 and 2000 BC in Ancient Egypt, the Indus Valley Civilization, and Mesopotamia (Ancient Iraq) when humans started to abandon a nomadic existence, creating a need for the construction of shelter. During this time, transportation became increasingly important leading to the development of the wheel and sailing.
Until modern times there was no clear distinction between civil engineering and architecture, and the term engineer and architect were mainly geographical variations referring to the same occupation, and often used interchangeably. The construction of pyramids in Egypt (circa 2700–2500 BC) were some of the first instances of large structure constructions. Other ancient historic civil engineering constructions include the Qanat water management system (the oldest is older than 3000 years and longer than 71 km,) the Parthenon by Iktinos in Ancient Greece (447–438 BC), the Appian Way by Roman engineers (c. 312 BC), the Great Wall of China by General Meng T'ien under orders from Ch'in Emperor Shih Huang Ti (c. 220 BC) and the stupas constructed in ancient Sri Lanka like the Jetavanaramaya and the extensive irrigation works in Anuradhapura. The Romans developed civil structures throughout their empire, including especially aqueducts, insulae, harbors, bridges, dams and roads.
In the 18th century, the term civil engineering was coined to incorporate all things civilian as opposed to military engineering. The first self-proclaimed civil engineer was John Smeaton, who constructed the Eddystone Lighthouse. In 1771 Smeaton and some of his colleagues formed the Smeatonian Society of Civil Engineers, a group of leaders of the profession who met informally over dinner. Though there was evidence of some technical meetings, it was little more than a social society.
In 1818 the Institution of Civil Engineers was founded in London, and in 1820 the eminent engineer Thomas Telford became its first president. The institution received a Royal Charter in 1828, formally recognising civil engineering as a profession. Its charter defined civil engineering as:
History of civil engineering education.
The first private college to teach civil engineering in the United States was Norwich University, founded in 1819 by Captain Alden Partridge. The first degree in civil engineering in the United States was awarded by Rensselaer Polytechnic Institute in 1835. The first such degree to be awarded to a woman was granted by Cornell University to Nora Stanton Blatch in 1905.
In the UK during the early 19th century, the division between civil engineering and military engineering (served by the Royal Military Academy, Woolwich), coupled with the demands of the Industrial Revolution, spawned new engineering education initiatives: the Royal Polytechnic Institution was founded in 1838, the private College for Civil Engineers in Putney was established in 1839, and the UK's first Chair of Engineering was established at the University of Glasgow in 1840.
History of civil engineering.
Civil engineering is the application of physical and scientific principles for solving the problems of society, and its history is intricately linked to advances in understanding of physics and mathematics throughout history. Because civil engineering is a wide ranging profession, including several separate specialized sub-disciplines, its history is linked to knowledge of structures, materials science, geography, geology, soils, hydrology, environment, mechanics and other fields.
Throughout ancient and medieval history most architectural design and construction was carried out by artisans, such as stonemasons and carpenters, rising to the role of master builder. Knowledge was retained in guilds and seldom supplanted by advances. Structures, roads and infrastructure that existed were repetitive, and increases in scale were incremental.
One of the earliest examples of a scientific approach to physical and mathematical problems applicable to civil engineering is the work of Archimedes in the 3rd century BC, including Archimedes Principle, which underpins our understanding of buoyancy, and practical solutions such as Archimedes' screw. Brahmagupta, an Indian mathematician, used arithmetic in the 7th century AD, based on Hindu-Arabic numerals, for excavation (volume) computations.
The civil engineer.
Education and licensure.
"Civil engineers" typically possess an academic degree in civil engineering. The length of study is three to five years, and the completed degree is designated as a bachelor of engineering, or a bachelor of science. The curriculum generally includes classes in physics, mathematics, project management, design and specific topics in civil engineering. After taking basic courses in most sub-disciplines of civil engineering, they move onto specialize in one or more sub-disciplines at advanced levels. While an undergraduate degree (BEng/BSc) normally provides successful students with industry-accredited qualification, some academic institutions offer post-graduate degrees (MEng/MSc), which allow students to further specialize in their particular area of interest.
In most countries, a bachelor's degree in engineering represents the first step towards professional certification, and a professional body certifies the degree program. After completing a certified degree program, the engineer must satisfy a range of requirements (including work experience and exam requirements) before being certified. Once certified, the engineer is designated as a professional engineer (in the United States, Canada and South Africa), a chartered engineer (in most Commonwealth countries), a chartered professional engineer (in Australia and New Zealand), or a European engineer (in most countries of the European Union). There are international agreements between relevant professional bodies to allow engineers to practice across national borders.
The benefits of certification vary depending upon location. For example, in the United States and Canada, "only a licensed professional engineer may prepare, sign and seal, and submit engineering plans and drawings to a public authority for approval, or seal engineering work for public and private clients." This requirement is enforced under provincial law such as the Engineers Act in Quebec.
No such legislation has been enacted in other countries including the United Kingdom. In Australia, state licensing of engineers is limited to the state of Queensland. Almost all certifying bodies maintain a code of ethics which all members must abide by.
Engineers must obey contract law in their contractual relationships with other parties. In cases where an engineer's work fails, he may be subject to the law of tort of negligence, and in extreme cases, criminal charges. An engineer's work must also comply with numerous other rules and regulations such as building codes and environmental law.
Sub-disciplines.
In general, civil engineering is concerned with the overall interface of human created fixed projects with the greater world. General civil engineers work closely with surveyors and specialized civil engineers to design grading, drainage, pavement, water supply, sewer service,dams, electric and communications supply. General civil engineering is also referred to as site engineering, a branch of civil engineering that primarily focuses on converting a tract of land from one usage to another. Site engineers spend time visiting project sites, meeting with stakeholders, and preparing construction plans. Civil engineers apply the principles of geotechnical engineering, structural engineering, environmental engineering, transportation engineering and construction engineering to residential, commercial, industrial and public works projects of all sizes and levels of construction.
Materials science and engineering.
"Materials science" is closely related to civil engineering. It studies fundamental characteristics of materials, and deals with ceramics such as concrete and mix asphalt concrete, strong metals such as aluminum and steel, and polymers including polymethylmethacrylate (PMMA) and carbon fibers.
"Materials engineering" involves protection and prevention (paints and finishes). Alloying combines two types of metals to produce another metal with desired properties. It incorporates elements of applied physics and chemistry. With recent media attention on nanoscience and nanotechnology, materials engineering has been at the forefront of academic research. It is also an important part of forensic engineering and failure analysis.
Coastal engineering.
"Coastal engineering" is concerned with managing coastal areas. In some jurisdictions, the terms sea defense and coastal protection mean defense against flooding and erosion, respectively. The term coastal defense is the more traditional term, but coastal management has become more popular as the field has expanded to techniques that allow erosion to claim land.
Construction engineering.
"Construction engineering" involves planning and execution, transportation of materials, site development based on hydraulic, environmental, structural and geotechnical engineering. As construction firms tend to have higher business risk than other types of civil engineering firms do, construction engineers often engage in more business-like transactions, for example, drafting and reviewing contracts, evaluating logistical operations, and monitoring prices of supplies.
Earthquake engineering.
"Earthquake engineering" involves designing structures to withstand hazardous earthquake exposures. Earthquake engineering is a sub-discipline of structural engineering. The main objectives of earthquake engineering are to understand interaction of structures on the shaky ground; foresee the consequences of possible earthquakes; and design, construct and maintain structures to perform at earthquake in compliance with building codes.
Environmental engineering.
"Environmental engineering" is the contemporary term for sanitary engineering, though sanitary engineering traditionally had not included much of the hazardous waste management and environmental remediation work covered by environmental engineering. Public health engineering and environmental health engineering are other terms being used.
Environmental engineering deals with treatment of chemical, biological, or thermal wastes, purification of water and air, and remediation of contaminated sites after waste disposal or accidental contamination. Among the topics covered by environmental engineering are pollutant transport, water purification, waste water treatment, air pollution, solid waste treatment, and hazardous waste management. Environmental engineers administer pollution reduction, green engineering, and industrial ecology. Environmental engineers also compile information on environmental consequences of proposed actions.
Geotechnical engineering.
"Geotechnical engineering" studies rock and soil supporting civil engineering systems. Knowledge from the field of soil science, materials science, mechanics, and hydraulics is applied to safely and economically design foundations, retaining walls, and other structures. Environmental efforts to protect groundwater and safely maintain landfills have spawned a new area of research called geoenvironmental engineering.
Identification of soil properties presents challenges to geotechnical engineers. Boundary conditions are often well defined in other branches of civil engineering, but unlike steel or concrete, the material properties and behavior of soil are difficult to predict due to its variability and limitation on investigation. Furthermore, soil exhibits nonlinear (stress-dependent) strength, stiffness, and dilatancy (volume change associated with application of shear stress), making studying soil mechanics all the more difficult.
Water resources engineering.
"Water resources engineering" is concerned with the collection and management of water (as a natural resource). As a discipline it therefore combines hydrology, environmental science, meteorology, geology, conservation, and resource management. This area of civil engineering relates to the prediction and management of both the quality and the quantity of water in both underground (aquifers) and above ground (lakes, rivers, and streams) resources. Water resource engineers analyze and model very small to very large areas of the earth to predict the amount and content of water as it flows into, through, or out of a facility. Although the actual design of the facility may be left to other engineers.
"Hydraulic engineering" is concerned with the flow and conveyance of fluids, principally water. This area of civil engineering is intimately related to the design of pipelines, water supply network, drainage facilities (including bridges, dams, channels, culverts, levees, storm sewers), and canals. Hydraulic engineers design these facilities using the concepts of fluid pressure, fluid statics, fluid dynamics, and hydraulics, among others.
Structural engineering.
"Structural engineering" is concerned with the structural design and structural analysis of buildings, bridges, towers, flyovers (overpasses), tunnels, off shore structures like oil and gas fields in the sea, aerostructure and other structures. This involves identifying the loads which act upon a structure and the forces and stresses which arise within that structure due to those loads, and then designing the structure to successfully support and resist those loads. The loads can be self weight of the structures, other dead load, live loads, moving (wheel) load, wind load, earthquake load, load from temperature change etc. The structural engineer must design structures to be safe for their users and to successfully fulfill the function they are designed for (to be "serviceable"). Due to the nature of some loading conditions, sub-disciplines within structural engineering have emerged, including wind engineering and earthquake engineering.
Design considerations will include strength, stiffness, and stability of the structure when subjected to loads which may be static, such as furniture or self-weight, or dynamic, such as wind, seismic, crowd or vehicle loads, or transitory, such as temporary construction loads or impact. Other considerations include cost, constructability, safety, aesthetics and sustainability.
Surveying.
"Surveying" is the process by which a surveyor measures certain dimensions that occur on or near the surface of the Earth. Surveying equipment, such as levels and theodolites, are used for accurate measurement of angular deviation, horizontal, vertical and slope distances. With computerisation, electronic distance measurement (EDM), total stations, GPS surveying and laser scanning have to a large extent supplanted traditional instruments. Data collected by survey measurement is converted into a graphical representation of the Earth's surface in the form of a map. This information is then used by civil engineers, contractors and realtors to design from, build on, and trade, respectively. Elements of a structure must be sized and positioned in relation to each other and to site boundaries and adjacent structures. Although surveying is a distinct profession with separate qualifications and licensing arrangements, civil engineers are trained in the basics of surveying and mapping, as well as geographic information systems. Surveyors also lay out the routes of railways, tramway tracks, highways, roads, pipelines and streets as well as position other infrastructure, such as harbors, before construction.
In the United States, Canada, the United Kingdom and most Commonwealth countries land surveying is considered to be a distinct profession. Land surveyors are not considered to be engineers, and have their own professional associations and licensing requirements. The services of a licensed land surveyor are generally required for boundary surveys (to establish the boundaries of a parcel using its legal description) and subdivision plans (a plot or map based on a survey of a parcel of land, with boundary lines drawn inside the larger parcel to indicate the creation of new boundary lines and roads), both of which are generally referred to as Cadastral surveying.
Construction surveying is generally performed by specialised technicians. Unlike land surveyors, the resulting plan does not have legal status. Construction surveyors perform the following tasks:
Transportation engineering.
"Transportation engineering" is concerned with moving people and goods efficiently, safely, and in a manner conducive to a vibrant community. This involves specifying, designing, constructing, and maintaining transportation infrastructure which includes streets, canals, highways, rail systems, airports, ports, and mass transit. It includes areas such as transportation design, transportation planning, traffic engineering, some aspects of urban engineering, queueing theory, pavement engineering, Intelligent Transportation System (ITS), and infrastructure management.
Forensic engineering.
"Forensic engineering" is the investigation of materials, products, structures or components that fail or do not operate or function as intended, causing personal injury or damage to property. The consequences of failure are dealt with by the law of product liability. The field also deals with retracing processes and procedures leading to accidents in operation of vehicles or machinery. The subject is applied most commonly in civil law cases, although it may be of use in criminal law cases. Generally the purpose of a Forensic engineering investigation is to locate cause or causes of failure with a view to improve performance or life of a component, or to assist a court in determining the facts of an accident. It can also involve investigation of intellectual property claims, especially patents.
Municipal or urban engineering.
"Municipal engineering" is concerned with municipal infrastructure. This involves specifying, designing, constructing, and maintaining streets, sidewalks, water supply networks, sewers, street lighting, municipal solid waste management and disposal, storage depots for various bulk materials used for maintenance and public works (salt, sand, etc.), public parks and cycling infrastructure. In the case of underground utility networks, it may also include the civil portion (conduits and access chambers) of the local distribution networks of electrical and telecommunications services. It can also include the optimizing of waste collection and bus service networks. Some of these disciplines overlap with other civil engineering specialties, however municipal engineering focuses on the coordination of these infrastructure networks and services, as they are often built simultaneously, and managed by the same municipal authority. Municipal engineers may also design the site civil works for large buildings, industrial plants or campuses (i.e. access roads, parking lots, potable water supply, treatment or pretreatment of waste water, site drainage, etc.)
Control engineering.
"Control engineering" (or "control systems engineering") is the branch of civil engineering discipline that applies control theory to design systems with desired behaviors. The practice uses sensors to measure the output performance of the device being controlled (often a vehicle) and those measurements can be used to give feedback to the input actuators that can make corrections toward desired performance. When a device is designed to perform without the need of human inputs for correction it is called automatic control (such as cruise control for regulating a car's speed). Multidisciplinary in nature, control systems engineering activities focus on implementation of control systems mainly derived by mathematical modeling of systems of a diverse range.

</doc>
<doc id="5763" url="https://en.wikipedia.org/wiki?curid=5763" title="Cantonese (disambiguation)">
Cantonese (disambiguation)

Cantonese is a language spoken primarily in south China.
Cantonese may also refer to:

</doc>
<doc id="5765" url="https://en.wikipedia.org/wiki?curid=5765" title="Çatalhöyük">
Çatalhöyük

Çatalhöyük (; also "Çatal Höyük" and "Çatal Hüyük"; from Turkish "çatal" "fork" + "höyük" "mound") was a very large Neolithic and Chalcolithic proto-city settlement in southern Anatolia, which existed from approximately 7500 BC to 5700 BC, and flourished around 7000 BC. It is the largest and best-preserved Neolithic site found to date. In July 2012, it was inscribed as a UNESCO World Heritage Site.
Çatalhöyük is located overlooking the Konya Plain, southeast of the present-day city of Konya (ancient Iconium) in Turkey, approximately 140 km (87 mi) from the twin-coned volcano of Mount Hasan. The eastern settlement forms a mound which would have risen about 20 m (66 ft) above the plain at the time of the latest Neolithic occupation. There is also a smaller settlement mound to the west and a Byzantine settlement a few hundred meters to the east. The prehistoric mound settlements were abandoned before the Bronze Age. A channel of the Çarşamba river once flowed between the two mounds, and the settlement was built on alluvial clay which may have been favourable for early agriculture.
Archaeology.
The site was first excavated by James Mellaart in 1958. He later led a team which excavated there for four seasons between 1961 and 1965. These excavations revealed this section of Anatolia as a centre of advanced culture in the Neolithic period.
Mellaart was banned from Turkey for his involvement in the Dorak affair in which he published drawings of supposedly important Bronze Age artifacts that later went missing. Excavation revealed 18 successive layers of buildings signifying various stages of the settlement and eras of history. The bottom layer of buildings signifies as early as 7500 BC while the top layer is of 5,600 B.C.
After this scandal, the site lay idle until 1993, when investigations began under the leadership of Ian Hodder, then at the University of Cambridge. These investigations are among the most ambitious excavation projects currently in progress according to, among others, Colin Renfrew. In addition to extensive use of archaeological science, psychological and artistic interpretations of the symbolism of the wall paintings have been employed. Hodder, a former student of Mellaart, chose the site as the first "real world" test of his then-controversial theory of post-processual archaeology.
Culture.
Çatalhöyük was composed entirely of domestic buildings, with no obvious public buildings. While some of the larger ones have rather ornate murals, the purpose of some rooms remains unclear.
The population of the eastern mound has been estimated to be, at maximum, 10,000 people, but the population likely varied over the community’s history. An average population of between 5,000 to 7,000 is a reasonable estimate. The sites were set up as large numbers of buildings clustered together. Households looked to their neighbors for help, trade, and possible marriage for their children. The inhabitants lived in mudbrick houses that were crammed together in an aggregate structure. No footpaths or streets were used between the dwellings, which were clustered in a honeycomb-like maze. Most were accessed by holes in the ceiling, with doors reached by ladders and stairs. The rooftops were effectively streets. The ceiling openings also served as the only source of ventilation, allowing smoke from the houses' open hearths and ovens to escape. 
Houses had plaster interiors characterized by squared-off timber ladders or steep stairs. These were usually on the south wall of the room, as were cooking hearths and ovens. The main rooms contained raised platforms that may have been used for a range of domestic activities. Typical houses contained two rooms for everyday activity, such as cooking and crafting. All interior walls and platforms were plastered to a smooth finish. Ancillary rooms were used as storage, and were accessed through low openings from main rooms.
All rooms were kept scrupulously clean. Archaeologists identified very little rubbish in the buildings, finding middens outside the ruins, with sewage and food waste, as well as significant amounts of wood ash. In good weather, many daily activities may also have taken place on the rooftops, which may have formed a plaza. In later periods, large communal ovens appear to have been built on these rooftops. Over time, houses were renewed by partial demolition and rebuilding on a foundation of rubble, which was how the mound was gradually built up. As many as eighteen levels of settlement have been uncovered.
As a part of ritual life, the people of Çatalhöyük buried their dead within the village. Human remains have been found in pits beneath the floors and, especially, beneath hearths, the platforms within the main rooms, and under beds. Bodies were tightly flexed before burial and were often placed in baskets or wound and wrapped in reed mats. Disarticulated bones in some graves suggest that bodies may have been exposed in the open air for a time before the bones were gathered and buried. In some cases, graves were disturbed, and the individual’s head removed from the skeleton. These heads may have been used in rituals, as some were found in other areas of the community. In a woman's grave spinning whorls were recovered and in a man's grave, stone axes. Some skulls were plastered and painted with ochre to recreate faces, a custom more characteristic of Neolithic sites in Syria and at Neolithic Jericho than at sites closer by.
Vivid murals and figurines are found throughout the settlement, on interior and exterior walls. Distinctive clay figurines of women, notably the Seated Woman of Çatalhöyük, have been found in the upper levels of the site. Although no identifiable temples have been found, the graves, murals, and figurines suggest that the people of Çatalhöyük had a religion rich in symbols. Rooms with concentrations of these items may have been shrines or public meeting areas. Predominant images include men with erect phalluses, hunting scenes, red images of the now extinct aurochs (wild cattle) and stags, and vultures swooping down on headless figures. Relief figures are carved on walls, such as of lionesses facing one another.
Heads of animals, especially of cattle, were mounted on walls. A painting of the village, with the twin mountain peaks of Hasan Dağ in the background, is frequently cited as the world's oldest map and the first landscape painting. However, some archaeologists question this interpretation. Stephanie Meece, for example, argues that it is more likely a painting of a leopard skin instead of a volcano, and a decorative geometric design instead of a map.
Çatalhöyük had no apparent social classes, as no houses with distinctive features (belonging to royalty or religious hierarchy, for example) have been found so far. The most recent investigations also reveal little social distinction based on gender, with men and women receiving equivalent nutrition and seeming to have equal social status, as typically found in Paleolithic cultures. Children observed domestic areas. They learned how to perform rituals and how to build or repair houses by watching the adults make statues, beads and other objects.
Çatalhöyük's spatial layout may be due to the close kin relations exhibited amongst the people. It can be seen, in the layout, that the people were "divided into two groups who lived on opposite sides of the town, separated by a gully." Furthermore, because no nearby towns were found from which marriage partners could be drawn, "this spatial separation must have marked two intermarrying kinship groups." This would help explain how a settlement so early on would become so large.
In upper levels of the site, it becomes apparent that the people of Çatalhöyük were gaining skills in agriculture and the domestication of animals. Female figurines have been found within bins used for storage of cereals, such as wheat and barley, and the figurines are presumed to be of a deity protecting the grain. Peas were also grown, and almonds, pistachios, and fruit were harvested from trees in the surrounding hills. Sheep were domesticated and evidence suggests the beginning of cattle domestication as well. However, hunting continued to be a major source of food for the community. Pottery and obsidian tools appear to have been major industries; obsidian tools were probably both used and also traded for items such as Mediterranean sea shells and flint from Syria.
Religion.
A striking feature of Çatalhöyük are its female figurines. Mellaart, the original excavator, argued that these well-formed, carefully made figurines, carved and molded from marble, blue and brown limestone, schist, calcite, basalt, alabaster, and clay, represented a female deity. Although a male deity existed as well, "statues of a female deity far outnumber those of the male deity, who moreover, does not appear to be represented at all after Level VI". To date, eighteen levels have been identified. These artfully-hewn figurines were found primarily in areas Mellaart believed to be shrines. The stately goddess seated on a throne flanked by two female lions ("illustration") was found in a grain bin, which Mellaart suggests might have been a means of ensuring the harvest or protecting the food supply. In later cultures, similar depictions are seen of Cybele, the sun goddess, pulled in her chariot by lions as she transverses the sky in the house of Leo at the height of her power during the months of July and August.
Whereas Mellaart excavated nearly two hundred buildings in four seasons, the current excavator, Ian Hodder, spent an entire season excavating one building alone. Hodder and his team, in 2004 and 2005, began to believe that the patterns suggested by Mellaart were false. They found one similar figurine, but the vast majority did not imitate the Mother Goddess style that Mellaart suggested. Instead of a Mother Goddess culture, Hodder points out that the site gives little indication of a matriarchy or patriarchy.
In an article in the "Turkish Daily News", Hodder is reported as denying that Çatalhöyük was a matriarchal society and quoted as saying "When we look at what they eat and drink and at their social statues, we see that men and women had the same social status. There was a balance of power. Another example is the skulls found. If one's social status was of high importance in Çatalhöyük, the body and head were separated after death. The number of female and male skulls found during the excavations is almost equal." In another article in the "Hurriyet Daily News" Hodder is reported to say "We have learned that men and women were equally approached".
In a report in September 2009 on the discovery of around 2000 figurines Hodder is quoted as saying:
Çatalhöyük was excavated in the 1960s in a methodical way, but not using the full range of natural science techniques that are available to us today. Sir James Mellaart who excavated the site in the 1960s came up with all sorts of ideas about the way the site was organised and how it was lived in and so on ... We’ve now started working there since the mid 1990s and come up with very different ideas about the site. One of the most obvious examples of that is that Çatalhöyük is perhaps best known for the idea of the mother goddess. But our work more recently has tended to show that in fact there is very little evidence of a mother goddess and very little evidence of some sort of female-based matriarchy. That’s just one of the many myths that the modern scientific work is undermining.
Professor Lynn Meskell explained that while the original excavations had found only 200 figures, the new excavations had uncovered 2000 figurines of which most were animals, with less than 5% of the figurines women.
Estonian folklorist Uku Masing has suggested as early as in 1976, that Çatalhöyük was probably a hunting and gathering religion and the Mother Goddess figurine did not represent a female deity. He implied that perhaps a longer period of time was needed in order to develop symbols for agricultural rites. His theory was developed in the paper "Some remarks on the mythology of the people of Catal Hüyük".
Archaeological project support.
The current archaeological investigations at Çatalhöyük are supported by the following institutions and organizations:

</doc>
<doc id="5766" url="https://en.wikipedia.org/wiki?curid=5766" title="Clement Attlee">
Clement Attlee

Clement Richard Attlee, 1st Earl Attlee, (3 January 1883 – 8 October 1967) was a British politician who was the Prime Minister of the United Kingdom from 1945 to 1951 and the Leader of the Labour Party from 1935 to 1955. Attlee was the first person to hold the office of Deputy Prime Minister of the United Kingdom, serving under Winston Churchill in the wartime coalition government, before going on to lead the Labour Party to a landslide election victory in 1945 and a narrow victory in 1950. He became the first Labour Prime Minister ever to serve a full five-year term, as well as the first to command a Labour majority in Parliament, and remains the longest-ever serving Leader of the Labour Party.
First elected to Parliament in 1922 as the MP for Limehouse, Attlee rose quickly to become a minister in the minority government led by Ramsay MacDonald in 1924. In 1931, after Labour suffered a heavy election defeat, he was elected its Deputy Leader. Four years later, he became the Leader of the Labour Party after the resignation of George Lansbury. At first advocating pacificism and appeasement, he later reversed his position and by 1938 became a strong critic of Neville Chamberlain's attempts to appease Adolf Hitler and Benito Mussolini. He subsequently took Labour into the Churchill war ministry in 1940. Initially serving as Lord Privy Seal, he was appointed Deputy Prime Minister two years later. With the end of the Second World War in Europe in May 1945, the coalition government was dissolved and Attlee led Labour to win a huge majority in the ensuing 1945 general election two months later.
The government he led built the post-war consensus, based upon the assumption that full employment would be maintained by Keynesian policies and that a greatly enlarged system of social services would be created – aspirations that had been outlined in the wartime Beveridge Report. Within this context, his government undertook the nationalisation of public utilities and major industries, as well as the creation of the National Health Service. After initial Conservative opposition to Keynesian fiscal policy, this settlement was broadly accepted by all parties for over three decades until Margaret Thatcher became Prime Minister in 1979. His government also presided over the decolonisation of a large part of the British Empire, granting British India, Burma, and Ceylon independence, as well as ending the British Mandates of Palestine and Jordan. He strongly supported the Cold War against Soviet Communism. When the budgetary crisis forced Britain out of Greece in 1947 he called on Washington to counter the Soviet Union with the Truman Doctrine. He avidly supported the Marshall Plan to rebuild Western Europe with American money, and the NATO military alliance against the Soviet bloc. He sent British troops to fight in the Malayan Emergency in 1948. After leading Labour to a narrow victory in the 1950 general election, he sent British troops to fight in the Korean War. Attlee was narrowly defeated by Churchill in the 1951 general election; he retired as Labour leader after losing the 1955 General Election and was elevated to the House of Lords.
In public, Attlee appeared modest and unassuming; he was ineffective at public relations and lacked charisma. His strengths emerged behind the scenes, especially in committees where his depth of knowledge, quiet demeanour, objectivity and pragmatism proved decisive. He saw himself as spokesman on behalf of his entire party, and successfully kept its multiple factions in harness. His reputation among scholars in recent decades has been much higher than during his years as Prime Minister, thanks to his role in forging the welfare state and opposing Stalin in the Cold War.
In 2004 he was voted the most successful British Prime Minister of the 20th Century by a poll of 139 academics organised by Ipsos MORI.
Early life and education.
Attlee was born in Putney, Surrey (now part of London), the seventh of eight children. His father was Henry Attlee (1841–1908), a solicitor, and his mother was Ellen Bravery Watson (1847–1920), daughter of Thomas Simons Watson, secretary for the Art Union of London. He was educated at Northaw School, a boys' preparatory school near Pluckley in Kent. He then attended Haileybury College, and University College, Oxford, where he graduated with a Second Class Honours BA in Modern History in 1904. Attlee then trained as a lawyer, and was called to the Bar at Inner Temple in 1906.
Early career.
From 1906 to 1909, Attlee worked as manager of Haileybury House, a charitable club for working-class boys in Stepney in the East End of London run by his old school. Until then, his political views had been more conservative. However, after his shock at the poverty and deprivation he saw while working with the slum children, he came to the view that private charity would never be sufficient to alleviate poverty and that only direct action and income redistribution by the state would have any serious effect. This sparked a process of political evolution that saw him develop into a full-fledged supporter of socialism. He subsequently joined the Independent Labour Party in 1908 and became active in local politics.
In 1909, he worked briefly as a secretary for Beatrice Webb, before becoming a secretary for Toynbee Hall. In 1911, he was employed by the UK Government as an "official explainer", touring the country to explain Chancellor of the Exchequer David Lloyd George's National Insurance Act. He spent the summer of that year touring Essex and Somerset on a bicycle, explaining the Act at public meetings. A year later, he became a lecturer at the London School of Economics and remained there until he applied for an army officer commission following the outbreak of the First World War in August 1914.
Military service during the First World War.
During the First World War, Attlee was commissioned and served with the South Lancashire Regiment in the Gallipoli Campaign in Turkey. His decision to fight caused a rift between him and his older brother Tom, who, as a conscientious objector, spent much of the war in prison. After a period fighting in Gallipoli, he became ill with dysentery and was sent to a hospital in Malta to recover. His hospitalisation coincided with the Battle of Sari Bair, which saw a large number of his comrades killed. Upon returning to action, he was informed that his regiment had been chosen to hold the final lines during the evacuation of Suvla. As such, he was the penultimate man to be evacuated from Suvla Bay, the last being General Frederick Stanley Maude.
The Gallipoli Campaign had been proposed by the First Lord of the Admiralty, Winston Churchill. Attlee believed that it was a bold strategy, which could have been successful if it had been better implemented on the ground. This gave him an admiration for Churchill as a military strategist, which would make their working relationship in later years productive.
He later served in the Mesopotamian Campaign where he was badly wounded at the Battle of Hanna, being hit in the leg by shrapnel while storming an enemy trench. He was sent back to Britain to recover, and spent most of 1917 training soldiers. From 2 to 9 July 1917 he was the temporary commanding officer of the newly formed L (later 10th) Battalion, the Tank Corps at Bovington Camp, Dorset. From 9 July he assumed command of 30th Company of the same battalion however he did not deploy to France with it in December 1917. That year, he was promoted to the rank of Major, leading him to be known as "Major Attlee" for the inter-war period. After recovering from his injuries, he was sent to France in June 1918 to serve on the Western Front for the final months of the war.
After the war he returned to lecturing at the London School of Economics, where he would remain until 1923.
Marriage and children.
Attlee met Violet Millar on a trip to Italy in 1921. They were engaged a few weeks after their return, and were later married at Christ Church, Hampstead on 10 January 1922. It would come to be a devoted marriage, until her death in 1964. They had four children:
Early political career.
Local politics.
Attlee returned to local politics in the immediate post-war period, becoming mayor of the Metropolitan Borough of Stepney, one of London's poorest inner-city boroughs, in 1919. During his time as mayor, the council undertook action to tackle slum landlords who charged high rents but refused to spend money on keeping their property in habitable condition. The council served and enforced legal orders on house-owners to repair their property. It also appointed health visitors and sanitary inspectors, and reduced the infant mortality rate.
In 1920, while mayor, he wrote his first book, "The Social Worker", which set out many of the principles that informed his political philosophy and that were to underpin the actions of his government in later years. The book attacked the idea that looking after the poor could be left to voluntary action. He wrote at page 30:
and went on to say at page 75:
A right established by law, such as that to an old age pension, is less galling than an allowance made by a rich man to a poor one, dependent on his view of the recipient's character, and terminable at his caprice.
He strongly supported the Poplar Rates Rebellion led by George Lansbury in 1921. This put him into conflict with many of the leaders of the London Labour Party, including Herbert Morrison.
Member of Parliament.
At the 1922 general election, Attlee became the Member of Parliament (MP) for the constituency of Limehouse in Stepney. At the time, he admired Ramsay MacDonald and helped MacDonald get elected as Labour Party leader at the 1922 Labour leadership election. He later regretted this decision. He served as Ramsay MacDonald's Parliamentary Private Secretary for the brief 1922 parliament. His first taste of ministerial office came in 1924, when he served as Under-Secretary of State for War in the short-lived first Labour government, led by MacDonald.
Attlee opposed the 1926 General Strike, believing that strike action should not be used as a political weapon. However, when it happened, he did not attempt to undermine it. At the time of the strike he was chairman of the Stepney Borough Electricity Committee. He negotiated a deal with the Electrical Trade Union so that they would continue to supply power to hospitals, but would end supplies to factories. One firm, Scammell and Nephew Ltd, took a civil action against Attlee and the other Labour members of the committee (although not against the Conservative members who had also supported this). The court found against Attlee and his fellow councillors and they were ordered to pay £300 damages. The decision was later reversed on appeal, but the financial problems caused by the episode almost forced Attlee out of politics.
In 1927 he was appointed a member of the multi-party Simon Commission, a Royal Commission set up to examine the possibility of granting self-rule to India. Due to the time he needed to devote to the commission, and contrary to a promise MacDonald made to Attlee to induce him to serve on the commission, he was not initially offered a ministerial post in the Second Labour Government. Attlee's unsought service on the Commission equipped him with a thorough exposure to India and many of its political leaders. He was later to decide the future of India as Prime Minister.
In 1930, Labour MP Oswald Mosley left the party after its rejection of his proposals for solving the unemployment problem. Attlee was given Mosley's post of Chancellor of the Duchy of Lancaster. He was Postmaster-General at the time of the 1931 crisis, during which most of the party's leaders lost their seats. During the course of the second Labour government, Attlee had become increasingly disillusioned with Ramsay MacDonald, whom he came to regard as vain and incompetent, and of whom he later wrote scathingly in his autobiography.
Opposition.
Deputy Leader.
After Ramsay MacDonald formed a National Government to combat the Great Depression in 1931, the ensuing election was a disaster for the Labour Party, which lost over 200 seats. The vast majority of the party's senior figures lost their seats, including the Leader Arthur Henderson. George Lansbury and Attlee were two of the very few surviving Labour MPs who had experience of government, and accordingly Lansbury was elected Leader with Attlee as his deputy.
Attlee would effectively serve as acting leader for nine months from December 1933, after Lansbury fractured his thigh in an accident, which raised Attlee's public profile considerably. It was during this period, however, that personal financial problems almost forced Attlee to quit politics altogether. His wife had become ill, and at that time there was no separate salary for the Leader of the Opposition. On the verge of resigning from Parliament, he was persuaded to stay by Stafford Cripps, a wealthy socialist, who agreed to pay him an additional salary until his wife recovered.
Leader of the Opposition.
George Lansbury, a committed pacifist, resigned as the Leader of the Labour Party at the 1935 Party Conference after delegates voted in favour of sanctions against Italy for its aggression against Abyssinia. Lansbury had strongly opposed the policy, and felt unable to continue leading the party. With a general election looming, the Parliamentary Labour Party appointed Attlee as interim leader, on the understanding that a leadership election would be held after the general election. Attlee therefore led Labour through the 1935 election, which saw the party stage a partial recovery from its disastrous 1931 performance, gaining over one hundred seats.
Attlee stood in the subsequent leadership election, held in October 1935, where he was opposed by Herbert Morrison and Arthur Greenwood. Morrison was seen as the favourite by many, but was distrusted by many sections of the party, especially the left-wing. Arthur Greenwood's leadership bid, meanwhile, was severely hampered by his alcohol problem. Attlee was able to come across as a competent and unifying figure, particularly having already led the party through a general election. He went on to come first in both the first and second ballots, formally being elected Leader of the Labour Party on 25 October 1935.
Throughout the 1920s and most of the 1930s, the Labour Party's official policy had been to oppose rearmament, instead supporting internationalism and collective security under the League of Nations. At the 1934 Labour Party Conference in Southport, Attlee declared that "We have absolutely abandoned any idea of nationalist loyalty. We are deliberately putting a world order before our loyalty to our own country. We say we want to see put on the statute book something which will make our people citizens of the world before they are citizens of this country". During a debate on defence in the House of Commons a year later, Attlee said "We are told (in the White Paper) that there is danger against which we have to guard ourselves. We do not think you can do it by national defence. We think you can only do it by moving forward to a new world. A world of law, the abolition of national armaments with a world force and a world economic system. I shall be told that that is quite impossible". Shortly after those comments, Adolf Hitler would give a speech in which he proclaimed that German rearmament offered no threat to world peace. Attlee responded the next day noting that Hitler's speech, although containing unfavourable references to the Soviet Union, created "A chance to call a halt in the armaments race...We do not think that our answer to Herr Hitler should be just rearmament. We are in an age of rearmaments, but we on this side cannot accept that position".
In April 1936, the Chancellor of the Exchequer, Neville Chamberlain, introduced a Budget which increased the amount spent on the armed forces. Attlee made a radio broadcast in opposition to it, saying the budget "was the natural expression of the character of the present Government. There was hardly any increase allowed for the services which went to build up the life of the people, education and health. Everything was devoted to piling up the instruments of death. The Chancellor expressed great regret that he should have to spend so much on armaments, but said that it was absolutely necessary and was due only to the actions of other nations. One would think to listen to him that the Government had no responsibility for the state of world affairs...The Government has now resolved to enter upon an arms race, and the people will have to pay for their mistake in believing that it could be trusted to carry out a policy of peace. ... This is a War Budget. We can look in the future for no advance in Social Legislation. All available resources are to be devoted to armaments."
In June 1936 the Conservative MP Duff Cooper called for an Anglo-French alliance against possible German aggression and called for all parties to support one. Attlee condemned this: "We say that any suggestion of an alliance of this kind—an alliance in which one country is bound to another, right or wrong, by some overwhelming necessity—is contrary to the spirit of the League of Nations, is contrary to the Covenant, is contrary to Locarno is contrary to the obligations which this country has undertaken, and is contrary to the professed policy of this Government". At the Labour Party conference at Edinburgh in October Attlee reiterated that "There can be no question of our supporting the Government in its rearmament policy".
However, with the rising threat from Nazi Germany, and the ineffectiveness of the League of Nations, this policy eventually lost credibility. By 1937, Labour had jettisoned its pacifist position and came to support rearmament and oppose Neville Chamberlain's policy of appeasement. In 1938, Attlee opposed the Munich Agreement in which Chamberlain negotiated with Hitler to give Germany the German-speaking parts of Czechoslovakia, the Sudetenland: We all feel relief that war has not come this time. Every one of us has been passing through days of anxiety; we cannot, however, feel that peace has been established, but that we have nothing but an armistice in a state of war. We have been unable to go in for care-free rejoicing. We have felt that we are in the midst of a tragedy. We have felt humiliation. This has not been a victory for reason and humanity. It has been a victory for brute force. At every stage of the proceedings there have been time limits laid down by the owner and ruler of armed force. The terms have not been terms negotiated; they have been terms laid down as ultimata. We have seen to-day a gallant, civilised and democratic people betrayed and handed over to a ruthless despotism. We have seen something more. We have seen the cause of democracy, which is, in our view, the cause of civilisation and humanity, receive a terrible defeat. ... The events of these last few days constitute one of the greatest diplomatic defeats that this country and France have ever sustained. There can be no doubt that it is a tremendous victory for Herr Hitler. Without firing a shot, by the mere display of military force, he has achieved a dominating position in Europe which Germany failed to win after four years of war. He has overturned the balance of power in Europe. He has destroyed the last fortress of democracy in Eastern Europe which stood in the way of his ambition. He has opened his way to the food, the oil and the resources which he requires in order to consolidate his military power, and he has successfully defeated and reduced to impotence the forces that might have stood against the rule of violence.
In 1937, Attlee visited Spain and visited the British Battalion of the International Brigades fighting in the Spanish Civil War. One of the companies was named the 'Major Attlee Company' in his honour.
Deputy Prime Minister.
Attlee remained as Leader of the Opposition when the Second World War broke out in September 1939. The ensuing disastrous Norwegian campaign would result in a motion of no confidence in Neville Chamberlain. Although Chamberlain survived this, the reputation of his administration was so badly and publicly damaged, that it became clear a coalition government would be necessary. Even if Attlee had personally been prepared to serve under Chamberlain in an emergency coalition government, he would never have been able to carry Labour with him. Consequently, Chamberlain tendered his resignation, and Labour and the Conservatives entered a coalition government led by Winston Churchill.
In the coalition government, three inter-connected committees effectively ran the country. Churchill chaired the first two, the War Cabinet and the Defence Committee, with Attlee deputising for him in these, and answering for the government in Parliament when Churchill was absent. Attlee himself chaired the third and final body, the Lord President's Committee, which was responsible for domestic affairs. As Churchill was most concerned with overseeing the war effort, this arrangement suited both men. Only Attlee and Churchill would remain in the War Cabinet from the formation of the Government of National Unity in May 1940 through to the election in May 1945. Attlee was initially the Lord Privy Seal, before becoming Britain's first ever Deputy Prime Minister in 1942, as well as becoming the Dominions Secretary and the Lord President of the Council. Attlee supported Churchill in his continuation of Britain's resistance after the French capitulation in 1940, and proved a loyal ally to Churchill throughout the conflict; when the War Cabinet had voted on whether to negotiate peace terms, Attlee—along with fellow Labour minister Arthur Greenwood—voted in favour of fighting, giving Churchill the majority he needed to continue the war.
Prime Minister.
1945 election.
Following the defeat of Nazi Germany and the end of the War in Europe in May 1945, Attlee and Churchill favoured the coalition government remaining in place until Japan had been defeated. However, Herbert Morrison made it clear that the Labour Party would not be willing to accept this, and Churchill was forced to tender his resignation as Prime Minister and call an immediate election. The war had set in motion profound social changes within Britain, and had ultimately led to a widespread popular desire for social reform. This mood was epitomised in the Beveridge Report of 1942, by the Liberal economist William Beveridge. The "Report" assumed that the maintenance of full employment would be the aim of post-war governments, and that this would provide the basis for the welfare state. Immediately on its release, it sold hundreds of thousands of copies. All major parties committed themselves to fulfilling this aim, but most historians say that Attlee's Labour Party were seen by the electorate as the most likely to follow it through.
Labour campaigned on the theme of "Let Us Face the Future", positioning themselves as the party best placed to rebuild Britain after the war, while the Conservative campaign focused entirely around Churchill. With Churchill's status as a "war hero", many predicted a Conservative victory. Churchill made some costly errors during the campaign. In particular, his suggestion during one radio broadcast that a future Labour Government would require "some form of a gestapo" to implement their policies was widely regarded as being in very bad taste, and massively backfired. Despite this, the results of the election came as a surprise to most, including Attlee himself, when they were announced on 26 July. Labour had won power by a huge landslide, winning 47.7% of the vote to the Conservatives' 36%. This gave them 393 seats in the House of Commons, a working majority of 146. This was the first time in history that the Labour Party had won a majority in Parliament. When Attlee went to see King George VI at Buckingham Palace to be appointed Prime Minister, the notoriously laconic Attlee and the famously tongue-tied King stood in silence; Attlee finally volunteered the remark, "I've won the election." The King replied "I know. I heard it on the Six O'Clock News."
As Prime Minister, Attlee appointed Hugh Dalton as Chancellor of the Exchequer, Ernest Bevin as Foreign Secretary, and Herbert Morrison as Deputy Prime Minister, with overall responsibility for nationalisation. Additionally, Stafford Cripps was made President of the Board of Trade, Aneurin Bevan became Minister of Health, and Ellen Wilkinson, the only woman to serve in Attlee's government, was appointed Minister of Education. The Attlee Government proved itself to be a radical, reforming government. From 1945 to 1948, over 200 public Acts of Parliament were passed, with eight major pieces of legislation placed on the statute book in 1946 alone.
Domestic policy.
Francis (1995) argues there was consensus both in the Labour's national executive committee and at party conferences on a definition of socialism that stressed moral improvement as well as material improvement. The Attlee government was committed to rebuilding British society as an ethical commonwealth, using public ownership and controls to abolish extremes of wealth and poverty. Labour's ideology contrasted sharply with the contemporary Conservative Party's defence of individualism, inherited privileges, and income inequality.
Health.
Attlee's Health Minister, Aneurin Bevan, fought hard against the general disapproval of the medical establishment, including the British Medical Association, by creating the National Health Service (NHS) in 1948. This was a publicly funded healthcare system, which offered treatment free of charge for all at the point of use. Reflecting pent-up demand that had long existed for medical services, the NHS treated some 8½ million dental patients and dispensed more than 5 million pairs of spectacles during its first year of operation. Consultants also benefited from the new system by being paid salaries that provided an acceptable standard of living without the need for them to resort to private practice. The NHS brought major improvements in the health of working-class people, with deaths from diphtheria, pneumonia, and TB significantly reduced. Although there were often disputes about its organisation and funding, British parties continued to voice their general support for the NHS in order to remain electable.
In the field of health care, funds were allocated towards modernisation and extension schemes aimed at improving administrative efficiency. Improvements were made in nursing accommodation in order to recruit more nurses and reduce labour shortages which were keeping 60,000 beds out of use, and efforts were made to reduce the imbalance "between an excess of fever and tuberculosis (TB) beds and a shortage of maternity beds." In addition, BCG vaccinations were introduced for the protection of medical students, midwives, nurses, and contacts of patients with TB, a pension scheme was set up for employees of the newly established NHS, and the Radioactive Substances Act of 1948 set out general provisions to control radioactive substances. Numerous lesser reforms were also introduced, some of which were of great benefit to certain segments of British society, such as the mentally deficient and the blind. During the period 1948–51, the Attlee Government increased spending on health from £6 billion to £11 billion, an increase of over 80%, and from 2.1% to 3.6% of GDP.
Welfare.
The government set about implementing William Beveridge's plans for the creation of a 'cradle to grave' welfare state, and set in place an entirely new system of social security. Among the most important pieces of legislation was the National Insurance Act 1946, in which people in work paid a flat rate of national insurance. In return, they (and the wives of male contributors) were eligible for flat-rate pensions, sickness benefit, unemployment benefit, and funeral benefit. Various other pieces of legislation provided for child benefit and support for people with no other source of income. In 1949, unemployment, sickness and maternity benefits were exempted from tax.
A block grant introduced in 1948 helped the social services provided by local authorities. Personal Social Services or welfare services were developed in 1948 for individual and families in general, particularly special groups such as the mentally disordered, deprived children, the elderly, and the handicapped. The Attlee Government also significantly increased pensions and other benefits, with pensions raised to become more of a living income than they had been. War pensions and allowances (for both world wars) were increased by an Act of 1946 which gave the wounded man with an allowance for his wife and children if he married after he had been wounded, thereby removing a grievance of more than twenty years standing. Other improvements were made in war pensions during Attlee’s tenure as prime minister. A Constant Attendance Allowance was tripled, an Unemployability Allowance was tripled from 10s to 30s a week, and a special hardship allowance of up to £1 a week was introduced. In addition, the 1951 Budget made further improvements in the supplementary allowances for many war pensioners. From 1945 onwards, three out of every four pension claims had been successful, whilst after the First World War only one pension claim in three was allowed. Under the Superannuation (Miscellaneous Provisions) Act of 1948, employees of a body representative of local authorities or of the officers of local authorities could be admitted “on suitable terms to the superannuation fund of a local authority.”
A more extensive system of social welfare benefits was established by the Attlee Government, which did much to reduce acute social deprivation. The cumulative impact of the Attlee's Government's health and welfare policies was such that all the indices of health (such as statistics of school medical or dental officers, or of medical officers of health) showed signs of improvement, with continual improvements in survival rates for infants and increased life expectancy for the elderly. The success of the Attlee Government's welfare legislation in reducing poverty was such that, in the general election of 1950, according to Kevin Jefferys, "Labour propaganda could make much of the claim that social security had eradicated the most abject destitution of the 1930s".
Housing.
The New Towns Act of 1946 set up development corporations to construct new towns, while the Town and Country Planning Act of 1947 instructed county councils to prepare development plans and also provided compulsory purchase powers. The Attlee Government also extended the powers of local authorities to requisition houses and parts of houses, and made the acquisition of land less difficult than before. The Housing (Scotland) Act of 1949 provided grants of 75% (87.5% in the highlands and islands) towards modernisation costs payable by Treasury to local authorities. In 1949, local authorities were empowered to provide people suffering from poor health with public housing at subsidised rents. To assist home ownership, the limit on the amount of money that people could borrow from their local authority in order to purchase or build a home was raised from £800 to £1,500 in 1945, and to £5,000 in 1949. Under the National Assistance act of 1948, local authorities had a duty "to provide emergency temporary accommodation for families which become homeless through no fault of their own."
A large house-building programme was carried out with the intention of providing millions of people with high-quality homes. A housing bill passed in 1946 increased Treasury subsidies for the construction of local authority housing in England and Wales. Four out of five houses constructed under Labour were council properties built to more generous specifications than before the Second World War, and subsidies kept down council rents. Altogether, these policies provided public-sector housing with its biggest ever boost up until that point, while low-wage earners particularly benefited from these developments. Although the Attlee Government failed to meet its targets, primarily due to economic constraints, over a million new homes were built between 1945 and 1951 (a significant achievement under the circumstances) which ensured that decent, affordable housing was available to many low-income families for the first time ever.
Women and children.
A number of reforms were embarked upon to improve conditions for women and children. In 1946, universal family allowances were introduced to provide financial support to households for raising children. These benefits had been legislated for the previous year by Churchill's Family Allowances Act 1945, and was the first measure pushed through parliament by Attlee's government. Conservatives would later criticise Labour for having been "too hasty" in introducing family allowances.
A Married Women (Restraint Upon Anticipation) Act was passed in 1949 "to equalise, to render inoperative any restrictions upon anticipation or alienation attached to the enjoyment of property by a woman," while the Married Women (Maintenance) Act of 1949 was enacted with the intention of improving the adequacy and duration of financial benefits for married women. The Criminal Law (Amendment) Act of 1950 amended an Act of 1885 to bring prostitutes within the law and safeguard them from abduction and abuse. The Criminal Justice Act of 1948 restricted imprisonment for juveniles and brought improvements to the probation and remand centres systems, while the passage of the Justices of the Peace Act of 1949 led to extensive reforms of magistrates courts. The Attlee Government also abolished the marriage bar in the Civil Service, thereby enabling married women to work in that institution.
In 1946, the government set up a National Institute of Houseworkers as a means of providing a social democratic variety of domestic service. The institute aimed to promote domestic service as a skilled craft, by means of training and by examination of those who already had the necessary qualifications. By the autumn of 1946, agreed standards of training were established, which was followed by the opening of a training headquarters and the opening of an additional 9 training centres in Wales, Scotland, and then throughout Great Britain. The National Health Service Act of 1946 indicated that domestic help should be provided for households where that help is required "owing to the presence of any person who is ill, lying-in, an expectant mother, mentally defective, aged or a child not over compulsory school age". 'Home help' therefore included the provision of home-helps for nursing and expectant mothers and for mothers with children under the age of five, and by 1952 some 20,000 women were engaged in this service.
Planning and development.
Development rights were nationalised while the government attempted to take all development profits for the State. Strong planning authorities were set up to control land use, and issued manuals of guidance which stressed the importance of safeguarding agricultural land. A strong chain of regional offices was set up within its planning ministry to provide a strong lead in regional development policies. Comprehensive Development Areas (CDAs), a designation under the Town and Country Planning Act of 1947, allowed local authorities to acquire property in the designated areas using powers of compulsory purchase in order to re-plan and develop urban areas suffering from urban blight or war damage.
Workers' rights.
Various measures were carried out to improve conditions in the workplace. Entitlement to sick leave was greatly extended, and sick pay schemes were introduced for local authority administrative, professional and technical workers in 1946 and for various categories of manual workers in 1948. Worker's compensation was also significantly improved. The Fair Wages Resolution of 1946 required any contractor working on a public project to at least match the pay rates and other employment conditions set in the appropriate collective agreement. In 1946, purchase tax was removed completely from kitchen fittings and crockery, while the rate was reduced on various gardening items.
The Fire Services Act 1947 introduced a new pension scheme for fire-fighters, while the Electricity Act 1947 introduced better retirement benefits for workers in that industry. A Workers' Compensation (Supplementation) Act was passed in 1948 that introduced benefits for workers with certain asbestos-related diseases which had occurred before 1948. The Merchant Shipping Act of 1948 and the Merchant Shipping (Safety Convention) Act of 1949 were passed to improve conditions for seamen. The Shops Act of 1950 consolidated previous legislation which provided that no one could be employed in a shop for more than six hours without having a break for at least 20 minutes. The legislation also required a lunch break of at least 45 minutes for anyone for worked between 11:30 am and 2:30 pm and a half-hour tea break for anyone working between 4 pm and 7 pm. The government also strengthened a Fair Wages Resolution, with a clause that required all employers getting government contracts to recognise the rights of their workers to join trade unions.
The Trades Disputes Act 1927 was repealed, and a Dock Labour Scheme was introduced in 1947 to put an end to the casual system of hiring labour in the docks. This scheme gave registered dockers the legal right to minimum work and decent conditions. Through the National Dock Labour Board (on which trade unions and employers had equal representation) the unions acquired control over recruitment and dismissal. Registered dockers laid off by employers within the Scheme had the right either to be taken on by another, or to generous compensation. All dockers were registered under the Dock Labour Scheme, giving them a legal right to minimum work, holidays and sick pay.
Wages for members of the police force were significantly increased. The introduction of a Miner's Charter in 1946 instituted a five-day work week for miners and a standardised day wage structure, and in 1948 a Colliery Workers Supplementary Scheme was approved, providing supplementary allowances to disabled coal-workers and their dependants. In 1948, a pension scheme was set up to provide pension benefits for employees of the new NHS, as well as their dependents. Under the Coal Industry Nationalisation (Superannuation) Regulations of 1950, a pension scheme for mineworkers was established. Improvements were also made in farmworkers' wages, and the Agricultural Wages Board in 1948 not only safeguarded wage levels, but also ensured that workers were provided with accommodation.
A number of regulations aimed at safeguarding the health and safety of people at work were also introduced during Attlee's time in office. Regulations were issued in February 1946 applying to factories involved with “manufacturing briquettes or blocks of fuel consisting of coal, coal dust, coke or slurry with pitch as a binding .substance,” and which concerned “dust and ventilation, washing facilities and clothing accommodation, medical supervision and examination, skin and eye protection and messrooms.” The Magnesium (Grinding of Castings and Other Articles) (Special Regulations) Order of December 1946 contained special measures “respecting the maintenance of plant and apparatus; precautions against causing sparks; the interception and removal of dust; automatic operation of appliances; protective clothing; and prohibition of smoking, open lights and fires.” For those workers engaged in luminising processes, the Factories (Luminising) Special Regulations (1947) prohibited the employment of those under the age of 18 and ordered “an initial medical examination to be carried out before the seventh day of employment ; subsequent examinations are to be carried out once a month.”Under the terms of the Blasting (Castings and Other Articles) Special Regulations (1949) “no sand or other substance containing free silica is to be employed in any blasting process,” while the Foundries (Parting Materials) Special Regulations (1950) prohibited the use of certain parting powders “which give rise to a substantial risk of silicosis.”
The Building (Safety, Health & Welfare) Regulations of 1948 required that measures should be taken to minimise exposure to potentially harmful dust or fumes, The Pottery (Health) Special Regulations (1947) prohibited the use “except in the manufacture of glazed tiles" of all “but leadless or low solubility glazes and prescribe certain processes in which ground or powdered flint or quartz are not to be employed.” while the Pottery (Health and Welfare) Special Regulations of 1950 made provision for the health and safety of workers employed in factories "in which there is carried on the manufacture or decoration of pottery or certain allied manufactures or processes."
Law.
Various law reforms were also carried out by Attlee's government. The Criminal Justice Act of 1948 provided for new methods to deal with offenders, and abolished hard labour, penal servitude, prison divisions and whipping. The Law Reform (Personal Injuries) Act 1948 enabled employees to sue their employers in cases where they experienced injury due to the negligence of a fellow employee. The Legal Aid and Advice Act of 1949 introduced a State aided scheme to assist those who couldn't afford legal services.
Nationalisation.
Attlee's government also carried out their manifesto commitment for nationalisation of basic industries and public utilities. The Bank of England and civil aviation were nationalised in 1946. Coal mining, the railways, road haulage, canals and Cable and Wireless were nationalised in 1947, electricity and gas followed in 1948. The steel industry was nationalised in 1951. By 1951 about 20% of the British economy had been taken into public ownership.
In spite of the hopes of many on the left, nationalisation failed to provide workers with a greater say in the running of the industries in which they worked. It did, however, bring about significant material gains for workers in the form of higher wages, reduced working hours, and improvements in working conditions, especially in regards to safety. As historian Eric Shaw noted of the years following nationalisation, the electricity and gas supply companies became "impressive models of public enterprise" in terms of efficiency, and the National Coal Board was not only profitable, but working conditions for miners had significantly improved as well. Within a few years of nationalisation, a number of progressive measures had been carried out which did much to improve conditions in the mines, including better pay, a five-day working week, a national safety scheme (with proper standards at all the collieries), a ban on boys under the age of 16 going underground, the introduction of training for newcomers before going down to the coalface, and the making of pithead baths into a standard facility. In addition, the newly established National Coal Board offered sick pay and holiday pay to miners. As noted by Martin Francis
"Union leaders saw nationalisation as a means to pursue a more advantageous position within a framework of continued conflict, rather than as an opportunity to replace the old adversarial form of industrial relations. Moreover, most workers in nationalised industries exhibited an essentially instrumentalist attitude, favouring public ownership because it secured job security and improved wages rather than because it promised the creation of a new set of socialists relationships in the workplace."
Agriculture.
The Attlee Government placed strong emphasis on improving the quality of life in rural areas, benefiting both farmers and other consumers. Security of tenure for farmers was introduced, while consumers were protected by food subsidies and the redistributive effects of deficiency payments. Between 1945 and 1951, the quality of rural life was improved by improvements in gas, electricity, and water services, as well as in leisure and public amenities. In addition, the 1947 Transport Act improved provision of rural bus services, while the Agriculture Act 1947 established a more generous subsidy system for farmers. Legislation was also passed in 1947 and 1948 which established a permanent Agricultural Wages Board to fix minimum wages for agricultural workers. The Attlee Government also made it possible for farm workers to borrow up to 90% of the cost of building their own houses, and received a subsidy of £15 a year for 40 years towards that cost. Grants were also made to meet up to half the cost of supplying water to farm buildings and fields, the government met half the cost of bracken eradication and lime spreading, and grants were paid for bringing hill farming land into use that had previously been considered unfit for farming purposes.
In 1946, the National Agricultural Advisory Service was set up to supply agricultural advice and information. The Hill Farming Act of 1946 introduced for upland areas a system of grants for buildings, land improvement, and infrastructural improvements such as roads and electrification. The Act also continued a system of headage payments for hill sheep and cattle that had been introduced during the war. The Agricultural Holdings Act of 1948 enabled (in effect) tenant farmers to have lifelong tenancies and made provision for compensation in the event of cessations of tenancies. In addition, the Livestock Rearing Act of March 1951 extended the provisions of the 1946 Hill Farming Act to the upland store cattle and sheep sector.
At a time of world food shortages, it was vital that farmers produced the maximum possible quantities. The government encouraged farmers via subsidies for modernisation, while the National Agricultural Advisory Service provided expertise and price guarantees. As a result of the Attlee Government's initiatives in agriculture, there was a 20% increase in output between 1947 and 1952, while Britain adopted one of the most mechanised and efficient farming industries in the world.
Education.
The Attlee Government ensured provisions of the Education Act 1944 were fully implemented, with free secondary education becoming a right for the first time. Fees in state grammar schools were eliminated, while new, modern secondary schools were constructed. The school leaving age was raised to 15 in 1947, an accomplishment helped brought into fruition by initiatives such as the H.O.R.S.A. ("Huts Operation for Raising the School-leaving Age") scheme and the S.F.O.R.S.A. (furniture) scheme. University scholarships were introduced to ensure that no one who was qualified “should be deprived of a university education for financial reasons,” while a large school building programme was organised. A rapid increase in the number of trained teachers took place, and the number of new school places was increased.
Increased Treasury funds were made available for education, particularly for upgrading school buildings suffering from years of neglect and war damage. Prefabricated classrooms were also built and 928 new primary schools were constructed between 1945 and 1950. The provision of free school meals was expanded, and opportunities for university entrants were increased. State scholarships to universities were increased, and the government adopted a policy of supplementing university scholarships awards to a level sufficient to cover fees plus maintenance. Many thousands of ex-servicemen were also assisted to go through college who could never have contemplated it before the war. Free milk was also made available to all schoolchildren for the first time. In addition, spending on technical education also rose, and the number of nursery schools was increased. Salaries for teachers were also improved, and funds were allocated towards improving existing schools. In 1947, the Arts Council of Great Britain was set up to encourage the arts.
A Ministry of Education was established, and free County Colleges were set up for the compulsory part-time instruction of teenagers between the ages of 15 and 18 who were not in full-time education. An Emergency Training Scheme was also introduced which turned out an extra 25,000 teachers in 1945–51. In 1947, Regional Advisory Councils were set up to bring together industry and education to find out the needs of young workers "and advise on the provision required, and to secure reasonable economy of provision." That same year, thirteen Area Training Organisations were set up in England and one in Wales to coordinate teacher training.
Despite these achievements, Attlee's government failed to introduce the comprehensive education for which many socialists had hoped (as a means of making the educational system based more on merit and less on hereditary privilege.) This reform was eventually carried out by Harold Wilson's government. During its time in office, the Attlee Government increased spending on education by over 50%, from £6.5 billion to £10 billion.
Economy.
The most significant problem facing Attlee and his ministers remained the economy, as the war effort had left Britain nearly bankrupt. The war had cost Britain about a quarter of her national wealth. Overseas investments had been used up to pay for the war. The transition to a peacetime economy, and the maintaining of strategic military commitments abroad led to continuous and severe problems with the balance of trade. This resulted in strict rationing of food and other essential goods continuing in the post war period to force a reduction in consumption in an effort to limit imports, boost exports, and stabilise the Pound Sterling so that Britain could trade its way out of its financial state.
The abrupt end of the American Lend-Lease program in August 1945 almost caused a crisis. Some relief was provided by the Anglo-American loan, negotiated in December 1945. The conditions attached to the loan included making the pound fully convertible to the US$. When this was introduced in July 1947, it led to a currency crisis and convertibility had to be suspended after just five weeks. Britain also benefited from the American Marshall Aid program in 1948, and the economic situation improved significantly. Another balance of payments crisis in 1949 forced Chancellor of the Exchequer, Stafford Cripps, into devaluation of the pound.
Despite these problems, one of the main achievements of Attlee's government was the maintenance of near full employment. The government maintained most of the wartime controls over the economy, including control over the allocation of materials and manpower, and unemployment rarely rose above 500,000, or 3% of the total workforce. Labour shortages proved to be a more frequent problem. The inflation rate was also kept low during his term. The rate of unemployment rarely rose above 2% during Attlee's time in office, whilst there was no hard-core of long-term unemployed. Both production and productivity rose as a result of new equipment, while the average working week was shortened.
The government was less successful in housing, which was the responsibility of Aneurin Bevan. The government had a target to build 400,000 new houses a year to replace those which had been destroyed in the war, but shortages of materials and manpower meant that less than half this number were built. Nevertheless, millions of people were rehoused as a result of the Attlee government's housing policies. Between August 1945 and December 1951, 1,016,349 new homes were completed in England, Scotland, and Wales.
When the Attlee Government was voted out of office in 1951, the economy had been improved compared to 1945. The period from 1946 to 1951 saw continuous full employment and steadily rising living standards, which increased by about 10% each year. During that same period, the economy grew by 3% a year, and by 1951 the United Kingdom had "the best economic performance in Europe, while output per person was increasing faster than in the United States." Careful planning after 1945 also ensured that demobilisation was carried out without having a negative impact upon economic recovery, and that unemployment stayed at very low levels. In addition, the number of motor cars on the roads rose from 3 million to 5 million from 1945 to 1951, and seaside holidays were taken by far more people than ever before. A Monopolies and Restrictive Practices (Inquiry and Control) Act was also passed (in 1948), which allowed for investigations of restrictive practices and monopolies.
Energy.
1947 proved to be a particularly difficult year for the government; an exceptionally cold winter that year caused coal mines to freeze and cease production, creating widespread power cuts and food shortages. The Minister of Fuel and Power, Emanuel Shinwell, became a scapegoat for the crisis and was widely blamed for failing to ensure adequate coal stocks, and soon resigned from his post. The Conservatives capitalised on the crisis with the slogan 'Starve with Strachey and shiver with Shinwell' (referring to the Minister of Food John Strachey).
The crisis led to an unsuccessful plot by Hugh Dalton to replace Attlee as Prime Minister with Ernest Bevin. Later that year Stafford Cripps tried to persuade Attlee to stand aside for Bevin. These plots petered out after Bevin refused to cooperate. Later that year, Hugh Dalton resigned as Chancellor after inadvertently leaking details of the budget to a journalist. He was replaced by Cripps.
Foreign policy.
Europe and the Cold War.
In foreign affairs, the Attlee Government was concerned with four main issues; post-war Europe, the onset of the Cold War, the establishment of the United Nations, and decolonisation. The first two were closely related, and Attlee was assisted by Foreign Secretary Ernest Bevin. Attlee also attended the later stages of the Potsdam Conference, where he negotiated with President Harry S. Truman and general secretary Joseph Stalin.
In the immediate aftermath of the war, the Government faced the challenge of managing relations with Britain's former war-time ally, Stalin and the Soviet Union. Ernest Bevin was a passionate anti-communist, based largely on his experience of fighting communist influence in the trade union movement. Bevin's initial approach to the USSR as Foreign Secretary was "wary and suspicious, but not automatically hostile". Attlee himself sought warm relations with Stalin. He put his trust in the United Nations, rejected notions that the Soviet Union was bent on world conquest, and warned that treating Moscow as an enemy would turn it into one. This put Attlee at sword's point with his foreign minister, the Foreign Office, and the military who all saw the Soviets as a growing threat to Britain's role in the Middle East. Suddenly in January 1947, Attlee reversed his position and agreed with Bevin on a hard-line anti-Soviet policy.
In an early "good-will" gesture that was later heavily criticised, the Attlee Government allowed the Soviets access, under the terms of a 1946 UK-USSR Trade Agreement, to several Rolls-Royce Nene jet engines. The Soviets, who at the time were well behind the West in jet technology, reverse-engineered the Nene and installed their own version in the MiG-15 interceptor, used to good effect against US-UK forces in the subsequent Korean War, as well as in several later MiG models.
After Stalin took political control of most of Eastern Europe, and began to subvert other governments in the Balkans, Attlee's and Bevin's worst fears of Soviet intentions were realized. The Attlee Government then became instrumental in the creation of the successful NATO defence alliance to protect Western Europe against any Soviet aggression. In a crucial contribution to the economic stability of post-war Europe, Attlee's Cabinet was instrumental in promoting the American Marshall Plan for the economic recovery of Europe.
A group of Labour MPs, organised under the banner of "Keep Left", urged the government to steer a middle way between the two emerging superpowers, and advocated the creation of a "third force" of European powers to stand between the US and USSR. However, deteriorating relations between Britain and the USSR, as well as Britain's economic reliance on America following the Marshall Plan, steered policy towards supporting the US. In January 1947, fear of Soviet and American nuclear intentions led to a secret meeting of the Cabinet, where the decision was made to press ahead with the development of Britain's independent nuclear deterrent, an issue which later caused a split in the Labour Party. Britain's first successful nuclear test, however, did not occur until 1952, one year after Attlee had left office.
The London dock strike of July 1949, led by Communists, was suppressed when the Attlee government sent in 13,000 Army troops and passed special legislation to promptly end the strike. His response reveals Attlee's growing concern that Soviet expansionism, supported by the British Communist Party, was a genuine threat to national security, and that the docks were highly vulnerable to sabotage ordered by Moscow. He noted that the strike was caused not by local grievances but in order to help Communist unions who were on strike in Canada. Attlee agreed with MI5 that he faced, "a very present menace."
Decolonisation.
The Attlee Government was responsible for beginning the process of decolonisation of the British Empire, starting by granting independence to India. Attlee appointed Lord Louis Mountbatten to be the Viceroy of India when he first became Prime Minister, and agreed to Mountbatten's request for plenipotentiary powers for negotiating Indian independence. In view of implacable demands by the political leadership of, in particular, the Islamic community in British India for a separate Muslim homeland, Mountbatten conceded the notion of two nations consisting of a Hindu-majority India and a Muslim-majority Pakistan (which incorporated East Pakistan, now Bangladesh).
The drawing of borders was accomplished at the cost of large-scale population movements and heavy communal bloodshed on both sides. The independence of Burma and Ceylon was also negotiated around this time. Some of the new countries became British Dominions, the genesis of the modern Commonwealth of Nations.
One of the most urgent problems concerned the future of the Palestine Mandate. British policies there were perceived by the Zionist movement and the Truman Administration as pro-Arab and anti-Jewish. In the face of an armed revolt of Jewish militant groups and increasing violence of the local Arab population, Britain had found itself unable to control events. This was a very unpopular commitment, and the evacuation of British troops and subsequent handing over of the issue to the United Nations was widely supported by the public.
The government's policies with regard to the other colonies, particularly those in Africa, were very different. A major military base was built in Kenya, and the African colonies came under an unprecedented degree of direct control from London. Development schemes were implemented to help solve Britain's desperate post-war balance of payments crisis and raise African living standards. This "new colonialism" was generally a failure, at times even spectacularly so, such as the Tanganyika groundnut scheme.
1950 election.
The 1950 election gave Labour a massively reduced majority of five seats compared to the triple-digit majority of 1945. Although re-elected, the result was seen by Attlee as very disappointing, and was widely attributed to the effects of post-war austerity denting Labour's appeal to middle class voters. With such a small majority leaving him dependent on a small number of MPs to govern, Attlee's second term would prove to be much tamer compared with his first. Some major reforms were nevertheless passed, particularly regarding industry in urban areas and regulations to limit air and water pollution.
1951 election.
By 1951, the Attlee Government was exhausted, with several of its most senior ministers ailing, ageing or deceased, and with a lack of new ideas. Attlee's record for settling internal differences in the Labour Party fell in April 1951, when there was a damaging split over an austerity Budget brought in by the Chancellor, Hugh Gaitskell, to pay for the cost of Britain's participation in the Korean War. Aneurin Bevan resigned to protest against the new charges for "teeth and spectacles" in the National Health Service introduced by that Budget, and was joined in this action by several senior ministers, including the future Prime Minister Harold Wilson, then the President of the Board of Trade.
Finding it increasingly impossible to govern, and not wanting to continue to be at the mercy of a small number of MPs, Attlee eventually called a snap election in October 1951, in the hope of achieving a more workable majority and to regain authority. However, Labour went on to narrowly lose the election to the Conservative Party, despite winning slightly more votes. Attlee tendered his resignation as Prime Minister the following day. His term of six years and three months as Prime Minister was the longest unbroken time spent by any Labour Leader as Prime Minister until Tony Blair more than 50 years later. Although Harold Wilson did manage a total of almost eight years as Prime Minister, that took place across two different spells between 1964 and 1976.
Retirement.
Following the defeat in 1951, Attlee continued to lead the party as Leader of the Opposition. His last four years as leader were, however, widely seen as one of the Labour Party's weaker periods. The party split between its right wing, led by Hugh Gaitskell, and its left, led by Aneurin Bevan. Many Labour MPs felt that Attlee should have retired after the 1951 election and allowed a younger man to lead the party. Bevan openly called for him to stand down in the summer of 1954. One of his main reasons for staying on as leader was to frustrate the leadership ambitions of Herbert Morrison, whom Attlee disliked for political and personal reasons. At one time, Attlee had favoured Aneurin Bevan to succeed him as leader, but this became problematic after Bevan almost irrevocably split the party.
In an interview with the "News Chronicle" columnist Percy Cudlipp in mid-September 1955, Attlee made clear his own thinking together with his preference for the leadership succession, stating: "Labour has nothing to gain by dwelling in the past. Nor do I think we can impress the nation by adopting a futile left-wingism. I regard myself as Left of Centre which is where a Party Leader ought to be. It is no use asking, 'What would Keir Hardie have done?' We must have at the top men brought up in the present age, not, as I was, in the Victorian Age."
Attlee, now 72 years of age, contested the 1955 general election against Anthony Eden, which saw the Conservative majority increase from seventeen to sixty. He retired as Leader of the Labour Party on 25 November 1955, having led Labour for twenty years, and was succeeded by Hugh Gaitskell.
He subsequently retired from the Commons and was elevated to the peerage to take his seat in the House of Lords as Earl Attlee and Viscount Prestwood on 16 December 1955. In 1958 he was, along with Bertrand Russell, one of a group of notables to establish the Homosexual Law Reform Society. The society campaigned for the decriminalisation of homosexual acts in private by consenting adults, a reform which was voted through parliament nine years later.
He attended Churchill's funeral in January 1965. He was elderly and frail by that time, and had to remain seated in the freezing cold as the coffin was carried, having tired himself out by standing at the rehearsal the previous day. After the service, Attlee had to be helped down the steps of St Paul's Cathedral by Sir Anthony Eden and a Guards officer.
A heavy pipe and cigarette smoker from an early age, Attlee had breathing problems in his later years. He lived to see Labour return to power under Harold Wilson in 1964, but also to see his old constituency of Walthamstow West fall to the Conservatives in a by-election in September 1967. He died of pneumonia at the age of 84 at Westminster Hospital on 8 October 1967.
On his death, the title passed to his son Martin Richard Attlee, 2nd Earl Attlee (1927–91). It is now held by Clement Attlee's grandson John Richard Attlee, 3rd Earl Attlee. The third earl (a member of the Conservative Party) retained his seat in the Lords as one of the hereditary peers to remain under an amendment to Labour's 1999 House of Lords Act.
When Attlee died, his estate was sworn for probate purposes at a value of £7,295, a relatively modest sum for so prominent a figure. He was cremated and his ashes buried in the nave of Westminster Abbey, close to those of Lord Passfield and Ernest Bevin.
Legacy.
"A modest man, but then he has so much to be modest about", is a quote about Attlee that is very commonly ascribed to Churchill (although Churchill in fact denied saying it, and respected Attlee's service in the War Cabinet). Attlee's modesty and quiet manner hid a great deal that has only come to light with historical reappraisal. In terms of the machinery of government, he was one of the most businesslike and effective of all the British prime ministers. Indeed, he is widely praised by his successors, both Labour and Conservative.
His leadership style of consensual government, acting as a chairman rather than a president, won him much praise from historians and politicians alike. Christopher Soames, Britain's Ambassador to France during the government of Edward Heath and cabinet minister under Margaret Thatcher, remarked that "Mrs. Thatcher was not really running a team. Every time you have a Prime Minister who wants to make all the decisions, it mainly leads to bad results. Attlee didn't. That's why he was so damn good." Even Thatcher herself wrote in her 1995 memoirs, which charted her beginnings in Grantham to her victory in the 1979 General Election, that she admired Attlee, writing: "Of Clement Attlee, however, I was an admirer. He was a serious man and a patriot. Quite contrary to the general tendency of politicians in the 1990s, he was all substance and no show".
Attlee's administration presided over the successful transition from a wartime economy to peacetime, tackling problems of demobilisation, shortages of foreign currency, and adverse deficits in trade balances and government expenditure. Further domestic policies that he brought about included the establishment of the National Health Service and post-war Welfare State, which became key to the reconstruction of post-war Britain. Attlee and his ministers did much to transform Britain into a more prosperous and egalitarian society during their time in office with reductions in poverty and a rise in the general economic security of the population.
In foreign affairs, he did much to assist with the post-war economic recovery of Europe. He proved a loyal ally of America at the onset of the cold war. Because of his style of leadership it was not he but Ernest Bevin who masterminded foreign policy. It was Attlee's government that decided Britain should have an independent atomic weapons programme, and work began on it in 1947. Bevin, Attlee's Foreign Secretary, famously stated that "We've got to have it and it's got to have a bloody Union Jack on it." The first operational British A Bomb was not detonated until October 1952, about one year after Attlee had left office. Independent British atomic research was prompted partly by the US McMahon Act, which nullified wartime expectations of postwar US-British collaboration in nuclear research, and prohibited Americans from communicating nuclear technology even to allied countries. British atomic bomb research was kept secret even from some members of Attlee's own cabinet, whose loyalty or discretion seemed uncertain.
Though a socialist, Attlee still believed in the British Empire of his youth. He thought of it as an institution that was a power for good in the world. Nevertheless, he saw that a large part of it needed to be self-governing. Using the Dominions of Canada, Australia, and New Zealand as a model, he continued the transformation of the Empire into the Commonwealth.
His greatest achievement, surpassing many of these, was, perhaps, the establishment of a political and economic consensus about the governance of Britain that all parties, whether Labour, Conservative or Liberal subscribed to for three decades, fixing the arena of political discourse until the later 1970s.
Several years after his death, a street on a new housing development in Tividale, West Midlands, was named Attlee Close in his memory. The Birks Holt social housing estate in Maltby, South Yorkshire has its streets named after Labour politicians, including Attlee, Sir Stafford Cripps, Hugh Gaitskell and George Lansbury.
On 30 November 1988, a bronze statue of Clement Attlee was unveiled by Harold Wilson (the next Labour prime minister after Attlee) outside Limehouse Library in his former constituency. By then Wilson was the last surviving member of Attlee's cabinet and the unveiling of the statue would be the last public appearance by Wilson, who was by then in the first stages of Alzheimer's Disease and who died in May 1995. In April 2011, Limehouse Library having closed in 2003, the Attlee statue was unveiled in its new home at Queen Mary University of London. Attlee was awarded an Honorary Fellowship of Queen Mary College on 15 December 1948.
A blue plaque unveiled in 1979 commemorates Attlee at 17 Monkhams Avenue, in Woodford Green in the London borough of Redbridge.
Public image.
In 1956 he was painted by the famous Scottish portrait artist, Cowan Dobson, who also later painted Harold Wilson.
Although possessed of a genial personality, Clement Attlee was notably taciturn in his relations with the press, sometimes offering only monosyllabic answers to reporters' questions. He was seldom referred to by his forename; usually he was referred to as "C. R. Attlee" or "Mr. Attlee".
Religious views.
Although one of his brothers became a clergyman and one of his sisters a missionary, Attlee himself is usually regarded as an agnostic. In an interview he described himself as "incapable of religious feeling", saying that he believed in "the ethics of Christianity" but not "the mumbo-jumbo". When asked whether he was an agnostic, Attlee replied "I don't know".
Appearance in popular culture.
Literature.
Attlee composed this limerick about himself to demonstrate how he was often underestimated:
<poem>Few thought he was even a starter.
There were many who thought themselves smarter.
But he finished PM,
CH and OM,
An earl and a Knight of the Garter.</poem>
An alternative version also exists, which may reflect Attlee's use of English more closely:
<poem>There were few who thought him a starter,
Many who thought themselves smarter.
But he ended PM,
CH and OM,
an Earl and a Knight of the Garter.</poem>

</doc>
<doc id="5768" url="https://en.wikipedia.org/wiki?curid=5768" title="Catullus">
Catullus

Gaius Valerius Catullus (; c. 84 – 54 BC) was a Latin poet of the late Roman Republic who wrote in the neoteric style of poetry. His surviving works are still read widely, and continue to influence poetry and other forms of art.
Catullus' poems were widely appreciated by other poets. He greatly influenced poets such as Ovid, Horace, and Virgil. After his rediscovery in the late Middle Ages, Catullus again found admirers. His explicit writing style has shocked many readers. Indeed, Catullus' work was never canonical in schools, although his body of work is still frequently read from secondary school to graduate programs across the world.
Life.
Gaius Valerius Catullus () was born to a leading equestrian family of Verona, in Cisalpine Gaul. The social prominence of the Catullus family allowed the father of Gaius Valerius to entertain Julius Caesar when he was the Promagistrate (proconsul) of both Gallic provinces. In a poem, Catullus describes his happy homecoming to the family villa at Sirmio, on Lake Garda, near Verona; he also owned a villa near the resort of Tibur (Tivoli).
Catullus appears to have spent most of his young adult years in Rome. His friends there included the poets Licinius Calvus, and Helvius Cinna, Quintus Hortensius (son of the orator and rival of Cicero) and the biographer Cornelius Nepos, to whom Catullus dedicated a "libellus" of poems, the relation of which to the extant collection remains a matter of debate. He appears to have been acquainted with the poet Marcus Furius Bibaculus. A number of prominent contemporaries appear in his poetry, including Cicero, Caesar and Pompey. According to an anecdote preserved by Suetonius, Caesar did not deny that Catullus's lampoons left an indelible stain on his reputation, but when Catullus apologized, he invited the poet for dinner the very same day.
It was probably in Rome that Catullus fell deeply in love with the "Lesbia" of his poems, who is usually identified with Clodia Metelli, a sophisticated woman from the aristocratic house of patrician family Claudii Pulchri, sister of the infamous Publius Clodius Pulcher, and wife to proconsul Quintus Caecilius Metellus Celer. In his poems Catullus describes several stages of their relationship: initial euphoria, doubts, separation, and his wrenching feelings of loss. Clodia had several other partners; “From the poems one can adduce no fewer than five lovers in addition to Catullus: Egnatius (poem 37), Gellius (poem 91), Quintius (poem 82), Rufus (poem 77), and Lesbius (poem 79).” There is also some question surrounding her husband’s mysterious death in 59 B.C., some critics believing he was domestically poisoned. Yet, a sensitive and passionate Catullus could not relinquish his flame for Clodia, regardless of her obvious indifference to his desire for a deep and permanent relationship. In his poems, Catullus wavers between devout, sweltering love and bitter, scornful insults that he directs at her blatant infidelity (as demonstrated in poems 11 and 58). His passion for her is unrelenting— yet it is unclear when exactly the couple split up for good. Catullus's poems about the relationship display striking depth and psychological insight.
He spent the provincial command year summer 57 to summer 56 BC in Bithynia on the staff of the commander Gaius Memmius. While in the East, he traveled to the Troad to perform rites at his brother's tomb, an event recorded in a moving poem.
There survives no ancient biography of Catullus: his life has to be pieced together from scattered references to him in other ancient authors and from his poems. Thus it is uncertain when he was born and when he died. St. Jerome says that he died in his 30th year, and was born in 87 BC. But the poems include references to events of 55 and 54 BC. Since the Roman consular fasti make it somewhat easy to confuse 87–57 BC with 84–54 BC, many scholars accept the dates 84 BC–54 BC, supposing that his latest poems and the publication of his "libellus" coincided with the year of his death. Other authors suggest 52 or 51 BC as the year of the poet's death. Though upon his elder brother's death Catullus lamented that their “whole house was buried along” with the deceased, the existence (and prominence) of "Valerii Catulli" is attested in the following centuries. T.P. Wiseman argues that after the brother's death Catullus could have married, and that, in this case, the later "Valerii Catulli" may have been his descendants.
Poetry.
Sources and organization.
Catullus's poems have been preserved in an anthology of 116 "carmina" (the actual number of poems may slightly vary in various editions), which can be divided into three parts according to their form: sixty short poems in varying meters, called "polymetra", eight longer poems, and forty-eight epigrams.
There is no scholarly consensus on whether Catullus himself arranged the order of the poems. The longer poems differ from the "polymetra" and the epigrams not only in length but also in their subjects: There are seven hymns and one mini-epic, or epyllion, the most highly prized form for the "new poets".
The "polymetra" and the epigrams can be divided into four major thematic groups (ignoring a rather large number of poems that elude such categorization):
All these poems describe the lifestyle of Catullus and his friends, who, despite Catullus's temporary political post in Bithynia, lived their lives withdrawn from politics. They were interested mainly in poetry and love. Above all other qualities, Catullus seems to have valued "venustas", or charm, in his acquaintances, a theme which he explores in a number of his poems. The ancient Roman concept of "virtus" (i.e. of virtue that had to be proved by a political or military career), which Cicero suggested as the solution to the societal problems of the late Republic, meant little to them.
However Catullus does not reject traditional notions, but rather their particular application to the "vita activa" of politics and war. Indeed, he tries to reinvent these notions from a personal point of view and to introduce them into human relationships. For example, he applies the word "fides", which traditionally meant faithfulness towards one's political allies, to his relationship with Lesbia and reinterprets it as unconditional faithfulness in love. So, despite seeming frivolity of his lifestyle, Catullus measured himself and his friends by quite ambitious standards.
Intellectual influences.
Catullus's poetry was influenced by the innovative poetry of the Hellenistic Age, and especially by Callimachus and the Alexandrian school, which had propagated a new style of poetry that deliberately turned away from the classical epic poetry in the tradition of Homer. Cicero called these local innovators "neoteroi" (νεώτεροι) or 'moderns' (in Latin "poetae novi" or 'new poets'), in that they cast off the heroic model handed down from Ennius in order to strike new ground and ring a contemporary note. Catullus and Callimachus did not describe the feats of ancient heroes and gods (except perhaps in re-evaluating and predominantly artistic circumstances, e.g. poems 63 and 64), focusing instead on small-scale personal themes. Although these poems sometimes seem quite superficial and their subjects often are mere everyday concerns, they are accomplished works of art. Catullus described his work as "expolitum", or polished, to show that the language he used was very carefully and artistically composed.
Catullus was also an admirer of Sappho, a female poet of the seventh century BC, and is the source for much of what we know or infer about her. Catullus 51 follows Sappho 31 so closely that some believe the later poem to be, in part, a direct translation of the earlier poem, and 61 and 62 are certainly inspired by and perhaps translated directly from lost works of Sappho. Both of the latter are "epithalamia", a form of laudatory or erotic wedding-poetry that Sappho had been famous for but that had gone out of fashion in the intervening centuries. Catullus twice used a meter that Sappho developed, called the Sapphic strophe in poems 11 and 51. In fact, Catullus may have brought about a substantial revival of that form in Rome.
Catullus, as was common to his era, was greatly influenced by stories from Greek and Roman myth. His longer poems—such as 63, 64, 65, 66, and 68—allude to mythology in various ways. Some stories he refers to are the wedding of Peleus and Thetis, the departure of the Argonauts, Theseus and the Minotaur, Ariadne's abandonment, Tereus and Procne, as well as Protesilaus and Laodamia.
Style.
Catullus wrote in many different meters including hendecasyllabic and elegiac couplets (common in love poetry). All of his poetry shows strong and occasionally wild emotions especially in regard to Lesbia. He also demonstrates a great sense of humour such as in Catullus 13.
Musical settings.
"Catullus Dreams" (2011) is a song cycle by David Glaser set to texts of Catullus. The cycle is scored for soprano and seven instruments. It was premiered at Symphony Space in New York by soprano Linda Larson and Sequitur Ensemble. 
"Catulli Carmina" is a cantata by Carl Orff to the texts of Catullus.
"Carmina Catulli" is a song cycle arranged from 17 of Catullus' poems by American composer Michael Linton. The cycle was recorded in December 2013 and premiered at Carnegie Hall's Weill Recital Hall in March 2014 by French baritone Edwin Crossley-Mercer and pianist Jason Paul Peterson.
Catullus 5, the love poem "Vivamus mea Lesbia atque amemus", in the translation by Ben Jonson was set to music (lute accompanied song) by Alfonso Ferrabosco the younger. Thomas Campion also wrote a lute-song using his own translation of the first six lines of Catullus 5 followed by two verses of his own. The translation by Richard Crashaw was set to music in a four-part glee by Samuel Webbe Jr. It was also set to music in a three-part glee by John Stafford Smith.
Finnish jazz singer Reine Rimón has recorded poems of Catullus set to standard jazz tunes.

</doc>
<doc id="5769" url="https://en.wikipedia.org/wiki?curid=5769" title="C. S. Forester">
C. S. Forester

Cecil Louis Troughton Smith (27 August 1899 – 2 April 1966), known by his pen name Cecil Scott "C. S." Forester, was an English novelist known for writing tales of naval warfare such as the 12-book Horatio Hornblower series, depicting a Royal Navy officer during the Napoleonic wars. His other works include "The African Queen" (1935; filmed in 1951 by John Huston), as well as "A Ship of the Line" and "Flying Colours", which were jointly awarded the James Tait Black Memorial Prize for fiction in 1938.
Early years.
Forester was born in Cairo and, after a family breakup at an early age, moved with his mother to London, where he was educated at Alleyn's School and Dulwich College, south London. At Alleyn's he was a contemporary of E.S. Hornblower, who died on active service with the Canadian Infantry in 1917. He began to study medicine at Guy's Hospital, London, but left without completing his degree. Forester had always worn glasses and been thin. Trying to enlist in the army, he failed his physical and was told there was not a chance that he would be accepted, even though he was of good height and somewhat athletic. Around 1921, after leaving Guy's, he began writing seriously using his pen name.
World War II.
During World War II, Forester moved to the United States where he worked for the British Information Service and wrote propaganda to encourage the US to join the Allies. He eventually settled in Berkeley, California. While living in Washington, D.C., he met a young British intelligence officer named Roald Dahl in early 1942, whose experiences in the RAF he had heard about, and encouraged him to write about them. According to Dahl's autobiographical "Lucky Break", Forester asked Dahl about his experiences as a fighter pilot. This prompted Dahl to write his first story, "A Piece of Cake".
Literary career.
Forester wrote many novels. He is best known for the 12-book Horatio Hornblower series, depicting a Royal Navy officer during the Napoleonic wars. He began the series with Hornblower fairly high in rank in the first novel, published in 1937. The last completed novel was published in 1962. With demand for more stories, Forester filled in Hornblower's life story, in effect. Hornblower's fictional feats were based on real events, but Forester wrote the body of the works carefully to avoid entanglements with real world history, so that Hornblower is always off on another mission when a great naval victory occurs during the Napoleonic Wars. This is a contrast to writer Patrick O'Brian, who began a series in the same historic era in 1969 but put his Royal Navy captain Jack Aubrey in the midst of some real victories, or watching others as a prisoner of war.
Forester's other novels include "The African Queen" (1935) and "The General" (1936); Peninsular War novels in "Death to the French" (published in the United States as "Rifleman Dodd") and "The Gun" (filmed as "The Pride and the Passion" in 1957); and seafaring stories that did not involve Hornblower, such as "Brown on Resolution" (1929), "The Captain from Connecticut" (1941), "The Ship" (1943), and "Hunting the Bismarck" (1959), which was used as the basis of the screenplay for the film "Sink the Bismarck!" (1960). Several of his works were filmed, including "The African Queen" (1951), directed by John Huston. Forester is also credited as story writer for several movies not based on his published fiction, including "Commandos Strike at Dawn" (1942).
He wrote several volumes of short stories set during the Second World War. Those in "The Nightmare" (1954) were based on events in Nazi Germany, ending at the Nuremberg Trials. Stories in "The Man in the Yellow Raft" (1969) followed the career of the destroyer USS "Boon", while many of those in "Gold from Crete" (1971) followed the destroyer HMS "Apache". The last of the stories in "Gold from Crete" was "If Hitler had invaded England", which offers an imagined sequence of events starting with Hitler's attempt to implement Operation Sea Lion, and culminating in the early military defeat of Nazi Germany in the summer of 1941. His non-fiction seafaring works include "The Age of Fighting Sail" (1956), an account of the sea battles between Great Britain and the United States in the War of 1812. 
In addition to his novels of seafaring life, Forester also published two crime novels ("Payment Deferred" (1926) and "Plain Murder" (1930)) and two children's books. "Poo-Poo and the Dragons" (1942) was created as a series of stories told to his younger son George to encourage him to finish his meals. George had mild food allergies that kept him feeling unwell, and he needed encouragement to eat. "The Barbary Pirates" (1953) is a children's history of early 19th-century pirates.
C. S. Forester appeared as a contestant on the TV quiz program "You Bet Your Life", hosted by Groucho Marx, in an episode telecast on 1 November 1956.
A "lost" novel of Forester's, "The Pursued", was discovered in 2003 and bought at an auction and was published by Penguin Classics on 3 November 2011.
Marriage.
He married Kathleen Belcher in 1926, having two sons (John and George Forester), but the couple divorced in 1945. His elder son John Forester wrote a two-volume biography of his father. In 1947, he married Dorothy Foster.

</doc>
<doc id="5770" url="https://en.wikipedia.org/wiki?curid=5770" title="List of country calling codes">
List of country calling codes

Country calling codes or country dial in codes are telephone dialing prefixes for the member countries of the International Telecommunication Union (ITU). They are defined by the ITU-T in standards E.123 and E.164. The prefixes enable international direct dialing (IDD), and are also referred to as "international subscriber dialing" (ISD) codes.
Country codes are a component of the international telephone numbering plan, and are necessary only when dialing a telephone number to establish a call to another country. Country codes are dialed before the national telephone number. By convention, international telephone numbers are represented by prefixing the country code with a plus sign (+), which also indicates to the subscriber that the local international dialing prefix must first be dialed. For example, the international dialing prefix in all countries belonging to the North American Numbering Plan is "011", while it is "00" in most European, Asian and African countries. On GSM (cellular) networks, the prefix may automatically be inserted when the user prefixes a dialed number with the plus sign.
Tree list.
Country calling codes are prefix codes; hence, they can be organized as a tree. In each row of the table below, the country codes given in the left-most column share the same first digit; then subsequent columns give the second digit in ascending order.
Ordered by code.
While there is a general geographic grouping to the zones, some exceptions exist for political and historical reasons. Thus, the geographical indicators below are approximations only.
Zone 1: North American Numbering Plan Area.
Countries within NANP (North American Numbering Plan) administered areas are assigned area codes as if they were all within one country. The codes below in format +1 XXX represent area code XXX within the +1 NANP zone – not a separate country code.
The North American Numbering Plan Area includes:
Zones 3-4: Europe.
Originally, larger countries such as Spain, the United Kingdom or France, were assigned two-digit codes to compensate for their usually longer domestic numbers. Small countries, such as Iceland, were assigned three-digit codes. Since the 1980s, all new assignments have been three-digit regardless of countries’ populations.
Locations with no country code.
In Antarctica, dialing is dependent on the parent country of each base:
Other places with no calling codes:

</doc>
<doc id="5771" url="https://en.wikipedia.org/wiki?curid=5771" title="Christopher Marlowe">
Christopher Marlowe

Christopher Marlowe, also known as Kit Marlowe (baptised 26 February 156430 May 1593), was an English playwright, poet and translator of the Elizabethan era. Marlowe was the foremost Elizabethan tragedian of his day. He greatly influenced William Shakespeare, who was born in the same year as Marlowe and who rose to become the pre-eminent Elizabethan playwright after Marlowe's mysterious early death. Marlowe's plays are known for the use of blank verse and their overreaching protagonists.
A warrant was issued for Marlowe's arrest on 18 May 1593. No reason was given for it, though it was thought to be connected to allegations of blasphemy—a manuscript believed to have been written by Marlowe was said to contain "vile heretical conceipts". On 20 May he was brought to the court to attend upon the Privy Council for questioning. There is no record of their having met that day, however, and he was commanded to attend upon them each day thereafter until "licensed to the contrary." Ten days later, he was stabbed to death by Ingram Frizer. Whether the stabbing was connected to his arrest has never been resolved.
Early life.
Marlowe was born in Canterbury to shoemaker John Marlowe and his wife Catherine. His date of birth is not known, but he was baptised on 26 February 1564, and is likely to have been born a few days before. Thus he was just two months older than his contemporary William Shakespeare, who was baptised on 26 April 1564 in Stratford-upon-Avon.
Marlowe attended The King's School in Canterbury (where a house is now named after him) and Corpus Christi College, Cambridge, where he studied on a scholarship and received his Bachelor of Arts degree in 1584. In 1587 the university hesitated to award him his Master of Arts degree because of a rumour that he intended to go to the English college at Rheims, presumably to prepare for ordination as a Roman Catholic priest. However, his degree was awarded on schedule when the Privy Council intervened on his behalf, commending him for his "faithful dealing" and "good service" to the Queen. The nature of Marlowe's service was not specified by the Council, but its letter to the Cambridge authorities has provoked much speculation, notably the theory that Marlowe was operating as a secret agent working for Sir Francis Walsingham's intelligence service. No direct evidence supports this theory, although the Council's letter is evidence that Marlowe had served the government in some secret capacity.
Literary career.
Of the dramas attributed to Marlowe, "Dido, Queen of Carthage" is believed to have been his first. It was performed by the Children of the Chapel, a company of boy actors, between 1587 and 1593. The play was first published in 1594; the title page attributes the play to Marlowe and Thomas Nashe.
Marlowe's first play performed on the regular stage in London, in 1587, was "Tamburlaine the Great", about the conqueror Tamburlaine, who rises from shepherd to war-lord. It is among the first English plays in blank verse, and, with Thomas Kyd's "The Spanish Tragedy", generally is considered the beginning of the mature phase of the Elizabethan theatre. "Tamburlaine" was a success, and was followed with "Tamburlaine the Great, Part II".
The two parts of "Tamburlaine" were published in 1590; all Marlowe's other works were published posthumously. The sequence of the writing of his other four plays is unknown; all deal with controversial themes.
Marlowe's plays were enormously successful, thanks in part, no doubt, to the imposing stage presence of Edward Alleyn. Alleyn was unusually tall for the time, and the haughty roles of Tamburlaine, Faustus, and Barabas were probably written especially for him. Marlowe's plays were the foundation of the repertoire of Alleyn's company, the Admiral's Men, throughout the 1590s.
Marlowe also wrote the poem "Hero and Leander" (published in 1598, and with a continuation by George Chapman the same year), the popular lyric "The Passionate Shepherd to His Love", and translations of Ovid's "Amores" and the first book of Lucan's "Pharsalia". In 1599, his translation of Ovid was banned and copies publicly burned as part of Archbishop Whitgift's crackdown on offensive material.
Legend.
As with other writers of the period, little is known about Marlowe. What evidence there is can be found in legal records and other official documents. This has not stopped writers of both fiction and non-fiction from speculating about his activities and character. Marlowe has often been described as a spy, a brawler, and a heretic, as well as a "magician", "duellist", "tobacco-user", "counterfeiter", and "rakehell". J. A. Downie and Constance Kuriyama have argued against the more lurid speculation, but J. B. Steane remarked, "it seems absurd to dismiss all of these Elizabethan rumours and accusations as 'the Marlowe myth.
Spying.
Marlowe is often alleged to have been a government spy (Park Honan's 2005 biography even had "Spy" in its title) The author Charles Nicholl speculates this was the case and suggests that Marlowe's recruitment took place when he was at Cambridge. As noted above, in 1587 the Privy Council ordered the University of Cambridge to award Marlowe his degree of Master of Arts, denying rumours that he intended to go to the English Catholic college in Rheims, saying instead that he had been engaged in unspecified "affaires" on "matters touching the benefit of his country". Surviving college records from the period also indicate that Marlowe had had a series of unusually lengthy absences from the university – much longer than permitted by university regulations – that began in the academic year 1584–1585. Surviving college buttery (provisions store) accounts indicate he began spending lavishly on food and drink during the periods he was in attendance – more than he could have afforded on his known scholarship income.
It has sometimes been theorised that Marlowe was the "Morley" who was tutor to Arbella Stuart in 1589. This possibility was first raised in a "TLS" letter by E. St John Brooks in 1937; in a letter to "Notes and Queries", John Baker has added that only Marlowe could be Arbella's tutor due to the absence of any other known "Morley" from the period with an MA and not otherwise occupied. If Marlowe was Arbella's tutor, (and some biographers think that the "Morley" in question may have been a brother of the musician Thomas Morley) it might indicate that he was there as a spy, since Arbella, niece of Mary, Queen of Scots, and cousin of James VI of Scotland, later James I of England, was at the time a strong candidate for the succession to Elizabeth's throne. Frederick Boas dismisses the possibility of this identification, based on surviving legal records which document his "residence in London between September and December 1589". He had been party to a fatal quarrel involving his neighbours in Norton Folgate, and was held in Newgate Prison for a fortnight. In fact the quarrel and his arrest was on 18 September, he was released on bail on 1 October, and he had to attend court – where he was cleared of any wrongdoing – on 3 December, but there is no record of where he was for the intervening two months.
In 1592 Marlowe was arrested in the town of Flushing (Vlissingen) in the Netherlands for his alleged involvement in the counterfeiting of coins, presumably related to the activities of seditious Catholics. He was sent to be dealt with by the Lord Treasurer (Burghley) but no charge or imprisonment resulted. This arrest may have disrupted another of Marlowe's spying missions: perhaps by giving the resulting coinage to the Catholic cause. He was to infiltrate the followers of the active Catholic plotter William Stanley and report back to Burghley.
Arrest and death.
In early May 1593 several bills were posted about London threatening Protestant refugees from France and the Netherlands who had settled in the city. One of these, the "Dutch church libel", written in rhymed iambic pentameter, contained allusions to several of Marlowe's plays and was signed, "Tamburlaine". On May 11 the Privy Council ordered the arrest of those responsible for the libels. The next day, Marlowe's colleague Thomas Kyd was arrested. Kyd's lodgings were searched and a fragment of a heretical tract was found. Kyd asserted that it had belonged to Marlowe, with whom he had been writing "in one chamber" some two years earlier. At that time they had both been working for an aristocratic patron, probably Ferdinando Stanley, Lord Strange. A warrant for Marlowe's arrest was issued on May 18, when the Privy Council apparently knew that he might be found staying with Thomas Walsingham, whose father was a first cousin of the late Sir Francis Walsingham, Elizabeth's principal secretary in the 1580s and a man more deeply involved in state espionage than any other member of the Privy Council. Marlowe duly presented himself on 20 May but, there apparently being no Privy Council meeting on that day, was instructed to "give his daily attendance on their Lordships, until he shall be licensed to the contrary". On Wednesday May 30, Marlowe was killed.
Various accounts of Marlowe's death were current over the next few years. In his "Palladis Tamia", published in 1598, Francis Meres says Marlowe was "stabbed to death by a bawdy serving-man, a rival of his in his lewd love" as punishment for his "epicurism and atheism." In 1917, in the "Dictionary of National Biography", Sir Sidney Lee wrote that Marlowe was killed in a drunken fight, and this is still often stated as fact today.
The official account came to light only in 1925 when the scholar Leslie Hotson discovered the coroner's report of the inquest on Marlowe's death, held two days later on Friday June 1, 1593, by the Coroner of the Queen's Household, William Danby. Marlowe had spent all day in a house in Deptford, owned by the widow Eleanor Bull, and together with three men: Ingram Frizer, Nicholas Skeres and Robert Poley. All three had been employed by one or other of the Walsinghams. Skeres and Poley had helped snare the conspirators in the Babington plot and Frizer would later describe Thomas Walsingham as his "master" at that time although his role was probably more that of a financial or business agent as he was for Walsingham's wife Audrey a few years later. These witnesses testified that Frizer and Marlowe had argued over payment of the bill (now famously known as the 'Reckoning') exchanging "divers malicious words" while Frizer was sitting at a table between the other two and Marlowe was lying behind him on a couch. Marlowe snatched Frizer's dagger and wounded him on the head. In the ensuing struggle, according to the coroner's report, Marlowe was stabbed above the right eye, killing him instantly. The jury concluded that Frizer acted in self-defence, and within a month he was pardoned. Marlowe was buried in an unmarked grave in the churchyard of St. Nicholas, Deptford immediately after the inquest, on June 1, 1593.
The complete text of the inquest report was published by Leslie Hotson in his book, "The Death of Christopher Marlowe", in the introduction to which Prof. G. L. Kittredge said "The mystery of Marlowe's death, heretofore involved in a cloud of contradictory gossip and irresponsible guess-work, is now cleared up for good and all on the authority of public records of complete authenticity and gratifying fullness", but this confidence proved fairly short-lived.
Hotson himself had considered the possibility that the witnesses had "concocted a lying account of Marlowe's behaviour, to which they swore at the inquest, and with which they deceived the jury" but came down against that scenario. Others, however, began to suspect that this was indeed the case. Writing to the "Times Literary Supplement" shortly after the book's publication, Eugénie de Kalb disputed that the struggle and outcome as described were even possible, and Samuel A. Tannenbaum (a graduate of the Columbia University College of Physicians and Surgeons) insisted the following year that such a wound could not have possibly resulted in instant death, as had been claimed. Even Marlowe's biographer John Bakeless acknowledged that "some scholars have been inclined to question the truthfulness of the coroner's report. There is something queer about the whole episode" and said that Hotson's discovery "raises almost as many questions as it answers." It has also been discovered more recently that the apparent absence of a local county coroner to accompany the Coroner of the Queen's Household would, if noticed, have made the inquest null and void.
One of the main reasons for doubting the truth of the inquest concerns the reliability of Marlowe's companions as witnesses. As an "agent provocateur" for the late Sir Francis Walsingham, Robert Poley was a consummate liar, the "very genius of the Elizabethan underworld", and is even on record as saying "I will swear and forswear myself, rather than I will accuse myself to do me any harm." The other witness, Nicholas Skeres, had for many years acted as a confidence trickster, drawing young men into the clutches of people in the money-lending racket, including Marlowe's apparent killer, Ingram Frizer, with whom he was currently engaged in just such a swindle. In other words, despite their being referred to as "generosi" (gentlemen) in the inquest report, they were all professional liars.
Some biographers, such as Kuriyama and Downie, nevertheless take the inquest to be a true account of what occurred, but in trying to explain what really happened if the account was "not" true, others have come up with a variety of murder theories.
There is even a theory that Marlowe's death was faked to save him from trial and execution for subversive atheism. However, since there are only written documents on which to base any conclusions, and since it is probable that the most crucial information about his death was never committed to writing at all, it is unlikely that the full circumstances of Marlowe's death will ever be known.
Atheism.
During his lifetime, Marlowe was reputed to be an atheist, which, at that time, held the dangerous implication of being an enemy of God and by association, the state. With the rise of public fears concerning The School of Night, or the then called "School of Atheism" in the late 16th century, accusations of Atheism were closely associated with disloyalty to the then Protestant monarchy of England.
Some modern historians, however, consider that Marlowe's professed atheism, as with his supposed Catholicism, may have been no more than an elaborate and sustained pretence adopted to further his work as a government spy. Contemporary evidence comes from Marlowe's accuser in Flushing, an informer called Richard Baines. The governor of Flushing had reported that each of the men had "of malice" accused the other of instigating the counterfeiting, and of intending to go over to the Catholic "enemy"; such an action was considered atheistic by the Protestants, who constituted the dominant religious faction in England at that time. Following Marlowe's arrest in 1593, Baines submitted to the authorities a "note containing the opinion of one Christopher Marly concerning his damnable judgment of religion, and scorn of God's word." Baines attributes to Marlowe a total of eighteen items which "scoff at the pretensions of the Old and New Testament" such as, "Christ was a bastard and his mother dishonest nchast", "the woman of Samaria and her sister were whores and that Christ knew them dishonestly", and, "St John the Evangelist was bedfellow to Christ and leaned always in his bosom" (cf. John 13:23–25), and, "that he used him as the sinners of Sodom". He also implies that Marlowe had Catholic sympathies. Other passages are merely sceptical in tone: "he persuades men to atheism, willing them not to be afraid of bugbears and hobgoblins". The final paragraph of Baines' document reads:
These thinges, with many other shall by good & honest witnes be aproved to be his opinions and Comon Speeches, and that this Marlowe doth not only hould them himself, but almost into every Company he Cometh he persuades men to Atheism willing them not to be afeard of bugbeares and hobgoblins, and vtterly scorning both god and his ministers as I Richard Baines will Justify & approue both by mine oth and the testimony of many honest men, and almost al men with whome he hath Conversed any time will testify the same, and as I think all men in Cristianity ought to indevor that the mouth of so dangerous a member may be stopped, he saith likewise that he hath quoted a number of Contrarieties oute of the Scripture which he hath giuen to some great men who in Convenient time shalbe named. When these thinges shalbe Called in question the witnes shalbe produced.
Similar examples of Marlowe's statements were given by Thomas Kyd after his imprisonment and possible torture (see above); both Kyd and Baines connect Marlowe with the mathematician Thomas Harriot and Walter Raleigh's circle. Another document claimed at around the same time that "one Marlowe is able to show more sound reasons for Atheism than any divine in England is able to give to prove divinity, and that ... he hath read the Atheist lecture to Sir Walter Raleigh and others."
Some critics believe that Marlowe sought to disseminate these views in his work and that he identified with his rebellious and iconoclastic protagonists. However, plays had to be approved by the Master of the Revels before they could be performed, and the censorship of publications was under the control of the Archbishop of Canterbury. Presumably these authorities did not consider any of Marlowe's works to be unacceptable (apart from the "Amores").
Sexuality.
Like his contemporary William Shakespeare, Marlowe is sometimes described today as homosexual. Others argue that the question of whether an Elizabethan was gay or homosexual in a modern sense is anachronistic. For the Elizabethans, what is often today termed homosexual or bisexual was more likely to be recognised as a sexual act, rather than an exclusive sexual orientation and identity. Some scholars argue that the evidence is inconclusive and that the reports of Marlowe's homosexuality may simply be exaggerated rumours produced after his death. Richard Baines reported Marlowe as saying: "All they that love not Tobacco and Boys are fools". David Bevington and Eric Rasmussen describe Baines's evidence as "unreliable testimony" and make the comment: "These and other testimonials need to be discounted for their exaggeration and for their having been produced under legal circumstances we would regard as a witch-hunt". One critic, J.B. Steane, remarked that he considers there to be "no "evidence" for Marlowe's homosexuality at all." Other scholars, however, point to homosexual themes in Marlowe's writing: in "Hero and Leander", Marlowe writes of the male youth Leander, "in his looks were all that men desire" and that when the youth swims to visit Hero at Sestos, the sea god Neptune becomes sexually excited, "magining that Ganymede, displeas'd, ad left the Heavens ... he lusty god embrac'd him, call'd him love ... He watched his arms and, as they opened wide t every stroke, betwixt them would he slide nd steal a kiss, ... And dive into the water, and there pry pon his breast, his thighs, and every limb, ... nd talk of love", while the boy, naive and unaware of Greek love practices, protests, You are deceiv'd, I am no woman, I.' Thereat smil'd Neptune." "Edward the Second" contains the following passage supporting homosexual relationships:
<poem>
The mightiest kings have had their minions;
Great Alexander loved Hephaestion,
The conquering Hercules for Hylas wept;
And for Patroclus, stern Achilles drooped.
And not kings only, but the wisest men:
The Roman Tully loved Octavius,
Grave Socrates, wild Alcibiades.
</poem>
Marlowe wrote the only play about the life of Edward II up to his time, taking the humanist literary discussion of male sexuality much further than his contemporaries. The play was extremely bold, dealing with a star-crossed love story between Edward II and Piers Gaveston. Though it was common practice at the time to reveal characters as gay to give audiences reason to suspect them as culprits of a given crime, Christopher Marlowe's Edward II is portrayed as a sympathetic character.
Reputation among contemporary writers.
Whatever the particular focus of modern critics, biographers and novelists, for his contemporaries in the literary world, Marlowe was above all an admired and influential artist. Within weeks of his death, George Peele remembered him as "Marley, the Muses' darling"; Michael Drayton noted that he "Had in him those brave translunary things / That the first poets had", and Ben Jonson wrote of "Marlowe's mighty line". Thomas Nashe wrote warmly of his friend, "poor deceased Kit Marlowe". So too did the publisher Edward Blount, in the dedication of "Hero and Leander" to Sir Thomas Walsingham.
Among the few contemporary dramatists to say anything negative about Marlowe was the anonymous author of the Cambridge University play "The Return From Parnassus" (1598) who wrote, "Pity it is that wit so ill should dwell, / Wit lent from heaven, but vices sent from hell."
The most famous tribute to Marlowe was paid by Shakespeare in "As You Like It", where he not only quotes a line from "Hero and Leander" ("Dead Shepherd, now I find thy saw of might, 'Who ever loved that loved not at first sight?) but also gives to the clown Touchstone the words "When a man's verses cannot be understood, nor a man's good wit seconded with the forward child, understanding, it strikes a man more dead than a great reckoning in a little room." This appears to be a reference to Marlowe's murder which involved a fight over the "reckoning", the bill, as well as to a line in Marlowe's "Jew of Malta""Infinite riches in a little room".
Shakespeare was heavily influenced by Marlowe in his work, as can be seen in the re-using of Marlovian themes in "Antony and Cleopatra", "The Merchant of Venice", "Richard II", and "Macbeth" ("Dido", "Jew of Malta", "Edward II" and "Dr Faustus" respectively). In "Hamlet", after meeting with the travelling actors, Hamlet requests the Player perform a speech about the Trojan War, which at 2.2.429–32 has an echo of Marlowe's "Dido, Queen of Carthage". In "Love's Labour's Lost" Shakespeare brings on a character "Marcade" (three syllables) in conscious acknowledgement of Marlowe's character "Mercury", also attending the King of Navarre, in "Massacre at Paris". The significance, to those of Shakespeare's audience who had read "Hero and Leander", was Marlowe's identification of himself with the god Mercury.
As Shakespeare.
A theory has arisen centred on the notion that Marlowe may have faked his death and then continued to write under the assumed name of William Shakespeare. However, orthodox academic consensus rejects alternative candidates for authorship, including Marlowe.
Memorial window in Poets' Corner.
In July 2002, a memorial window to Marlowe – a gift of the Marlowe Society – was unveiled in Poets' Corner in Westminster Abbey. Controversially, a question mark was added to the generally accepted date of death. On 25 October 2011 a letter from Paul Edmondson and Stanley Wells was published by "The Times" newspaper, in which they called on the Dean and Chapter to remove the question mark on the grounds that it "flew in the face of a mass of unimpugnable evidence". In 2012, they renewed this call in their e-book "Shakespeare Bites Back", adding that it "denies history", and again the following year in their book "Shakespeare Beyond Doubt".
Works.
The dates of composition are approximate.
The play "Lust's Dominion" was attributed to Marlowe upon its initial publication in 1657, though scholars and critics have almost unanimously rejected the attribution.

</doc>
<doc id="5772" url="https://en.wikipedia.org/wiki?curid=5772" title="Cricket (disambiguation)">
Cricket (disambiguation)

Cricket is a bat-and-ball sport contested by two teams.
Cricket may also refer to:

</doc>
<doc id="5776" url="https://en.wikipedia.org/wiki?curid=5776" title="Caving">
Caving

Caving — also traditionally known as spelunking in the United States and Canada and potholing in the United Kingdom and Ireland — is the recreational pastime of exploring wild (generally non-commercial) cave systems. In contrast, speleology is the scientific study of caves and the cave environment.
The challenges involved in caving vary according to the cave being visited, but – in addition to the total absence of light beyond the entrance – often include the negotiation of pitches, squeezes, and water hazards. Cave diving is a distinct, and much more hazardous, sub-specialty undertaken by a small minority of technically proficient (and daring) cavers. In an area of overlap between recreational pursuit and scientific study, the most devoted and serious-minded cavers become accomplished at the surveying and mapping of caves and the formal (though usually private) publication of their efforts.
Caving became widely popular in the 1940s and '50s when a substantial caving community developed in the United States. In recent decades, the pursuit has changed considerably due to the availability of modern protective wear and other equipment. Sometimes categorized as an "extreme sport", it is not commonly considered as such by long-time enthusiasts, who may dislike the term for its connotation of disregard for safety.
Many caving skills overlap with those involved in mine and urban exploration.
Motivation.
Caving is often undertaken for the enjoyment of the outdoor activity or for physical exercise, as well as original exploration, similar to mountaineering or diving. Physical or biological science is also an important goal for some cavers, while others are engaged in cave photography. Virgin cave systems comprise some of the last unexplored regions on Earth and much effort is put into trying to locate, enter and survey them. In well-explored regions (such as most developed nations), the most accessible caves have already been explored, and gaining access to new caves often requires cave digging or cave diving.
Caving, in certain areas, has also been utilized as a form of eco and adventure tourism. Tour companies have established an industry leading and guiding tours into and through caves. Depending on the type of cave and the type of tour, the experience could be adventure-based or ecological-based. In Bend, Oregon, there are tours led through lava tubes by a guiding service. Special permits are required in some situations to guide tours.
Caving has also been described as an "individualist's team sport" by some, as cavers can often make a trip without direct physical assistance from others but will generally go in a group for companionship or to provide emergency help if needed. Some however consider the assistance cavers give each other as a typical team sport activity.
Too much emphasis on the labeling of caving as a sport can narrow the goals of caving as a whole. Caving often puts the needs and welfare of a cave before those of the active participants. It is fair to say that while caving shares some attributes of sport activities, for many it transcends sports as many cavers pursue cave science, mapping, photography, and the management and conservation of cave resources.
Etymology.
Clay Perry, an American caver of the 1940s, wrote about a group of men and boys who explored and studied caves throughout New England. This group referred to themselves as "spelunkers", a term derived from the Latin "" "cave, cavern, den", itself from the Greek "spēlynks" "cave". This is regarded as the first use of the word in the Americas. Throughout the 1950s, "spelunking" was the general term used for exploring caves in US English. It was used freely, without any positive or negative connotations, although only rarely outside the US.
In the 1960s, the terms "spelunking" and "spelunker" began to be considered déclassé among experienced enthusiasts. In 1985, Steve Knutson — editor of the National Speleological Society (NSS) publication "American Caving Accidents" — made the following distinction:
This sentiment is exemplified by bumper stickers and T-shirts displayed by some cavers: "Cavers rescue spelunkers". Nevertheless, outside the caving community, "spelunking" and "spelunkers" predominately remain neutral terms referring to the practice and practitioners, without any respect to skill level.
"Potholing" refers to the act of exploring "potholes", a word originating in the north of England for predominantly vertical caves. The term is often used as a synonym for caving.
History.
Caving was pioneered by Édouard-Alfred Martel (1859–1938), who first achieved the descent and exploration of the Gouffre de Padirac, in France, as early as 1889 and the first complete descent of a 110-metre wet vertical shaft at Gaping Gill, in Yorkshire, England, in 1895. He developed his own techniques based on ropes and metallic ladders. Martel visited Kentucky and notably Mammoth Cave National Park in October 1912. In the 1920s famous US caver Floyd Collins made important explorations in the area and in the 1930s, as caving became increasingly popular, small exploration teams both in the Alps and in the karstic high plateaus of southwest France (Causses and Pyrenees) transformed cave exploration in both a scientific and recreational activity. Robert de Joly, Guy de Lavaur and Norbert Casteret were prominent figures of that time. They surveyed mostly caves in Southwest France. During World War II, an alpine team composed of Pierre Chevalier, Fernand Petzl, Charles Petit-Didier and others explored the Dent de Crolles cave system near Grenoble, France which became the deepest explored system in the world (-658m) at that time. The lack of available equipment during the war forced Pierre Chevalier and the rest of the team to develop their own equipment, leading to technical innovation. The scaling-pole (1940), nylon ropes (1942), use of explosives in caves (1947) and mechanical rope-ascenders (Henri Brenot's "monkeys", first used by Chevalier and Brenot in a cave in 1934) can be directly associated to the exploration of the Dent de Crolles cave system.
In 1941, American cavers organized themselves into the National Speleological Society (NSS) to advance the exploration, conservation, study, and understanding of caves in the United States. American caver Bill Cuddington, known as "Vertical Bill", developed the single rope technique (SRT) in the late 1950s. In 1958, two Swiss alpinists, Juesi and Marti teamed together, creating the first rope ascender known as the Jumar. In 1968 Bruno Dressler asked Fernand Petzl, who worked as a metals machinist, to build a rope-ascending tool, today known as the Petzl Croll, that he had developed by adapting the Jumar to pit caving. Pursuing these developments, Petzl started in the 1970s a caving equipment manufacturing company named Petzl. The development of the rappel rack and the evolution of mechanical ascension systems extended the practice and safety of pit exploration to a larger venue of cavers.
Practice and equipment.
Hard hats are worn to protect the head from bumps and falling rocks. The caver's primary light source is usually mounted on the helmet in order to keep the hands free. Electric LED lights are most common. Many cavers carry two or more sources of light - one as primary and the others as backup in case the first fails. More often than not, a second light will be mounted to the helmet for quick transition if the primary fails. Carbide lamp systems are an older form of illumination, inspired by miner's equipment, and are still used by some cavers.
The type of clothes worn underground varies according to the environment of the cave being explored, and the local culture. In cold caves, the caver may wear a warm base layer that retains its insulating properties when wet, such as a fleece ("furry") suit and/or polypropylene underwear, and an oversuit of hard-wearing (e.g., cordura) and/or waterproof (e.g., PVC) material. Lighter clothing may be worn in warm caves, particularly if the cave is dry, and in tropical caves thin polypropylene clothing is used, to provide some abrasion protection whilst remaining as cool as possible. Wetsuits may be worn if the cave is particularly wet or involves stream passages. On the feet boots are worn - hiking-style boots in drier caves, or rubber boots (such as wellies) often with neoprene socks ("wetsocks") in wetter caves. Knee-pads (and sometimes elbow-pads) are popular for protecting joints during crawls. Depending on the nature of the cave, gloves are sometimes worn to protect the hands against abrasion and/or cold. In pristine areas and for restoration, clean oversuits and powder-free, non-latex surgical gloves are used to protect the cave itself from contaminants.
Ropes are used for descending or ascending pitches (single rope technique or SRT) or for protection. Knots commonly used in caving are the figure-of-eight- (or figure-of-nine-) loop, bowline, alpine butterfly, and Italian hitch. Ropes are usually rigged using bolts, slings, and carabiners. In some cases cavers may choose to bring and use a flexible metal ladder.
In addition to the equipment already described, cavers frequently carry packs containing first-aid kits, emergency equipment, and food. Containers for securely transporting urine are also commonly carried. On longer trips, containers for securely transporting feces out of the cave are carried.
During very long trips, it may be necessary to camp in the cave - some cavers have stayed underground for many days, or in particularly extreme cases, for weeks at a time. This is particularly the case when exploring or mapping very extended cave systems, where it would be impractical to retrace the route back to the surface regularly. Such long trips necessitate the cavers carrying provisions, sleeping and cooking equipment.
Safety.
Caves can be dangerous places; hypothermia, falling, flooding, falling rocks and physical exhaustion are the main risks. Rescuing people from underground is difficult and time-consuming, and requires special skills, training, and equipment. Full-scale cave rescues often involve the efforts of dozens of rescue workers (often other long-time cavers who have participated in specialized courses, as normal rescue staff are not sufficiently experienced in cave environments), who may themselves be put in jeopardy in effecting the rescue. This said, caving is not necessarily a high-risk sport (especially if it does not involve difficult climbs or diving). As in all physical sports, knowing one's limitations is key.
Caving in the Mississippi and Ohio River valleys carries the risk of contracting histoplasmosis, a fungal infection that is contracted from bird or bat droppings. It can cause pneumonia and can disseminate in the body to cause continued infections.
Safety risks while caving can be minimized by using a number of techniques:
Cave conservation.
Many cave environments are very fragile. Many speleothems can be damaged by even the slightest touch and some by impacts as slight as a breath. Research suggests that increased carbon dioxide levels can lead to "a higher equilibrium concentration of calcium within the drip waters feeding the speleothems, and hence causes dissolution of existing features." In 2008, researchers found evidence that respiration from cave visitors may generate elevated carbon dioxide concentrations in caves, leading to increased temperatures of up to 3 °C and a dissolution of existing features.
Pollution is also of concern. Since water that flows through a cave eventually comes out in streams and rivers, any pollution may ultimately end up in someone's drinking water, and can even seriously affect the surface environment, as well. Even minor pollution such as dropping organic material can have a dramatic effect on the cave biota.
Cave-dwelling species are also very fragile, and often, a particular species found in a cave may live within that cave alone, and be found nowhere else in the world, such as Alabama cave shrimp. Cave-dwelling species are accustomed to a near-constant climate of temperature and humidity, and any disturbance can be disruptive to the species' life cycles. Though cave wildlife may not always be immediately visible, it is typically nonetheless present in most caves.
Bats are one such fragile species of cave-dwelling animal. Bats which hibernate are most vulnerable during the winter season, when no food supply exists on the surface to replenish the bat's store of energy should it be awakened from hibernation. Bats which migrate are most sensitive during the summer months when they are raising their young. For these reasons, visiting caves inhabited by hibernating bats is discouraged during cold months; and visiting caves inhabited by migratory bats is discouraged during the warmer months when they are most sensitive and vulnerable. Due to an affliction affecting bats in the northeastern US known as white nose syndrome (WNS), the US Fish & Wildlife Service has called for a moratorium effective March 26, 2009 on caving activity in states known to have hibernacula (MD, NY, VT, NH, MA, CT, NJ, PA, VA, and WV) affected by WNS, as well as adjoining states.
Some cave passages may be marked with flagging tape or other indicators to show biologically, aesthetically, or archaeologically sensitive areas. Marked paths may show ways around notably fragile areas such as a pristine floor of sand or silt which may be thousands of years old, dating from the last time water flowed through the cave. Such deposits may easily be spoiled forever by a single misplaced step. Active formations such as flowstone can be similarly marred with a muddy footprint or handprint, and ancient human artifacts, such as fiber products, may even crumble to dust under all but the most gentle touch.
In 1988, concerned that cave resources were becoming increasingly damaged through unregulated use, Congress enacted the Federal Cave Resources Protection Act, giving land management agencies in the United States expanded authority to manage cave conservation on public land.
In Europe, some panoramic 360° realizations have been done to share some of interesting caves or quarries:
Caving organizations.
Cavers in many countries have created organizations for the administration and oversight of caving activities within their nations. The oldest of these is the French Federation of Speleology (originally Société de spéléologie) founded by Édouard-Alfred Martel in 1895, which produced the first periodical journal in speleology, "Spelunca". The National Speleological Society of the USA was later founded in 1941 (originally formed as the Speleological Society of the District of Columbia on May 6, 1939) and the Swiss Society of Speleology created in 1939 in Geneva, but the first speleological institute in the world was founded in 1920 in Cluj-Napoca, Romania, by Emil Racovita, a Romanian biologist, zoologist, speleologist and explorer of Antarctica. The Pakistan Cave Research & Caving Federation was founded in Pakistan in 1997. Similarly in India, the National Cave Research and Protection Organization has been functioning since 2006.

</doc>
<doc id="5778" url="https://en.wikipedia.org/wiki?curid=5778" title="Cave">
Cave

A cave or cavern is a hollow place in the ground, especially a natural underground space large enough for a human to enter. Caves form naturally by the weathering of rock and often extend deep underground. The word "cave" can also refer to much smaller openings such as sea caves, rock shelters, and grottos.
Speleology is the science of exploration and study of all aspects of caves and the cave environment. Visiting or exploring caves for recreation may be called "caving", "potholing", or "spelunking".
Types and formation.
The formation and development of caves is known as "speleogenesis". Caves are formed by various geologic processes and can be variable sizes. These may involve a combination of chemical processes, erosion from water, tectonic forces, microorganisms, pressure, and atmospheric influences.
It is estimated that the maximum depth of a cave cannot be more than due to the pressure of overlying rocks. For karst caves the maximum depth is determined on the basis of the lower limit of karst forming processes, coinciding with the base of the soluble carbonate rocks.
Most caves are formed in limestone by dissolution.
Solutional cave.
Solutional caves are the most frequently occurring caves and such caves form in rock that is soluble, such as limestone, but can also form in other rocks, including chalk, dolomite, marble, salt, and gypsum. Rock is dissolved by natural acid in groundwater that seeps through bedding planes, faults, joints, and comparable features. Over geological epochs cracks expand to become caves and cave systems.
The largest and most abundant solutional caves are located in limestone. Limestone dissolves under the action of rainwater and groundwater charged with HCO (carbonic acid) and naturally occurring organic acids. The dissolution process produces a distinctive landform known as "karst", characterized by sinkholes and underground drainage. Limestone caves are often adorned with calcium carbonate formations produced through slow precipitation. These include flowstones, stalactites, stalagmites, helictites, soda straws and columns. These secondary mineral deposits in caves are called "speleothems".
The portions of a solutional cave that are below the water table or the local level of the groundwater will be flooded.
Lechuguilla Cave in New Mexico and nearby Carlsbad Cavern are now believed to be examples of another type of solutional cave. They were formed by HS (hydrogen sulfide) gas rising from below, where reservoirs of oil give off sulfurous fumes. This gas mixes with ground water and forms HSO (sulfuric acid). The acid then dissolves the limestone from below, rather than from above, by acidic water percolating from the surface.
Primary cave.
Caves formed at the same time as the surrounding rock are called primary caves.
Lava tubes are formed through volcanic activity and are the most common primary caves. As lava flows downhill, its surface cools and solidifies. Hot liquid lava continues to flow under that crust, and if most of it flows out, a hollow tube remains. Examples of such caves can be found in the Canary Islands, Jeju-do, the basaltic plains of Eastern Idaho, and other places. Kazumura Cave near Hilo, Hawaii is a remarkably long and deep lava tube; it is .
Lava caves include but are not limited to lava tubes. Other caves formed through volcanic activity include rift caves, lava mold caves, open vertical volcanic conduits, and inflationary caves.
Sea cave or littoral cave.
Sea caves are found along coasts around the world. A special case is littoral caves, which are formed by wave action in zones of weakness in sea cliffs. Often these weaknesses are faults, but they may also be dykes or bedding-plane contacts. Some wave-cut caves are now above sea level because of later uplift. Elsewhere, in places such as Thailand's Phang Nga Bay, solutional caves have been flooded by the sea and are now subject to littoral erosion. Sea caves are generally around in length, but may exceed .
Corrasional cave or erosional cave.
Corrasional or erosional caves are those that form entirely by erosion by flowing streams carrying rocks and other sediments. These can form in any type of rock, including hard rocks such as granite. Generally there must be some zone of weakness to guide the water, such as a fault or joint. A subtype of the erosional cave is the wind or aeolian cave, carved by wind-born sediments. Many caves formed initially by solutional processes often undergo a subsequent phase of erosional or vadose enlargement where active streams or rivers pass through them.
Glacier cave.
Glacier caves are formed by melting ice and flowing water within and under glaciers. The cavities are influenced by the very slow flow of the ice, which tends to collapse the caves again. Glacier caves are sometimes misidentified as "ice caves", though this latter term is properly reserved for bedrock caves that contain year-round ice formations.
Fracture cave.
Fracture caves are formed when layers of more soluble minerals, such as gypsum, dissolve out from between layers of less soluble rock. These rocks fracture and collapse in blocks of stone.
Talus cave.
Talus caves are formed by the openings among large boulders that have fallen down into a random heap, often at the bases of cliffs. These unstable deposits are called talus or scree, and may be subject to frequent rockfalls and landslides.
Anchialine cave.
Anchialine caves are caves, usually coastal, containing a mixture of freshwater and saline water (usually sea water). They occur in many parts of the world, and often contain highly specialized and endemic fauna.
Geographic distribution.
Caves are found throughout the world, but only a small portion of them have been explored and documented by cavers. The distribution of documented cave systems is widely skewed toward countries where caving has been popular for many years (such as France, Italy, Australia, the UK, the United States, etc.). As a result, explored caves are found widely in Europe, Asia, North America and Oceania, but are sparse in South America, Africa, and Antarctica.
This is a rough generalization, as large expanses of North America and Asia contain no documented caves, whereas areas such as the Madagascar dry deciduous forests and parts of Brazil contain many documented caves. As the world's expanses of soluble bedrock are researched by cavers, the distribution of documented caves is likely to shift. For example, China, despite containing around half the world's exposed limestone—more than —has relatively few documented caves.
Ecology.
Cave-inhabiting animals are often categorized as troglobites (cave-limited species), troglophiles (species that can live their entire lives in caves, but also occur in other environments), trogloxenes (species that use caves, but cannot complete their life cycle fully in caves) and accidentals (animals not in one of the previous categories). Some authors use separate terminology for aquatic forms (for example, stygobites, stygophiles, and stygoxenes).
Of these animals, the troglobites are perhaps the most unusual organisms. Troglobitic species often show a number of characteristics, termed troglomorphic, associated with their adaptation to subterranean life. These characteristics may include a loss of pigment (often resulting in a pale or white coloration), a loss of eyes (or at least of optical functionality), an elongation of appendages, and an enhancement of other senses (such as the ability to sense vibrations in water). Aquatic troglobites (or stygobites), such as the endangered Alabama cave shrimp, live in bodies of water found in caves and get nutrients from detritus washed into their caves and from the feces of bats and other cave inhabitants. Other aquatic troglobites include cave fish, and cave salamanders such as the olm and the Texas blind salamander.
Cave insects such as Oligaphorura (formerly Archaphorura) schoetti are troglophiles, reaching in length. They have extensive distribution and have been studied fairly widely. Most specimens are female, but a male specimen was collected from St Cuthberts Swallet in 1969.
Bats, such as the gray bat and Mexican free-tailed bat, are trogloxenes and are often found in caves; they forage outside of the caves. Some species of cave crickets are classified as trogloxenes, because they roost in caves by day and forage above ground at night.
Because of the fragile nature of the cave ecosystem, and the fact that cave regions tend to be isolated from one another, caves harbor a number of endangered species, such as the Tooth cave spider, liphistius trapdoor spider, and the gray bat.
Caves are visited by many surface-living animals, including humans. These are usually relatively short-lived incursions, due to the lack of light and sustenance.
Cave entrances often have typical florae. For instance, in the eastern temperate United States, cave entrances are most frequently (and often densely) populated by the bulblet fern, "Cystopteris bulbifera".
Archaeological and cultural importance.
Throughout history, primitive peoples have made use of caves. The earliest human fossils found in caves come from a series of caves near Krugersdorp and Mokopane in South Africa. The cave sites of Sterkfontein, Swartkrans, Kromdraai B, Drimolen, Malapa, Cooper's D, Gladysvale, Gondolin and Makapansgat have yielded a range of early human species dating back to between three and one million years ago, including Australopithecus africanus, Australopithecus sediba and Paranthropus robustus. However, it is not generally thought that these early humans were living in the caves, but that they were brought into the caves by carnivores that had killed them.
The first early hominid ever found in Africa, the Taung Child in 1924, was also thought for many years to come from a cave, where it had been deposited after being predated on by an eagle. However, this is now debated (Hopley et al., 2013; Am. J. Phys. Anthrop.). Caves do form in the dolomite of the Ghaap Plateau, including the Early, Middle and Later Stone Age site of Wonderwerk Cave; however, the caves that form along the escarpment's edge, like that hypothesised for the Taung Child, are formed within a secondary limestone deposit called tufa. There is numerous evidence for other early human species inhabiting caves from at least one million years ago in different parts of the world, including Homo erectus in China at Zhoukoudian, Homo rhodesiensis in South Africa at the Cave of Hearths (Makapansgat), Homo neandertalensis and Homo heidelbergensis in Europe at Archaeological Site of Atapuerca, Homo floresiensis in Indonesia, and the Denisovans in southern Siberia.
In southern Africa, early modern humans regularly used sea caves as shelter starting about 180,000 years ago when they learned to exploit the sea for the first time (Marean et al., 2007; Nature). The oldest known site is PP13B at Pinnacle Point. This may have allowed rapid expansion of humans out of Africa and colonization of areas of the world such as Australia by 60-50,000 years ago. Throughout southern Africa, Australia, and Europe, early modern humans used caves and rock shelters as sites for rock art, such as those at Giants Castle. Caves such as the yaodong in China were used for shelter; other caves were used for burials (such as rock-cut tombs), or as religious sites (such as ). Among the known sacred caves are China's Cave of a Thousand Buddhas and the sacred caves of Crete.

</doc>
<doc id="5781" url="https://en.wikipedia.org/wiki?curid=5781" title="Chinese numerals">
Chinese numerals

Chinese numerals are words and characters used to denote numbers in Chinese.
Today speakers of Chinese use three written numeral systems: the system of Arabic numerals used world-wide, and two indigenous systems. The more familiar indigenous system is based on Chinese characters that correspond to numerals in the spoken language. These are shared with other languages of the Chinese cultural sphere such as Japanese, Korean and Vietnamese. Most people and institutions in China primarily use the Arabic system for convenience, with traditional Chinese numerals used in finance, mainly for writing amounts on checks, banknotes, some ceremonial occasions, some boxes, and on commercials.
The other indigenous system is the Suzhou numerals, or "huama", a positional system, the only surviving form of the rod numerals. These were once used by Chinese mathematicians, and later in Chinese markets, such as those in Hong Kong before the 1990s, but have been gradually supplanted by Arabic (and also Roman) numerals.
Characters used to represent numbers.
The Chinese character numeral system consists of the Chinese characters used by the Chinese written language to write spoken numerals. Similar to spelling-out numbers in English (e.g., "one thousand nine hundred forty-five"), it is not an independent system "per se". Since it reflects spoken language, it does not use the positional system as in Arabic numerals, in the same way that spelling out numbers in English does not.
Standard numbers.
There are characters representing the numbers zero through nine, and other characters representing larger numbers such as tens, hundreds, thousands and so on. There are two sets of characters for Chinese numerals: one for everyday writing and one for use in commercial or financial contexts known as "dàxiě" (). The latter arose because the characters used for writing numerals are geometrically simple, so simply using those numerals cannot prevent forgeries in the same way spelling numbers out in English would. A forger could easily change the everyday characters 三十 (30) to 五千 (5000) just by adding a few strokes. That would not be possible when writing using the financial characters 參拾(30) and 伍仟 (5000). They are also referred to as "banker's numerals", "anti-fraud numerals", or "banker's anti-fraud numerals". For the same reason, rod numerals were never used in commercial records.
T denotes Traditional Chinese characters, S denotes Simplified Chinese characters.
Characters with military usage.
In the PLA, some numbers will have altered names when used for clearer radio communications. They are:
Large numbers.
For numbers larger than 10,000, similarly to the long and short scales in the West, there have been four systems in ancient and modern usage. The original one, with unique names for all powers of ten up to the 14th, is ascribed to the Yellow Emperor in the 6th century book by Zhen Luan, "Wujing suanshu" (Arithmetic in Five Classics). In modern Chinese only the second system is used, in which the same ancient names are used, but each represents a number 10,000 (myriad, 萬 wàn) times the previous:
In practice, this situation does not lead to ambiguity, with the exception of 兆 (zhào), which means 10 according to the system in common usage throughout the Chinese communities as well as in Japan and Korea, but has also been used for 10 in recent years (especially in mainland China for megabyte). To avoid problems arising from the ambiguity, the PRC government never uses this character in official documents, but uses 万亿 (wànyì) instead. The ROC government in Taiwan uses 兆 (zhào) to mean 10 in official documents.
Numbers from Buddhism.
Numerals beyond 載 zài come from Buddhist texts in Sanskrit, but are mostly found in ancient texts.
Small numbers.
The following are characters used to denote small order of magnitude in Chinese historically. With the introduction of SI units, some of them have been incorporated as SI prefixes, while the rest have fallen into disuse.
SI prefixes.
In the People's Republic of China, the translations for the SI prefixes in 1981 were different from those used today. The larger (兆, 京, 垓, 秭, 穰) and smaller Chinese numerals (微, 纖, 沙, 塵, 渺) were defined as translations for the SI prefixes as "mega", "giga", "tera", "peta", "exa", "micro", "nano", "pico", "femto", "atto", resulting in the creation of yet more values for each numeral.
The Republic of China (Taiwan) defined 百萬 as the translation for "mega". This translation is widely used in official documents, academic communities, informational industries, etc. However, the civil broadcasting industries sometimes use 兆赫 to represent "megahertz".
Today, the governments of both China and Taiwan use phonetic transliterations for the SI prefixes. However, the governments have each chosen different Chinese characters for certain prefixes. The following table lists the two different standards together with the early translation.
Reading and transcribing numbers.
Whole numbers.
Multiple-digit numbers are constructed using a multiplicative principle; first the digit itself (from 1 to 9), then the place (such as 10 or 100); then the next digit.
In Mandarin, the multiplier 兩 (liǎng) is often used rather than 二 (èr) for all numbers 200 and greater with the "2" numeral (although as noted earlier this varies from dialect to dialect and person to person). Use of both 兩 (liǎng) or 二 (èr) are acceptable for the number 200. When writing in the Cantonese dialect, 二 (yi) is used to represent the "2" numeral for all numbers. In the southern Min dialect of Chaozhou (Teochew), 兩 (no) is used to represent the "2" numeral in all numbers from 200 onwards. Thus:
For the numbers 11 through 19, the leading "one" (一) is usually omitted. In some dialects, like Shanghainese, when there are only two significant digits in the number, the leading "one" and the trailing zeroes are omitted. Sometimes, the one before "ten" in the middle of a number, such as 213, is omitted. Thus:
Notes:
In certain older texts like the Protestant Bible or in poetic usage, numbers such as 114 may be "written" as 0 (百十四).
For numbers larger than a myriad, the same grouping system used in English applies, except in groups of four places (myriads) rather than in groups of three (thousands). Hence it is more convenient to think of numbers here as in groups of four, thus 1,234,567,890 is regrouped here as 12,3456,7890. Larger than a myriad, each number is therefore four zeroes longer than the one before it, thus 10000 × wàn (萬) = yì (億). If one of the numbers is between 10 and 19, the leading "one" is omitted as per the above point. Hence (numbers in parentheses indicate that the number has been written as one number rather than expanded):
Interior zeroes before the unit position (as in 1002) must be spelt explicitly. The reason for this is that trailing zeroes (as in 1200) are often omitted as shorthand, so ambiguity occurs. One zero is sufficient to resolve the ambiguity. Where the zero is before a digit other than the units digit, the explicit zero is not ambiguous and is therefore optional, but preferred. Thus:
Fractional values.
To construct a fraction, the denominator is written first, followed by 分之 ("parts of") and then the numerator. This is the opposite of how fractions are read in English, which is numerator first. Each half of the fraction is written the same as a whole number. Mixed numbers are written with the whole-number part first, followed by 又 ("and"), then the fractional part.
Percentages are constructed similarly, using 百 (100) as the denominator. The 一 (one) before 百 is omitted.
Decimal numbers are constructed by first writing the whole number part, then inserting a point (), and finally the decimal expression. The decimal expression is written using only the digits for 0 to 9, without multiplicative words.
Ordinal numbers.
Ordinal numbers are formed by adding 第 "dì" ("sequence") before the number.
Negative numbers.
Negative numbers are formed by adding (, Pinyin: fù, Jyuping: fu6) before the number.
Usage.
Chinese grammar requires the use of classifiers (measure words) when a numeral is used together with a noun to express a quantity. For example, "three people" is expressed as 三个人 "sān ge rén", "three GE person", where 个 "ge" is a classifier. There exist many different classifiers, for use with different sets of nouns, although 个 is the most common, and may be used informally in place of other classifiers.
Chinese uses cardinal numbers in certain situations in which English would use ordinals. For example, 三楼 "sān lóu" (literally "three story") means "third floor" ("second floor" in British numbering). Likewise, 二十一世纪 "èrshí yī shìjì" (literally "twenty-one century") is used for "21st century".
Numbers of years are commonly spoken as a sequence of digits, as in 二零零一 "èr líng líng yī" ("two zero zero one") for the year 2001. Names of months and days (in the Western system) are also expressed using numbers: 一月 "yīyuè" ("one month") for January, etc.; and 星期一 "xīngqīyī" ("week one") for Monday, etc. (although Sunday is 星期日 "xīngqīrì", or informally 星期天 "xīngqītiān", "week day"). Full dates are usually written in the format 2001年1月20日 for January 20, 2001 (using 年 "nián" "year", 月 "yuè" "month", and 日 "rì" "day") – all the numbers are read as cardinals, not ordinals, with no leading zeroes, and the year is read as a sequence of digits. For brevity the year "yuè" and "rì" may be dropped to give a date composed of just numbers, so for example 64, in Chinese is six-four, short for month six-day four i.e. June Fourth, a common Chinese shorthand for the Tiananmen Square protests of 1989.
Counting rod and Suzhou numerals.
In the same way that Roman numerals were standard in ancient and medieval Europe for mathematics and commerce, the Chinese formerly used the rod numerals, which is a positional system. The Suzhou numerals () system is a variation of the Southern Song rod numerals. Nowadays, the "huāmǎ" system is only used for displaying prices in Chinese markets or on traditional handwritten invoices.
Hand gestures.
There is a common method of using of one hand to signify the numbers one to ten. While the five digits on one hand can express the numbers one to five, six to ten have special signs that can be used in commerce or day-to-day communication.
Historical use of numerals in China.
Most Chinese numerals of later periods were descendants of the Shang dynasty oracle numerals of the 14th century BC. The oracle bone script numerals were found on tortoise shell and animal bones. In early civilizations, the Shang were able to express any numbers, however large, with only nine symbols and a counting board.
Some of the bronze script numerals such as 1, 2, 3, 4, 10, 11, 12, and 13 became part of the system of rod numerals.
In this system, horizontal rod numbers are used for the tens, thousands, hundred thousands etc. Sun Tzu wrote that "one is vertical, ten is horizontal".
The counting rod numerals system has place value and decimal numerals for computation, and was used widely by Chinese merchants, mathematicians and astronomers from the Han dynasty to the 16th century.
Alexander Wylie, Christian missionary to China, in 1853 already refuted the notion that "the Chinese numbers were written in words at length", and stated that in ancient China, calculation was carried out by means of counting rods, and "the written character is evidently a rude presentation of these". After being introduced to the rod numerals, he said "Having thus obtained a simple but effective system of figures, we find the Chinese in actual use of a method of notation depending on the theory of local value .e. place-valu, several centuries before such theory was understood in Europe, and while yet the science of numbers had scarcely dawned among the Arabs."
During the Ming and Qing dynasties (after Arabic numerals were introduced into China), some Chinese mathematicians used Chinese numeral characters as positional system digits. After the Qing period, both the Chinese numeral characters and the Suzhou numerals were replaced by Arabic numerals in mathematical writings.
Cultural influences.
Traditional Chinese numeric characters are also used in Japan and Korea and were used in Vietnam before the 20th century. In vertical text (that is, read top to bottom), using characters for numbers is the norm, while in horizontal text, Arabic numerals are most common. Chinese numeric characters are also used in much the same formal or decorative fashion that Roman numerals are in Western cultures. Chinese numerals may appear together with Arabic numbers on the same sign or document.

</doc>
<doc id="5783" url="https://en.wikipedia.org/wiki?curid=5783" title="Computer program">
Computer program

A computer program is a collection of instructions that performs a specific task when executed by a computer. A computer requires programs to function, and typically executes the program's instructions in a central processing unit.
A computer program is usually written by a computer programmer in a programming language. From the program in its human-readable form of source code, a compiler can derive machine code—a form consisting of instructions that the computer can directly execute. Alternatively, a computer program may be executed with the aid of an interpreter.
A part of a computer program that performs a well-defined task is known as an algorithm. A collection of computer programs, libraries and related data are referred to as software. Computer programs may be categorized along functional lines, such as application software or system software.
History.
Early programmable machines.
The earliest programmable machines preceded the invention of the digital computer. In 1801, Joseph-Marie Jacquard devised a loom that would weave a pattern by following a series of perforated cards. Patterns, including flowers and leaves, could be weaved and repeated by arranging the cards.
Analytical Engine.
In 1837, Charles Babbage was inspired by Jacquard's loom to attempt to build the Analytical Engine.
The names of the components of the calculating device were borrowed from the textile industry. In the textile industry, yarn was brought from the store to be milled. The device would have had a "store"—memory to hold 1,000 numbers of 40 decimal digits each. Numbers from the "store" would then have then been transferred to the "mill" (analogous to the CPU of a modern machine), for processing. It was programmed using two sets of perforated cards—one to direct the operation and the other for the input variables.
During a nine-month period in 1842–43, Ada Lovelace translated the memoir of Italian mathematician Luigi Menabrea. The memoir covered the Analytical Engine. The translation contained Note G which completely detailed a method for calculating Bernoulli numbers using the Analytical Engine. This note is recognized by some historians as the world's first written computer program.
Universal Turing machine.
In 1936, Alan Turing introduced the Universal Turing machine—a theoretical device that can model every computation that can be performed on a Turing complete computing machine.
It is a finite-state machine that has an infinitely long read/write tape. The machine can move the tape back and forth, changing its contents as it performs an algorithm. The machine starts in the initial state, goes through a sequence of steps, and halts when it encounters the halt state.
This machine is considered by some to be the origin of the stored-program computer—used by John von Neumann (1946) for the "Electronic Computing Instrument" that now bears the von Neumann architecture name.
Early programmable computers.
The Z3 computer, invented by Konrad Zuse (1941) in Germany, was a digital and programmable computer. A digital computer uses electricity as the calculating component. The Z3 contained 2,400 relays to create the circuits. The circuits provided a binary, floating-point, nine-instruction computer. Programming the Z3 was through a specially designed keyboard and punched tape.
The Electronic Numerical Integrator And Computer (Fall 1945) was a Turing complete, general-purpose computer that used 17,468 vacuum tubes to create the circuits. At its core, it was a series of Pascalines wired together. Its 40 units weighed 30 tons, occupied 1,800 square feet, and consumed $650 per hour (in 1940s currency) in electricity when idle. It had 20 base-10 accumulators. Programming the ENIAC took up to two months. Three function tables were on wheels and needed to be rolled to fixed function panels. Function tables were connected to function panels using heavy black cables. Each function table had 728 rotating knobs. Programming the ENIAC also involved setting some of the 3,000 switches. Debugging a program took a week. The ENIAC featured parallel operations. Different sets of accumulators could simultaneously work on different algorithms. It used punched card machines for input and output, and it was controlled with a clock signal. It ran for eight years, calculating hydrogen bomb parameters, predicting weather patterns, and producing firing tables to aim artillery guns.
The Manchester Small-Scale Experimental Machine (June 1948) was a stored-program computer. Programming transitioned away from moving cables and setting dials; instead, a computer program was stored in memory as numbers. Only three bits of memory were available to store each instruction, so it was limited to eight instructions. 32 switches were available for programming.
Later computers.
Computers manufactured until the 1970s had front-panel switches for programming. The computer program was written on paper for reference. An instruction was represented by a configuration of on/off settings. After setting the configuration, an execute button was pressed. This process was then repeated. Computer programs also were manually input via paper tape or punched cards. After the medium was loaded, the starting address was set via switches and the execute button pressed.
In 1961, the Burroughs B5000 was built specifically to be programmed in the ALGOL 60 language. The hardware featured circuits to ease the compile phase.
In 1964, the IBM System/360 was a line of six computers each having the same instruction set architecture. The Model 30 was the smallest and least expensive. Customers could upgrade and retain the same application software. Each System/360 model featured multiprogramming. With operating system support, multiple programs could be in memory at once. When one was waiting for input/output, another could compute. Each model also could emulate other computers. Customers could upgrade to the System/360 and retain their IBM 7094 or IBM 1401 application software.
Computer programming.
Computer programming is the process of writing or editing source code. Editing source code involves testing, analyzing, refining, and sometimes coordinating with other programmers on a jointly developed program. A person who practices this skill is referred to as a computer programmer, software developer, and sometimes coder.
The sometimes lengthy process of computer programming is usually referred to as software development. The term software engineering is becoming popular as the process is seen as an engineering discipline.
Programming languages.
Computer programs can be categorized by the programming language paradigm used to produce them. Two of the main paradigms are imperative and declarative.
Imperative languages.
"Imperative programming languages" specify a sequential algorithm using declarations, expressions, and statements:
One criticism of imperative languages is the side effect of an assignment statement on a class of variables called non-local variables.
Declarative languages.
"Declarative programming languages" describe "what" computation should be performed and not "how" to compute it. Declarative programs omit the control flow and are considered "sets" of instructions. Two broad categories of declarative languages are functional languages and logical languages. The principle behind functional languages (like Haskell) is to not allow side effects, which makes it easier to reason about programs like mathematical functions. The principle behind logical languages (like Prolog) is to define the problem to be solved – the goal – and leave the detailed solution to the Prolog system itself. The goal is defined by providing a list of subgoals. Then each subgoal is defined by further providing a list of its subgoals, etc. If a path of subgoals fails to find a solution, then that subgoal is backtracked and another path is systematically attempted.
Compilation and interpretation.
A computer program in the form of a human-readable, computer programming language is called source code. Source code may be converted into an executable image by a compiler or executed immediately with the aid of an interpreter.
Compilers are used to translate source code from a programming language into either object code or machine code. Object code needs further processing to become machine code, and machine code consists of the central processing unit's native instructions, ready for execution. Compiled computer programs are commonly referred to as executables, binary images, or simply as binaries – a reference to the binary file format used to store the executable code.
Interpreters are used to execute source code from a programming language immediately, without an intermediate file. The interpreter decodes each statement and performs its behavior. One advantage of interpreters is the ability to generate an interactive session. The programmer is presented with a prompt, and individual lines of code are typed in and performed immediately.
The main disadvantage of interpreters is computer programs run slower than when compiled. Interpreting code is slower because the interpreter must decode each statement and then perform it. However, software development may be faster using an interpreter because testing is immediate when the compiling step is omitted. Another disadvantage of interpreters is an interpreter must be present on the executing computer. By contrast, compiled computer programs need no compiler present during execution.
Just in time compilers pre-compile computer programs ahead of time and interpret them later. For example, Java computer programs are pre-compiled into a file containing bytecode. Bytecode is then executed by an interpreter called a virtual machine.
Either compiled or interpreted programs might be executed in a batch process without human interaction. Batch programming languages are called scripting languages. One common scripting language is Unix shell, and its executing environment is called the command-line interface.
No properties of a programming language require it to be exclusively compiled or exclusively interpreted. The categorization usually reflects the most popular method of language execution. For example, BASIC is thought of as an interpreted language and C a compiled language, despite the existence of BASIC compilers and C interpreters.
Storage and execution.
Typically, computer programs are stored in non-volatile memory until requested either directly or indirectly to be executed by the computer user. Upon such a request, the program is loaded into random-access memory, by a computer program called an operating system, where it can be accessed directly by the central processor. The central processor then executes ("runs") the program, instruction by instruction, until termination. A program in execution is called a process. Termination is either by normal self-termination or by error – software or hardware error.
Simultaneous execution.
Many operating systems support multitasking which enables many computer programs to appear to run simultaneously on one computer. Operating systems may run multiple programs through process scheduling – a software mechanism to switch the CPU among processes often so users can interact with each program while it runs. Within hardware, modern day multiprocessor computers or computers with multicore processors may run multiple programs.
Multiple lines of the same computer program may be simultaneously executed using threads. Multithreading processors are optimized to execute multiple threads efficiently.
Self-modifying programs.
A computer program in execution is normally treated as being different from the data the program operates on. However, in some cases, this distinction is blurred when a computer program modifies itself. The modified computer program is subsequently executed as part of the same program. Self-modifying code is possible for programs written in machine code, assembly language, Lisp, C, COBOL, PL/1, and Prolog.
Functional categories.
Computer programs may be categorized along functional lines. The main functional categories are application software and system software. System software includes the operating system which couples computer hardware with application software. The purpose of the operating system is to provide an environment in which application software executes in a convenient and efficient manner. In addition to the operating system, system software includes embedded programs, boot programs, and micro programs. Application software designed for end users have a user interface. Application software not designed for the end user includes middleware, which couples one application with another. Application software also includes utility programs. The distinction between system software and application software is under debate.
Application software.
There are many types of application software:
Utility programs.
Utility programs are application programs designed to aid system administrators and computer programmers.
Operating system.
An operating system is a computer program that acts as an intermediary between a user of a computer and the computer hardware.
In the 1950s, the programmer, who was also the operator, would write a program and run it.
After the program finished executing, the output may have been printed, or it may have been punched onto paper tape or cards for later processing.
More often than not the program did not work.
The programmer then looked at the console lights and fiddled with the console switches. If less fortunate, a memory printout was made for further study.
In the 1960s, programmers reduced the amount of wasted time by automating the operator's job. A program called an "operating system" was kept in the computer at all times.
Originally, operating systems were programmed in assembly; however, modern operating systems are typically written in C.
Boot program.
A stored-program computer requires an initial computer program stored in its read-only memory to boot. The boot process is to identify and initialize all aspects of the system, from processor registers to device controllers to memory contents. Following the initialization process, this initial computer program loads the operating system and sets the program counter to begin normal operations.
Embedded programs.
Independent of the host computer, a hardware device might have embedded firmware to control its operation. Firmware is used when the computer program is rarely or never expected to change, or when the program must not be lost when the power is off.
Microcode programs.
Microcode programs control some central processing units and some other hardware. This code moves data between the registers, buses, arithmetic logic units, and other functional units in the CPU. Unlike conventional programs, microcode is not usually written by, or even visible to, the end users of systems, and is usually provided by the manufacturer, and is considered internal to the device.

</doc>
<doc id="5785" url="https://en.wikipedia.org/wiki?curid=5785" title="Crime">
Crime

In ordinary language, the term crime denotes an unlawful act punishable by a state. The term "crime" does not, in modern criminal law, have any simple and universally accepted definition, though statutory definitions have been provided for certain purposes. The most popular view is that crime is a category created by law; in other words, something is a crime if declared as such by the relevant and applicable law. One proposed definition is that a crime or offence (or criminal offence) is an act harmful not only to some individual or individuals but also to a community, society or the state ("a public wrong"). Such acts are forbidden and punishable by law.
The notion that acts such as murder, rape and theft are to be prohibited exists worldwide. What precisely is a criminal offence is defined by criminal law of each country. While many have a catalogue of crimes called the criminal code, in some common law countries no such comprehensive statute exists.
The state (government) has the power to severely restrict one's liberty for committing a crime. In modern societies, there are procedures to which investigations and trials must adhere. If found guilty, an offender may be sentenced to a form of reparation such as a community sentence, or, depending on the nature of their offence, to undergo imprisonment, life imprisonment or, in some jurisdictions, execution.
Usually, to be classified as a crime, the "act of doing something criminal" ("actus reus") must – with certain exceptions – be accompanied by the "intention to do something criminal" ("mens rea").
While every crime violates the law, not every violation of the law counts as a crime. Breaches of private law (torts and breaches of contract) are not automatically punished by the state, but can be enforced through civil procedure.
Overview.
When informal relationships and sanctions prove insufficient to establish and maintain a desired social order, a government or a state may impose more formalized or stricter systems of social control. With institutional and legal machinery at their disposal, agents of the State can compel populations to conform to codes and can opt to punish or attempt to reform those who do not conform.
Authorities employ various mechanisms to regulate (encouraging or discouraging) certain behaviors in general. Governing or administering agencies may for example codify rules into laws, police citizens and visitors to ensure that they comply with those laws, and implement other policies and practices that legislators or administrators have prescribed with the aim of discouraging or preventing crime. In addition, authorities provide remedies and sanctions, and collectively these constitute a criminal justice system. Legal sanctions vary widely in their severity; they may include (for example) incarceration of temporary character aimed at reforming the convict. Some jurisdictions have penal codes written to inflict permanent harsh punishments: legal mutilation, capital punishment or life without parole.
Usually a natural person perpetrates a crime, but legal persons may also commit crimes. Conversely, at least under U.S. law, nonpersons such as animals cannot commit crimes.
The sociologist Richard Quinney has written about the relationship between society and crime. When Quinney states "crime is a social phenomenon" he envisages both how individuals conceive crime and how populations perceive it, based on societal norms.
Etymology.
The word "crime" is derived from the Latin root "cernō", meaning "I decide, I give judgment". Originally the Latin word "crīmen" meant "charge" or "cry of distress." The Ancient Greek word "krima" (κρίμα), from which the Latin cognate derives, typically referred to an intellectual mistake or an offense against the community, rather than a private or moral wrong.
In 13th century English "crime" meant "sinfulness", according to etymonline.com. It was probably brought to England as Old French "crimne" (12th century form of Modern French "crime"), from Latin "crimen" (in the genitive case: "criminis"). In Latin, "crimen" could have signified any one of the following: "charge, indictment, accusation; crime, fault, offense".
The word may derive from the Latin "cernere" – "to decide, to sift" (see crisis, mapped on Kairos and Chronos). But Ernest Klein (citing Karl Brugmann) rejects this and suggests *cri-men, which originally would have meant "cry of distress". Thomas G. Tucker suggests a root in "cry" words and refers to English plaint, plaintiff, and so on. The meaning "offense punishable by law" dates from the late 14th century. The Latin word is glossed in Old English by "facen", also "deceit, fraud, treachery", f. fak. "Crime wave" is first attested in 1893 in American English.
Definition.
England and Wales.
Whether a given act or omission constitutes a crime does not depend on the nature of that act or omission. It depends on the nature of the legal consequences that may follow it. An act or omission is a crime if it is capable of being followed by what are called criminal proceedings.
History
The following definition of "crime" was provided by the Prevention of Crimes Act 1871, and applied for the purposes of section 10 of the Prevention of Crime Act 1908:
Scotland.
For the purpose of section 243 of the Trade Union and Labour Relations (Consolidation) Act 1992, a crime means an offence punishable on indictment, or an offence punishable on summary conviction, and for the commission of which the offender is liable under the statute making the offence punishable to be imprisoned either absolutely or at the discretion of the court as an alternative for some other punishment.
Sociology.
A normative definition views crime as deviant behavior that violates prevailing normscultural standards prescribing how humans ought to behave normally. This approach considers the complex realities surrounding the concept of crime and seeks to understand how changing social, political, psychological, and economic conditions may affect changing definitions of crime and the form of the legal, law-enforcement, and penal responses made by society.
These structural realities remain fluid and often contentious. For example: as cultures change and the political environment shifts, societies may criminalise or decriminalise certain behaviours, which directly affects the statistical crime rates, influence the allocation of resources for the enforcement of laws, and (re-)influence the general public opinion.
Similarly, changes in the collection and/or calculation of data on crime may affect the public perceptions of the extent of any given "crime problem". All such adjustments to crime statistics, allied with the experience of people in their everyday lives, shape attitudes on the extent to which the State should use law or social engineering to enforce or encourage any particular social norm. Behaviour can be controlled and influenced by a society in many ways without having to resort to the criminal justice system.
Indeed, in those cases where no clear consensus exists on a given norm, the drafting of criminal law by the group in power to prohibit the behaviour of another group may seem to some observers an improper limitation of the second group's freedom, and the ordinary members of society have less respect for the law or laws in general — whether the authorities actually enforce the disputed law or not.
Other definitions.
Legislatures can pass laws (called "mala prohibita") that define crimes against social norms. These laws vary from time to time and from place to place: note variations in gambling laws, for example, and the prohibition or encouragement of duelling in history. Other crimes, called "mala in se", count as outlawed in almost all societies, (murder, theft and rape, for example).
English criminal law and the related criminal law of Commonwealth countries can define offences that the courts alone have developed over the years, without any actual legislation: common law offences. The courts used the concept of "malum in se" to develop various common law offences.
Criminalization.
One can view criminalization as a procedure deployed by society as a preemptive harm-reduction device, using the threat of punishment as a deterrent to anyone proposing to engage in the behavior causing harm. The State becomes involved because governing entities can become convinced that the costs of not criminalizing (through allowing the harms to continue unabated) outweigh the costs of criminalizing it (restricting individual liberty, for example, to minimize harm to others).
Criminalization may provide future harm reduction at least to the outside population, assuming those shamed or incarcerated or otherwise restrained for committing crimes start out more prone to criminal behaviour. Likewise, one might assume that criminalizing acts that in themselves do not harm other people ("victimless crimes") may prevent subsequent harmful acts (assuming that people "prone" to commit these acts may tend to commit harmful actions in general). Some see the criminalization of "victimless crimes" as a pretext for imposing personal, religious or moral convictions on otherwise productive citizens or taxpayers.
Some commentators may see criminalization as a way to make potential criminals pay or suffer for their prospective crimes. In this case, criminalization becomes a way to set the price that one must pay to society for certain actions considered detrimental to society as a whole. An extreme view might see criminalization as State-sanctioned revenge.
States control the process of criminalization because:
Labelling theory.
The label of "crime" and the accompanying social stigma normally confine their scope to those activities seen as injurious to the general population or to the State, including some that cause serious loss or damage to individuals. Those who apply the labels of "crime" or "criminal" intend to assert the hegemony of a dominant population, or to reflect a consensus of condemnation for the identified behavior and to justify any punishments prescribed by the State (in the event that standard processing tries and convicts an accused person of a crime).
Natural-law theory.
Justifying the State's use of force to coerce compliance with its laws has proven a consistent theoretical problem. One of the earliest justifications involved the theory of natural law. This posits that the nature of the world or of human beings underlies the standards of morality or constructs them. Thomas Aquinas wrote in the 13th century: "the rule and measure of human acts is the reason, which is the first principle of human acts" (Aquinas, ST I-II, Q.90, A.I). He regarded people as by nature rational beings, concluding that it becomes morally appropriate that they should behave in a way that conforms to their rational nature. Thus, to be valid, any law must conform to natural law and coercing people to conform to that law is morally acceptable. In the 1760s William Blackstone (1979: 41) described the thesis:
But John Austin (1790–1859), an early positivist, applied utilitarianism in accepting the calculating nature of human beings and the existence of an objective morality. He denied that the legal validity of a norm depends on whether its content conforms to morality. Thus in Austinian terms a moral code can objectively determine what people ought to do, the law can embody whatever norms the legislature decrees to achieve social utility, but every individual remains free to choose what to do. Similarly, Hart (1961) saw the law as an aspect of sovereignty, with lawmakers able to adopt any law as a means to a moral end.
Thus the necessary and sufficient conditions for the truth of a proposition of law simply involved internal logic and consistency, and that the state's agents used state power with responsibility. Ronald Dworkin (2005) rejects Hart's theory and proposes that all individuals should expect the equal respect and concern of those who govern them as a fundamental political right. He offers a theory of compliance overlaid by a theory of deference (the citizen's duty to obey the law) and a theory of enforcement, which identifies the legitimate goals of enforcement and punishment. Legislation must conform to a theory of legitimacy, which describes the circumstances under which a particular person or group is entitled to make law, and a theory of legislative justice, which describes the law they are entitled or obliged to make.
Indeed, despite everything, the majority of natural-law theorists have accepted the idea of enforcing the prevailing morality as a primary function of the law. This view entails the problem that it makes any moral criticism of the law impossible: if conformity with natural law forms a necessary condition for legal validity, all valid law must, by definition, count as morally just. Thus, on this line of reasoning, the legal validity of a norm necessarily entails its moral justice.
One can solve this problem by granting some degree of moral relativism and accepting that norms may evolve over time and, therefore, one can criticize the continued enforcement of old laws in the light of the current norms. People may find such law acceptable, but the use of State power to coerce citizens to comply with that law lacks moral justification. More recent conceptions of the theory characterise crime as the violation of individual rights.
Since society considers so many rights as natural (hence the term "right") rather than man-made, what constitutes a crime also counts as natural, in contrast to laws (seen as man-made). Adam Smith illustrates this view, saying that a smuggler would be an excellent citizen, "...had not the laws of his country made that a crime which nature never meant to be so."
Natural-law theory therefore distinguishes between "criminality" (which derives from human nature) and "illegality" (which originates with the interests of those in power). Lawyers sometimes express the two concepts with the phrases "malum in se" and "malum prohibitum" respectively. They regard a "crime "malum in se"" as inherently criminal; whereas a "crime "malum prohibitum"" (the argument goes) counts as criminal only because the law has decreed it so.
It follows from this view that one can perform an illegal act without committing a crime, while a criminal act could be perfectly legal. Many Enlightenment thinkers (such as Adam Smith and the American Founding Fathers) subscribed to this view to some extent, and it remains influential among so-called classical liberals and libertarians.
History.
Some religious communities regard sin as a crime; some may even highlight the crime of sin very early in legendary or mythological accounts of origins — note the tale of Adam and Eve and the theory of original sin. What one group considers a crime may cause or ignite war or conflict. However, the earliest known civilizations had codes of law, containing both civil and penal rules mixed together, though not always in recorded form.
The Sumerians produced the earliest surviving written codes. Urukagina (reigned c. 2380 BC–2360 BC, short chronology) had an early code that has not survived; a later king, Ur-Nammu, left the earliest extant written law system, the Code of Ur-Nammu (c. 2100-2050 BC), which prescribed a formal system of penalties for specific cases in 57 articles. The Sumerians later issued other codes, including the "code of Lipit-Ishtar". This code, from the 20th century BCE, contains some fifty articles, and scholars have reconstructed it by comparing several sources. 
Successive legal codes in Babylon, including the code of Hammurabi (c. 1790 BC), reflected Mesopotamian society's belief that law derived from the will of the gods (see Babylonian law).
Many states at this time functioned as theocracies, with codes of conduct largely religious in origin or reference. In the Sanskrit texts of Dharmaśāstra (c. 1250 B.C.), issues such as legal and religious duties, code of conduct, penalties and remedies, etc. have been discussed and forms one of the elaborate and earliest source of legal code.
Sir Henry Maine (1861) studied the ancient codes available in his day, and failed to find any criminal law in the "modern" sense of the word. While modern systems distinguish between offences against the "State" or "community", and offences against the "individual", the so-called penal law of ancient communities did not deal with "crimes" (Latin: "crimina"), but with "wrongs" (Latin: "delicta"). Thus the Hellenic laws treated all forms of theft, assault, rape, and murder as private wrongs, and left action for enforcement up to the victims or their survivors. The earliest systems seem to have lacked formal courts.
The Romans systematized law and applied their system across the Roman Empire. Again, the initial rules of Roman law regarded assaults as a matter of private compensation. The most significant Roman law concept involved "dominion". The "pater familias" owned all the family and its property (including slaves); the "pater" enforced matters involving interference with any property. The "Commentaries" of Gaius (written between 130 and 180 AD) on the Twelve Tables treated "furtum" (in modern parlance: "theft") as a tort.
Similarly, assault and violent robbery involved trespass as to the "pater's" property (so, for example, the rape of a slave could become the subject of compensation to the "pater" as having trespassed on his "property"), and breach of such laws created a "vinculum juris" (an obligation of law) that only the payment of monetary compensation (modern "damages") could discharge. Similarly, the consolidated Teutonic laws of the Germanic tribes, included a complex system of monetary compensations for what courts would consider the complete range of criminal offences against the person, from murder down.
Even though Rome abandoned its Britannic provinces around 400 AD, the Germanic mercenarieswho had largely become instrumental in enforcing Roman rule in Britanniaacquired ownership of land there and continued to use a mixture of Roman and Teutonic Law, with much written down under the early Anglo-Saxon kings. But only when a more centralized English monarchy emerged following the Norman invasion, and when the kings of England attempted to assert power over the land and its peoples, did the modern concept emerge, namely of a crime not only as an offence against the "individual", but also as a wrong against the "State".
This idea came from common law, and the earliest conception of a criminal act involved events of such major significance that the "State" had to usurp the usual functions of the civil tribunals, and direct a special law or "privilegium" against the perpetrator. All the earliest English criminal trials involved wholly extraordinary and arbitrary courts without any settled law to apply, whereas the civil (delictual) law operated in a highly developed and consistent manner (except where a king wanted to raise money by selling a new form of writ). The development of the idea that the "State" dispenses justice in a court only emerges in parallel with or after the emergence of the concept of sovereignty.
In continental Europe, Roman law persisted, but with a stronger influence from the Christian Church.
Coupled with the more diffuse political structure based on smaller feudal units, various legal traditions emerged, remaining more strongly rooted in Roman jurisprudence, but modified to meet the prevailing political climate.
In Scandinavia the effect of Roman law did not become apparent until the 17th century, and the courts grew out of the "things" — the assemblies of the people. The people decided the cases (usually with largest freeholders dominating). This system later gradually developed into a system with a royal judge nominating a number of the most esteemed men of the parish as his board, fulfilling the function of "the people" of yore.
From the Hellenic system onwards, the policy rationale for requiring the payment of monetary compensation for wrongs committed has involved the avoidance of feuding between clans and families.
If compensation could mollify families' feelings, this would help to keep the peace. On the other hand, the institution of oaths also played down the threat of feudal warfare. Both in archaic Greece and in medieval Scandinavia, an accused person walked free if he could get a sufficient number of male relatives to swear him not guilty. (Compare the United Nations Security Council, in which the veto power of the permanent members ensures that the organization does not become involved in crises where it could not enforce its decisions.)
These means of restraining private feuds did not always work, and sometimes prevented the fulfillment of justice. But in the earliest times the "state" did not always provide an independent policing force. Thus criminal law grew out of what 21st-century lawyers would call torts; and, in real terms, many acts and omissions classified as crimes actually overlap with civil-law concepts.
The development of sociological thought from the 19th century onwards prompted some fresh views on crime and criminality, and fostered the beginnings of criminology as a study of crime in society. Nietzsche noted a link between crime and creativityin "The Birth of Tragedy" he asserted: "The best and brightest that man can acquire he must obtain by crime". In the 20th century Michel Foucault in "Discipline and Punish" made a study of criminalization as a coercive method of state control.
Classification and categorisation.
Categorisation by type.
The following classes of offences are used, or have been used, as legal terms of art:
Researchers and commentators have classified crimes into the following categories, in addition to those above:
Categorisation by penalty.
One can categorise crimes depending on the related punishment, with sentencing tariffs prescribed in line with the perceived seriousness of the offence. Thus fines and noncustodial sentences may address the crimes seen as least serious, with lengthy imprisonment or (in some jurisdictions) capital punishment reserved for the most serious.
Common law.
Under the common law of England, crimes were classified as either treason, felony or misdemeanour, with treason sometimes being included with the felonies. This system was based on the perceived seriousness of the offence. It is still used in the United States but the distinction between felony and misdemeanour is abolished in England and Wales and Northern Ireland.
Classification by mode of trial.
The following classes of offence are based on mode of trial:
Classification by origin.
In common law countries, crimes may be categorised into common law offences and statutory offences. In the US, Australia and Canada (in particular), they are divided into federal crimes and under state crimes.
U.S. classification.
In the United States since 1930, the FBI has tabulated Uniform Crime Reports (UCR) annually from crime data submitted by law enforcement agencies across the United States.
Officials compile this data at the city, county, and state levels into the UCR. They classify violations of laws based on common law as Part I (index) crimes in UCR data. These are further categorized as violent or property crimes. Part I violent crimes include murder and criminal homicide (voluntary manslaughter), forcible rape, aggravated assault, and robbery; while Part I property crimes include burglary, arson, larceny/theft, and motor-vehicle theft. All other crimes count come under Part II.
For convenience, such lists usually include infractions although, in the U.S., they may come into the sphere not of the criminal law, but rather of the civil law. Compare tortfeasance.
Booking arrests require detention for a time-frame ranging 1 to 24 hours.
Offence in common law jurisdictions.
In England and Wales, as well as in Hong Kong, the term "offence" means the same thing as, and is interchangeable with, the term "crime", They are further split into:
Causes and correlates of crime.
Many different causes and correlates of crime have been proposed with varying degree of empirical support. They include socioeconomic, psychological, biological, and behavioral factors. Controversial topics include media violence research and effects of gun politics.
Emotional state (both chronic and current) have a tremendous impact on individual thought processes and, as a result, can be linked to criminal activities. The positive psychology concept of Broaden and Build posits that cognitive functioning expands when an individual is in a good-feeling emotional state and contracts as emotional state declines. In positive emotional states an individual is able to consider more possible solutions to problems, but in lower emotional states fewer solutions can be ascertained. The narrowed thought-action repertoires can result in the only paths perceptible to an individual being ones they would never use if they saw an alternative, but if they can't conceive of the alternatives that carry less risk they will choose one that they can see. Criminals who commit even the most horrendous of crimes, such as mass murders, did not see another solution. 
Crimes in international law.
Crimes defined by treaty as crimes against international law include:
From the point of view of State-centric law, extraordinary procedures (usually international courts) may prosecute such crimes. Note the role of the International Criminal Court at The Hague in the Netherlands.
Popular opinion in the Western world and former Soviet Union often associates international law with the concept of opposing terrorism — seen as a crime as distinct from warfare.
Religion and crime.
Different religious traditions may promote distinct norms of behaviour, and these in turn may clash or harmonise with the perceived interests of a state. Socially accepted or imposed religious morality has influenced secular jurisdictions on issues that may otherwise concern only an individual's conscience. Activities sometimes criminalized on religious grounds include (for example) alcohol consumption (prohibition), abortion and stem-cell research. In various historical and present-day societies, institutionalized religions have established systems of earthly justice that punish crimes against the divine will and against specific devotional, organizational and other rules under specific codes, such as Roman Catholic canon law.
Military jurisdictions and states of emergency.
In the military sphere, authorities can prosecute both regular crimes and specific acts (such as mutiny or desertion) under martial-law codes that either supplant or extend civil codes in times of (for example) war.
Many constitutions contain provisions to curtail freedoms and criminalize otherwise tolerated behaviors under a state of emergency in the event of war, natural disaster or civil unrest. Undesired activities at such times may include assembly in the streets, violation of curfew, or possession of firearms.
Employee crime.
Two common types of employee crime exist: embezzlement and wage theft.
The complexity and anonymity of computer systems may help criminal employees camouflage their operations. The victims of the most costly scams include banks, brokerage houses, insurance companies, and other large financial institutions.
Nationally it is estimated that workers are not paid at least $19 billion every year in overtime and that in total $40 billion to $60 billion are lost annually due to all forms of wage theft. This compares to national annual losses of $340 million due to robbery, $4.1 billion due to burglary, $5.3 billion due to larceny, and $3.8 billion due to auto theft in 2012. In Singapore, as in the United States, wage theft was found to be widespread and severe. In a 2014 survey it was found that as many as one-third of low wage male foreign workers in Singapore, or about 130,000, were affected by wage theft from partial to full denial of pay.

</doc>
<doc id="5786" url="https://en.wikipedia.org/wiki?curid=5786" title="California Institute of Technology">
California Institute of Technology

The California Institute of Technology (abbreviated Caltech) is a private research university located in Pasadena, California, United States. Although founded as a preparatory and vocational school by Amos G. Throop in 1891, the college attracted influential scientists such as George Ellery Hale, Arthur Amos Noyes, and Robert Andrews Millikan in the early 20th century. The vocational and preparatory schools were disbanded and spun off in 1910, and the college assumed its present name in 1921. In 1934, Caltech was elected to the Association of American Universities, and the antecedents of NASA's Jet Propulsion Laboratory, which Caltech continues to manage and operate, were established between 1936 and 1943 under Theodore von Kármán. The university is one among a small group of Institutes of Technology in the United States which tends to be primarily devoted to the instruction of technical arts and applied sciences.
Caltech has six academic divisions with strong emphasis on science and engineering, managing $332 million in 2011 in sponsored research. Its primary campus is located approximately northeast of downtown Los Angeles. First-year students are required to live on campus, and 95% of undergraduates remain in the on-campus house system. Although Caltech has a strong tradition of practical jokes and pranks, student life is governed by an honor code which allows faculty to assign take-home examinations. The Caltech Beavers compete in 13 intercollegiate sports in the NCAA Division III's Southern California Intercollegiate Athletic Conference.
Caltech is frequently cited as one of the world's best universities. Despite its small size, 33 Caltech alumni and faculty have won a total of 34 Nobel Prizes (Linus Pauling being the only individual in history to win two unshared prizes) and 71 have won the United States National Medal of Science or Technology. There are 112 faculty members who have been elected to the National Academies. In addition, numerous faculty members are associated with the Howard Hughes Medical Institute as well as NASA.
History.
Throop College.
Caltech started as a vocational school founded in Pasadena in 1891 by local businessman and politician Amos G. Throop. The school was known successively as Throop University, Throop Polytechnic Institute (and Manual Training School), and Throop College of Technology, before acquiring its current name in 1920. The vocational school was disbanded and the preparatory program was split off to form an independent Polytechnic School in 1907.
At a time when scientific research in the United States was still in its infancy, George Ellery Hale, a solar astronomer from the University of Chicago, founded the Mount Wilson Observatory in 1904. He joined Throop's board of trustees in 1907, and soon began developing it and the whole of Pasadena into a major scientific and cultural destination. He engineered the appointment of James A. B. Scherer, a literary scholar untutored in science but a capable administrator and fund raiser, to Throop's presidency in 1908. Scherer persuaded retired businessman and trustee Charles W. Gates to donate $25,000 in seed money to build Gates Laboratory, the first science building on campus.
World Wars.
In 1910, Throop moved to its current site. Arther Fleming donated the land for the permanent campus site. Theodore Roosevelt delivered an address at Throop Institute on March 21, 1911, and he declared:
I want to see institutions like Throop turn out perhaps ninety-nine of every hundred students as men who are to do given pieces of industrial work better than any one else can do them; I want to see those men do the kind of work that is now being done on the Panama Canal and on the great irrigation projects in the interior of this country—and the one-hundredth man I want to see with the kind of cultural scientific training that will make him and his fellows the matrix out of which you can occasionally develop a man like your great astronomer, George Ellery Hale.
In the same year, a bill was introduced in the California Legislature calling for the establishment of a publicly funded "California Institute of Technology", with an initial budget of a million dollars, ten times the budget of Throop at the time. The board of trustees offered to turn Throop over to the state, but the presidents of Stanford University and the University of California successfully lobbied to defeat the bill, which allowed Throop to develop as the only scientific research-oriented education institute in southern California, public or private, until the onset of the World War II necessitated the broader development of research-based science education. The promise of Throop attracted physical chemist Arthur Amos Noyes from MIT to develop the institution and assist in establishing it as a center for science and technology.
With the onset of World War I, Hale organized the National Research Council to coordinate and support scientific work on military problems. While he supported the idea of federal appropriations for science, he took exception to a federal bill that would have funded engineering research at land-grant colleges, and instead sought to raise a $1 million national research fund entirely from private sources. To that end, as Hale wrote in "The New York Times":
Throop College of Technology, in Pasadena California has recently afforded a striking illustration of one way in which the Research Council can secure co-operation and advance scientific investigation. This institution, with its able investigators and excellent research laboratories, could be of great service in any broad scheme of cooperation. President Scherer, hearing of the formation of the council, immediately offered to take part in its work, and with this object, he secured within three days an additional research endowment of one hundred thousand dollars.
Through the National Research Council, Hale simultaneously lobbied for science to play a larger role in national affairs, and for Throop to play a national role in science. The new funds were designated for physics research, and ultimately led to the establishment of the Norman Bridge Laboratory, which attracted experimental physicist Robert Andrews Millikan from the University of Chicago in 1917. During the course of the war, Hale, Noyes and Millikan worked together in Washington on the NRC. Subsequently, they continued their partnership in developing Caltech.
Under the leadership of Hale, Noyes and Millikan (aided by the booming economy of Southern California), Caltech grew to national prominence in the 1920s and concentrated on the development of Roosevelt's "Hundredth Man". On November 29, 1921, the trustees declared it to be the express policy of the Institute to pursue scientific research of the greatest importance and at the same time "to continue to conduct thorough courses in engineering and pure science, basing the work of these courses on exceptionally strong instruction in the fundamental sciences of mathematics, physics, and chemistry; broadening and enriching the curriculum by a liberal amount of instruction in such subjects as English, history, and economics; and vitalizing all the work of the Institute by the infusion in generous measure of the spirit of research." In 1923, Millikan was awarded the Nobel Prize in Physics. In 1925, the school established a department of geology and hired William Bennett Munro, then chairman of the division of History, Government, and Economics at Harvard University, to create a division of humanities and social sciences at Caltech. In 1928, a division of biology was established under the leadership of Thomas Hunt Morgan, the most distinguished biologist in the United States at the time, and discoverer of the role of genes and the chromosome in heredity. In 1930, Kerckhoff Marine Laboratory was established in Corona del Mar under the care of Professor George MacGinitie. In 1926, a graduate school of aeronautics was created, which eventually attracted Theodore von Kármán. Kármán later helped create the Jet Propulsion Laboratory, and played an integral part in establishing Caltech as one of the world's centers for rocket science. In 1928, construction of the Palomar Observatory began.
Millikan served as "Chairman of the Executive Council" (effectively Caltech's president) from 1921 to 1945, and his influence was such that the Institute was occasionally referred to as "Millikan's School." Millikan initiated a visiting-scholars program soon after joining Caltech. Scientists who accepted his invitation include luminaries such as Paul Dirac, Erwin Schrödinger, Werner Heisenberg, Hendrik Lorentz and Niels Bohr. Albert Einstein arrived on the Caltech campus for the first time in 1931 to polish up his Theory of General Relativity, and he returned to Caltech subsequently as a visiting professor in 1932 and 1933.
During World War II, Caltech was one of 131 colleges and universities nationally that took part in the V-12 Navy College Training Program which offered students a path to a Navy commission. The United States Navy also maintained a naval training school for aeronautical engineering, resident inspectors of ordnance and naval material, and a liaison officer to the National Defense Research Committee on campus.
Post-war growth.
In the 1950s–1970s, Caltech was the home of Murray Gell-Mann and Richard Feynman, whose work was central to the establishment of the Standard Model of particle physics. Feynman was also widely known outside the physics community as an exceptional teacher and colorful, unconventional character.
During Lee A. DuBridge's tenure as Caltech's president (1946–1969), Caltech's faculty doubled and the campus tripled in size. DuBridge, unlike his predecessors, welcomed federal funding of science. New research fields flourished, including chemical biology, planetary science, nuclear astrophysics, and geochemistry. A 200-inch telescope was dedicated on nearby Palomar Mountain in 1948 and remained the world's most powerful optical telescope for over forty years.
Caltech opened its doors to female undergraduates during the presidency of Harold Brown in 1970, and they made up 14% of the entering class. The fraction of female undergraduates has been increasing since then.
Caltech undergraduates have historically been so apathetic to politics that there has been only one organized student protest in January 1968 outside the Burbank studios of NBC, in response to rumors that NBC was to cancel Star Trek. In 1973, the students from Dabney House protested a presidential visit with a sign on the library bearing the simple phrase "Impeach Nixon". The following week, Ross McCollum, president of the National Oil Company, wrote an open letter to Dabney House stating that in light of their actions he had decided not to donate one million dollars to Caltech. The Dabney family, being Republicans, disowned Dabney House after hearing of the prank.
21st century.
Since 2000, the Einstein Papers Project has been located at Caltech. The project was established in 1986 to assemble, preserve, translate, and publish papers selected from the literary estate of Albert Einstein and from other collections.
In fall 2008, the freshman class was 42% female, a record for Caltech's undergraduate enrollment. In the same year, the Institute concluded a six-year-long fund-raising campaign. The campaign raised more than $1.4 billion from about 16,000 donors. Nearly half of the funds went into the support of Caltech programs and projects.
In 2010, Caltech, in partnership with Lawrence Berkeley National Laboratory and headed by Professor Nathan Lewis, established a DOE Energy Innovation Hub aimed at developing revolutionary methods to generate fuels directly from sunlight. This hub, the Joint Center for Artificial Photosynthesis, will receive up to $122 million in federal funding over five years.
Since 2012, Caltech began to offer classes through MOOCs under Coursera, and from 2013, edX.
Jean-Lou Chameau, the eighth president, announced on February 19, 2013, that he would be stepping down to accept the presidency at King Abdullah University of Science and Technology. Thomas F. Rosenbaum was announced to be the ninth president of Caltech on October 24, 2013, and his term began on July 1, 2014.
Campus.
Caltech's primary campus is located in Pasadena, California, approximately northeast of downtown Los Angeles. It is within walking distance of Old Town Pasadena and the Pasadena Playhouse District and therefore the two locations are frequent getaways for Caltech students.
In 1917 Hale hired architect Bertram Goodhue to produce a master plan for the campus. Goodhue conceived the overall layout of the campus and designed the physics building, Dabney Hall, and several other structures, in which he sought to be consistent with the local climate, the character of the school, and Hale's educational philosophy. Goodhue's designs for Caltech were also influenced by the traditional Spanish mission architecture of Southern California.
In 1971 a magnitude-6.6 earthquake in San Fernando caused some damage to the Caltech campus. Engineers who evaluated the damage found that two historic buildings dating from the early days of the Institute—Throop Hall and the Goodhue-designed Culbertson Auditorium—had cracked. These were some of the first reinforced concrete buildings, and their plans did not contain enough details (such as how much reinforcing bar had been embedded in the concrete) to be sure they were safe, so the engineers recommended demolition. However, demolishing these historic structures required considerably more effort than would have been necessary had they been in real danger of collapse. A large wrecking ball was used to demolish Throop Hall, and smashing the concrete revealed massive amounts of rebar, far in excess of safety requirements. The rebar had to be cut up before the pieces could be hauled away, and the process took much longer than expected.
New additions to the campus include the Cahill Center for Astronomy and Astrophysics and the Walter and Leonore Annenberg Center for Information Science and Technology, which opened in 2009, and the Warren and Katherine Schlinger Laboratory for Chemistry and Chemical Engineering followed in March 2010. The Institute also concluded an upgrading of the south houses in 2006. In late 2010, Caltech completed a 1.3 MW solar array projected to produce approximately 1.6 GWh in 2011.
Organization and administration.
The mission statement of Caltech reads:
"The mission of the California Institute of Technology is to expand human knowledge and benefit society through research integrated with education. We investigate the most challenging, fundamental problems in science and technology in a singularly collegial, interdisciplinary atmosphere, while educating outstanding students to become creative members of society."
Caltech is incorporated as a non-profit corporation and is governed by a privately appointed 46-member board of trustees who serve five-year terms of office and retire at the age of 72. The current board is chaired by David L. Lee, co-founder of Global Crossing Ltd. The Trustees elect a President to serve as the chief executive officer of the Institute and administer the affairs on the Institute on behalf of the board, a Provost who serves as the chief academic officer of the Institute below the President, and ten other vice presidential and other senior positions. Former Georgia Tech provost Jean-Lou Chameau became the eighth president of Caltech on September 1, 2006, replacing David Baltimore who had served since 1997. Dr. Chameau's compensation for 2008–2009 totaled $799,472. Chameau served until June 30, 2013. Thomas F. Rosenbaum was announced to be the ninth president of Caltech on October 24, 2013, and his term began on July 1, 2014. Caltech's endowment is governed by a permanent Trustee committee and administered by an Investment Office.
The Institute is organized into six primary academic divisions: Biology and Biological Engineering, Chemistry and Chemical Engineering, Engineering and Applied Science, Geological and Planetary Sciences, Humanities and Social Sciences, and Physics, Mathematics, and Astronomy. The voting faculty of Caltech include all professors, instructors, research associates and fellows, and the University Librarian. Faculty are responsible for establishing admission requirements, academic standards, and curricula. The Faculty Board is the faculty's representative body and consists of 18 elected faculty representatives as well as other senior administration officials. Full-time professors are expected to teach classes, conduct research, advise students, and perform administrative work such as serving on committees.
The Jet Propulsion Laboratory (JPL) is a federally funded research and development center (FFRDC) owned by NASA and operated as a division of Caltech through a contract between NASA and Caltech. In 2008, JPL spent over $1.6 billion on research and development and employed over 5,000 project-related and support employees. The JPL Director also serves as a Caltech Vice President and is responsible to the President of the Institute for the management of the Laboratory.
Academics.
Caltech is a small four-year, highly residential research university with a slight majority in graduate programs. The Institute has been accredited by the Western Association of Schools and Colleges since 1949. Caltech is on the quarter system: the fall term starts in late September and ends before Christmas, the second term starts after New Years Day and ends in mid-March, and the third term starts in late March or early April and ends in early June.
Caltech is ranked 1st internationally since 2011 by the "Times Higher Education World University Rankings". Caltech was ranked as the best university in the world in two categories: Engineering & Technology and Physical Sciences. It was also found to have the highest faculty citation rate in the world.
Shanghai Ranking Consultancy's "Academic Ranking of World Universities" (ARWU), a ranking with an emphasis on bibliometric data and scientific research, ranked Caltech 7th in the world and 6th in the U.S. for 2014.
In 2012, the Center for World University Rankings ranked Caltech fifth in the world and fourth nationally in its CWUR World University Rankings.
"Money Magazine" ranked Caltech 10th in the country out of the nearly 1500 schools it evaluated for its 2014 Best Colleges ranking.
"The Daily Beast" ranked Caltech 12th in the country out of the nearly 2000 schools it evaluated for its 2014 Best Colleges ranking.
"U.S. News & World Report" ranked Caltech as tied for the 10th best university in the United States in their 2016 national rankings, with the graduate programs in chemistry and earth sciences ranked first in the nation.
The United States National Research Council released its latest "Assessment of Research Doctorate Programs" in 2010, and 23 of the 24 graduate programs of Caltech were ranked within the top four programs in the nation in their size quartile as determined by both the R95 and S95 rankings. Of particular note, programs that were placed within the top 10% of all size programs in that field based on an average of the R95 and S95 rank order include Aeronautics, Astrophysics, Biochemistry and Molecular Biophysics, Bioengineering, Biology, Chemical Engineering, Chemistry, Electrical Engineering, Environmental Science and Engineering, Geology, Geophysics, Materials Science, Mechanical Engineering, Physics, Planetary Science, and Social Science (Economics).
In 2013 a paper published in the Quarterly Journal of Economics ranked Caltech 2nd in the U.S. (after Harvard) for revealed preference. The sample of the study was 3240 high achieving students (students ranked in the top 10% of public high school class or top 20% of private high school class). Rankings were based on number of students who accepted offered admission.
Admissions.
For the Class of 2018 (enrolled fall 2014), Caltech received 6,524 applications and accepted 529 (8.1%). In 2013, the number enrolling was 249; the yield rate (the percentage of accepted students who enroll) was 43%. Of the 31% of entering freshmen who submitted class rank, 98% were in the top 10% of their high school classes; 100% ranked in the top quarter. The middle 50% range of SAT scores for enrolled freshmen were 720-780 for critical reading, 770-800 for math, and 730-800 for writing. The middle 50% range ACT Composite score was 33-35.
Tuition and financial aid.
Undergraduate tuition for the 2013–2014 school year was $39,990 and total annual costs were estimated to be $58,755. In 2012–2013, Caltech awarded $17.1 million in need-based aid, $438k in non-need-based aid, and $2.51 million in self-help support to enrolled undergraduate student. The average financial aid package of all students eligible for aid was $38,756 and students graduated with an average debt of $15,090.
Undergraduate program.
The full-time, four-year undergraduate program emphasizes instruction in the arts and sciences and has high graduate coexistence. Caltech offers 24 majors (called "options") and six minors across all six academic divisions. Caltech also offers interdisciplinary programs in Applied Physics, Biochemistry, Bioengineering, Computation and Neural Systems, Control and Dynamical Systems, Environmental Science and Engineering, Geobiology and Astrobiology, Geochemistry, and Planetary Astronomy. The most popular options are Chemical Engineering, Computer Science, Electrical Engineering, Mechanical Engineering and Physics.
Prior to the entering class of 2017, Caltech requires students to take a core curriculum of 30 classes: five terms of mathematics, five terms of physics, two terms of chemistry, one term of biology, a freshman elective "menu" course, two terms of introductory lab courses, 2 terms of science writing, and 12 terms of humanities. The new core curriculum effective from the 2013-2014 academic year reduces the math and physics requirements to three terms each, but most option requirements still require about five terms of math and physics.
A typical class is worth 9 academic units and given the extensive core curriculum requirements in addition to individual options' degree requirements, students need to take an average of 40.5 units per term (more than four classes) in order to graduate in four years. 36 units is the minimum full-time load, 48 units is considered a heavy load, and registrations above 54 units require an overload petition. Approximately 20 percent of students double-major. This is achievable since the humanities and social sciences majors have been designed to be done in conjunction with a science major. Although choosing two options in the same division is discouraged, it is still possible.
First year students are enrolled in first-term classes based upon results of placement exams in math, physics, chemistry, and writing and take all classes in their first two terms on a Pass/Fail basis. There is little competition; collaboration on homework is encouraged and the Honor System encourages take-home tests and flexible homework schedules. Caltech offers co-operative programs with other schools, such as the Pasadena Art Center College of Design and Occidental College.
Upon graduation, Caltech alumni have one of the highest median starting salary among graduates of other colleges or universities in 2012–2013, of $67,400, according to PayScale. The mid-career median pay is $120,700. Caltech was found to offer the second highest return of investment of college education, at $1,991,000 over a 30-year period, according to the same study.
Caltech offers Army and Air Force ROTC in cooperation with the University of Southern California.
Graduate program.
The graduate instructional programs emphasize doctoral studies and are dominated by science, technology, engineering, and mathematics fields. The Institute offers graduate degree programs for the Master of Science, Engineer's Degree, Doctor of Philosophy, BS/MS and MD/PhD, with the majority of students in the PhD program. The most popular options are Chemistry, Physics, Biology, Electrical Engineering and Chemical Engineering. Applicants for graduate studies are required to take the GRE. GRE Subject scores are either required or strongly recommended by several options. A joint program between Caltech and the UCLA David Geffen School of Medicine grants MD/PhD degrees. Students in this program do their preclinical and clinical work at UCLA, and their PhD work with any member of the Caltech faculty, including the Biology, Chemistry, and Engineering and Applied Sciences Divisions. The MD degree would be from UCLA and the PhD would be awarded from Caltech.
The research facilities at Caltech are available to graduate students, but there are opportunities for students to work in facilities of other universities, research centers as well as private industries. The graduate student to faculty ratio is 4:1.
Approximately 99 percent of doctoral students have full financial support. Financial support for graduate students comes in the form of fellowships, research assistantships, teaching assistantships or a combination of fellowship and assistantship support.
Graduate students are bound by the Honor Code, as are the undergraduates, and the Graduate Honor Council oversees any violations of the code.
Research.
Caltech was elected to the Association of American Universities in 1934 and remains a research university with "very high" research activity, primarily in STEM fields. Caltech manages research expenditures of $270 million annually, 66th among all universities in the U.S. and 17th among private institutions without medical schools for 2008. The largest federal agencies contributing to research are NASA, National Science Foundation, Department of Health and Human Services, Department of Defense, and Department of Energy. Caltech received $144 million in federal funding for the physical sciences, $40.8 million for the life sciences, $33.5 million for engineering, $14.4 million for environmental sciences, $7.16 million for computer sciences, and $1.97 million for mathematical sciences in 2008.
The Institute was awarded an all-time high funding of $357 million in 2009. Active funding from the National Science Foundation Directorate of Mathematical and Physical Science (MPS) for Caltech stands at $343 million as of 2011, the highest for any educational institution in the nation, and higher than the total funds allocated to any state except California and New York.
In 2005, Caltech had dedicated to research: to physical sciences, to engineering, and to biological sciences.
In addition to managing JPL, Caltech also operates the Palomar Observatory in San Diego County, the Owens Valley Radio Observatory in Bishop, California, the Submillimeter Observatory and W. M. Keck Observatory at the Mauna Kea Observatory, the Laser Interferometer Gravitational-Wave Observatory at Livingston, Louisiana and Richland, Washington, and Kerckhoff Marine Laboratory in Corona del Mar, California. The Institute launched the Kavli Nanoscience Institute at Caltech in 2006, the Keck Institute for Space Studies in 2008, and is also the current home for the Einstein Papers Project. The Spitzer Science Center (SSC), part of the Infrared Processing and Analysis Center located on the Caltech campus, is the data analysis and community support center for NASA's Spitzer Space Telescope.
Caltech partnered with UCLA to establish a Joint Center for Translational Medicine (UCLA-Caltech JCTM), which conducts experimental research into clinical applications, including the diagnosis and treatment of diseases such as cancer.
Undergraduates at Caltech are also encouraged to participate in research. About 80% of the class of 2010 did research through the annual Summer Undergraduate Research Fellowships (SURF) program at least once during their stay, and many continued during the school year. Students write and submit SURF proposals for research projects in collaboration with professors, and about 70 percent of applicants are awarded SURFs. The program is open to both Caltech and non-Caltech undergraduate students. It serves as preparation for graduate school and helps to explain why Caltech has the highest percentage of alumni who go on to receive a PhD of all the major universities.
The licensing and transferring of technology to the commercial sector is managed by the Office of Technology Transfer (OTT). OTT protects and manages the intellectual property developed by faculty members, students, other researchers, and JPL technologists. Caltech receives more invention disclosures per faculty member than any other university in the nation. As of 2008, 1891 patents were granted to Caltech researchers since 1969.
Student life.
House system.
During the early 20th century, a Caltech committee visited several universities and decided to transform the undergraduate housing system from regular fraternities to a house system. Four south houses (or "hovses") were built: Blacker House, Dabney House, Fleming House and Ricketts House. In the 1960s, three north houses were built: Lloyd House, Page House, and Ruddock House, and during the 1990s, Avery House. The four south houses closed for renovation in 2005 and reopened in 2006. All first year students live in the house system and 95% of undergraduates remain affiliated with a house.
Athletics.
Caltech has athletic teams in baseball, men's and women's basketball, cross country, fencing, men's soccer, swimming and diving, men's and women's tennis, track and field, women's volleyball, and men's and women's water polo. Caltech's mascot is the Beaver, an homage to nature's engineer. Its teams (with the exception of the fencing team) play in the Southern California Intercollegiate Athletic Conference, which Caltech co-founded in 1915. The fencing team competes in the NCAA's Division I, facing teams from UCLA, USC, UCSD, and Stanford, among others.
On January 6, 2007, the Beavers' men's basketball team snapped a 207-game losing streak to Division III schools, beating Bard College 81–52. It was their first Division III victory since 1996.
Until their win over Occidental on February 22, 2011 the team had not won a game in conference play since 1985. Ryan Elmquist's free throw with 3.3 seconds in regulation gave the Beavers the victory. The documentary film "Quantum Hoops" concerns the events of the Beavers' 2005–06 season.
On January 13, 2007, the Caltech women's basketball team snapped a 50-game losing streak, defeating the Pomona–Pitzer Sagehens 55–53. The women's program, which entered the SCIAC in 2002, garnered their first conference win. On the bench as honorary coach for the evening was Dr. Robert Grubbs, 2005 Nobel laureate in Chemistry. The team went on to beat Whittier College on February 10, for its second SCIAC win, and placed its first member on the All Conference team. The 2006–2007 season is the most successful season in the history of the program.
In 2007, 2008, and 2009, the women's table tennis team (a club team) competed in nationals. The women's Ultimate club team, known as "Snatch", has also been very successful in recent years, ranking 44 of over 200 college teams in the Ultimate Player's Association.
On February 2, 2013, the Caltech baseball team ended a 228-game losing streak, the team's first win in nearly 10 years.
The track and field team plays at the South Athletic Field in Tournament Park, the site of the first Rose Bowl Game.
The school also sponsored a football team prior to 1976, which played part of its home schedule at the Rose Bowl, or, as Caltech students put it, "to the largest number of empty seats in the nation".
Performing and visual arts.
The Caltech/Occidental College Orchestra is a full seventy-piece orchestra composed of students, faculty, and staff at Caltech and nearby Occidental College. The orchestra gives three pairs of concerts annually, at both Caltech and Occidental College. There are also two Caltech Jazz Bands and a Concert Band. For vocal music, Caltech has Men's and Women's Glee Clubs. The theater program at Caltech is known as TACIT, or Theater Arts at the California Institute of Technology. There are two to three plays organized by TACIT per year, and they were involved in the production of the , released in 2011.
Student life traditions.
Annual events.
Every Halloween, Dabney House conducts the infamous "Millikan pumpkin-drop experiment" from the top of Millikan Library, the highest point on campus. According to tradition, a claim was once made that the shattering of a pumpkin frozen in liquid nitrogen and dropped from a sufficient height would produce a triboluminescent spark. This yearly event involves a crowd of observers, who try to spot the elusive spark. The title of the event is an oblique reference to the famous Millikan oil-drop experiment which measured "e", the elemental unit of electrical charge.
On Ditch Day, the seniors ditch school, leaving behind elaborately designed tasks and traps at the doors of their rooms to prevent underclassmen from entering. Over the years this has evolved to the point where many seniors spend months designing mechanical, electrical, and software obstacles to confound the underclassmen. Each group of seniors designs a "stack" to be solved by a handful of underclassmen. The faculty have been drawn into the event as well, and cancel all classes on Ditch Day so the underclassmen can participate in what has become a highlight of the academic year.
Another long-standing tradition is the playing of Wagner's "Ride of the Valkyries" at 7:00 each morning during finals week with the largest, loudest speakers available. The playing of that piece is not allowed at any other time (except if one happens to be listening to the entire 14 hours and 5 minutes of "The Ring Cycle"), and any offender is dragged into the showers to be drenched in cold water fully dressed. The playing of the "Ride" is such a strong tradition that the music was used during Apollo 17 to awaken Astronaut Harrison Schmitt, a Caltech alumnus. Unfortunately, the tradition arose at different times in different Houses, so Schmitt did not react as expected. Instead, he just became confused.
Pranks.
Caltech students have been known for the many pranks (also known as "RFs").
The two most famous in recent history are the changing of the Hollywood Sign to read "Caltech", by judiciously covering up certain parts of the letters, and the changing of the scoreboard to read Caltech 38, MIT 9 during the 1984 Rose Bowl Game. But the most famous of all occurred during the 1961 Rose Bowl Game, where Caltech students altered the flip-cards that were raised by the stadium attendees to display "Caltech", and several other "unintended" messages. This event is now referred to as the Great Rose Bowl Hoax.
In recent years, pranking has been officially encouraged by Tom Mannion, Caltech's Assistant VP for Student Affairs and Campus Life. "The grand old days of pranking have gone away at Caltech, and that's what we are trying to bring back," reported the "Boston Globe".
In December 2011, Caltech students went to New York and pulled a prank on Manhattan's Greenwich Village. The prank involved making The Cube sculpture look like the Aperture Science Weighted Companion Cube from the video game "Portal".
Caltech pranks have been documented in three Legends of Caltech books, the most recent of which was edited by alumni Autumn Looijen '99 and Mason A. Porter '98 and published in May 2007.
Rivalry with MIT.
In 2005, a group of Caltech students pulled a string of pranks during MIT's Campus Preview Weekend for admitted students. These include covering up the word Massachusetts in the "Massachusetts Institute of Technology" engraving on the main building façade with a banner so that it read "That Other Institute of Technology". A group of MIT hackers responded by altering the banner so that the inscription read "The Only Institute of Technology." Caltech students also passed out T-shirts to MIT's incoming freshman class that had MIT written on the front and "... because not everyone can go to Caltech" along with an image of a palm tree on the back.
MIT retaliated in April 2006, when students posing as the Howe & Ser (Howitzer) Moving Company stole the 130-year-old, 1.7-ton Fleming House cannon and moved it over 3000 miles to their campus in Cambridge, Massachusetts for their 2006 Campus Preview Weekend, repeating a similar prank performed by nearby Harvey Mudd College in 1986. Thirty members of Fleming House traveled to MIT and reclaimed their cannon on April 10, 2006.
On April 13, 2007 (Friday the 13th), a group of students from "The California Tech", Caltech's campus newspaper, arrived and distributed fake copies of "The Tech", MIT's campus newspaper, while prospective students were visiting for their Campus Preview Weekend. Articles included "MIT Invents the Interweb," "Architects Deem Campus 'Unfortunate'," and "Infinite Corridor Not Actually Infinite."
In December 2009, some Caltech students declared that MIT had been sold and had become the Caltech East campus. A "sold" banner was hung on front of the MIT dome building and a "Welcome to Caltech East: School of the Humanities" banner over the Massachusetts Avenue Entrance. Newspapers and T-shirts were distributed, and door labels and fliers in the infinite corridor were put up in accordance with the "curriculum change."
In September 2010, MIT students attempted to put a TARDIS, the time machine from the BBC's "Doctor Who", onto a roof. Caught in midact, the prank was aborted. In January 2011, Caltech students in conjunction with MIT students helped put the TARDIS on top of Baxter. Caltech students then moved the TARDIS to UC Berkeley and Stanford.
In April 2014, during MIT's Campus Preview Weekend, a group of Caltech students handed out mugs emblazoned with the MIT logo on the front and the words "The Institute of Technology" on the back. When heated, the mugs turn orange, display a palm tree, and read "Caltech The Hotter Institute of Technology." Identical mugs continue to be sold at the Caltech campus store.
Honor code.
Life in the Caltech community is governed by the honor code, which simply states: "No member of the Caltech community shall take unfair advantage of any other member of the Caltech community." This is enforced by a Board of Control, which consists of undergraduate students, and by a similar body at the graduate level, called the Graduate Honor Council.
The honor code aims at promoting an atmosphere of respect and trust that allows Caltech students to enjoy privileges that make for a more relaxed atmosphere. For example, the honor code allows professors to make the majority of exams as take-home, allowing students to take them on their own schedule and in their preferred environment.
Through the late 1990s, the only exception to the honor code, implemented earlier in the decade in response to changes in federal regulations, concerned the sexual harassment policy. Today, there are myriad exceptions to the honor code in the form of new institute policies such as the fire policy, and alcohol policy. Though both policies are presented in the Honor System Handbook given to new members of the Caltech community, some undergraduates regard them as a slight against the honor code and the implicit trust and respect it represents within the community.
People.
As of 2015, Caltech has 34 Nobel laureates to its name. This figure includes 20 alumni, 14 non-alumni professors, and 4 professors who are also alumni (Carl D. Anderson, Linus Pauling, William A. Fowler, and Edward B. Lewis). The number of awards is 35, because Pauling received prizes in both Chemistry and Peace. Seven faculty and alumni have received a Crafoord Prize from the Royal Swedish Academy of Sciences, while 58 have been awarded the U.S. National Medal of Science, and 13 have received the National Medal of Technology. One alumnus, Stanislav Smirnov, won the Fields Medal in 2010. Other distinguished researchers have been affiliated with Caltech as postdoctoral scholars (for example, Barbara McClintock, James D. Watson, Sheldon Glashow and John Gurdon) or visiting professors (for example, Albert Einstein, Stephen Hawking and Edward Witten).
Students.
Caltech enrolled 983 undergraduate students and 1,226 graduate students for the 2014–2015 school year. Women made up 36% of the undergraduate and 26% of the graduate student body.
The four-year graduation rate is 79% and the six-year rate is 92%, which is low compared to most leading U.S. universities, but substantially higher than it was in the 1960s and 1970s. Students majoring in STEM fields traditionally have graduation rates below 70%.
Faculty and staff.
Richard Feynman was among the most well-known physicists associated with Caltech, having published the "Feynman Lectures on Physics", an undergraduate physics text, and a few other popular science texts such as "Six Easy Pieces" for the general audience. The promotion of physics made him a public figure of science, although his Nobel-winning work in quantum electrodynamics was already very established in the scientific community. Murray Gell-Mann, a Nobel-winning physicist, introduced a classification of hadrons and went on to postulate the existence of quarks, which is currently accepted as part of the Standard Model. Long-time Caltech President Robert Andrews Millikan was the first to calculate the charge of the electron with his well-known oil-drop experiment, while Richard Chace Tolman is remembered for his contributions to cosmology and statistical mechanics. 2004 Nobel Prize in Physics winner H. David Politzer is a current professor at Caltech, as is astrophysicist and author Kip Thorne and eminent mathematician Barry Simon. Linus Pauling pioneered quantum chemistry and molecular biology, and went on to discover the nature of the chemical bond in 1939. Seismologist Charles Richter, also an alumnus, developed the magnitude scale that bears his name, the Richter magnitude scale for measuring the power of earthquakes. One of the founders of the geochemistry department, Clair Patterson was the first to accurately determine the age of the Earth via lead:uranium ratio in meteorites. In engineering, Theodore von Kármán made many key advances in aerodynamics, notably his work on supersonic and hypersonic airflow characterization. A repeating pattern of swirling vortices is named after him, the von Kármán vortex street. Participants in von Kármán's GALCIT project included Frank Malina, who helped develop the WAC Corporal which was the first U.S. rocket to reach the edge of space, Jack Parsons, a pioneer in the development of liquid and solid rocket fuels who designed the first castable composite-based rocket motor, and Qian Xuesen, who was dubbed the "Father of Chinese Rocketry". More recently, Michael Brown, a professor of planetary astronomy, discovered many trans-Neptunian objects, most notably the dwarf planet Eris, which prompted the International Astronomical Union to redefine the term "planet".
David Baltimore, the Robert A. Millikan Professor of Biology, and Alice Huang, Senior Faculty Associate in Biology, have served as the President of AAAS from 2007–2008 and 2010–2011 respectively.
33% of the faculty are members of the National Academy of Science or Engineering and/or fellows of the American Academy of Arts and Sciences. This is the highest percentage of any faculty in the country with the exception of the graduate institution Rockefeller University.
The average salary for assistant professors at Caltech is $111,300, associate professors $121,300, and full professors $172,800. Caltech faculty are highly productive in the fields of applied physics, astronomy and astrophysics, biology, biochemistry, biological engineering, chemical engineering, computer science, geology, mechanical engineering and physics.
Alumni.
20 alumni and 14 non-alumni faculty have won the Nobel Prize. The Turing Award, the "Nobel Prize of Computer Science", has been awarded to six alumni, and one has won the Fields Medal.
Alumni have participated in scientific research. Some have concentrated their studies on the very small universe of atoms and molecules. Nobel laureate Carl D. Anderson (BS 1927, PhD 1930) proved the existence of positrons and muons, Nobel laureate Edwin McMillan (BS 1928, MS 1929) synthesized the first transuranium element, Nobel laureate Leo James Rainwater (BS 1939) investigated the non-spherical shapes of atomic nuclei, and Nobel laureate Douglas D. Osheroff (BS 1967) studied the superfluid nature of helium-3. Donald Knuth (PhD 1963), the "father" of the analysis of algorithms, wrote "The Art of Computer Programming" and created the TeX computer typesetting system, which is commonly used in the scientific community. Narendra Karmarkar (MS 1979) is known for the interior point method, a polynomial algorithm for linear programming known as Karmarkar's algorithm.
Other alumni have turned their gaze to the universe. C. Gordon Fullerton (BS 1957, MS 1958) piloted the third space shuttle mission and orbited the earth in Skylab. Astronaut (and later, United States Senator) Harrison Schmitt (BS 1957) was the only geologist to have ever walked on the surface of the moon. Astronomer Eugene Merle Shoemaker (BS 1947, MS 1948) co-discovered Comet Shoemaker-Levy 9 (a comet which crashed into the planet Jupiter) and was the first person buried on the moon (by having his ashes crashed into the moon).
Undergraduate alumni founded, or co-founded, companies such as LCD manufacturer Varitronix, Hotmail, Compaq, and MathWorks (which created Matlab), while graduate students founded, or co-founded, companies such as Intel, TRW, and the non-profit educational organization, the Exploratorium.
Arnold Beckman (PhD 1928) invented the pH meter and commercialized it with the founding of Beckman Instruments. His success with that company enabled him to provide seed funding for William Shockley (BS 1932), who had co-invented semiconductor transistors and wanted to commercialize them. Shockley became the founding Director of the Shockley Semiconductor Laboratory division of Beckman Instruments. Shockley had previously worked at Bell Labs, whose first president was another alumnus, Frank Jewett (BS 1898). Because his aging mother lived in Palo Alto, California, Shockley established his laboratory near her in Mountain View, California. Shockley was a co-recipient of the Nobel Prize in physics in 1956, but his aggressive management style and odd personality at the Shockley Lab became unbearable. In late 1957, eight of his researchers resigned and with support from Sherman Fairchild formed Fairchild Semiconductor. Among the "traitorous eight" was Gordon E. Moore (PhD 1954), who later left Fairchild to co-found Intel. Other offspring companies of Fairchild Semiconductor include National Semiconductor and Advanced Micro Devices, which in turn spawned more technology companies in the area. Shockley's decision to use silicon – instead of germanium – as the semiconductor material, coupled with the abundance of silicon semiconductor related companies in the area, gave rise to the term "Silicon Valley" to describe that geographic region surrounding Palo Alto.
Caltech alumni also held public offices, with Mustafa A.G. Abushagur (PhD 1984) the Deputy Prime Minister of Libya, James Fletcher (PhD 1948) the 4th and 7th Administrator of NASA, Steven Koonin (PhD 1972) the Undersecretary of Energy for Science, and Regina Dugan (PhD 1993) the 19th director of DARPA. The 20th director for DARPA, Arati Prabhakar, is also a Caltech alumna (PhD 1984). Arvind Virmani is a former Chief Economic Adviser to the Government of India. In 2013, President Obama announced the nomination of France Cordova (PhD 1979) as the director of the National Science Foundation and Ellen Williams (PhD 1982) as the director for ARPA-E.
Caltech startups.
Over the years Caltech has actively promoted the commercialization of technologies developed within its walls. Through its Office of Technology Transfer & Corporate Partnerships, scientific breakthroughs have led to the transfer of numerous technologies in a wide variety of scientific-related fields such as photovoltaic, radio-frequency identification (RFID), semiconductors, hyperspectral imaging, electronic devices, protein design, solid state amplifiers, and many more. Companies such as Contour Energy Systems, Impinj, Fulcrum Microsystems, Nanosys, Inc., Photon etc., Xencor, Wavestream Wireless have emerged from Caltech.
In media and popular culture.
Caltech has appeared in many works of popular culture, both as itself and in disguised form. As with MIT, a Caltech reference is often used to establish a character's high level of intelligence or a technical background; for example, in the novel "Contact" by Carl Sagan, Eleanor Arroway holds a Ph.D. in radio astronomy from there. On television, the four male lead characters of the sitcom "The Big Bang Theory" are all employed at the Institute. Caltech is also the inspiration, and frequent film location, for the California Institute of Science of "Numb3rs". On film, the Pacific Tech of "The War of the Worlds" and "Real Genius" is based on Caltech.
In nonfiction, two 2007 documentaries examine aspects of Caltech: "Curious", its researchers, and "Quantum Hoops", its men's basketball team.
Given its Los Angeles-area location, the grounds of the Institute are often host to short scenes in movies and television. The Athenaeum dining club appears in the "Beverly Hills Cop" series, "The X-Files", "True Romance", and "The West Wing".

</doc>
<doc id="5790" url="https://en.wikipedia.org/wiki?curid=5790" title="Carlo Goldoni">
Carlo Goldoni

Carlo Osvaldo Goldoni (; 25 February 1707 – 6 February 1793) was an Italian playwright and librettist from the Republic of Venice. His works include some of Italy's most famous and best-loved plays. Audiences have admired the plays of Goldoni for their ingenious mix of wit and honesty. His plays offered his contemporaries images of themselves, often dramatizing the lives, values, and conflicts of the emerging middle classes. Though he wrote in French and Italian, his plays make rich use of the Venetian language, regional vernacular, and colloquialisms. Goldoni also wrote under the pen name and title "Polisseno Fegeio, Pastor Arcade," which he claimed in his memoirs the "Arcadians of Rome" bestowed on him.
One of his best known works is the comic play "Servant of Two Masters", which has been translated and adapted internationally numerous times. In 2011, Richard Bean adapted the play for the National Theatre of Great Britain as "One Man, Two Guvnors". Its popularity led to a transfer to the West End and in 2012 to Broadway.
Biography.
Memoirs.
There is an abundance of autobiographical information on Goldoni, most of which comes from the introductions to his plays and from his "Memoirs". However, these memoirs are known to contain many errors of fact, especially about his earlier years.
In these memoirs, he paints himself as a born comedian, careless, light-hearted and with a happy temperament, proof against all strokes of fate, yet thoroughly respectable and honorable.
Early life and studies.
Goldoni was born in Venice in 1707, the son of Margherita and Giulio Goldoni. In his memoirs, Goldoni describes his father as a physician, and claims that he was introduced to theatre by his grandfather Carlo Alessandro Goldoni. In reality, it seems that Giulio was an apothecary; as for the grandfather, he had died four years before Carlo's birth. In any case, Goldoni was deeply interested in theatre from his earliest years, and all attempts to direct his activity into other channels were of no avail; his toys were puppets, and his books, plays.
His father placed him under the care of the philosopher Caldini at Rimini but the youth soon ran away with a company of strolling players and returned to Venice. In 1723 his father matriculated him into the stern Collegio Ghislieri in Pavia, which imposed the tonsure and monastic habits on its students. However, he relates in his "Memoirs" that a considerable part of his time was spent in reading Greek and Latin comedies. He had already begun writing at this time and, in his third year, he composed a libellous poem ("Il colosso") in which he ridiculed the daughters of certain Pavian families. As a result of that incident (and/or of a visit paid with some schoolmates to a local brothel) he was expelled from the school and had to leave the city (1725). He studied law at Udine, and eventually took his degree at University of Modena. He was employed as a law clerk at Chioggia and Feltre, after which he returned to his native city and began practicing.
Educated as a lawyer, and holding lucrative positions as secretary and counsellor, he seemed, indeed, at one time to have settled down to the practice of law, but following an unexpected summons to Venice, after an absence of several years, he changed his career, and thenceforth he devoted himself to writing plays and managing theatres. His father died in 1731. In 1732, to avoid an unwanted marriage, he left the town for Milan and then for Verona where the theatre manager Giuseppe Imer helped him on his way to becoming a comical poet as well as introducing him to his future wife, Nicoletta Conio. Goldoni returned with her to Venice, where he stayed until 1743.
Theatrical career.
Goldoni entered the Italian theatre scene with a tragedy, "Amalasunta", produced in Milan. The play was a critical and financial failure.
Submitting it to Count Prata, director of the opera, he was told that his piece "was composed with due regard for the rules of Aristotle and Horace, but not according to those laid down for the Italian drama." "In France", continued the count, "you can try to please the public, but here in Italy it is the actors and actresses whom you must consult, as well as the composer of the music and the stage decorators. Everything must be done according to a certain form which I will explain to you."
Goldoni thanked his critic, went back to his inn and ordered a fire, into which he threw the manuscript of his "Amalasunta".
His next play, "Belisario", written in 1734, was more successful, though of its success he afterward professed himself ashamed.
During this period he also wrote librettos for opera seria and served for a time as literary director of the San Giovanni Grisostomo, Venice's most distinguished opera house.
He wrote other tragedies for a time, but he was not long in discovering that his bent was for comedy. He had come to realize that the Italian stage needed reforming; adopting Molière as his model, he went to work in earnest and in 1738 produced his first real comedy, "L'uomo di mondo" ("The Man of the World"). During his many wanderings and adventures in Italy, he was constantly at work and when, at Livorno, he became acquainted with the manager Medebac, he determined to pursue the profession of playwriting in order to make a living. He was employed by Medebac to write plays for his theater in Venice. He worked for other managers and produced during his stay in that city some of his most characteristic works. He also wrote "Momolo Cortesan" in 1738. By 1743, he had perfected his hybrid style of playwriting (combining the model of Molière with the strengths of Commedia dell'arte and his own wit and sincerity). This style was typified in "La Donna di garbo", the first Italian comedy of its kind.
After 1748, Goldoni collaborated with the composer Baldassare Galuppi, making significant contributions to the new form of 'opera buffa'. Galuppi composed the score for more than twenty of Goldoni's librettos. As with his comedies, Goldoni's "opera buffa" integrate elements of the Commedia dell'arte with recognisable local and middle-class realities. His operatic works include two of the most successful musical comedies of the eighteenth century, "Il filosofo di campagna" ("The Country Philosopher"), set by Galuppi (1752) and "La buona figliuola" ("The Good Girl"), set by Niccolò Piccinni (1760).
In 1753, following his return from Bologna he defected to the Teatro San Luca of the Vendramin family where he performed most of his plays to 1762.
Move to France and death.
In 1757, he engaged in a bitter dispute with playwright Carlo Gozzi, which left him utterly disgusted with the tastes of his countrymen; so much so that in 1761 he moved to Paris, where he received a position at court and was put in charge of the Theatre Italien. He spent the rest of his life in France, composing most of his plays in French and writing his memoirs in that language.
Among the plays which he wrote in French, the most successful was "Le bourru bienfaisant", produced on the occasion of the marriage of Louis XVI and Marie Antoinette in 1771. He enjoyed considerable popularity in France; when he retired to Versailles, the King gave him a pension. He lost this pension after the French Revolution. The Convention eventually voted to restore his pension the day after his death. It was restored to his widow, at the pleading of the poet André Chénier; "She is old", he urged, "she is seventy-six, and her husband has left her no heritage save his illustrious name, his virtues and his poverty."
Goldoni's impact on Italian theatre.
In his "Memoirs" Goldoni amply discusses the state of Italian comedy when he began writing. At that time, Italian comedy revolved around the conventionality of the Commedia dell'arte, or improvised comedy. Goldoni took to himself the task of superseding the comedy of masks and the comedy of intrigue by representations of actual life and manners through the characters and their behaviors. He rightly maintained that Italian life and manners were susceptible of artistic treatment such as had not been given them before.
His works are a lasting monument to the changes that he initiated: a dramatic revolution that had been attempted but not achieved before. Goldoni's importance lay in providing good examples rather than precepts. Goldoni says that he took for his models the plays of Molière and that whenever a piece of his own succeeded he whispered to himself: "Good, but not yet Molière." Goldoni's plays are gentler and more optimistic in tone than Molière's.
It was this very success that was the object of harsh critiques by Carlo Gozzi, who accused Goldoni of having deprived the Italian theatre of the charms of poetry and imagination. The great success of Gozzi's fairy dramas so irritated Goldoni that it led to his self-exile to France.
Goldoni gave to his country a classical form, which, though it has since been cultivated, has yet to be cultivated by a master.
Themes.
Goldoni's plays that were written while he was still in Italy ignore religious and ecclesiastical subjects. This may be surprising, considering his staunch Catholic upbringing. No thoughts are expressed about death or repentance in his memoirs or in his comedies. After his move to France, his position became clearer, as his plays took on a clear anti-clerical tone and often satirized the hypocrisy of monks and of the Church.
Goldoni was inspired by his love of humanity and the admiration he had for his fellow men. He wrote, and was obsessed with, the relationships that humans establish with one another, their cities and homes, the Humanist movement, and the study of philosophy. The moral and civil values that Goldoni promotes in his plays are those of rationality, civility, humanism, the importance of the rising middle-class, a progressive stance to state affairs, honor and honesty. Goldoni had a dislike for arrogance, intolerance and the abuse of power.
Goldoni's main characters are no abstract examples of human virtue, nor monstrous examples of human vice. They occupy the middle ground of human temperament. Goldoni maintains an acute sensibility for the differences in social classes between his characters as well as environmental and generational changes. Goldoni pokes fun at the arrogant nobility and the pauper who lacks dignity.
Venetian and Tuscan.
As in other theatrical works of the time and place, the characters in Goldoni's Italian comedies spoke originally either the literary Tuscan variety (which became modern Italian) or the Venetian dialect, depending on their station in life. However, in some printed editions of his plays he often turned the Venetian texts into Tuscan, too.
Selected works.
The following is a small sampling of Goldoni's enormous output.

</doc>
<doc id="5793" url="https://en.wikipedia.org/wiki?curid=5793" title="Cumulative distribution function">
Cumulative distribution function

In probability theory and statistics, the cumulative distribution function (CDF), or just distribution function, evaluated at 'x', is the probability that a real-valued random variable "X" will take a value less than or equal to "x". In other words, CDF(x) = Pr(X≤x), where Pr denotes probability.
In the case of a continuous distribution, it gives the area under the probability density function from minus infinity to "x". Cumulative distribution functions are also used to specify the distribution of multivariate random variables.
Definition.
The cumulative distribution function of a real-valued random variable "X" is the function given by
where the right-hand side represents the probability that the random variable "X" takes on a value less than or
equal to "x". The probability that "X" lies in the semi-closed interval ("a", "b"], where "a"  <  "b", is therefore
In the definition above, the "less than or equal to" sign, "≤", is a convention, not a universally used one (e.g. Hungarian literature uses "<"), but is important for discrete distributions. The proper use of tables of the binomial and Poisson distributions depends upon this convention. Moreover, important formulas like Paul Lévy's inversion formula for the characteristic function also rely on the "less than or equal" formulation.
If treating several random variables "X", "Y", ... etc. the corresponding letters are used as subscripts while, if treating only one, the subscript is usually omitted. It is conventional to use a capital "F" for a cumulative distribution function, in contrast to the lower-case "f" used for probability density functions and probability mass functions. This applies when discussing general distributions: some specific distributions have their own conventional notation, for example the normal distribution.
The CDF of a continuous random variable "X" can be expressed as the integral of its probability density function ƒ as follows:
In the case of a random variable "X" which has distribution having a discrete component at a value "b",
If "F" is continuous at "b", this equals zero and there is no discrete component at "b".
Properties.
Every cumulative distribution function "F" is non-decreasing and right-continuous, which makes it a càdlàg function. Furthermore,
Every function with these four properties is a CDF, i.e., for every such function, a random variable can be defined such that the function is the cumulative distribution function of that random variable.
If "X" is a purely discrete random variable, then it attains values "x", "x", ... with probability "p" = P("x"), and the CDF of "X" will be discontinuous at the points "x" and constant in between:
If the CDF "F" of a real valued random variable "X" is continuous, then "X" is a continuous random variable; if furthermore "F" is absolutely continuous, then there exists a Lebesgue-integrable function "f"("x") such that
for all real numbers "a" and "b". The function "f" is equal to the derivative of "F" almost everywhere, and it is called the probability density function of the distribution of "X".
Examples.
As an example, suppose formula_8 is uniformly distributed on the unit interval , .
Then the CDF of formula_8 is given by
Suppose instead that formula_8 takes only the discrete values 0 and 1, with equal probability.
Then the CDF of formula_8 is given by
Derived functions.
Complementary cumulative distribution function (tail distribution).
Sometimes, it is useful to study the opposite question and ask how often the random variable is "above" a particular level. This is called the complementary cumulative distribution function (ccdf) or simply the tail distribution or exceedance, and is defined as
This has applications in statistical hypothesis testing, for example, because the one-sided p-value is the probability of observing a test statistic "at least" as extreme as the one observed. Thus, provided that the test statistic, "T", has a continuous distribution, the one-sided p-value is simply given by the ccdf: for an observed value "t" of the test statistic
In survival analysis, formula_16 is called the survival function and denoted formula_17, while the term "reliability function" is common in engineering.
Folded cumulative distribution.
While the plot of a cumulative distribution often has an S-like shape, an alternative illustration is the folded cumulative distribution or mountain plot, which folds the top half of the graph over,
thus using two scales, one for the upslope and another for the downslope. This form of illustration emphasises the median and dispersion (the mean absolute deviation from the median) of the distribution or of the empirical results.
Inverse distribution function (quantile function).
If the CDF "F" is strictly increasing and continuous then formula_26 is the unique real number formula_27 such that formula_28. In such a case, this defines the inverse distribution function or quantile function.
Some distributions do not have a unique inverse (for example in the case where formula_29 for all formula_30 to be constant). This problem can be solved by defining, for formula_31, the generalized inverse distribution function: 
Some useful properties of the inverse cdf (which are also preserved in the definition of the generalized inverse distribution function) are:
The inverse of the cdf can be used to translate results obtained for the uniform distribution to other distributions.
Multivariate case.
When dealing simultaneously with more than one random variable the "joint" cumulative distribution function can also be defined. For example, for a pair of random variables "X,Y", the joint CDF formula_44 is given by
where the right-hand side represents the probability that the random variable "X" takes on a value less than or
equal to "x" and that "Y" takes on a value less than or
equal to "y".
Every multivariate CDF is:
Use in statistical analysis.
The concept of the cumulative distribution function makes an explicit appearance in statistical analysis in two (similar) ways. Cumulative frequency analysis is the analysis of the frequency of occurrence of values of a phenomenon less than a reference value. The empirical distribution function is a formal direct estimate of the cumulative distribution function for which simple statistical properties can be derived and which can form the basis of various statistical hypothesis tests. Such tests can assess whether there is evidence against a sample of data having arisen from a given distribution, or evidence against two samples of data having arisen from the same (unknown) population distribution.
Kolmogorov–Smirnov and Kuiper's tests.
The Kolmogorov–Smirnov test is based on cumulative distribution functions and can be used to test to see whether two empirical distributions are different or whether an empirical distribution is different from an ideal distribution. The closely related Kuiper's test is useful if the domain of the distribution is cyclic as in day of the week. For instance Kuiper's test might be used to see if the number of tornadoes varies during the year or if sales of a product vary by day of the week or day of the month.

</doc>
<doc id="5794" url="https://en.wikipedia.org/wiki?curid=5794" title="Central tendency">
Central tendency

In statistics, a central tendency (or, more commonly, a measure of central tendency) is a central or typical value for a probability distribution. It may also be called a center or location of the distribution. Colloquially, measures of central tendency are often called "averages." The term "central tendency" dates from the late 1920s.
The most common measures of central tendency are the arithmetic mean, the median and the mode. A central tendency can be calculated for either a finite set of values or for a theoretical distribution, such as the normal distribution. Occasionally authors use central tendency to denote "the tendency of quantitative data to cluster around some central value." 
The central tendency of a distribution is typically contrasted with its "dispersion" or "variability"; dispersion and central tendency are the often characterized properties of distributions. Analysts may judge whether data has a strong or a weak central tendency based on its dispersion.
Measures of central tendency.
The following may be applied to one-dimensional data. Depending on the circumstances, it may be appropriate to transform the data before calculating a central tendency. Examples are squaring the values or taking logarithms. Whether a transformation is appropriate and what it should be depend heavily on the data being analyzed.
Any of the above may be applied to each dimension of multi-dimensional data, but the results may not be invariant to rotations of the multi-dimensional space. In addition, there is the
The Quadratic mean (often known as the root mean square) is useful in engineering, but is not often used in statistics. This is because it is not a good indicator of the center of the distribution when the distribution includes negative values.
Solutions to variational problems.
Several measures of central tendency can be characterized as solving a variational problem, in the sense of the calculus of variations, namely minimizing variation from the center. That is, given a measure of statistical dispersion, one asks for a measure of central tendency that minimizes variation: such that variation from the center is minimal among all choices of center. In a quip, "dispersion precedes location". In the sense of "L" spaces, the correspondence is:
Thus standard deviation about the mean is lower than standard deviation about any other point, and the maximum deviation about the midrange is lower than the maximum deviation about any other point. The uniqueness of this characterization of mean follows from convex optimization. Indeed, for a given (fixed) data set "x", the function
represents the dispersion about a constant value "c" relative to the "L" norm. Because the function "ƒ" is a strictly convex coercive function, the minimizer exists and is unique.
Note that the median in this sense is not in general unique, and in fact any point between the two central points of a discrete distribution minimizes average absolute deviation. The dispersion in the "L" norm, given by
is not "strictly" convex, whereas strict convexity is needed to ensure uniqueness of the minimizer. In spite of this, the minimizer is unique for the "L" norm.
Relationships between the mean, median and mode.
For unimodal distributions the following bounds are known and are sharp:
where "μ" is the mean, "ν" is the median, "θ" is the mode, and "σ" is the standard deviation.
For every distribution,

</doc>
<doc id="5796" url="https://en.wikipedia.org/wiki?curid=5796" title="Celebrity">
Celebrity

Celebrity is fame and public attention in the media, usually applied to a person, group of people (celebrity couple, family etc.), or occasionally, to animals or fictional entities. Celebrity status is often associated with wealth (commonly referred to as "fame and fortune") and fame can often provide opportunities to make money.
Successful careers in sports and entertainment are commonly associated with celebrity status; political leaders often become celebrities. People may also become celebrities due to media attention for their lifestyle, wealth, or controversial actions, or for their connection to a famous person.
History.
Throughout recorded history there are accounts of people who attracted the trappings of celebrity which would be recognized today.
Athletes in Ancient Greece were welcomed home as heroes, had songs and poems written in their honour and received free food and gifts from those seeking celebrity endorsement. Ancient Rome similarly lauded actors and notorious gladiators and Julius Caesar appeared on a coin in his own lifetime (a departure from the usual depiction of battles and divine lineage).
In the 12th century, Thomas Becket became famous following his murder. He was promoted by the Christian Church as a martyr and images of him and scenes from his life became widespread in just a few years. In a pattern often repeated, what started out as an explosion of popularity (often referred to with the suffix 'mania') turned into a long-lasting fame: pilgrimages to Canterbury Cathedral where he was killed became instantly fashionable and the fascination with his life and death have inspired plays and films.
The cult of personality (particularly in the west) can be traced back to the Romantics in the 18th Century, whose livelihood as artists and poets depended on the currency of their reputation. The establishment of cultural hot-spots became an important factor in the process of generating fame: for example, London and Paris in the 18th and 19th Centuries. Newspapers started including gossip columns and certain clubs and events became places to be seen in order to receive publicity.
The movie industry spread around the globe in the first half of the 20th Century and with it the now familiar concept of the instantly recognizable faces of its superstars. Yet, celebrity wasn't always tied to actors in films, especially when cinema was starting out as a medium. As Paul McDonald states in "The Star System: Hollywood's Production of Popular Identities", "in the first decade of the twentieth century, American film production companies withheld the names of film performers, despite requests from audiences, fearing that public recognition would drive performers to demand higher salaries." Public fascination went well beyond the on-screen exploits of movie stars and their private lives became headline news: for example, in Hollywood the marriages of Elizabeth Taylor and in Bollywood the affairs of Raj Kapoor in the 1950s.
The second half of the century saw television and popular music bring new forms of celebrity. The rock star and pop group epitomised by Elvis Presley and The Beatles respectively. John Lennon's quote: "We're more popular than Jesus now" gives an insight into both the adulation and notoriety that fame can bring. Unlike movies, television created celebrities who were not primarily actors; for example, presenters, talk show hosts and news readers. However, most of these are only famous within the regions reached by their particular broadcaster, and only a few such as Oprah Winfrey, Jerry Springer or David Frost could be said to have broken through into a wider stardom.
Regional and cultural implications.
Cultures and regions with a significant population may have their own independent celebrity systems, with distinct hierarchies. For example, the Canadian province of Quebec, which is French-speaking, has its own system of French-speaking television, movie and music celebrities. A person who garners a degree of fame in one culture may be considered less famous or obscure in another. Some nationwide celebrities might command some attention outside their own nation; for example, the singer Lara Fabian is widely known in the French-speaking world, but only had a couple of "Billboard" hits in the US, whereas the francophone Canadian singer Celine Dion is well known in both the French-speaking world and in the US.
Regions within a country, or cultural communities (linguistic, ethnic, religious) can also have their own celebrity systems, especially in linguistically or culturally distinct regions such as Quebec or Wales. Regional radio personalities, newscasters, politicians or community leaders may be local or regional celebrities.
English-speaking media commentators and journalists will sometimes refer to celebrities as belonging to the "A-List" or state that a certain actor belongs to the "B-List", the latter being a disparaging context. These informal rankings indicate a placing within a hierarchy. However, due to differing levels of celebrity in different regions, it is difficult to place people within one bracket. A Brazilian actor might be a B-list action film actor in the US, but an A-list star in Portugal.
Some elements are associated with fame, such as appearing on the cover of "Time", being spoofed in "Mad", having a wax statue in "Madame Tussauds", or receiving a star on the "Hollywood Walk of Fame."
Certain people are known even to people unfamiliar with the area in which they excelled. If one has to name a famous boxer, they are more likely to name Muhammad Ali or Mike Tyson, since their fame expanded beyond the sport itself. Pablo Picasso's style and name are known even to people who are not interested in art; likewise many know that Harry Houdini was an illusionist, Tiger Woods a golfer, Bill Gates an entrepreneur, Albert Einstein a scientist; Mozart and Beethoven classical composers; Luciano Pavarotti an opera singer.
Fictional implications.
The same phenomenon is true for fictional characters. Superman, Spider-Man, and Batman represent super heroes to a far wider audience than that of the comics and graphic novels in which they appear. Disney have themeparks around the world which rely on the fame of its creations headed by Mickey Mouse. Sherlock Holmes and James Bond continue to be portrayed in film, television and literature decades after the original stories were published. Some characters from video and computer games have developed a celebrity life beyond these media, such as Lara Croft and Mario.
Becoming a celebrity in the U.S..
People may become celebrities in a wide range of ways; from their professions, following appearances in the media, committing a mass murder, or by complete accident. The term "instant celebrity" describes someone who becomes a celebrity in a very short period of time. Someone who achieves a small amount of transient fame (through, say, hype or mass media) may become labeled a "B-grade celebrity". Often, the generalization extends to someone who falls short of mainstream or persistent fame but who seeks to extend or exploit it.
Success.
There are no guarantees of success for an individual to become a celebrity. Though celebrities come from many different working fields, most celebrities are typically associated with the fields of sports and entertainment or a person may be a public figure who is commonly recognizable in mass media.
Though glamor and wealth certainly plays a role for only famous celebrities, most people in the sports and entertainments sphere, be it music, film, television, radio, modelling, comedy, literature etc. live in obscurity and only a small percentage achieve fame and fortune.
Difficulty.
A large number of athletes who are unable to turn professional take a second job or even sometimes abandon their athletic aspirations in order to make ends meet. A small percentage of entertainers and athletes are able to make a decent living but a vast majority will spend their careers toiling from hard work, determination, rejection and frequent unemployment. For minor league to amateur athletes, earnings are usually on the lower end of the pay-scale. Many of them take second jobs on the side or even venture into other occupations within the field of sports such as coaching, general management, refereeing or recruiting and scouting up-and-coming athletes.
The Screen Actors Guild, a union well known for representing actors and actresses throughout Hollywood reports that the average television and film actor earns less than US$5000 annually. Actors sometimes alternate between theater, television and film or even branch into other occupations within the entertainment industry such as becoming a singer, comedian, producer, or a television host in order to be monetarily diversified, as doing one gig pays comparatively very little. For instance, David Letterman is well known for branching into late night television as a talk show host while honing his skills as a stand-up comedian, Barbra Streisand ventured into acting while operating as a singer, or Clint Eastwood, who achieved even greater fame in Hollywood for being a film director and a producer than for his acting credentials.
According to American entertainment magnate Master P, entertainers and professional athletes make up less than 1% of all millionaires in the entire world. Less than 1% of all runway models are known to make more than US$1000 for every fashion showcase. According to the US Bureau of Labor Statistics the median wage for commercial and print models was only $11.22 per hour in 2006 and was also listed one of the top ten worst jobs in the United States. Most models only draw in around US$500 every showcase and only famous models that are high in demand such as Miranda Kerr or Gisele Bündchen earn multimillion-dollar salaries. Freelance writers and authors who aspire to be the next Stephen King and Dan Brown are known to submit manuscripts of their latest literary creations hoping for their big break are only to be bombarded with numerous rejection letters from major publishing houses. Many aspiring comedians who dream of becoming the next Louis C.K. and Jerry Seinfeld never see the inside of a movie or television studio, but rather spend most of their careers doing stand-up in comedy clubs and other small venues, hoping to be discovered. Because gigs can be infrequent, it can be very difficult to make a living as a freelance entertainer. As a result, many supplement their income by holding down other jobs on the side.
Wealth.
Forbes Celebrity 100.
Forbes Magazine releases an annual Forbes Celebrity 100 list of the highest paid celebrities in the world. The total earnings for all top celebrity 100 earners totaled $4.5 billion over the course of 2010 alone.
For instance, Forbes ranked media mogul and talk show host, Oprah Winfrey as the top earner "Forbes magazine’s annual ranking of the most powerful celebrities", with earnings of $290 million in the past year. Forbes cites that Lady Gaga reportedly earned over $90 million in 2010. In 2010, golfer Tiger Woods was one of highest-earning celebrity athletes, with an income of $75 million and is consistently ranked one of the highest paid athletes in the world. In 2013, Madonna was ranked as the fifth most powerful and the highest earning celebrity of the year with earnings of $125 million. She has consistently been among the most powerful and highest earning celebrities in the world, occupying the third place in Forbes Celebrity 100 2009 with $110 million of earnings, and getting the tenth place in the 2010 edition of the list with annual earnings equal to $58 million.
Entrepreneurship and endorsements.
Celebrity endorsements have proven very successful around the world where, due to increasing consumerism, an individual is considered to own a status symbol when they purchase a celebrity-endorsed product. Though become the commonplace for celebrities to place their name with endorsements onto products just for quick money. However, some celebrities have gone beyond merely using their big names and have decided to put their entrepreneurial spirit to work by becoming entrepreneurs by attaching themselves in the business aspects of entertainment and building their own business brand beyond their traditional salaried activities. Along with investing their salaried wages into growing business endeavors, a number of celebrities have become innovative business leaders in their respective industries, gaining the admiration of their peers and contributing to the country’s economy.
Numerous celebrities have ventured into becoming business moguls and established themselves as entrepreneurs, idolizing many well known American business leaders such as Bill Gates, Warren Buffett, and Donald Trump. For instance, basketball legend, Michael Jordan became an active entrepreneur involved with many sports related ventures including investing a minority stake in the Charlotte Bobcats, Paul Newman started his own salad dressing business after leaving behind a distinguished acting career, or rap musician, Birdman started his own record label, clothing line, and an oil business while maintaining a career as a rap artist. Other celebrities such as Tyler Perry, George Lucas, and Steven Spielberg have become successful entrepreneurs through starting their own film production companies and running their own movie studios beyond their traditional activities of screenwriting, directing, animating, producing, and acting.
Various examples of celebrity turned entrepreneurs included in the table below are:
Tabloid magazines and talk TV shows bestow a great deal of attention on celebrities. To stay in the public eye and build wealth in addition to their salaried labor, numerous celebrities have participating and branching into various business ventures and endorsements. Many celebrities have participated in many different endorsement opportunities that include: animation, publishing, fashion designing, cosmetics, consumer electronics, household items and appliances, cigarettes, soft drinks and alcoholic beverages, hair care, hairdressing, jewelry design, fast food, credit cards, video games, writing, and toys.
In addition to various endorsements, a number of celebrities have been involved with some business and investment related ventures also include: and toddler related items, sports team ownership, fashion retailing, establishments such as restaurants, cafes, hotels, and casinos, movie theaters, advertising and event planning, management related ventures such as sports management, financial services, model management, and talent management, record labels, film production, television production, publishing such as book and music publishing, massage therapy, salons, health and fitness, and real estate.
Although some celebrities have achieved additional financial success from various business ventures, the vast majority of celebrities are not successful businesspeople and still rely on salaried labored wages in order earn a living. Most businesses and investments are well known to have a 90 to 95 percent failure rate within the first five years of operation. Not all celebrities eventually succeed with their own businesses and other related side ventures. Some celebrities either went broke or filed for bankruptcy as result of dabbling with such side businesses or endorsements. Though some might question such a validity since celebrities themselves are already well known, have mass appeal, and are well exposed to the general public. The average entrepreneur who is not well known and reputable to general public doesn't the same marketing flexibility and status-quo as most celebrities allow and have. Therefore, compared to the average person who starts a business, celebrities already have all the cards and odds stacked in their favor. This means they can have an unfair advantage to expose their business ventures and endorsements and can easily capture a more significant amount of market share than the average entrepreneur.
As a mass media phenomenon.
Celebrities often have fame comparable to royalty. As a result, there is a strong public curiosity about their private affairs. The release of Kim Kardashian's sex tape with rapper Ray J in 2003 brought her to a new level of fame, leading to magazine covers, book deals, and reality TV series.
Celebrities may be resented for their accolades, and the public may have a love/hate relationship with celebrities. Due to the high visibility of celebrities' private lives, their successes and shortcomings are often made very public. Celebrities are alternately portrayed as glowing examples of perfection, when they garner awards, or as decadent or immoral if they become associated with a scandal. When seen in a positive light, celebrities are frequently portrayed as possessing skills and abilities beyond average people; for example, celebrity actors are routinely celebrated for acquiring new skills necessary for filming a role within a very brief time, and to a level that amazes the professionals who train them. Similarly, some celebrities with very little formal education can sometimes be portrayed as experts on complicated issues. Some celebrities have been very vocal with their political views. For example, Matt Damon expressed his displeasure with 2008 US vice presidential nominee Sarah Palin, as well as with the 2011 United States debt-ceiling crisis.
Famous for being famous.
Famous for being famous, in popular culture terminology, refers to someone who attains celebrity status for no particular identifiable reason, or who achieves fame through association with a celebrity. The term is a pejorative, suggesting that the individual has no particular talents or abilities. Even when their fame arises from a particular talent or action on their part, the term will sometimes still apply if their fame is perceived as disproportionate to what they earned through their own talent or work.
The coinages "famesque" and "celebutante" are of similar pejorative gist.
Families.
Another example of celebrity is a family that has notable ancestors or is known for its wealth. In some cases, a well-known family is associated with a particular field. For example, the Kennedy family is associated with US politics; The House of Windsor with royalty; The Osbournes, The Jacksons, Chaplin, and Barrymore families with entertainment.
Restricted access.
Access to celebrities is strictly controlled by their entourage of staff which includes managers, publicists, agents, personal assistants, and bodyguards. Even journalists find it difficult to access celebrities for interviews. An interview with writer and actor Michael Musto cites:
Celebrities often hire one or more bodyguards (or close protection officer) to protect themselves and their families from threats ranging from the mundane (intrusive paparazzi photographers or autograph-seeking fans) to serious (assault, kidnapping, assassination, or stalking). The bodyguard travels with the celebrity during professional activities (movie shoots or concerts) and personal activities such as recreation and errands.
Celebrities also typically have security staff at their home, to protect them from similar threats.
Cult of celebrity.
15 minutes of fame.
"See also: 15 minutes of fame, One-hit wonder"
Andy Warhol famously coined the phrase "15 minutes of fame" in reference to a short-lived publicity. Certain "15 minutes of fame" celebrities can be average people seen with an A-list celebrity, who are sometimes noticed on entertainment news channels such as E! News. These persons are ordinary people becoming celebrities, often based on the ridiculous things they do. "In fact, many reality show contestants fall into this category: the only thing that qualifies them to be on TV is that they're real."
Certain people are only remembered today because of a movie portrayal, certain story or urban legend surrounding their life and less for their accomplishments. Antonio Salieri was a famous and well-known 18th-century composer, but his fictional portrayal as an antagonist (for example, in the musical and film "Amadeus") has been more famous than his music since the end of the 20th century. Roscoe "Fatty" Arbuckle and O. J. Simpson are more notorious for their association with murder trials than for their respective movie and sports careers. Ronald Reagan is more famous as a politician today than as a movie actor. Centuries after his death, Andrea Mantegna is now better known as the mentor of Leonardo da Vinci than for his own paintings.
Social networking.
Celebrities have been flocking to social networking and video hosting sites such as YouTube, Twitter, Facebook, Instagram, Google+, and MySpace. Social networking sites allow celebrities to communicate directly with their fans, removing the middle-man known as traditional media. Social media humanizes celebrities in a way that arouses public fascination as evident by the success of magazines such as "Us Weekly" and "People Weekly". Celebrity blogging have also spawned stars such as Perez Hilton who is well known for not only blogging, but also outing celebrities.
Social media sites have also contributed to the fame of some celebrities, such as Tila Tequila who became known through MySpace.
Health implications.
John Cleese said being famous offers some advantages such as financial wealth and easier access to things that are more difficult for non-famous people to access, such as the ability to more easily meet other famous or powerful people, but that being famous also often comes with the disadvantage of creating the conditions in which the celebrity finds themselves acting, at least temporarily (although sometimes over extended periods of time), in a superficial, inauthentic fashion.
Common threats such as stalking have spawned celebrity worship syndrome where a person becomes overly involved with the details of a celebrity's personal life. Psychologists have indicated that though many people obsess over glamorous film, television, sport and pop stars, the disparity in salaries in society seems to value professional athletes and entertainment industry based professionals. One study found that singers, musicians, actors and sportspeople die younger on average than writers, composers, academics, politicians and businesspeople, with a greater incidence of cancer and especially lung cancer. However, it was remarked that the reasons for this remained unclear, with theories including innate tendencies towards risk-taking as well as the pressure or opportunities of particular types of fame.
Furthermore, some have said fame might have negative psychological effects, and may lead to increasingly selfish tendencies and psychopathy. 
Recently, there has been more attention toward the impact celebrities have on health decisions of the population at large. It is believed that the public will follow celebrities' health advice to some extent. This can have positive impacts when the celebrities give solid, evidence informed health advice, however it can also have detrimental affects if the health advice is not accurate enough.

</doc>
<doc id="5797" url="https://en.wikipedia.org/wiki?curid=5797" title="Cluster sampling">
Cluster sampling

Cluster sampling is a sampling technique used when "natural" but relatively heterogeneous groupings are evident in a statistical population. It is often used in marketing research. In this technique, the total population is divided into these groups (or clusters) and a simple random sample of the groups is selected. Then the required information is collected from a simple random sample of the elements within each selected group. A subsample of elements may be selected within each of these groups. A common motivation for cluster sampling is to reduce the total number of interviews and costs given the desired accuracy. Assuming a fixed sample size, the technique gives more accurate results when most of the variation in the population is within the groups, not between them.
Cluster elements.
The population within a cluster should ideally be as heterogeneous as possible, but there should be homogeneity between cluster means. Each cluster should be a small-scale representation of the total population. The clusters should be mutually exclusive and collectively exhaustive. A random sampling technique is then used on any relevant clusters to choose which clusters to include in the study. In single-stage cluster sampling, all the elements from each of the selected clusters are used. In two-stage cluster sampling, a random sampling technique is applied to the elements from each of the selected clusters.
The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so analysis is done on a population of clusters (at least in the first stage). In stratified sampling, the analysis is done on elements within strata. In stratified sampling, a random sample is drawn from each of the strata, whereas in cluster sampling only the selected clusters are studied. The main objective of cluster sampling is to reduce costs by increasing sampling efficiency. This contrasts with stratified sampling where the main objective is to increase precision.
There also exists multistage sampling, here more than two steps are taken in selecting clusters from clusters.
Aspects of cluster sampling.
One version of cluster sampling is area sampling or geographical cluster sampling. Clusters consist of geographical areas. Because a geographically dispersed population can be expensive to survey, greater economy than simple random sampling can be achieved by treating several respondents within a local area as a cluster. It is usually necessary to increase the total sample size to achieve equivalent precision in the estimators, but cost savings may make that feasible.
In some situations, cluster analysis is only appropriate when the clusters are approximately the same size. This can be achieved by combining clusters. If this is not possible, probability proportionate to size sampling is used. In this method, the probability of selecting any cluster varies with the size of the cluster, giving larger clusters a greater probability of selection and smaller clusters a lower probability. However, if clusters are selected with probability proportionate to size, the same number of interviews should be carried out in each sampled cluster so that each unit sampled has the same probability of selection.
Cluster sampling is used to estimate high mortalities in cases such as wars, famines and natural disasters.
Errors: The other probabilistic methods give fewer errors than this method. For this reason, it is discouraged for beginners.
More on cluster sampling.
Two-stage cluster sampling.
Two-stage cluster sampling, a simple case of multistage sampling, is obtained by selecting cluster samples in the first stage and then selecting sample of elements from every sampled cluster. Consider a population of "N" clusters in total. In the first stage, "n" clusters are selected using ordinary cluster sampling method. In the second stage, simple random sampling is usually used. It is used separately in every cluster and the numbers of elements selected from different clusters are not necessarily equal. The total number of clusters "N", number of clusters selected "n", and numbers of elements from selected clusters need to be pre-determined by the survey designer. Two-stage cluster sampling aims at minimizing survey costs and at the same time controlling the uncertainty related to estimates of interest. This method can be used in health and social sciences. For instance, researchers used two-stage cluster sampling to generate a representative sample of the Iraqi population to conduct mortality surveys. Sampling in this method can be quicker and more reliable than other methods, which is why this method is now used frequently.

</doc>
<doc id="5804" url="https://en.wikipedia.org/wiki?curid=5804" title="Charles Baudelaire">
Charles Baudelaire

Charles Pierre Baudelaire (; ; April 9, 1821 – August 31, 1867) was a French poet who also produced notable work as an essayist, art critic, and pioneering translator of Edgar Allan Poe.
His most famous work, "Les Fleurs du mal" ("The Flowers of Evil"), expresses the changing nature of beauty in modern, industrializing Paris during the 19th century. Baudelaire's highly original style of prose-poetry influenced a whole generation of poets including Paul Verlaine, Arthur Rimbaud and Stéphane Mallarmé among many others. He is credited with coining the term "modernity" ("modernité") to designate the fleeting, ephemeral experience of life in an urban metropolis, and the responsibility art has to capture that experience.
Baudelaire the poet.
Baudelaire is one of the major innovators in French literature. His poetry is influenced by the French romantic poets of the earlier 19th century, although its attention to the formal features of verse connects it more closely to the work of the contemporary "Parnassians". As for theme and tone, in his works we see the rejection of the belief in the supremacy of nature and the fundamental goodness of man as typically espoused by the romantics and expressed by them in rhetorical, effusive and public voice in favor of a new urban sensibility, an awareness of individual moral complexity, an interest in vice (linked with decadence) and refined sensual and aesthetic pleasures, and the use of urban subject matter, such as the city, the crowd, individual passers-by, all expressed in highly ordered verse, sometimes through a cynical and ironic voice. Formally, the use of sound to create atmosphere, and of "symbols" (images that take on an expanded function within the poem), betray a move towards considering the poem as a self-referential object, an idea further developed by the Symbolists Verlaine and Mallarmé, who acknowledge Baudelaire as a pioneer in this regard.
Beyond his innovations in versification and the theories of symbolism and "correspondences", an awareness of which is essential to any appreciation of the literary value of his work, aspects of his work that regularly receive (or have received) much critical discussion include the role of women, the theological direction of his work and his alleged advocacy of "satanism", his experience of drug-induced states of mind, the figure of the dandy, his stance regarding democracy and its implications for the individual, his response to the spiritual uncertainties of the time, his criticisms of the bourgeois, and his advocacy of modern music and painting (e.g., Wagner, Delacroix).
Early life.
Baudelaire was born in Paris, France, on April 9, 1821, and baptized two months later at Saint-Sulpice Roman Catholic Church. His father, François Baudelaire, a senior civil servant and amateur artist, was 34 years older than Baudelaire's mother. François died during Baudelaire's childhood, in 1827. The following year, Caroline married Lieutenant Colonel Jacques Aupick, who later became a French ambassador to various noble courts. Biographers have often seen this as a crucial moment, considering that finding himself no longer the sole focus of his mother's affection left him with a trauma, which goes some way to explaining the excesses later apparent in his life. He stated in a letter to her that, "There was in my childhood a period of passionate love for you." Baudelaire regularly begged his mother for money throughout his career, often promising that a lucrative publishing contract or journalistic commission was just around the corner.
Baudelaire was educated in Lyon, where he boarded. Baudelaire at fourteen was described by a classmate: "He was much more refined and distinguished than any of our fellow pupils . we are bound to one another . by shared tastes and sympathies, the precocious love of fine works of literature." Baudelaire was erratic in his studies, at times diligent, at other times prone to "idleness". Later, he attended the Lycée Louis-le-Grand in Paris, studying law, a popular course for those not yet decided on any particular career. Baudelaire began to frequent prostitutes and may have contracted gonorrhea and syphilis during this period. Baudelaire began to run up debts, mostly for clothes. Upon gaining his degree in 1839, he told his brother "I don't feel I have a vocation for anything." His stepfather had in mind a career in law or diplomacy, but instead Baudelaire decided to embark upon a literary career. His mother later recalled: "Oh, what grief! If Charles had let himself be guided by his stepfather, his career would have been very different... He would not have left a name in literature, it is true, but we should have been happier, all three of us."
His stepfather sent him on a voyage to Calcutta, India, in 1841 in the hope of ending his dissolute habits. The trip provided strong impressions of the sea, sailing, and exotic ports, that he later employed in his poetry. (Baudelaire later exaggerated his aborted trip to create a legend about his youthful travels and experiences, including "riding on elephants".) Baudelaire returned to the taverns where he began to compose some of the poems of "Les Fleurs du Mal". At 21, he received a good-sized inheritance but squandered much of it within a few years. His family obtained a decree to place his property in trust, which he resented bitterly, at one point arguing that allowing him to fail alone financially would have been the one sure way of teaching him the value of maintaining well-ordered finances.
Baudelaire became known in artistic circles as a dandy and free-spender. During this time, Jeanne Duval became his mistress. His mother thought Duval a "Black Venus" who "tortured him in every way" and drained him of money at every opportunity. She was rejected by his family. He made a suicide attempt during this time.
Baudelaire took part in the Revolutions of 1848 and wrote for a revolutionary newspaper. However, his interest was passing, as he was later to note in his political writings in his journals.
In the early 1850s, Baudelaire struggled with poor health, pressing debts, and irregular literary output. He often moved from one lodging to another to escape creditors. He received many projects that he was unable to complete, though he did finish translations of stories by Edgar Allan Poe.
Upon the death of his stepfather in 1857, Baudelaire received no mention in the will but he was heartened nonetheless that the division with his mother might now be mended. At 36 he wrote her: "believe that I belong to you absolutely, and that I belong only to you".
Published career.
His first published work was his art review "Salon of 1845", which attracted immediate attention for its boldness. Many of his critical opinions were novel in their time, including his championing of Delacroix, and some of his views seem remarkably in tune with the future theories of the Impressionist painters.
In 1846, Baudelaire wrote his second Salon review, gaining additional credibility as an advocate and critic of Romanticism. His support of Delacroix as the foremost Romantic artist gained widespread notice. The following year Baudelaire's novella "La Fanfarlo" was published.
"The Flowers of Evil".
Baudelaire was a slow and fastidious worker, often sidetracked by indolence, emotional distress and illness, and it was not until 1857 that he published his first and most famous volume of poems, "Les Fleurs du mal" ("The Flowers of Evil"). Some of these poems had already appeared in the "Revue des deux mondes" ("Review of Two Worlds") in 1855, when they were published by Baudelaire's friend Auguste Poulet Malassis. Some of the poems had also previously appeared a "fugitive verse" in various French magazines during the previous decade.
The poems found a small, appreciative audience, but greater public attention was given to their subject matter. The effect on fellow artists was, as Théodore de Banville stated, "immense, prodigious, unexpected, mingled with admiration and with some indefinable anxious fear". Gustave Flaubert, recently attacked in a similar fashion for "Madame Bovary" (and acquitted), was impressed and wrote to Baudelaire: "You have found a way to rejuvenate Romanticism... You are as unyielding as marble, and as penetrating as an English mist."
The principal themes of sex and death were considered scandalous. He also touched on lesbianism, sacred and profane love, metamorphosis, melancholy, the corruption of the city, lost innocence, the oppressiveness of living, and wine. Notable in some poems is Baudelaire's use of imagery of the sense of smell and of fragrances, which is used to evoke feelings of nostalgia and past intimacy.
The book, however, quickly became a byword for unwholesomeness among mainstream critics of the day. Some critics called a few of the poems "masterpieces of passion, art and poetry" but other poems were deemed to merit no less than legal action to suppress them. J. Habas writing in "Le Figaro", led the charge against Baudelaire, writing: "Everything in it which is not hideous is incomprehensible, everything one understands is putrid." Then Baudelaire responded to the outcry, in a prophetic letter to his mother:
"You know that I have always considered that literature and the arts pursue an aim independent of morality. Beauty of conception and style is enough for me. But this book, whose title ("Fleurs du mal") says everything, is clad, as you will see, in a cold and sinister beauty. It was created with rage and patience. Besides, the proof of its positive worth is in all the ill that they speak of it. The book enrages people. Moreover, since I was terrified myself of the horror that I should inspire, I cut out a third from the proofs. They deny me everything, the spirit of invention and even the knowledge of the French language. I don't care a rap about all these imbeciles, and I know that this book, with its virtues and its faults, will make its way in the memory of the lettered public, beside the best poems of V. Hugo, Th. Gautier and even Byron."
Baudelaire, his publisher and the printer were successfully prosecuted for creating an offense against public morals. They were fined, but Baudelaire was not imprisoned. Six of the poems were suppressed, but printed later as "Les Épaves" ("The Wrecks") (Brussels, 1866). Another edition of "Les Fleurs du mal", without these poems, but with considerable additions, appeared in 1861. Many notables rallied behind Baudelaire and condemned the sentence. Victor Hugo wrote to him: "Your "fleurs du mal" shine and dazzle like stars... I applaud your vigorous spirit with all my might." Baudelaire did not appeal the judgment, but his fine was reduced. Nearly 100 years later, on May 11, 1949, Baudelaire was vindicated, the judgment officially reversed, and the six banned poems reinstated in France.
In the poem "Au lecteur" ("To the Reader") that prefaces "Les Fleurs du mal", Baudelaire accuses his readers of hypocrisy and of being as guilty of sins and lies as the poet:
Final years.
Baudelaire next worked on a translation and adaptation of Thomas De Quincey's "Confessions of an English Opium Eater". Other works in the years that followed included "Petits Poèmes en prose" ("Small Prose poems"); a series of art reviews published in the "Pays, Exposition universelle" ("Country, World Fair"); studies on Gustave Flaubert (in "L'Artiste", October 18, 1857); on Théophile Gautier ("Revue contemporaine", September 1858); various articles contributed to Eugene Crepet's "Poètes francais"; "Les Paradis artificiels: opium et haschisch" ("French poets; Artificial Paradises: opium and hashish") (1860); and "Un Dernier Chapitre de l'histoire des oeuvres de Balzac" ("A Final Chapter of the history of works of Balzac") (1880), originally an article "Comment on paye ses dettes quand on a du génie" ("How one pays one's debts when one has genius"), in which his criticism turns against his friends Honoré de Balzac, Théophile Gautier, and Gérard de Nerval.
By 1859, his illnesses, his long-term use of laudanum, his life of stress and poverty had taken a toll and Baudelaire had aged noticeably. But at last, his mother relented and agreed to let him live with her for a while at Honfleur. Baudelaire was productive and at peace in the seaside town, his poem "Le Voyage" being one example of his efforts during that time. In 1860, he became an ardent supporter of Richard Wagner.
His financial difficulties increased again, however, particularly after his publisher Poulet Malassis went bankrupt in 1861. In 1864, he left Paris for Belgium, partly in the hope of selling the rights to his works and also to give lectures. His long-standing relationship with Jeanne Duval continued on-and-off, and he helped her to the end of his life. Baudelaire's relationships with actress Marie Daubrun and with courtesan Apollonie Sabatier, though the source of much inspiration, never produced any lasting satisfaction. He smoked opium, and in Brussels he began to drink to excess. Baudelaire suffered a massive stroke in 1866 and paralysis followed. After more than a year of aphasia, he received the last rites of the Catholic Church. The last two years of his life were spent, in a semi-paralyzed state, in "maisons de santé" in Brussels and in Paris, where he died on August 31, 1867. Baudelaire is buried in the Cimetière du Montparnasse, Paris.
Many of Baudelaire's works were published posthumously. After his death, his mother paid off his substantial debts, and at last she found some comfort in Baudelaire's emerging fame. "I see that my son, for all his faults, has his place in literature." She lived another four years.
Critiques.
Baudelaire was an active participant in the artistic life of his times. As critic and essayist, he wrote extensively and perceptively about the luminaries and themes of French culture. He was frank with friends and enemies, rarely took the diplomatic approach and sometimes responded violently verbally, which often undermined his cause. His associations were numerous and included: Gustave Courbet, Honoré Daumier, Franz Liszt, Champfleury, Victor Hugo, Gustave Flaubert, Balzac and the artists and writers that follow.
Edgar Allan Poe.
In 1847, Baudelaire became acquainted with the works of Poe, in which he found tales and poems that had, he claimed, long existed in his own brain but never taken shape. Baudelaire had much in common with Poe (who died in 1849 at age forty). The two poets display a similar sensibility of the macabre and supernatural turn of mind; each struggled with illness, poverty, and melancholy. Like Poe, Baudelaire believed in the doctrine of original sin, denounced democracy and the idea of progress and of man's natural goodness, and Poe held a disdainful aristocratic attitude similar to Baudelaire's dandy. Baudelaire saw in Poe a precursor and tried to be his French contemporary counterpart. From this time until 1865, he was largely occupied with translating Poe's works; his translations were widely praised. Baudelaire was not the first French translator of Poe, but his "scrupulous translations" were considered among the best. These were published as "Histoires extraordinaires" ("Extraordinary stories") (1852), "Nouvelles histoires extraordinaires" ("New extraordinary stories") (1857), "Aventures d'Arthur Gordon Pym", "", and "Histoires grotesques et sérieuses" ("Grotesque and serious stories") (1865). Two essays on Poe are to be found in his "Oeuvres complètes" ("Complete works") (vols. v. and vi.).
Eugène Delacroix.
A strong supporter of the Romantic painter Delacroix, Baudelaire called him "a poet in painting". Baudelaire also absorbed much of Delacroix's aesthetic ideas as expressed in his journals. As Baudelaire elaborated in his "Salon of 1846", "As one contemplates his series of pictures, one seems to be attending the celebration of some grievous mystery... This grave and lofty melancholy shines with a dull light ... plaintive and profound like a melody by Weber." Delacroix, though appreciative, kept his distance from Baudelaire, particularly after the scandal of "Les Fleurs du mal". In private correspondence, Delacroix stated that Baudelaire "really gets on my nerves" and he expressed his unhappiness with Baudelaire's persistent comments about "melancholy" and "feverishness".
Richard Wagner.
Baudelaire had no formal musical training, and knew little of composers beyond Beethoven and Carl Maria von Weber. Weber was in some ways Wagner's precursor, using the leitmotif and conceiving the idea of the "total art work" ("Gesamtkunstwerk"), both of which gained Baudelaire's admiration. Before even hearing Wagner's music, Baudelaire studied reviews and essays about him, and formulated his impressions. Later, Baudelaire put them into his non-technical analysis of Wagner, which was highly regarded, particularly his essay "Richard Wagner et Tannhäuser à Paris". Baudelaire's reaction to music was passionate and psychological. "Music engulfs (possesses) me like the sea." After attending three Wagner concerts in Paris in 1860, Baudelaire wrote to the composer: "I had a feeling of pride and joy in understanding, in being possessed, in being overwhelmed, a truly sensual pleasure like that of rising in the air." Baudelaire's writings contributed to the elevation of Wagner and to the cult of Wagnerism that swept Europe in the following decades.
Théophile Gautier.
Gautier, writer and poet, earned Baudelaire's respect for his perfection of form and his mastery of language, though Baudelaire thought he lacked deeper emotion and spirituality. Both strove to express the artist's inner vision, which Heinrich Heine had earlier stated: "In artistic matters, I am a supernaturalist. I believe that the artist can not find all his forms in nature, but that the most remarkable are revealed to him in his soul." Gautier's frequent meditations on death and the horror of life are themes which influenced Baudelaire writings. In gratitude for their friendship and commonality of vision, Baudelaire dedicated "Les Fleurs du mal" to Gautier.
Édouard Manet.
Manet and Baudelaire became constant companions from around 1855. In the early 1860s, Baudelaire accompanied Manet on daily sketching trips and often met him socially. Manet also lent Baudelaire money and looked after his affairs, particularly when Baudelaire went to Belgium. Baudelaire encouraged Manet to strike out on his own path and not succumb to criticism. "Manet has great talent, a talent which will stand the test of time. But he has a weak character. He seems to me crushed and stunned by shock." In his painting "Music in the Tuileries", Manet includes portraits of his friends Théophile Gautier, Jacques Offenbach, and Baudelaire. While it's difficult to differentiate who influenced whom, both Manet and Baudelaire discussed and expressed some common themes through their respective arts. Baudelaire praised the modernity of Manet's subject matter: "almost all our originality comes from the stamp that 'time' imprints upon our feelings." When Manet's famous "Olympia" (1863), a portrait of a nude prostitute, provoked a scandal for its blatant realism mixed with an imitation of Renaissance motifs, Baudelaire worked privately to support his friend, though he offered no public defense (he was, however, ill at the time). When Baudelaire returned from Belgium after his stroke, Manet and his wife were frequent visitors at the nursing home and she would play passages from Wagner for Baudelaire on the piano.
Nadar.
Nadar (Félix Tournachon) was a noted caricaturist, scientist and important early photographer. Baudelaire admired Nadar, one of his closest friends, and wrote: "Nadar is the most amazing manifestation of vitality." They moved in similar circles and Baudelaire made many social connections through him. Nadar's ex-mistress Jeanne Duval became Baudelaire's mistress around 1842. Baudelaire became interested in photography in the 1850s and, denouncing it as an art form, advocated its return to "its real purpose, which is that of being the servant to the sciences and arts". Photography should not, according to Baudelaire, encroach upon "the domain of the impalpable and the imaginary". Nadar remained a stalwart friend right to Baudelaire's last days and wrote his obituary notice in "Le Figaro".
Philosophy.
Many of Baudelaire's philosophical proclamations were considered scandalous and intentionally provocative in his time. He wrote on a wide range of subjects, drawing criticism and outrage from many quarters.
Love.
"There is an invincible taste for prostitution in the heart of man, from which comes his horror of solitude. He wants to be 'two'. The man of genius wants to be 'one'... It is this horror of solitude, the need to lose oneself in the external flesh, that man nobly calls 'the need to love'."
Marriage.
"Unable to suppress love, the Church wanted at least to disinfect it, and it created marriage."
The artist.
"The more a man cultivates the arts, the less randy he becomes... Only the brute is good at coupling, and copulation is the lyricism of the masses. To copulate is to enter into another—and the artist never emerges from himself."
Pleasure.
"Personally, I think that the unique and supreme delight lies in the certainty of doing 'evil'–and men and women know from birth that all pleasure lies in evil."
"But what can eternity of damnation matter to someone who has felt, if only for a second, the infinity of delight??"
Politics.
Along with Poe, Baudelaire named the arch-reactionary Joseph de Maistre as his maître à penser and adopted increasingly aristocratic views. In his journals, he wrote "There is no form of rational and assured government save an aristocracy. A monarchy or a republic, based upon democracy, are equally absurd and feeble. The immense nausea of advertisements. There are but three beings worthy of respect: the priest, the warrior and the poet. To know, to kill and to create. The rest of mankind may be taxed and drudged, they are born for the stable, that is to say, to practise what they call professions."
The public.
"In this regards, my friend, you're like the public, to whom one should never offer a delicate perfume. It exasperates them. Give them only carefully selected garbage." 
Time.
"The will to work must dominate, for art is long and time is brief." (quoting Hippocrates)
"Each man bears within himself his own dose of natural opium, incessantly secreted and renewed, and, from birth to death, how many hours can we count filled with pleasure,with prosperous and effective action?" 
Influence.
Baudelaire's influence on the direction of modern French (and English) language literature was considerable. The most significant French writers to come after him were generous with tributes; four years after his death, Arthur Rimbaud praised him in a letter as 'the king of poets, a true God'. In 1895, Stéphane Mallarmé published a sonnet in Baudelaire's memory, 'Le Tombeau de Charles Baudelaire'. Marcel Proust, in an essay published in 1922, stated that along with Alfred de Vigny, Baudelaire was 'the greatest poet of the nineteenth century'.
In the English-speaking world, Edmund Wilson credited Baudelaire as providing an initial impetus for the Symbolist movement, by virtue of his translations of Poe. In 1930, T. S. Eliot, while asserting that Baudelaire had not yet received a "just appreciation" even in France, claimed that the poet had "great genius" and asserted that his "technical mastery which can hardly be overpraised ... has made his verse an inexhaustible study for later poets, not only in his own language". Eliot also alluded to Baudelaire's poetry directly in his own poetry. For example, he quoted the last line of Baudelaire's 'Au Lecteur' in the last line of Section I of 'The Waste Land.'
At the same time that Eliot was affirming Baudelaire's importance from a broadly conservative and explicitly Christian viewpoint, left-wing critics such as Wilson and Walter Benjamin were able to do so from a dramatically different perspective. Benjamin translated Baudelaire's "Tableaux Parisiens" into German and published a major essay on translation as the foreword.
In the late 1930s, Benjamin used Baudelaire as a starting point and focus for his monumental attempt at a materialist assessment of 19th-century culture, "Das Passagenwerk." For Benjamin, Baudelaire's importance lay in his anatomies of the crowd, of the city and of modernity. François Porche published a poetry collection called "Charles Baudelaire: Poetry Collection" in memory of Baudelaire.
In 1982, avant-garde performance artist and vocalist Diamanda Galás recorded an adaptation of his poem "The Litanies of Satan" ("Les Litanies de Satan").
The song "How Beautiful You Are" by The Cure from their 1987 album "Kiss Me, Kiss Me, Kiss Me" was inspired by and based on Baudelaire's poem "The Eyes of the Poor".
The 1998 Spanglish classic novel "Yo-Yo Boing!" by Giannina Braschi features a debate between artists and writers on the greatness of Baudlelaire versus Arthur Rimbaud and Antonin Artaud.
in 2002, alt-rock band ...And You Will Know Us by the Trail of Dead included a song titled "Baudelaire" on their album "Source Tags and Codes".
In 2008, the Italian band Baustelle dedicates to him the song "Baudelaire" on its album "Amen".
The 2011 Latin American postcolonial novel "United States of Banana" by Giannina Braschi features cameo appearances by Baudelaire, along with fellow poets Antonin Artaud, Arthur Rimbaud, César Vallejo and Rubén Darío.
The Baudelaires, protagonists of Lemony Snicket's "A Series of Unfortunate Events", were named after him.
Vanderbilt University has "assembled one of the world's most comprehensive research collections on ... Baudelaire".
The Japanese comic or "manga" "Aku no Hana", by Shūzō Oshimi, is inspired by Baudelaire's "Les Fleurs du mal". The anime was aired in 2013 and drew attention due to its heavy use of rotoscope animation. The protagonist in both manga and the anime, Takao Kasuga, is a bookworm whose favorite book is "Les fleurs du mal," translated in Japanese as "Aku no Hana".
"Les Fleurs du mal" has a number of scholarly references.

</doc>
<doc id="5808" url="https://en.wikipedia.org/wiki?curid=5808" title="Casey at the Bat">
Casey at the Bat

"Casey at the Bat: A Ballad of the Republic Sung in the Year 1888" is a baseball poem written in 1888 by Ernest Thayer. First published in "The San Francisco Examiner" (then called the "The Daily Examiner") on June 3, 1888, it was later popularized by DeWolf Hopper in many vaudeville performances. It has become one of the best-known poems in American literature. The poem was originally published anonymously (under the pen name "Phin", based on Thayer's college nickname, "Phinney").
Synopsis.
A baseball team from the fictional town of "Mudville" (implied to be the home team) is losing by two runs in its last inning. Both the team and its fans (a crowd of 5,000, according to the poem) believe they can win if Mighty Casey, Mudville's star player, gets up to bat. However, Casey is scheduled to be the fifth batter of the inning, and the first two batters (Cooney and Barrows) fail to get on base. The next two batters (Flynn and Jimmy Blake) are perceived to be weak hitters with little chance of reaching base to allow Casey a chance to bat.
Surprisingly, Flynn hits a single, and Blake follows with a double that allows Flynn to reach third base. Both runners are now in scoring position and Casey represents the potential winning run. Casey is so sure of his abilities that he does not swing at the first two pitches, both called strikes. On the last pitch, the overconfident Casey strikes out swinging, ending the game and sending the crowd home unhappy.
Text.
The text is filled with references to baseball as it was in 1888, which in many ways is not far removed from today's version. As a work, the poem encapsulates much of the appeal of baseball, including the involvement of the crowd. It also has a fair amount of baseball jargon that can pose challenges for the uninitiated.
This is the complete poem as it originally appeared in "The San Francisco Examiner". After publication, various versions with minor changes were produced.
Inspiration.
Thayer said he chose the name "Casey" after a non-player of Irish ancestry he once knew, and it is open to debate who, if anyone, he modeled the character after.
One candidate is National League star Mike "King" Kelly, who became famous when Boston paid Chicago a record $10,000 for him. He had a personality that fans liked to cheer or jeer. After the 1887 season, Kelly went on a playing tour to San Francisco. Thayer, who wrote "Casey" in 1888, covered the San Francisco leg for the "San Francisco Examiner".
Thayer, in a letter he wrote in 1905, singles out Kelly as showing "impudence" in claiming to have written the poem. The author of the 2004 definitive bio of Kelly – which included a close tracking of his vaudeville career—did not find Kelly claiming to have been the author.
Plagiarism.
A month after the poem was published, it was reprinted as "Kelly at the Bat" in the "New York Sporting Times".
Aside from leaving off the first five verses, the only changes from the original are substitutions of Kelly for Casey, and Boston for Mudville. King Kelly, then of the Boston Beaneaters, was one of baseball's two biggest stars at the time (along with Cap Anson).
In 1897, "Current Literature" noted the two versions and said, "The locality, as originally given, is Mudville, not Boston; the latter was substituted to give the poem local color."
Pre-publication version.
Sportswriter Leonard Koppett claimed in a 1979 article that the published poem omits 18 lines penned by Thayer which change the entire point of the poem. Koppett said the full version of the poem takes the pitch count on Casey to full as his uncle Arnold stirs up wagering action in the stands before a wink passes between them and Casey throws the game.
Live performances.
DeWolf Hopper gave the poem's first stage recitation on August 14, 1888, at New York's Wallack Theatre as part of the comic opera "Prinz Methusalem" in the presence of the Chicago and New York baseball teams, the White Stockings and the Giants; August 14, 1888 was also Thayer's 25th birthday. Hopper became known as an orator of the poem, and recited it more than 10,000 times (by his count—some tabulations are as much as four times higher) before his death.
On stage in the early 1890s, baseball star Kelly recited the original "Casey" a few dozen times and not the parody. For example, in a review of a variety show he was in, in 1893, the Indianapolis News said, "Many who attended the performance had heard of Kelly's singing and his reciting, and many had heard De Wolf Hopper recite 'Casey at the Bat' in his inimitable way. Kelly recited this in a sing-song, school-boy fashion." Upon Kelly's death, a writer would say he gained "considerable notoriety by his ludicrous rendition of 'Casey at the Bat,' with which he concluded his `turn’ c at each performance."
During the 1980s, the magic/comedy team Penn & Teller performed a version of "Casey at the Bat" with Teller (the "silent" partner) struggling to escape a straitjacket while suspended upside-down over a platform of sharp steel spikes. The set-up was that Penn Jillette would leap off his chair upon finishing the poem, releasing the rope which supported Teller, and send his partner to a gruesome death if he wasn't free by that time. The drama of the performance was taken up a notch after the third or fourth stanza, when Penn Jillette began to read out the rest of the poem much faster than the opening stanzas, greatly reducing the time that Teller had left to work free from his bonds.
On July 4, 2008, Jack Williams recited the poem accompanied by the Boston Pops during the annual Boston Pops Fireworks Spectacular at Boston's 4 July Celebration.
On July 14, 2013, the jam rock band Furthur performed the poem as part of a second-set medley in center field of Doubleday Field in Cooperstown, New York.
Recordings.
The first recorded version of "Casey at the Bat" was made by Russell Hunting, speaking in a broad Irish accent, in 1893; an 1898 cylinder recording of the text made for the Columbia Graphophone label by Hunting can be accessed from the Cylinder Preservation and Digitization Project at the University of California, Santa Barbara Library.
DeWolf Hopper's more famous recorded recitation was released in October 1906.
In 1946, Walt Disney released a recording of the narration of the poem by Jerry Colonna, which accompanied the studio's animated cartoon adaptation of the poem (see below).
In 1973, the Cincinnati Symphony Orchestra commissioned its former Composer-in-Residence, Frank Proto, to create a work to feature Baseball Hall-of-Famer Johnny Bench with the orchestra. The result "Casey At The Bat – an American Folk Tale for Narrator and Orchestra" was an immediate hit and recorded by Bench and the orchestra. It has since been performed over 800 times by nearly every Major and Metropolitan orchestra in the U.S. and Canada.
In 1980, baseball pitcher Tug McGraw recorded Casey at The Bat by Frank Proto with Peter Nero and the Philly Pops.
In 1996, film star James Earl Jones recorded the poem with Arranger/Composer Steven Reineke and the Cincinnati Pops Orchestra.
In 2013, Dave Jageler and Charlie Slowes, both radio announcers for the Washington Nationals, each made recordings of the poem for the Library of Congress to mark the 125th anniversary of its first publication.
Mudville.
A rivalry of sorts has developed between two cities claiming to be the Mudville described in the poem.
Residents of Holliston, Massachusetts, where there is a neighborhood called Mudville, claim it as the Mudville described in the poem. Thayer grew up in nearby Worcester, Massachusetts, where he wrote the poem in 1888; his family owned a woolen mill less than a mile from Mudville's baseball field.
However, residents of Stockton, California — which was known for a time as Mudville prior to incorporation in 1850—also lay claim to being the inspiration for the poem. In 1887, Thayer covered baseball for "The San Francisco Examiner"—owned by his Harvard classmate William Randolph Hearst—and is said to have covered the local California League team, the Stockton Ports. For the 1902 season, after the poem became popular, Stockton's team was renamed the Mudville Nine. The team reverted to the Mudville Nine moniker for the 2000 and 2001 seasons. The Visalia Rawhide, another California League team, currently keep Mudville alive by playing in Mudville jerseys on June 3 each year.
Despite the towns' rival claims, Thayer himself told the "Syracuse Post-Standard" that "the poem has no basis in fact."
Adaptations.
The poem has been adapted to diverse types of media:
Derivations.
For a relatively short poem apparently dashed off quickly (and denied by its author for years), "Casey at the Bat" had a profound effect on American popular culture. It has been recited, re-enacted, adapted, dissected, parodied and subjected to just about every other treatment one could imagine.
Sequels.
"Casey's Revenge", by Grantland Rice (1907), gives Casey another chance against the pitcher who had struck him out in the original story. It was written in 1906, and its first known publication was in the quarterly magazine "The Speaker" in June 1907, under the pseudonym of James Wilson. In this version, Rice cites the nickname "Strike-Out Casey", hence the influence on Casey Stengel's name. Casey's team is down "three" runs by the last of the ninth, and once again Casey is down to two strikes—with the bases full this time. However, he connects, hits the ball so far that it is never found.
Here is the original version of the poem:
In response to the popularity of the 1946 Walt Disney animated adaptation, Disney made a sequel, "Casey Bats Again" (1954), in which Casey's nine daughters redeem his reputation.
In 1988, on the 100th anniversary of the poem, "Sports Illustrated" writer Frank Deford constructed a fanciful story (later expanded to book form) which posited Katie Casey, the subject of the song "Take Me Out to the Ball Game", as being the daughter of the famous slugger from the poem.
In 2010, Ken Eagle wrote "The Mudville Faithful", covering a century of the Mudville nine's ups and downs since Casey struck out. Faithful fans still root for the perpetually losing team, and are finally rewarded by a trip to the World Series, led by Casey's great-grandson who is also named Casey.
Parodies.
Of the many parodies made of the poem, some of the notable ones include:
Translations.
There are three known translations of the poem into a foreign language, one in French, written in 2007 by French Canadian linguist Paul Laurendeau, with the title "Casey au bâton", and two in Hebrew. One by the sports journalist Menachem Less titled "התור של קייסי לחבוט" ator Shel Casey Lachbo, and the other more recent and more true to the original cadence and style by Jason H. Elbaum called קֵיסִי בַּמַּחְבֵּט asey BaMachbay.
Names.
Casey Stengel describes in his autobiography how his nickname of "K.C." (for his hometown, Kansas City, Missouri) evolved into "Casey". It was influenced not just by the name of the poem, which was widely popular in the 1910s, but also because he tended to strike out frequently in his early career so fans and writers started calling him "strikeout Casey".
Contemporary culture.
Games.
The poem is referenced in the Super Nintendo Entertainment System game "EarthBound", where a weapon is named the Casey Bat, which is the strongest weapon in the game, but will only hit 25% of the time.
Television.
A one-time character in the "Pokémon" anime, a girl who is a very enthusiastic fan of baseball, is named "Casey" in the English version in reference to the poem.
Postage stamp.
On July 11, 1996, the United States Postal Service issued a commemorative stamp depicting "Mighty Casey." The stamp was part of a set commemorating American folk heroes. Other stamps in the set depicted Paul Bunyan, John Henry, and Pecos Bill.

</doc>
<doc id="5810" url="https://en.wikipedia.org/wiki?curid=5810" title="Classical guitar">
Classical guitar

The classical guitar (also called the "Spanish guitar") is the member of the guitar family used in classical music. It is an acoustical wooden guitar with strings made of nylon as opposed to the metal strings used in acoustic and electric guitars. The traditional classical guitar has 12 frets clear of the body and is held on the left leg so that the hand falls at the back of the soundhole. The steel string guitar, on the other hand, has fourteen frets clear of the body and is played off the hip.
In addition to the instrument, the phrase "classical guitar" can refer to two other concepts:
The shape, construction, and material of classical guitars vary, but typically they have a "modern classical guitar" shape, or "historic classical guitar" shape resembling early romantic guitars from France and Italy. Classical guitar strings were once made of catgut and nowadays are made of polymers such as nylon, with a fine silver wire wrap on the bass strings.
A guitar family tree can be identified. The flamenco guitar derives from the modern classical, but has differences in material, construction and sound.
The term "modern classical guitar" is sometimes used to distinguish the classical guitar from older forms of guitar, which are in their broadest sense also called "classical", or more specifically: "early guitars". Examples of early guitars include the 6-string early romantic guitar (c. 1790–1880), and the earlier baroque guitars with 5 courses.
Today's "modern classical guitar" was established by the late designs of the 19th-century Spanish luthier Antonio Torres Jurado.
Contexts.
The classical guitar has a long history and one is able to distinguish various:
Both instrument and repertoire can be viewed from a combination of various perspectives:
Historical (chronological period of time)
Geographical
Cultural
Historical perspective.
Early guitars.
While "classical guitar" is today mainly associated with the modern classical guitar design, there is an increasing interest in early guitars; and understanding the link between historical repertoire and the particular period guitar that was originally used to perform this repertoire. The musicologist and author Graham Wade writes:
"Nowadays it is customary to play this repertoire on reproductions of instruments authentically modelled on concepts of musicological research with appropriate adjustments to techniques and overall interpretation. Thus over recent decades we have become accustomed to specialist artists with expertise in the art of vihuela (a 16th-century type of guitar popular in Spain), lute, Baroque guitar, 19th-century guitar, etc."
Different types of guitars have different sound aesthetics, e.g. different colour-spectrum characteristics (the way the sound energy is spread in the fundamental frequency and the overtones), different response, etc. These differences are due to differences in construction, for example modern classical guitars usually use a different bracing (fan-bracing), than was used in earlier guitars (they had ladder-bracing); and a different voicing was used by the luthier.
It is interesting to note the historical parallel between musical styles (baroque, classical, romantic, flamenco, jazz) and the style of "sound aesthetic" of the musical instruments used, for example: Robert de Visée played on a baroque guitar with a very different sound aesthetic than the guitars used by Mauro Giuliani and Legnani - they used 19th century guitars. These guitars in turn sound different from the Torres models used by Segovia, that are suited for interpretations of romantic-modern works such as Moreno Torroba.
When considering the guitar from a historical perspective, the musical instrument used is just as important as the musical language and style of the particular period. As an example: It is impossible to play a historically informed de Visee or Corbetta (baroque guitarist-composers) on a modern classical guitar. The reason is that the baroque guitar used courses, which are two strings close together (in unison), that are plucked together. This gives baroque guitars an unmistakable sound characteristic and tonal texture that is an integral part of an interpretation. Additionally the sound aesthetic of the baroque guitar (with its strong overtone presence) is very different from modern classical type guitars, as is shown below.
Today's use of Torres and post-Torres type guitars for repertoire of all periods is sometimes critically viewed: Torres and post-Torres style modern guitars (with their fan-bracing and design) have a thick and strong tone, very suitable for modern-era repertoire. However, they are considered to emphasize the fundamental too heavily (at the expense of overtone partials) for earlier repertoire (Classical/Romantic: Carulli, Sor, Giuliani, Mertz, ...; Baroque: de Visee, ...; etc.). "Andrés Segovia presented the Spanish guitar as a versatile model for all playing styles", to the extent, that still today, "many guitarists have tunnel-vision of the world of the guitar, coming from the modern Segovia tradition".
While fan-braced modern classical Torres and post-Torres style instruments coexisted with traditional ladder-braced guitars at the beginning of the 20th century; the traditional forms eventually fell away. Some attribute this to the popularity of Segovia, considering him "the catalyst for change toward the Spanish design and the so-called 'modern' school in the 1920's and beyond". The styles of music performed on ladder-braced guitars were becoming more and more unfashionable; and, e.g. in Germany, musicians were in part turning towards folkstyle music (Schrammel-music and the Contraguitar), but this only remained localized in Germany and Austria and became unfashionable again. On the other hand, Segovia was playing in concerts around the world, popularizing his modern classical guitar, as well as a new style of music in the 1920s: Spanish romantic-modern style, with guitar works by Moreno Torroba, de Falla, etc. Some people consider it to have been this influence of Segovia, which eventually led to the domination of the Torres instrument - factories all over the world began producing them in large numbers.
It was the 19th century classical guitarist Francisco Tárrega who first popularized the Torres design as a classical solo instrument.
Style periods.
Renaissance.
Composers of the Renaissance period who wrote for four course guitar include Alonso Mudarra, Miguel de Fuenllana, Adrian Le Roy, Gregoire Brayssing, Guillaume de Morlaye, and Simon Golier.
Four-course guitar
Baroque.
Some well known composers of the baroque guitar were Gaspar Sanz, Robert de Visée and Francesco Corbetta.
Classical and Romantic.
From approximately 1780 to 1850, the guitar had numerous composers and performers including: 
Hector Berlioz studied the guitar as a teenager, Franz Schubert owned at least two and wrote for the instrument, Ludwig van Beethoven, after hearing Giuliani play, commented the instrument was "a miniature orchestra in itself". Niccolò Paganini was also a guitar virtuoso and composer. He once wrote: "I love the guitar for its harmony; it is my constant companion in all my travels". He also said, on another occasion: "I do not like this instrument, but regard it simply as a way of helping me to think" 
Francisco Tárrega.
The guitarist and composer Francisco Tárrega (b. Vilareal, Spain in November 29, 1852-d. December 15, 1909) was one of the great guitar virtuosos and teachers and is considered the father of modern classical guitar playing. As professor of guitar at the conservatories of Madrid and Barcelona, he defined many elements of the modern classical technique and elevated the importance of the guitar in the classical music tradition.
Modern period.
At the beginning of the 1920s, Andrés Segovia popularized the guitar with tours and early phonograph recordings. Segovia collaborated with the composers Federico Moreno Torroba and Joaquin Turina with the aim of extending the guitar repertoire with new music. Segovia's tour of South America revitalized public interest in the guitar and helped the guitar music of Manuel Ponce and Heitor Villa-Lobos reach a wider audience. The composers Alexandre Tansman and Mario Castelnuovo-Tedesco were commissioned by Segovia to write new pieces for the guitar. Luiz Bonfá popularized Brazilian musical styles such as the newly created Bossa Nova, which was well received by audiences in the USA.
"New music" - avant-garde.
The classical guitar repertoire also includes modern contemporary works – sometimes termed "New Music" – such as Elliott Carter's "Changes", Cristóbal Halffter's "Codex I", Luciano Berio's "Sequenza XI", Maurizio Pisati's "Sette Studi", Maurice Ohana's "Si Le Jour Paraît", Sylvano Bussotti's "Rara (eco sierologico)", Ernst Krenek's "Suite für Guitarre allein, Op. 164", Franco Donatoni's "Algo: Due pezzi per chitarra", etc.
Performers who are known for including modern repertoire include Jürgen Ruck, Elena Càsoli, Leo Brouwer (when he was still performing), John Schneider, Reinbert Evers, Maria Kämmerling, Siegfried Behrend, David Starobin, Mats Scheidegger, Magnus Andersson, etc.
This type of repertoire is usually performed by guitarists who have particularly chosen to focus on the avant-garde in their performances.
Within the contemporary music scene itself, there are also works which are generally regarded as extreme. These include works such as Brian Ferneyhough's "Kurze Schatten II", Sven-David Sandström's "away from" and Rolf Riehm's "Toccata Orpheus", etc. which are notorious for their extreme difficulty.
There are also a variety of databases documenting modern guitar works such as Sheer Pluck and others.
Background information.
The evolution of the classical guitar and its repertoire spans more than four centuries. It has a history that was shaped by contributions from earlier instruments, such as the lute, the vihuela, and the baroque guitar.
History.
Overview of the classical guitar's history.
The ancestries of the modern guitar, like numerous other chordophones, track back through many instruments and thousands of years to ancient central Asia. Guitar like instruments appear in ancient carvings and statues recovered from the old Persian capital of Susa. This means that the contemporary Iranian instruments such as the tanbur and setar are distantly related to the European guitar, as they all derive ultimately from the same ancient origins, but by very different historical routes and influences.
During the late Middle Ages, gitterns called "guitars" were in use, but their construction and tuning was different from modern guitars. The "Guitarra Latina" in Spain, had curved sides and a single hole. The "Guitarra Morisca", which appears to have had Moorish influences, had an oval soundbox and many sound holes on its soundboard. By the 15th century, a four course double-string instrument called the vihuela de mano, that had tuning like the later modern guitar except on one string and similar construction, first appeared in Spain and spread to France and Italy. In the 16th century, a fifth double-string was added. During this time, composers wrote mostly in tablature notation. In the middle of the 16th century, influences from the vihuela and the renaissance guitar were combined and the baroque five string guitar appeared in Spain. The baroque guitar quickly superseded the vihuela in popularity in Spain, France and Italy and Italian players and composers became prominent. In the late 18th century the six string guitar quickly became popular at the expense of the five string guitars. During the 19th century the Spanish luthier and player Antonio de Torres gave the modern classical guitar its definitive form, with a broadened body, increased waist curve, thinned belly, improved internal bracing. The modern classical guitar replaced older form for the accompaniment of song and dance called flamenco, and a modified version, known as the flamenco guitar, was created.
Renaissance guitar.
Alonso de Mudarra's book Tres Libros de Música, published in Spain in 1546, contains the earliest known written pieces for a four-course guitarra. This four-course "guitar" was popular in France, Spain and Italy. In France this instrument gained popularity among aristocrats and a considerable volume of music was published in Paris from the 1550s to the 1570s: Simon Gorlier's Le Troysième Livre . . . mis en tablature de Guiterne was published in 1551. In 1551 Adrian le Roy also published his Premier Livre de Tablature de Guiterne, and in the same year he also published Briefve et facile instruction pour apprendre la tablature a bien accorder, conduire, et disposer la main sur la Guiterne. Robert Ballard, Grégoire Brayssing from Augsburg, and Guillaume Morlaye (c. 1510 - c. 1558) significantly contributed to its repertoire. Morlaye's Le Premier Livre de Chansons, Gaillardes, Pavannes, Bransles, Almandes, Fantasies – which has a four-course instrument illustrated on its title page – was published in partnership with Michel Fedenzat, and amongst other music, they published six books of tablature by the lutenist Albert de Rippe (who was very likely Guillaume's teacher).
Vihuela.
The written history of the classical guitar can be traced back to the early 16th century with the development of the "vihuela" in Spain. While the lute was then becoming popular in other parts of Europe, the Spaniards did not take to it well because of its association with the Moors. Instead, the lute like vihuela appeared with two more strings that gave it more range and complexity. In its most developed form, the vihuela was a guitar-like instrument with six double strings made of gut, tuned like a modern classical guitar with the exception of the third string, which was tuned half a step lower. It has a high sound and is rather large to hold. Few have survived and most of what is known today comes from diagrams and paintings.
"Early romantic guitar" or "Guitar during the Classical music era".
The earliest extant six-string guitar is believed to have seen built in 1779 by Gaetano Vinaccia (1759 - after 1831) in Naples, Italy; however, the date on the label is a little ambiguous. The Vinaccia family of luthiers is known for developing the mandolin. This guitar has been examined and does not show tell-tale signs of modifications from a double-course guitar.
The authenticity of guitars allegedly produced before the 1790s is often in question. This also corresponds to when Moretti's 6-string method appeared, in 1792.
Contemporary classical guitar.
Contemporary concert guitars occasionally follow the Smallman design, which replaces fan braces with a much lighter balsa brace attached to the back of the sound board with carbon fiber. The balsa brace has a lattice pattern and allows the (now much thinner) sound board to support more vibrational modes. This leads to greater volume and longer sustain but compromises the subtle tonalities of the Spanish sound.
Performance.
The modern classical guitar is usually played in a seated position, with the instrument resting on the left lap - and the left foot placed on a footstool. Alternatively - if a footstool is not used - a "guitar support" can be placed between the guitar and the left lap (the support usually attaches to the instrument's side with suction cups). (There are of course exceptions, with some performers choosing to hold the instrument another way.)
Right-handed players use the fingers of the right hand to pluck the strings, with the thumb plucking from the top of a string downwards (downstroke) and the other fingers plucking from the bottom of string upwards (upstroke). The little finger in classical technique as it evolved in the 20th century is used only to ride along with the ring finger without striking the strings and to thus physiologically facilitate the ring finger's motion.
In contrast, Flamenco technique, and classical compositions evoking Flamenco, employ the little finger semi-independently in the Flamenco four-finger rasgueado, that rapid strumming of the string by the fingers in reverse order employing the back of the fingernail—a familiar characteristic of Flamenco.
Flamenco technique, in the performance of the rasgueado also uses the upstroke of the four fingers and the downstroke of the thumb: the string is hit not only with the inner, fleshy side of the fingertip but also with the outer, fingernail side. This was also used in a technique of the vihuela called dedillo which has recently begun to be introduced on the classical guitar.
Some modern guitarists, such as Štěpán Rak and Kazuhito Yamashita, use the little finger independently, compensating for the little finger's shortness by maintaining an extremely long fingernail.
Štěpán Rak and Kazuhito Yamashita have also generalized the use of the upstroke of the four fingers and the downstroke of the thumb (the same technique as in the rasgueado of the Flamenco: as explained above the string is hit not only with the inner, fleshy side of the fingertip but also with the outer, fingernail side) both as a free stroke and as a rest stroke.
Direct contact with strings.
As with other plucked instruments (such as the lute), the musician directly touches the strings (usually plucking) to produce the sound. This has important consequences: Different tone/timbre (of a single note) can be produced by plucking the string in different manners and in different positions.
Fingering notation.
In guitar "scores" the five fingers of the right-hand (which pluck the strings) are designated by the first letter of their Spanish names namely p = thumb ("pulgar"), i = index finger ("índice"), m = middle finger ("mayor"), a = ring finger ("anular"), c = little finger or pinky ("chiquito")
The four fingers of the left hand (which stop the strings) are designated 1 = index, 2 = major, 3 = ring finger, 4 = little finger; 0 designates an open string, that is a string that is not stopped by a finger of the left hand and whose full length thus vibrates when plucked. On the classical guitar thumb of the left hand is never used to stop strings from above (as is done on the electric guitar): the neck of a classical guitar is too wide and the normal position of the thumb used in classical guitar technique do not make that possible.
Scores (contrary to "tablatures") do not systematically indicate the string to be plucked (although in most cases the choice is obvious). When an indication of the string is required the strings are designated 1 to 6 (from the 1st the high E to the 6th the low E) with figures 1 to 6 inside circles.
The positions (that is where on the fretboard the first finger of the left hand is placed) are also not systematically indicated, but when they are (mostly in the case of the execution of "barrés") these are indicated with Roman numerals from the first position I (index finger of the left hand placed on the 1st fret: F-B flat-E flat-A flat-C-F) to the twelfth position XII (the index finger of the left hand placed on the 12th fret: E-A-D-G-B-E; the 12th fret is placed where the body begins) or even higher up to position XIX (the classical guitar most often having 19 frets, with the 19th fret being most often split and not being usable to fret the 3rd and 4th strings).
Alternation.
To achieve tremolo effects and rapid, fluent scale passages, the player must practice alternation, that is, never plucking a string with the same finger twice in a row. 
Using p to indicate the thumb, i the index finger, m the middle finger and a the ring finger, common alternation patterns include:
Repertoire.
Music written specifically for the classical guitar dates from the addition of the sixth string (the baroque guitar normally had five pairs of strings) in the late 18th century.
A guitar recital may include a variety of works, e.g. works written originally for the lute or vihuela by composers such as John Dowland (b. England 1563) and Luis de Narváez (b. Spain c. 1500), and also music written for the harpsichord by Domenico Scarlatti (b. Italy 1685), for the baroque lute by Sylvius Leopold Weiss (b. Germany 1687), for the baroque guitar by Robert de Visée (b. France c. 1650) or even Spanish-flavored music written for the piano by Isaac Albéniz (b. Spain 1860) and Enrique Granados (b. Spain 1867). The most important composer who did not write for the guitar but whose music is often played on it is Johann Sebastian Bach (b. Germany 1685), whose baroque lute works have proved highly adaptable to the instrument.
Of music written originally for guitar, the earliest important composers are from the classical period and include Fernando Sor (b. Spain 1778) and Mauro Giuliani (b. Italy 1781), both of whom wrote in a style strongly influenced by Viennese classicism. In the 19th century guitar composers such as Johann Kaspar Mertz (b. Slovakia, Austria 1806) were strongly influenced by the dominance of the piano. Not until the end of the nineteenth century did the guitar begin to establish its own unique identity. Francisco Tárrega (b. Spain 1852) was central to this, sometimes incorporating stylized aspects of flamenco's Moorish influences into his romantic miniatures. This was part of late 19th century mainstream European musical nationalism. Albéniz and Granados were central to this movement; their evocation of the guitar was so successful that their compositions have been absorbed into standard guitar repertoire.
The steel-string and electric guitars characteristic to the rise of rock and roll in the post-WWII era became more widely played in North America and the English speaking world. Barrios composed many works and brought into the mainstream the characteristics of Latin American music, as did the Brazilian composer Heitor Villa-Lobos. Andrés Segovia commissioned works from Spanish composers such as Federico Moreno Torroba and Joaquín Rodrigo, Italians such as Mario Castelnuovo-Tedesco and Latin American composers such as Manuel Ponce of Mexico. Other prominent Latin American composers are Leo Brouwer of Cuba, Antonio Lauro of Venezuela and Enrique Solares of Guatemala. Julian Bream of Britain managed to get nearly every British composer from William Walton to Benjamin Britten to Peter Maxwell Davies to write significant works for guitar. Bream's collaborations with tenor Peter Pears also resulted in song cycles by Britten, Lennox Berkeley and others. There are significant works by composers such as Hans Werner Henze of Germany, Gilbert Biberian of England and Roland Chadwick of Australia.
The classical guitar also became widely used in popular music and rock & roll in the 1960s after guitarist Mason Williams popularized the instrument in his instrumental hit Classical Gas. Guitarist Christopher Parkening is quoted in the book "Classical Gas: The Music of Mason Williams" as saying that it is the most requested guitar piece besides Malagueña and perhaps the best known instrumental guitar piece today.
In the field of New Flamenco, the works and performances of Spanish composer and player Paco de Lucía are known worldwide.
Not many classical guitar concertos were written through the guitar history. Nevertheless, some guitar concertos are nowadays wide known and popular, especially Joaquín Rodrigo's "Concierto de Aranjuez" (with the famous theme from 2nd movement) and "Fantasía para un gentilhombre". Composers, who also wrote famous guitar concertos are: Antonio Vivaldi (originally for mandolin or lute), Mauro Giuliani, Heitor Villa-Lobos, Mario Castelnuovo-Tedesco, Manuel Ponce, Leo Brouwer, Lennox Berkeley... 
Nowadays, more and more contemporary composers decide to write a guitar concerto.
Physical characteristics.
The classical guitar is distinguished by a number of characteristics:
Parts.
Fretboard.
The fretboard (also called the fingerboard) is a piece of wood embedded with metal frets that constitutes the top of the neck. It is flat or slightly curved. The curvature of the fretboard is measured by the fretboard radius, which is the radius of a hypothetical circle of which the fretboard's surface constitutes a segment. The smaller the fretboard radius, the more noticeably curved the fretboard is. Fretboards are most commonly made of ebony, but may also be made of rosewood or of phenolic composite ("micarta").
Frets.
Frets are the metal strips (usually nickel alloy or stainless steel) embedded along the fingerboard and placed at points that divide the length of string mathematically. The strings' vibrating length is determined when the strings are pressed down behind the frets. Each fret produces a different pitch and each pitch spaced a half-step apart on the 12 tone scale. The ratio of the widths of two consecutive frets is the twelfth root of two (formula_1), whose numeric value is about 1.059463. The twelfth fret divides the string in two exact halves and the 24th fret (if present) divides the string in half yet again. Every twelve frets represents one octave. This arrangement of frets results in equal tempered tuning.
Neck.
A classical guitar's frets, fretboard, tuners, headstock, all attached to a long wooden extension, collectively constitute its neck. The wood for the fretboard usually differs from the wood in the rest of the neck. The bending stress on the neck is considerable, particularly when heavier gauge strings are used.
Neck joint or 'heel'.
This is the point where the neck meets the body. In the traditional Spanish neck joint the neck and block are one piece with the sides inserted into slots cut in the block. Other necks are built separately and joined to the body either with a dovetail joint, mortise or flush joint. These joints are usually glued and can be reinforced with mechanical fasteners. Recently many manufacturers use bolt on fasteners. Bolt on neck joints were once associated only with less expensive instruments but now some top manufacturers and hand builders are using variations of this method. Some people believed that the Spanish style one piece neck/block and glued dovetail necks have better sustain, but testing has failed to confirm this.
While most traditional Spanish style builders use the one piece neck/heel block, Fleta, a prominent Spanish builder, used a dovetail joint due to the influence of his early training in violin making.
One reason for the introduction of the mechanical joints was to make it easier to repair necks. This is more of a problem with steel string guitars than with nylon strings, which have about half the string tension. This is why nylon string guitars often don't include a truss rod either.
Body.
The body of the instrument is a major determinant of the overall sound variety for acoustic guitars. The guitar top, or soundboard, is a finely crafted and engineered element often made of spruce, red cedar, redwood or mahogany. This thin (often 2 or 3 mm thick) piece of wood, strengthened by different types of internal bracing, is considered the most prominent factor in determining the sound quality of a guitar. The majority of the sound is caused by vibration of the guitar top as the energy of the vibrating strings is transferred to it. Different patterns of wood bracing have been used through the years by luthiers (Torres, Hauser, Ramírez, Fleta, and C.F. Martin being among the most influential designers of their times); to not only strengthen the top against collapsing under the tremendous stress exerted by the tensioned strings, but also to affect the resonation of the top. Some contemporary guitar makers have introduced new construction concepts such as "double-top" consisting of two extra-thin wooden plates separated by Nomex, or carbon-fiber reinforced lattice - pattern bracing. The back and sides are made out of a variety of woods such as mahogany, Indian rosewood and highly regarded Brazilian rosewood ("Dalbergia nigra"). Each one is chosen for its aesthetic effect and structural strength, and such choice can also play a significant role in determining the instrument's timbre. These are also strengthened with internal bracing, and decorated with inlays and purfling.
The body of a classical guitar is a resonating chamber that projects the vibrations of the body through a "sound hole", allowing the acoustic guitar to be heard without amplification. The sound hole is normally a single round hole in the top of the guitar (under the strings), though some have different placement, shapes, or numbers of holes. How much air an instrument can move determines its maximum volume.
Binding, purfling and kerfing.
The top, back and sides of a classical guitar body are very thin, so a flexible piece of wood called "kerfing" (because it is often scored, or "kerfed" so it bends with the shape of the rim) is glued into the corners where the rim meets the top and back. This interior reinforcement provides 5 to 20 mm of solid gluing area for these corner joints.
During final construction, a small section of the outside corners is carved or routed out and filled with binding material on the outside corners and decorative strips of material next to the binding, which are called "purfling". This binding serves to seal off the endgrain of the top and back. Binding and purfling materials are generally made of either wood or high quality plastic materials.
Bridge.
The main purpose of the bridge on a classical guitar is to transfer the vibration from the strings to the soundboard, which vibrates the air inside of the guitar, thereby amplifying the sound produced by the strings. The bridge holds the strings in place on the body. Also, the position of the saddle, usually a strip of bone or plastic that supports the strings off the bridge, determines the distance to the nut (at the top of the fingerboard).
Sizes.
The modern full size classical guitar has a scale length of around 650 mm (25.6 inches), with an overall instrument length of 965–1016 mm (38-40 inches). The scale length has remained quite consistent since it was chosen by the originator of the instrument, Antonio de Torres. This length may have been chosen because it's twice the length of a violin string. As the guitar is tuned to one octave below that of the violin, the same size gut could be used for the first strings of both instruments.
Smaller-scale instruments are produced to assist children in learning the instrument as the smaller scale leads to the frets being closer together, making it easier for smaller hands. The scale-size for the smaller guitars is usually in the range 484–578 mm (19-22.5 inches), with an instrument length of 785–915 mm (31-36 inches). Full-size instruments are sometimes referred to as 4/4, while the smaller sizes are 3/4, 1/2 or 1/4.
These sizes are not absolute, as luthiers may choose variations around these nominal scale-lengths;
Guitars can be described in size from largest to smallest as:
- Contra or Octave bass;
- Bass baritone or Quint bass;
- Prime or Quart bass;
- Terz treble;
- Alto Requinto;
- Quart;
- Quint;
- Soprano, Octave or Piccolo.
Tuning.
A variety of different tunings are used. The most common by far, which one could call the "standard tuning" is:
The above order, is the tuning from the "1st string" (highest-pitched string e'—spatially the bottom string in playing position) to the "6th string" - lowest-pitched string E—spatially the upper string in playing position, and hence comfortable to pluck with the thumb.
The explanation for this "asymmetrical" tuning (in the sense that the maj 3rd is not between the two middle strings as say in the tuning of the viola da gamba) is probably that the guitar originated as a 4-string instrument (actually an instrument with 4 double courses of strings, see above) with a maj 3rd between the 2nd and 3rd strings and that it only became a 6-string instrument by gradual addition of a 5th string and then a 6th string tuned a 4th apart:
""The Evolution of tuning The development of the modern tuning can be traced in stages. One of the tunings from the 16th century is C-F-A-D. This is equivalent to the top four strings of the modern guitar tuned a tone lower. However, the absolute pitch for these notes is not equivalent to modern "concert pitch". The tuning of the four-course guitar was moved up by a tone and toward the end of the 16th century, five-course instruments were in use with an added lower string tuned to A. This produced A-D-G-B-E, one of a wide number of variant tunings of the period. The low E string was added during the 18th century.""
This tuning is such that neighboring strings are at most 5 semitones apart.
There are also a variety of commonly used alternate tunings.

</doc>
<doc id="5813" url="https://en.wikipedia.org/wiki?curid=5813" title="C. S. Lewis">
C. S. Lewis

Clive Staples Lewis (29 November 1898 – 22 November 1963) was a British novelist, poet, academic, medievalist, literary critic, essayist, lay theologian, broadcaster, lecturer, and Christian apologist. He held academic positions at both Oxford University (Magdalen College), 1925–54, and Cambridge University (Magdalene College), 1954–63. He is best known for his fictional work, especially "The Screwtape Letters", "The Chronicles of Narnia", and "The Space Trilogy", and for his non-fiction Christian apologetics, such as "Mere Christianity", "Miracles", and "The Problem of Pain".
Lewis and fellow novelist J. R. R. Tolkien were close friends. They both served on the English faculty at Oxford University, and were active in the informal Oxford literary group known as the Inklings. According to Lewis' memoir "Surprised by Joy", he was baptised in the Church of Ireland, but fell away from his faith during adolescence. Lewis returned to the Anglican Communion at the age of 32, owing to the influence of Tolkien and other friends, and he became an "ordinary layman of the Church of England". His faith profoundly affected his work, and his wartime radio broadcasts on the subject of Christianity brought him wide acclaim.
In 1956, he married American writer Joy Davidman; she died of cancer four years later at the age of 45. Lewis died on 22 November 1963 from renal failure, one week before his 65th birthday. Media coverage of his death was minimal, as he and fellow British author Aldous Huxley died on the same day that US President John F. Kennedy was assassinated. In 2013, on the 50th anniversary of his death, Lewis was honoured with a memorial in Poets' Corner in Westminster Abbey.
Lewis's works have been translated into more than 30 languages and have sold millions of copies. The books that make up "The Chronicles of Narnia" have sold the most and have been popularised on stage, TV, radio, and cinema. His works entered the public domain in 2014 in countries where copyright expires 50 years after the death of the creator, such as Canada.
Biography.
Childhood.
Clive Staples Lewis was born in Belfast, Ireland, on 29 November 1898. His father was Albert James Lewis (1863–1929), a solicitor whose father Richard had come to Ireland from Wales during the mid-19th century. His mother was Florence Augusta Lewis, Hamilton (1862–1908), known as Flora, the daughter of a Church of Ireland (Anglican) priest. He had an elder brother, Warren Hamilton Lewis. When he was four, his dog Jacksie was killed by a car, and he announced that his name was now Jacksie. At first, he would answer to no other name, but later accepted Jack, the name by which he was known to friends and family for the rest of his life. When he was seven, his family moved into "Little Lea", the family home of his childhood, in the Strandtown area of East Belfast.
As a boy, Lewis was fascinated with anthropomorphic animals; he fell in love with Beatrix Potter's stories and often wrote and illustrated his own animal stories. He and his brother Warnie created the world of Boxen, inhabited and run by animals. Lewis loved to read. His father's house was filled with books, and he felt that finding a book to read was as easy as walking into a field and "finding a new blade of grass".
Lewis was schooled by private tutors before being sent to the Wynyard School in Watford, Hertfordshire, in 1908, just after his mother's death from cancer. Lewis's brother had enrolled there three years previously. The school was closed not long afterwards due to a lack of pupils; the headmaster Robert "Oldie" Capron was soon after committed to a psychiatric hospital. Lewis then attended Campbell College in the east of Belfast about a mile from his home, but he left after a few months due to respiratory problems. He was then sent to the health-resort town of Malvern, Worcestershire, where he attended the preparatory school Cherbourg House, which Lewis calls "Chartres" in his autobiography. It was during this time that Lewis abandoned his childhood Christian faith and became an atheist, becoming interested in mythology and the occult. In September 1913, Lewis enrolled at Malvern College, where he remained until the following June. He found the school socially competitive. After leaving Malvern, he studied privately with William T. Kirkpatrick, his father's old tutor and former headmaster of Lurgan College.
As a teenager, Lewis was wonder-struck by the songs and legends of what he called "Northernness", the ancient literature of Scandinavia preserved in the Icelandic sagas. These legends intensified an inner longing he later called "joy". He also grew to love nature; its beauty reminded him of the stories of the North, and the stories of the North reminded him of the beauties of nature. His teenage writings moved away from the tales of Boxen, and he began using different art forms, including epic poetry and opera, to try to capture his new-found interest in Norse mythology and the natural world. Studying with Kirkpatrick ("The Great Knock", as Lewis afterwards called him) instilled in him a love of Greek literature and mythology and sharpened his debate and reasoning skills. In 1916, Lewis was awarded a scholarship at University College, Oxford. Within months of entering Oxford, the British Army shipped him to France to fight in the First World War. His experience of the horror of war confirmed his atheism.
"My Irish life".
Lewis experienced a certain cultural shock on first arriving in England: "No Englishman will be able to understand my first impressions of England", Lewis wrote in "Surprised by Joy". "The strange English accents with which I was surrounded seemed like the voices of demons. But what was worst was the English landscape ... I have made up the quarrel since; but at that moment I conceived a hatred for England which took many years to heal."
From boyhood, Lewis immersed himself in Norse and Greek mythology, and later in Irish mythology and literature. He also expressed an interest in the Irish language, though there is not much evidence that he laboured to learn it. He developed a particular fondness for , in part because of Yeats's use of Ireland's Celtic heritage in poetry. In a letter to a friend, Lewis wrote, "I have here discovered an author exactly after my own heart, whom I am sure you would delight in, W. B. Yeats. He writes plays and poems of rare spirit and beauty about our old Irish mythology".
In 1921, Lewis met Yeats twice, since Yeats had moved to Oxford. Lewis was surprised to find his English peers indifferent to Yeats and the Celtic Revival movement, and wrote: "I am often surprised to find how utterly ignored Yeats is among the men I have met: perhaps his appeal is purely Irish – if so, then thank the gods that I am Irish." Early in his career, Lewis considered sending his work to the major Dublin publishers, writing: "If I do ever send my stuff to a publisher, I think I shall try Maunsel, those Dublin people, and so tack myself definitely onto the Irish school." After his conversion to Christianity, his interests gravitated towards Christian theology and away from pagan Celtic mysticism.
Lewis occasionally expressed a somewhat tongue-in-cheek chauvinism toward the English. Describing an encounter with a fellow Irishman, he wrote: "Like all Irish people who meet in England, we ended by criticisms on the invincible flippancy and dullness of the Anglo-Saxon race. After all, there is no doubt, "ami", that the Irish are the only people: with all their faults, I would not gladly live or die among another folk". Throughout his life, he sought out the company of other Irish people living in England and visited Northern Ireland regularly, even spending his honeymoon there in 1958 at the Old Inn, Crawfordsburn. He called this "my Irish life".
Various critics have suggested that it was Lewis's dismay over sectarian conflict in his native Belfast which led him to eventually adopt such an ecumenical brand of Christianity. As one critic has said, Lewis "repeatedly extolled the virtues of all branches of the Christian faith, emphasising a need for unity among Christians around what the Catholic writer called 'Mere Christianity', the core doctrinal beliefs that all denominations share." On the other hand, Paul Stevens of the University of Toronto has written that "Lewis's mere Christianity masked many of the political prejudices of an old-fashioned Ulster Protestant, a native of middle-class Belfast for whom British withdrawal from Northern Ireland even in the 1950s and 1960s was unthinkable".
First World War and Oxford University.
Soon after Lewis entered Oxford in the 1917 summer term, he joined the Officers' Training Corps at the university as his "most promising route into the army". From there, he was drafted into a Cadet Battalion for training. After his training, he was commissioned into the Third Battalion of the Somerset Light Infantry of the British Army as a Second Lieutenant. On his nineteenth birthday he arrived at the front line in the Somme Valley in France, where he experienced trench warfare. On 15 April 1918, Lewis was wounded and two of his colleagues were killed by a British shell falling short of its target. He suffered from depression and homesickness during his convalescence and, upon his recovery in October, he was assigned to duty in Andover, England. He was demobilised in December 1918 and soon restarted his studies.
After Lewis returned to Oxford University, he received a First in Honour Moderations (Greek and Latin literature) in 1920, a First in Greats (Philosophy and Ancient History) in 1922, and a First in English in 1923. In 1924 he became a philosophy tutor at University College and, in 1925, was elected a Fellow and Tutor in English Literature at Magdalen College, where he served for 29 years until 1954.
Jane Moore.
During his army training, Lewis shared a room with another cadet, Edward Courtnay Francis "Paddy" Moore (1898–1918). Maureen Moore, Paddy's sister, said that the two made a mutual pact that if either died during the war, the survivor would take care of both their families. Paddy was killed in action in 1918 and Lewis kept his promise. Paddy had earlier introduced Lewis to his mother, Jane King Moore, and a friendship quickly sprang up between Lewis, who was eighteen when they met, and Jane, who was forty-five. The friendship with Moore was particularly important to Lewis while he was recovering from his wounds in hospital, as his father did not visit him.
Lewis lived with and cared for Moore until she was hospitalised in the late 1940s. He routinely introduced her as his mother, and referred to her as such in letters. Lewis' own mother had died when he was a child, and his father was distant, demanding, and eccentric, and he developed a deeply affectionate friendship with Moore.
Speculation regarding their relationship re-surfaced with the 1990 publication of A. N. Wilson's biography of Lewis. Wilson (who never met Lewis) attempted to make a case for their having been lovers for a time. Wilson's biography was not the first to address the question of Lewis's relationship with Moore. George Sayer knew Lewis for 29 years, and he had sought to shed light on the relationship during the period of 14 years prior to Lewis's conversion to Christianity. In his biography "Jack: A Life of C. S. Lewis", he wrote:
Later Sayer changed his mind. In the introduction to the 1997 edition of his biography of Lewis he wrote:
Lewis spoke well of Mrs. Moore throughout his life, saying to his friend George Sayer, "She was generous and taught me to be generous, too." In December 1917, Lewis wrote in a letter to his childhood friend Arthur Greeves that Jane and Greeves were "the two people who matter most to me in the world".
In 1930, Lewis moved into "The Kilns" with his brother Warnie, Mrs. Moore, and her daughter Maureen. The Kilns was a house in the district of Headington Quarry on the outskirts of Oxford, now part of the suburb of Risinghurst. They all contributed financially to the purchase of the house, which passed to Maureen, who by then was Dame Maureen Dunbar, when Warren died in 1973.
Jane Moore suffered from dementia in her later years and was eventually moved into a nursing home, where she died in 1951. Lewis visited her every day in this home until her death.
Conversion to Christianity.
Lewis was raised in a religious family that attended the Church of Ireland. He became an atheist at age 15, though he later described his young self as being paradoxically "angry with God for not existing". His early separation from Christianity began when he started to view his religion as a chore and a duty; around this time, he also gained an interest in the occult, as his studies expanded to include such topics. Lewis quoted Lucretius ("De rerum natura", 5.198–9) as having one of the strongest arguments for atheism:
<poem>
</poem>
<poem>Had God designed the world, it would not be
A world so frail and faulty as we see.</poem>
Lewis's interest in the works of George MacDonald was part of what turned him from atheism. This can be seen particularly well through this passage in Lewis's "The Great Divorce", chapter nine, when the semi-autobiographical main character meets MacDonald in Heaven:
He eventually returned to Christianity, having been influenced by arguments with his Oxford colleague and friend J. R. R. Tolkien, whom he seems to have met for the first time on 11 May 1926, and by the book "The Everlasting Man" by G. K. Chesterton. Lewis vigorously resisted conversion, noting that he was brought into Christianity like a prodigal, "kicking, struggling, resentful, and darting his eyes in every direction for a chance to escape." He described his last struggle in "Surprised by Joy":
After his conversion to theism in 1929, Lewis converted to Christianity in 1931, following a long discussion and late-night walk with his close friends Tolkien and Hugo Dyson. He records making a specific commitment to Christian belief while on his way to the zoo with his brother. He became a member of the Church of England – somewhat to the disappointment of Tolkien, who had hoped that he would join the Catholic Church.
Lewis was a committed Anglican who upheld a largely orthodox Anglican theology, though in his apologetic writings, he made an effort to avoid espousing any one denomination. In his later writings, some believe that he proposed ideas such as purification of venial sins after death in purgatory ("The Great Divorce" and "Letters to Malcolm") and mortal sin ("The Screwtape Letters"), which are generally considered to be Roman Catholic teachings, although they are also widely held in Anglicanism (particularly in high church Anglo-Catholic circles). Regardless, Lewis considered himself an entirely orthodox Anglican to the end of his life, reflecting that he had initially attended church only to receive communion and had been repelled by the hymns and the poor quality of the sermons. He later came to consider himself honoured by worshipping with men of faith who came in shabby clothes and work boots and who sang all the verses to all the hymns.
Second World War.
After the outbreak of the war in 1939, the Lewises took child evacuees from London and other cities into The Kilns.
Lewis was only 40 when the war started, and he tried to re-enter military service, offering to instruct cadets; but his offer was not accepted. He rejected the recruiting office's suggestion of writing columns for the Ministry of Information in the press, as he did not want to "write lies" to deceive the enemy. He later served in the local Home Guard in Oxford.
From 1941 to 1943, Lewis spoke on religious programmes broadcast by the BBC from London while the city was under periodic air raids. These broadcasts were appreciated by civilians and servicemen at that stage. For example, Air Chief Marshal Sir Donald Hardman wrote:
The broadcasts were anthologised in "Mere Christianity". From 1941, he was occupied at his summer holiday weekends visiting R.A.F. stations to speak on his faith, invited by the R.A.F.'s Chaplain-in-Chief Maurice Edwards.
It was also during the same wartime period that Lewis was invited to become first President of the Oxford Socratic Club in January 1942, a position that he enthusiastically held until he resigned on appointment to Cambridge University in 1954.
Honour declined.
Lewis was named on the last list of honours by George VI in December 1951 as a Member of the Order of the British Empire (MBE) but declined so as to avoid association with any political issues.
Chair at Cambridge University.
In 1954, Lewis accepted the newly founded chair of Mediaeval and Renaissance Literature at Magdalene College, Cambridge, where he finished his career. He maintained a strong attachment to the city of Oxford, keeping a home there and returning on weekends until his death in 1963.
Joy Davidman.
In later life, Lewis corresponded with Joy Davidman Gresham, an American writer of Jewish background, a former Communist, and a convert from atheism to Christianity. She was separated from her alcoholic and abusive husband, novelist William L. Gresham, and came to England with her two sons, David and Douglas. Lewis at first regarded her as an agreeable intellectual companion and personal friend, and it was on this level that he agreed to enter into a civil marriage contract with her so that she could continue to live in the UK. The civil marriage took place at the register office, 42 St Giles', Oxford, on 23 April 1956. Lewis's brother Warren wrote: "For Jack the attraction was at first undoubtedly intellectual. Joy was the only woman whom he had met ... who had a brain which matched his own in suppleness, in width of interest, and in analytical grasp, and above all in humour and a sense of fun." After complaining of a painful hip, she was diagnosed with terminal bone cancer, and the relationship developed to the point that they sought a Christian marriage. Since she was divorced, this was not straightforward in the Church of England at the time, but a friend, the Rev. Peter Bide, performed the ceremony at her bed in the Churchill Hospital on 21 March 1957.
Gresham's cancer soon went into remission, and the couple lived together as a family with Warren Lewis until 1960, when recurrence of the cancer caused her death. Earlier that year, the couple took a brief holiday in Greece and the Aegean; Lewis was fond of walking but not of travel, and this marked his only crossing of the English Channel after 1918. Lewis's book "A Grief Observed" describes his experience of bereavement in such a raw and personal fashion that he originally released it under the pseudonym N. W. Clerk to keep readers from associating the book with him. Ironically, many friends recommended the book to Lewis as a method for dealing with his own grief. After Lewis's death, his authorship was made public by Faber's, with the permission of the executors.
Lewis continued to raise Gresham's two sons after her death. Douglas Gresham is a Christian like Lewis and his mother, while David Gresham turned to the faith into which his mother had been born, becoming Orthodox Jewish in his beliefs. His mother's writings had featured the Jews in an unsympathetic manner, particularly one "shohet" (ritual slaughterer). David informed Lewis that he was going to become a ritual slaughterer to present this type of Jewish religious functionary to the world in a more favourable light. In a 2005 interview, Douglas Gresham acknowledged that he and his brother were not close, but he did say that they are in email contact. Douglas remains involved in the affairs of the Lewis estate.
Illness and death.
In early June 1961, Lewis began suffering from inflammation of the kidneys, which resulted in blood poisoning. His illness caused him to miss the autumn term at Cambridge, though his health gradually began improving in 1962 and he returned that April. Lewis's health continued to improve, and according to his friend George Sayer, Lewis was fully himself by early 1963. On 15 July that year he fell ill and was admitted to hospital. The next day at 5:00 pm, Lewis suffered a heart attack and lapsed into a coma, unexpectedly awaking the following day at 2:00 pm. After he was discharged from the hospital, Lewis returned to the Kilns, though he was too ill to return to work. As a result, he resigned from his post at Cambridge in August. Lewis's condition continued to decline, and in mid-November he was diagnosed with end-stage renal failure. On 22 November, exactly one week before his 65th birthday, Lewis collapsed in his bedroom at 5:30 pm and died a few minutes later. He is buried in the churchyard of Holy Trinity Church, Headington, Oxford. His brother Warren Hamilton "Warnie" Lewis, who died on 9 April 1973, was later buried in the same grave.
Media coverage of his death was almost completely overshadowed by news of the assassination of US President John F. Kennedy, which occurred on the same day (approximately 55 minutes following Lewis' collapse), as did the death of English writer Aldous Huxley, author of "Brave New World". This coincidence was the inspiration for Peter Kreeft's book "Between Heaven and Hell: A Dialog Somewhere Beyond Death with John F. Kennedy, C. S. Lewis, & Aldous Huxley". C. S. Lewis is commemorated on 22 November in the church calendar of the Episcopal Church.
Career.
Scholar.
Lewis began his academic career as an undergraduate student at Oxford University, where he won a triple first, the highest honours in three areas of study. He was then elected a Fellow of Magdalen College, Oxford, where he worked for nearly thirty years, from 1925 to 1954. In 1954, he was awarded the newly founded chair of Mediaeval and Renaissance Literature at Cambridge University, and was elected a fellow of Magdalene College. Concerning his appointed academic field, he argued that there was no such thing as an English Renaissance. Much of his scholarly work concentrated on the later Middle Ages, especially its use of allegory. His "The Allegory of Love" (1936) helped reinvigorate the serious study of late medieval narratives such as the "Roman de la Rose".
Lewis wrote several prefaces to works of literature and poetry, such as Layamon's "Brut". His book "A Preface to "Paradise Lost"" is still one of the most valuable criticisms of that work. His last academic work, "The Discarded Image: An Introduction to Medieval and Renaissance Literature" (1964), is a summary of the medieval world view, a reference to the "discarded image" of the cosmos.
Lewis was a prolific writer, and his circle of literary friends became an informal discussion society known as the "Inklings", including J. R. R. Tolkien, Nevill Coghill, Lord David Cecil, Charles Williams, Owen Barfield, and his brother Warren Lewis. At least one scholar points to December 1929 as the Inklings' beginning date. Lewis's friendship with Coghill and Tolkien grew during their time as members of the Kolbítar, an Old Norse reading group that Tolkien founded and which ended around the time of the inception of the Inklings. At Oxford, he was the tutor of poet John Betjeman, critic Kenneth Tynan, mystic Bede Griffiths, novelist Roger Lancelyn Green and Sufi scholar Martin Lings, among many other undergraduates. Curiously, the religious and conservative Betjeman detested Lewis, whereas the anti-establishment Tynan retained a lifelong admiration for him.
Of Tolkien, Lewis writes in "Surprised by Joy":
Novelist.
In addition to his scholarly work, Lewis wrote several popular novels, including the science fiction Space Trilogy for adults and the Narnia fantasies for children. Most deal implicitly with Christian themes such as sin, humanity's fall from grace, and redemption.
"The Pilgrim's Regress".
His first novel after becoming a Christian was "The Pilgrim's Regress" (1933), which depicted his experience with Christianity in the style of John Bunyan's "The Pilgrim's Progress". The book was poorly received by critics at the time, although David Martyn Lloyd-Jones, one of Lewis's contemporaries at Oxford, gave him much-valued encouragement. Asked by Lloyd-Jones when he would write another book, Lewis replied, "When I understand the meaning of prayer."
"Space Trilogy" novels.
The "Space Trilogy" (also called the "Cosmic Trilogy" or "Ransom Trilogy") dealt with what Lewis saw as the dehumanising trends in contemporary science fiction. The first book, "Out of the Silent Planet", was apparently written following a conversation with his friend J.R.R. Tolkien about these trends. Lewis agreed to write a "space travel" story and Tolkien a "time travel" one, but Tolkien never completed "The Lost Road", linking his Middle-earth to the modern world. Lewis's main character Elwin Ransom is based in part on Tolkien, a fact to which Tolkien alludes in his letters.
The second novel, "Perelandra", depicts a new Garden of Eden on the planet Venus, a new Adam and Eve, and a new "serpent figure" to tempt Eve. The story can be seen as an account of what might have happened if the terrestrial Adam had defeated the serpent and avoided the Fall of Man, with Ransom intervening in the novel to "ransom" the new Adam and Eve from the deceptions of the enemy. The third novel, "That Hideous Strength", develops the theme of nihilistic science threatening traditional human values, embodied in Arthurian legend.
Many ideas in the trilogy, particularly opposition to de-humanization as portrayed in the third book, are presented more formally in "The Abolition of Man", based on a series of lectures by Lewis at Durham University in 1943. Lewis stayed in Durham, where he was overwhelmed by the cathedral. "That Hideous Strength" is in fact set in the environs of "Edgestow" university, a small English university like Durham, though Lewis disclaims any other resemblance between the two.
Walter Hooper, Lewis's literary executor, discovered a fragment of another science-fiction novel by Lewis called "The Dark Tower". Ransom appears in the story but it is not clear whether the book was intended as part of the same series of novels. The manuscript was eventually published in 1977, though Lewis scholar Kathryn Lindskoog doubts its authenticity.
"The Chronicles of Narnia".
"The Chronicles of Narnia" is a series of seven fantasy novels for children and is considered a classic of children's literature. Written between 1949 and 1954 and illustrated by Pauline Baynes, the series is Lewis's most popular work, having sold over 100 million copies in 41 languages . It has been adapted several times, complete or in part, for radio, television, stage and cinema.
The books contain Christian ideas intended to be easily accessible to young readers. In addition to Christian themes, Lewis also borrows characters from Greek and Roman mythology, as well as traditional British and Irish fairy tales.
Other works.
Lewis wrote several works on Heaven and Hell. One of these, "The Great Divorce", is a short novella in which a few residents of Hell take a bus ride to Heaven, where they are met by people who dwell there. The proposition is that they can stay if they choose, in which case they can call the place where they had come from "Purgatory", instead of "Hell". But many find it not to their taste. The title is a reference to William Blake's "The Marriage of Heaven and Hell", a concept that Lewis found a "disastrous error" . This work deliberately echoes two other more famous works with a similar theme: the "Divine Comedy" of Dante Alighieri, and Bunyan's "The Pilgrim's Progress".
Another short work, "The Screwtape Letters", consists of suave letters of advice from senior demon Screwtape to his nephew Wormwood on the best ways to tempt a particular human and secure his damnation. Lewis's last novel was "Till We Have Faces", which he thought of as his most mature and masterly work of fiction but which was never a popular success. It is a retelling of the myth of Cupid and Psyche from the unusual perspective of Psyche's sister. It is deeply concerned with religious ideas, but the setting is entirely pagan, and the connections with specific Christian beliefs are left implicit.
Before Lewis's conversion to Christianity, he published two books: "Spirits in Bondage", a collection of poems, and "Dymer", a single narrative poem. Both were published under the pen name Clive Hamilton. Other narrative poems have since been published posthumously, including "Launcelot", "The Nameless Isle", and "The Queen of Drum".
He also wrote "The Four Loves", which rhetorically explains four categories of love: friendship, eros, affection, and charity.
In 2009, a partial draft was discovered of "Language and Human Nature", which Lewis had begun co-writing with J.R.R. Tolkien, but which was never completed.
Christian apologist.
Lewis is also regarded by many as one of the most influential Christian apologists of his time, in addition to his career as an English professor and an author of fiction. "Mere Christianity" was voted best book of the twentieth century by "Christianity Today" in 2000. He has been called "The Apostle to the Skeptics" due to his approach to religious belief as a sceptic, and his following conversion.
Lewis was very interested in presenting a reasonable case for Christianity. "Mere Christianity", "The Problem of Pain", and "Miracles" were all concerned, to one degree or another, with refuting popular objections to Christianity, such as the question, "How could a good God allow pain to exist in the world?" He also became a popular lecturer and broadcaster, and some of his writing originated as scripts for radio talks or lectures (including much of "Mere Christianity").
According to George Sayer, losing a 1948 debate with Elizabeth Anscombe, also a Christian, led Lewis to re-evaluate his role as an apologist, and his future works concentrated on devotional literature and children's books. Anscombe had a completely different recollection of the debate's outcome and its emotional effect on Lewis. Victor Reppert also disputes Sayer, listing some of Lewis's post-1948 apologetic publications, including the second and revised edition of his "Miracles" in 1960, in which Lewis addressed Anscombe's criticism.<ref name="Reppert 2005 https://books.google.com/books?id=hn1gaNlri1cC&pg=PA266 266"></ref> Noteworthy too is Roger Teichman's suggestion in "The Philosophy of Elizabeth Anscombe" that the intellectual impact of Anscombe's paper on Lewis's philosophical self-confidence should not be over-rated: "... it seems unlikely that he felt as irretrievably crushed as some of his acquaintances have made out; the episode is probably an inflated legend, in the same category as the affair of Wittgenstein's Poker. Certainly Anscombe herself believed that Lewis's argument, though flawed, was getting at something very important; she thought that this came out more in the improved version of it that Lewis presented in a subsequent edition of "Miracles" – though that version also had 'much to criticize in it'."
Lewis also wrote an autobiography titled "Surprised by Joy", which places special emphasis on his own conversion. (It was written before he met his wife, Joy Gresham; the title of the book came from the first line of a poem by William Wordsworth.) His essays and public speeches on Christian belief, many of which were collected in "God in the Dock" and "The Weight of Glory and Other Addresses", remain popular today.
His most famous works, the "Chronicles of Narnia", contain many strong Christian messages and are often considered allegory. Lewis, an expert on the subject of allegory, maintained that the books were not allegory, and preferred to call the Christian aspects of them "suppositional". As Lewis wrote in a letter to a Mrs. Hook in December 1958:
If Aslan represented the immaterial Deity in the same way in which Giant Despair lt;/nowiki>a character in "[[The Pilgrim's Progress]]" represents despair, he would be an allegorical figure. In reality, he is an invention giving an imaginary answer to the question, 'What might Christ become like, if there really were a world like Narnia and He chose to be incarnate and die and rise again in that world as He actually has done in ours?' This is not allegory at all.
"Trilemma".
In a much-cited passage from "Mere Christianity", Lewis challenged the view that Jesus was a great moral teacher but not God. He argued that Jesus made several implicit claims to divinity, which would logically exclude that claim.
I am trying here to prevent anyone saying the really foolish thing that people often say about Him: 'I'm ready to accept Jesus as a great moral teacher, but I don't accept his claim to be God.' That is the one thing we must not say. A man who was merely a man and said the sort of things Jesus said would not be a great moral teacher. He would either be a lunatic – on the level with the man who says he is a poached egg – or else he would be the Devil of Hell. You must make your choice. Either this man was, and is, the Son of God, or else a madman or something worse. You can shut him up for a fool, you can spit at him and kill him as a demon or you can fall at his feet and call him Lord and God, but let us not come with any patronising nonsense about his being a great human teacher. He has not left that open to us. He did not intend to.
Lewis did not invent this argument, but developed and popularized it – although it is sometimes referred to as "Lewis's trilemma". It has been used by Christian apologist Josh McDowell in his book "More Than a Carpenter" . It has been widely repeated in Christian apologetic literature, but largely ignored by professional theologians and biblical scholars.
Lewis's Christian apologetics, and this argument in particular, have been criticised. Philosopher John Beversluis described Lewis's arguments as "textually careless and theologically unreliable," and this particular argument as logically unsound and an example of false dilemma. Theologian John Hick argues that New Testament scholars do not now support the view that Jesus claimed to be God. New Testament scholar N. T. Wright criticises Lewis for failing to recognise the significance of Jesus' Jewish identity and setting – an oversight which "at best, drastically short-circuits the argument" and which lays Lewis open to criticism that his argument "doesn't work as history, and it backfires dangerously when historical critics question his reading of the gospels," although he believes this "doesn't undermine the eventual claim."
Lewis used a similar argument in "The Lion, the Witch and the Wardrobe", when Digory Kirke advises the young heroes that their sister's claims of a magical world must logically be taken as either lies, madness, or truth.
Universal morality.
One of the main theses in Lewis's apologia is that there is a common morality known throughout humanity. In the first five chapters of "Mere Christianity" Lewis discusses the idea that people have a standard of behaviour to which they expect people to adhere. This standard has been called Universal Morality or Natural Law. Lewis claims that people all over the earth know what this law is and when they break it. He goes on to claim that there must be someone or something behind such a universal set of principles.
These then are the two points that I wanted to make. First, that human beings, all over the earth, have this curious idea that they ought to behave in a certain way, and cannot really get rid of it. Secondly, that they do not in fact behave in that way. They know the Law of Nature; they break it. These two facts are the foundation of all clear thinking about ourselves and the universe we live in.
Lewis also portrays Universal Morality in his works of fiction. In "The Chronicles of Narnia" he describes Universal Morality as the "deep magic" which everyone knew.
In the second chapter of "Mere Christianity" Lewis recognises that "many people find it difficult to understand what this Law of Human Nature ... is". And he responds first to the idea "that the Moral Law is simply our herd instinct" and second to the idea "that the Moral Law is simply a social convention". In responding to the second idea Lewis notes that people often complain that one set of moral ideas is better than another, but that this actually argues for there existing some "Real Morality" to which they are comparing other moralities. Finally he notes that sometimes differences in moral codes are exaggerated by people who confuse differences in beliefs about morality with differences in beliefs about facts:
Lewis also had fairly progressive views on the topic of "animal morality", in particular the suffering of animals, as is evidenced by several of his essays: most notably, "On Vivisection" and "On the Pains of Animals."
Legacy.
Lewis continues to attract a wide readership. In 2008, "The Times" ranked him eleventh on their list of "the 50 greatest British writers since 1945". Readers of his fiction are often unaware of what Lewis considered the Christian themes of his works. His Christian apologetics are read and quoted by members of many Christian denominations. In 2013, on the 50th anniversary of his death, Lewis joined some of Britain's greatest writers recognised at Poets' Corner, Westminster Abbey. The dedication service, at noon on 22 November 2013, included a reading from "The Last Battle" by Douglas Gresham, younger stepson of Lewis. Flowers were laid by Walter Hooper, trustee and literary advisor to the Lewis Estate. An address was delivered by former Archbishop of Canterbury Rowan Williams. The floor stone inscription is a quotation from an address by Lewis: I believe in Christianity as I believe that the Sun has risen, not only because I see it but because by it I see everything else.
Lewis has been the subject of several biographies, a few of which were written by close friends, such as Roger Lancelyn Green and George Sayer. In 1985, the screenplay "Shadowlands" by William Nicholson dramatized Lewis's life and relationship with Joy Davidman Gresham. It was aired on British television starring Joss Ackland and Claire Bloom. This was also staged as a theatre play starring Nigel Hawthorne in 1989, and made into the 1993 feature film "Shadowlands" starring Anthony Hopkins and Debra Winger. In 2005, a one-hour television movie entitled "C. S. Lewis: Beyond Narnia" provided a general synopsis of Lewis's life, starring Anton Rodgers.
Many books have been inspired by Lewis, including "A Severe Mercy" by his correspondent and friend Sheldon Vanauken. "The Chronicles of Narnia" have been particularly influential. Modern children's literature has been more or less influenced by Lewis's series, such as Daniel Handler's "A Series of Unfortunate Events", Eoin Colfer's "Artemis Fowl", Philip Pullman's "His Dark Materials", and J. K. Rowling's "Harry Potter" . Pullman is an atheist and so fierce a critic of Lewis's work as to be dubbed "the anti-Lewis". He considers C. S. Lewis a negative influence and has accused Lewis of featuring religious propaganda, misogyny, racism, and emotional sadism in his books. Authors of adult fantasy literature such as Tim Powers have also testified to being influenced by Lewis's work.
The alleged misogyny of Lewis' fiction and Christian apologetics has also been characterized as rooted in phallogocentrism and homoeroticism. Post-third-wave feminist writers have found in Lewis' works a tendency to privilege the masculine in understanding meaning or gender relations. 
Most of Lewis's posthumous work has been edited by his literary executor Walter Hooper. Kathryn Lindskoog, an independent Lewis scholar, argued that Hooper's scholarship is not reliable and that he has made false statements and attributed forged works to Lewis. C. S. Lewis's stepson Douglas Gresham denies the forgery claims, saying, "The whole controversy thing was engineered for very personal reasons ... Her fanciful theories have been pretty thoroughly discredited."
A bronze statue of Lewis's character Digory from "The Magician's Nephew" stands in Belfast's Holywood Arches in front of the Holywood Road Library.
Lewis was strongly opposed to the creation of live-action versions of his works. His major concern was that the anthropomorphic animal characters "when taken out of narrative into actual visibility, always turn into buffoonery or nightmare".
Several C. S. Lewis Societies exist around the world, including one which was founded in Oxford in 1982 to discuss papers on the life and works of Lewis and the other Inklings, and generally appreciate all things Lewisian. His name is also used by a variety of Christian organisations, often with a concern for maintaining conservative Christian values in education or literary studies.
Film adaptations have been made of three of "The Chronicles of Narnia: " (2005), ' (2008) and ' (2010).
Lewis is featured as a main character in "The Chronicles of the Imaginarium Geographica" series by James A. Owen. He is one of two characters in Mark St. Germain's 2009 play "Freud's Last Session", which imagines a meeting between Lewis, aged 40, and Sigmund Freud, aged 83, at Freud's house in Hampstead, London, in 1939, as the Second World War is about to break out.

</doc>
<doc id="5814" url="https://en.wikipedia.org/wiki?curid=5814" title="Chinese dominoes">
Chinese dominoes

Chinese dominoes are used in several tile-based games, namely, Tien Gow, Pai Gow, Tiu U and Kap Tai Shap. In Cantonese they are called "Gwat Pai" (骨牌), which literally means "bone tiles"; it is also the name of a northern Chinese game, where the rules are quite different from the southern Chinese game Tien Gow. References to Chinese domino tiles can be traced to writings from the Song Dynasty (AD 1120).
Deck Composition and Ranking.
Each tile pattern in the Chinese domino set is made up of the outcome of a throw of two six-sided dice. Each combination is only used once, so there are 21 unique possible patterns. Eleven of these 21 unique patterns are repeated to make a total of 32 tiles in a Chinese dominoes set. The tile set consists of 32 tiles in two "suits" or groups called "military" and "civilian". The civilian suit was originally called the "Chinese" suit and the military suit was called the "barbarian" suit but was changed during the Qing dynasty (1644-1912) to avoid offending the ruling Manchus. There are no markings on the tiles to distinguish these suits; a player must simply remember which tiles belong to which group.
The tile set contains two each of eleven civilian suit tiles (6-6, 1-1, 4-4, 1-3, 5-5, 3-3, 2-2, 5-6, 4-6, 1-6, 1-5) and one each of ten military suit tiles (3-6, 4-5; 2-6, 3-5; 2-5, 3-4; 2-4; 1-4, 2-3; 1-2). Each civilian tile also has a Chinese name (and common rough translation to English): The 6-6 is "tin" (天 heaven), 1-1 is "dei" (地 earth), 4-4 is "yan" (人 man), 1-3 is "ngo" (鵝 goose or 和 harmony), 5-5 is "mui" (梅 plum flower), 3-3 is "cheung" (長 long), 2-2 is "ban" (板 board), 5-6 is "fu" (斧 hatchet), 4-6 is "ping" (屏 partition), 1-6 is "tsat" (七) (long leg seven), and 1-5 is "luk" (六) (big head six).
The civilian tiles are ranked according to the Chinese cultural significance of the tile names, and must be memorized. For example, heaven ranks higher than earth; earth ranks higher than man etc. Remembering the suits and rankings of the tiles is easier if one understands the Chinese names of the tiles and the symbolism behind them.
The military tiles are named and ranked according to the total points on the tiles. For example, the "nines" (3-6 and 4-5) rank higher than the "eights" (2-6 and 3-5).
The military tiles (since there is only one each) are also considered to be five mixed "pairs" (for example, the 3-6 and 4-5 tiles "match" because they have same total points and both in the military suit). Among the military tiles, individual tiles of the same pair (such as 1-4 and 2-3) rank equally. The 2-4 and 1-2 are an odd pair. They are the only tiles in the whole set that don't match other tiles in the normal sense. This pair when played together is considered a suit on its own, called the Gee Joon (至尊 Supreme). It is the highest ranking pair in the game of Pai Gow, though the tiles rank low individually (in their normal order). When a tile of this pair is played individually in the game of Tien Gow, each takes its regular ranking among other military suit tiles according to the total points. The rankings of the individual tiles are similar in most games. However, the ranking of combination tiles is slightly different in Pai Gow and Tien Gow.
Using the same coloring scheme of the traditional Chinese dice, every half-domino with 1 or 4 spots has those spots colored red (for example, the 4-5 domino has four red spots and five white spots). The only exception is the pair of 6-6 tiles. Half of the spots on the 6-6 domino are colored red to make them stand out as the top ranking tiles.
Tiles with blank ends, like those found in western "double-six" dominoes, once existed during the 17th-century. These games employed two sets of "double-six" tiles.

</doc>
<doc id="5816" url="https://en.wikipedia.org/wiki?curid=5816" title="Cenozoic">
Cenozoic

The Cenozoic Era (; also Cænozoic, Caenozoic or Cainozoic ; meaning "new life", from Greek "kainos" "new", and "zoe" "life") is the current and most recent of the three Phanerozoic geological eras, following the Mesozoic Era and covering the period from 66 million years ago to present day.
The Cenozoic is also known as the Age of Mammals, because the extinction of many groups allowed mammals to greatly diversify.
Early in the Cenozoic, following the K-Pg event, the planet was dominated by relatively small fauna, including small mammals, birds, reptiles, and amphibians. From a geological perspective, it did not take long for mammals and birds to greatly diversify in the absence of the large reptiles that had dominated during the Mesozoic. Some flightless birds grew larger than the average human. These species are sometimes referred to as "terror birds," and were formidable predators. Mammals came to occupy almost every available niche (both marine and terrestrial), and some also grew very large, attaining sizes not seen in most of today's terrestrial mammals.
Climate-wise, the Earth had begun a drying and cooling trend, culminating in the glaciations of the Pleistocene Epoch, and partially offset by the Paleocene-Eocene Thermal Maximum. The continents also began looking roughly familiar at this time and moved into their current positions.
Subdivisions.
The Cenozoic is divided into three periods: The Paleogene, Neogene, and Quaternary; and seven epochs: The Paleocene, Eocene, Oligocene, Miocene, Pliocene, Pleistocene, and Holocene. The Quaternary Period was officially recognized by the International Commission on Stratigraphy in June 2009, and the former Tertiary Period was officially disused in 2004 because of the necessity to divide the Cenozoic into periods more like that of the previous Paleozoic and Mesozoic eras. The common use of epochs during the Cenozoic helps paleontologists better organize and group the many significant events that occurred during this comparatively short interval of time. There is also more detailed knowledge of this era than any other because of the relatively young strata associated with it.
Paleogene.
The Paleogene spans from the extinction of the dinosaurs, some 66 million years ago, to the dawn of the Neogene twenty three million years ago. It features three epochs: the Paleocene, Eocene and Oligocene. 
The Paleocene ranged from 65 million to 55 million years ago. The Paleocene is a transitional point between the devastation that is the K-T extinction, to the rich jungles environment that is the Early Eocene. The Early Paleocene saw the recovery of the earth. The continents began to take their modern shape, but all the continents and subcontinent India were separated from each other. Afro-Eurasia was separated by the Tethys Sea, and the Americas were separated by the strait of Panama, as the isthmus had not yet formed. This epoch featured a general warming trend, with jungles eventually reaching the poles. The oceans were dominated by sharks as the large reptiles that had once ruled went extinct. Archaic mammals filled the world such as creodonts and early primates that evolved during the Mesozoic, and as a result, there was nothing over 10 kilograms. Mammals were still quite small.
The Eocene Epoch ranged from 55 million years to 33 million years ago. In the Early-Eocene, life was small and lived in cramped jungles, much like the Paleocene. There was nothing over the weight of 10 kilograms. Among them were early primates, whales and horses along with many other early forms of mammals. At the top of the food chains were huge birds, such as "Gastornis". It is the only time that birds ruled the world (excluding their ancestors, the dinosaurs). The temperature was 30 degrees Celsius with little temperature gradient from pole to pole. In the Mid-Eocene, the circum-Antarctic current between Australia and Antarctica formed which disrupted ocean currents worldwide and as a result caused a global cooling effect, shrinking the jungles. This allowed mammals to grow to mammoth proportions, such as whales which, by that time, were almost fully aquatic. Mammals like "Andrewsarchus" were at the top of the food-chain and sharks were replaced by whales such as "Basilosaurus" as rulers of the seas. The Late Eocene saw the rebirth of seasons, which caused the expansion of savanna-like areas, along with the evolution of grass.
The Oligocene Epoch spans from 33 million to 23 million years ago. The Oligocene featured the expansion of grass which had led to many new species to evolve, including the first elephants, cats, dogs, marsupials and many other species still prevalent today. Many other species of plants evolved in this period too, such as the evergreen trees. A cooling period was still in effect and seasonal rains were as well. Mammals still continued to grow larger and larger. "Paraceratherium", the largest land mammal to ever live evolved during this period, along with many perissodactyls in an event known as the Grande Coupure.
Neogene.
The Neogene spans from 23 million to 3 million years ago, and is the shortest geological period in the Phanerozoic Eon. It features 2 epochs: the Miocene, and the Pliocene.
The Miocene spans from 23 to 5 million years ago and is a period in which grass spread further across, effectively dominating a large portion of the world, diminishing forests in the process. Kelp forests evolved, leading to the evolution of new species, such as sea otters. During this time, perissodactyls thrived, and evolved into many different varieties. Alongside them were the apes, which evolved into a staggering 30 species. Overall, arid and mountainous land dominated most of the world, as did grazers. The Tethys Sea finally closed with the creation of the Arabian Peninsula and in its wake left the Black, Red, Mediterranean and Caspian Seas. This only increased aridity. Many new plants evolved, and 95% of modern seed plants evolved in the mid-Miocene.
The Pliocene lasted from 5 to 2 million years ago. The Pliocene featured dramatic climactic changes, which ultimately led to modern species and plants. The Mediterranean Sea dried up for several million years. Along with these major geological events, "Australopithecus" evolved in Africa, beginning the human branch. The isthmus of Panama formed, and animals migrated between North and South America, wreaking havoc on the local ecology. Climatic changes brought savannas that are still continuing to spread across the world, Indian monsoons, deserts in East Asia, and the beginnings of the Sahara desert. The earth's continents and seas moved into their present shapes. The world map has not changed much since.
Quaternary.
The Quaternary spans from 3 million to present day, and features modern animals, and dramatic changes in the climate. It is divided into two epochs: the Pleistocene and the Holocene. 
The Pleistocene lasted from 3 million to 12,000 years ago. This epoch was marked by ice ages as a result of the cooling trend that started in the Mid-Eocene. There were at least four separate glaciation periods marked by the advance of ice caps as far south as 40 degrees N latitude in mountainous areas. Meanwhile, Africa experienced a trend of desiccation which resulted in the creation of the Sahara, Namib, and Kalahari deserts. Many animals evolved including mammoths, giant ground sloths, dire wolves, saber-toothed cats, and most famously "Homo sapiens". 100,000 years ago marked the end of one of the worst droughts of Africa, and led to the expansion of primitive man. As the Pleistocene drew to a close, a major extinction wiped out much of the world's megafauna, including some of the hominid species, such as Neanderthals. All the continents were affected, but Africa to a lesser extent. The continent retains many large animals, such as hippos.
The Holocene began 12,000 years ago and lasts until to present day. Also known as "the Age of Man", the Holocene is marked by the rise of man on his path to sentience. All recorded history and "the history of the world" lies within the boundaries of the Holocene epoch. Human activity is blamed for a mass extinction that began roughly 10,000 years ago, though the species becoming extinct have only been recorded since the Industrial Revolution. This is sometimes referred to as the "Sixth Extinction". 322 species have become extinct due to human activity since the Industrial Revolution.
Tectonics.
Geologically, the Cenozoic is the era when the continents moved into their current positions. Australia-New Guinea, having split from Pangea during the early Cretaceous, drifted north and, eventually, collided with South-east Asia; Antarctica moved into its current position over the South Pole; the Atlantic Ocean widened and, later in the era, South America became attached to North America with the isthmus of Panama.
India collided with Asia creating the Himalayas; Arabia collided with Eurasia, closing the Tethys ocean and creating the Zagros Mountains, around .
Climate.
The Paleocene–Eocene Thermal Maximum of was a significant global warming event; however, since the Azolla event of , the Cenozoic Era has been a period of long-term cooling. After the tectonic creation of Drake Passage, when South America fully detached from Antarctica during the Oligocene, the climate cooled significantly due to the advent of the Antarctic Circumpolar Current which brought cool deep Antarctic water to the surface. The cooling trend continued in the Miocene, with relatively short warmer periods. When South America became attached to North America creating the Isthmus of Panama, the Arctic region cooled due to the strengthening of the Humboldt and Gulf Stream currents, eventually leading to the glaciations of the Quaternary ice age, the current interglacial of which is the Holocene Epoch.
Recent analysis of the geomagnetic reversal frequency, oxygen isotope record, and tectonic plate subduction rate, which are indicators of the changes in the heat flux at the core mantle boundary, climate and plate tectonic activity, shows that all these changes indicate similar rhythms on million years’ timescale in the Cenozoic Era occurring with the common fundamental periodicity of ∼13 Myr during most of the time.
Life.
During the Cenozoic, mammals proliferated from a few small, simple, generalized forms into a diverse collection of terrestrial, marine, and flying animals, giving this period its other name, the Age of Mammals, despite the fact that birds still outnumbered mammals two to one. The Cenozoic is just as much the age of savannas, the age of co-dependent flowering plants and insects, and the age of birds. Grass also played a very important role in this era, shaping the evolution of the birds and mammals that fed on it. One group that diversified significantly in the Cenozoic as well were the snakes. Evolving in the Cenozoic, the variety of snakes increased tremendously, resulting in many colubrids, following the evolution of their current primary prey source, the rodents.
In the earlier part of the Cenozoic, the world was dominated by the gastornithid birds, terrestrial crocodiles like "Pristichampsus", and a handful of primitive large mammal groups like uintatheres, mesonychids, and pantodonts. But as the forests began to recede and the climate began to cool, other mammals took over.
The Cenozoic is full of mammals both strange and familiar, including chalicotheres, creodonts, whales, primates, entelodonts, saber-toothed cats, mastodons and mammoths, three-toed horses, giant rhinoceros like "Indricotherium", the rhinoceros-like brontotheres, various bizarre groups of mammals from South America, such as the vaguely elephant-like pyrotheres and the dog-like marsupial relatives called borhyaenids and the monotremes and marsupials of Australia.

</doc>
<doc id="5820" url="https://en.wikipedia.org/wiki?curid=5820" title="Confucianism">
Confucianism

Confucianism, also known as Ruism, is a system of philosophical and "ethical-sociopolitical teachings" sometimes described as a religion. Confucianism developed during the Spring and Autumn Period from the teachings of the Chinese philosopher Confucius (551–479 BCE), who considered himself a retransmitter of Zhou values. Its metaphysical and cosmological elements developed in the Han Dynasty following the replacement of its contemporary, the more Taoistic Huang-Lao, as the official ideology. More privately, Chinese emperors would still make use of the historical Realpolitik of the Chinese, termed Legalism. The disintegration of the Han in the second century CE opened the way for the soteriological doctrines of Buddhism and Taoism to dominate intellectual life at that time.
A Confucian revival began during the Tang dynasty of 618-907. In the late Tang, Confucianism developed in response to Buddhism and Taoism and was reformulated as Neo-Confucianism. This reinvigorated form was adopted as the basis of the imperial exams and the core philosophy of the scholar official class in the Song dynasty (960-1297). The abolition of the examination system in 1905 marked the end of official Confucianism. The New Culture intellectuals of the early twentieth century blamed Confucianism for China's weaknesses. They searched for new doctrines to replace Confucian teachings; some of these new ideologies include the "Three Principles of the People" with the establishment of the Republic of China, and then Maoism under the People's Republic of China. In the late twentieth century, some people credited Confucianism with the rise of the East Asian economy and it enjoyed a rise in popularity both in China and abroad.
With particular emphasis on the importance of the family and social harmony, rather than on an otherworldly soteriology, the core of Confucianism is humanistic. According to Herbert Fingarette's concept of "the secular as sacred", Confucianism regards the ordinary activities of human life — and especially in human relationships as a manifestation of the sacred, because they are the expression of our moral nature ("xing" 性), which has a transcendent anchorage in Heaven ("tian" 天) and a proper respect of the gods ("shen"). While Heaven ("tian") has some characteristics that overlap the category of deity, it is primarily an "impersonal" absolute, like "dao" and "Brahman". Confucian liturgy (that is called 儒 "rú", or sometimes 正统 "zhèngtǒng", meaning "orthoprax" ritual style) led by Confucian priests or ritual masters (礼生 "lǐshēng") to worship the gods in public and ancestral Chinese temples, is preferred in special occasions over Taoist or popular ritual.
The this-worldly concern of Confucianism rests on the belief that human beings are fundamentally good, and teachable, improvable, and perfectible through personal and communal endeavor especially self-cultivation and self-creation. Confucian thought focuses on the cultivation of virtue and maintenance of ethics. Some of the basic Confucian ethical concepts and practices include "rén", "yì", and "lǐ", and "zhì". "Ren" ("humaneness") is the essence of the human being which manifests as compassion, it is the virtue-form of Heaven. "Yi" is the upholding of righteousness and the moral disposition to do good. "Li" is a system of ritual norms and propriety that determines how a person should properly act in everyday life according to the law of Heaven. "Zhi" is the ability to see what is right and fair, or the converse, in the behaviors exhibited by others. Confucianism holds one in contempt, either passively or actively, for failure to uphold the cardinal moral values of "ren" and "yi".
Historically, cultures and countries strongly influenced by Confucianism include mainland China, Taiwan, Hong Kong, Macau, Korea, Japan, and Vietnam, as well as various territories settled predominantly by Chinese people, such as Singapore. In the 20th century Confucianism’s influence reduced greatly . More recently, there have been talks of a "Confucian Revival" in the academic and the scholarly community and there has been a grassroots proliferation of various types of Confucian churches. In late 2015 many Confucian leaders formally established a national Holy Confucian Church (孔圣会 "Kǒngshènghuì") in China to unify the many Confucian congregations and civil society organisations.
Names and terminology.
Strictly speaking, there is no term in Chinese which directly corresponds to "Confucianism". In the Chinese language, the character "rú" 儒 meaning "scholar" is generally used both in the past and the present to refer to things related to Confucianism. The word "ru" in ancient China has diverse meanings. Some examples include, "weak", "soft", "to tame", "to comfort" and "to educate". Several different terms are used in different situations, several of which are of modern origin:
Three of these use "rú". These names do not use the name "Confucius" at all, but instead center on the figure or ideal of the Confucian scholar; however, the suffixes "jiā", "jiào" and "xué" carry different implications as to the nature of Confucianism itself.
"Rújiā" contains the character "jiā", which literally means "house" or "family". In this context, it is more readily construed as meaning "school of thought", since it is also used to construct the names of philosophical schools contemporary with Confucianism: for example, the Chinese names for Legalism and Mohism end in "jiā".
"Rújiào" and "Kǒngjiào" contain the Chinese character "jiào", the noun "teach", used in such terms as "education", or "educator". The term, however, is notably used to construct the names of religions in Chinese: the terms for Islam, Judaism, Christianity, and other religions in Chinese all end with "jiào".
"Rúxué" contains "xué", "study". The term is parallel to "-ology" in English, being used to construct the names of academic fields: the Chinese names of fields such as physics, chemistry, biology, political science, economics, and sociology all end in "xué".
The use of the term Confucianism has been avoided by some modern scholars, who favor "Ruism" or "Ruists" in lieu of Confucianism. Robert Eno argues that the term has been "burdened... with the ambiguities and irrelevant traditional associations". Ruism, as he states, is more faithful to the original Chinese name for the school.
The Five Classics ("Wujing") and the Confucian vision.
Traditionally, Confucius was thought to be the author or editor of the Five Classics which were the basic texts of Confucianism. The scholar Yao Xinzhong allows that there are good reasons to believe that Confucian classics took shape in the hands of Confucius, but that “nothing can be taken for granted in the matter of the early versions of the classics.” Yao reports that perhaps most scholars today hold the “pragmatic” view that Confucius and his followers, although they did not intend to create a system of classics, “contributed to their formation.” In any case, it is undisputed that for most of the last 2,000 years, Confucius was believed to have either written or edited these texts.
The scholar Tu Weiming explains these classics as embodying “five visions" which underlie the development of Confucianism:
Doctrines.
Theory and theology.
By the words of Tu Weiming and other Confucian scholars, who recover the work of Kang Youwei, Confucianism revolves around the pursuit of the unity of the self and "Tian" (Heaven, or the God of the Universe in European terminology, although in a nontheistic sense), and the relationship of humankind to the Heaven. The principle of Heaven ("Tian li" or "Tao"), is the order of the creation and divine authority, monistic in its structure. Individuals can realise their humanity and become one with Heaven through the contemplation of this order. This transformation of the self can be extended to the family and society to create a harmonious fiduciary community.
The moral-spiritual ideal of Confucianism conciles both the inner and outer polarities of self-cultivation and world redemption, synthesised in the ideal of "sageliness within and kingliness without". "Ren", translated as "humaneness" or the essence proper of a human being, is the character of compassionate mind; it is the virtue endowed by Heaven and at the same time what allows man to achieve oneness with Heaven—in the "Datong shu" it is defined as "to form one body with all things" and "when the self and others are not separated ... compassion is aroused".
"Tian" and the gods.
"Tian" (天), commonly translated as "Heaven" or "Sky", but philologically meaning the "Great One", "Great Whole", is a key concept in Confucianism. It denotes the source of reality, the cosmos, and nature in Chinese religions and philosophies. The Confucians mean by "Tian" and "li" (order) what the Taoists mean by "Tao". The "Tian" can also be compared to the "Brahman" of Hindu and Vedic traditions.
In "Analects" 9.5 Confucius says that a person can know the movement of the Tian, and speaks about his own sense of having a special place in the universe. In 7.19 he says that he is able to understand the order of Tian.
Zigong, a disciple of Confucius, said that Tian had set the master on the path to become a wise man ("Analects" 9.6). In "Analects" 7.23 Confucius says that he has no doubt left that the Tian gave him life, and from it he had developed the virtue ("de"). In "Analects" 8.19 he says that the lives of the sages and their communion with Tian are interwoven.
Regarding personal gods ("shen", energies who emanate from and reproduce the "Tian") enliving nature, in "Analects" 6.22 Confucius says that it is appropriate ("yi") for people to worship ("jing") them, though through proper rites ("li"), implying respect of positions and discretion. Confucius himself was a ritual and sacrificial master. In "Analects" 3.12 he explains that religious rituals produce meaningful experiences. Rites and sacrifices to the gods have an ethical importance: they generate good life, because taking part in them leads to the overcoming of the self. Analects 10.11 tells that Confucius always took a small part of his food and placed it on the sacrificial bowls as an offering to his ancestors.
In original Confucianism the concept of Tian expresses a form of pantheism. Other philosophical currents, like Mohism, developed a more theistic idea of the Tian.
Ethics.
Confucian ethics are described as humanistic. This ethical philosophy can be practiced by all the members of a society. Confucian ethics is characterized by the promotion of virtues, encompassed by the Five Constants, or the "Wuchang" (五常), extrapolated by Confucian scholars during the Han Dynasty. The Five Constants are:
These are accompanied by the classical "Sìzì" (四字), that singles out four virtues, one of which is included among the Five Constants:
There are still many other elements, such as "chéng" (誠, honesty), "shù" (恕, kindness and forgiveness), "lián" (廉, honesty and cleanness), "chǐ" (恥, shame, judge and sense of right and wrong),"yǒng" (勇, bravery), "wēn" (溫, kind and gentle), "liáng" (良, good, kindhearted), "gōng" (恭, respectful, reverent), "jiǎn" (儉, frugal), "ràng" (讓, modestly, self-effacing).
Humaneness.
"Ren" (, "rén") is the Confucian virtue denoting the good feeling a virtuous human experiences when being altruistic. It is exemplified by a normal adult's protective feelings for children. It is considered the essence of the human being, endowed by heaven, and at the same time the mean by which man can act according to the principle of Heaven ("li") and become one with it.
Yan Hui, Confucius's most outstanding student, once asked his master to describe the rules of "ren" and Confucius replied, "one should see nothing improper, hear nothing improper, say nothing improper, do nothing improper". Confucius also defined "ren" in the following way: "wishing to be established himself, seeks also to establish others; wishing to be enlarged himself, he seeks also to enlarge others".
Another meaning of "ren" is "not to do to others as you would not wish done to yourself". Confucius also said, ""ren" is not far off; he who seeks it has already found it". "Ren" is close to man and never leaves him.
Rite, right.
"Li" (禮) is a classical Chinese word which finds its most extensive use in Confucian and post-Confucian Chinese philosophy. "Li" is variously translated as "rite" or "reason", "ratio" in the pure sense of Vedic "ṛta" ("right", "order") when referring to the cosmic law, but when referring to its realisation in the context of human individual and social behavior it has also been translated as "custom", "mores", and "rules", among other terms.
"Li" embodies the entire web of interaction between humanity, human objects, and nature. Confucius includes in his discussions of "li" such diverse topics as learning, tea drinking, titles, mourning, and governance. Xunzi cites "songs and laughter, weeping and lamentation... rice and millet, fish and meat... the wearing of ceremonial caps, embroidered robes, and patterned silks, or of fasting clothes and mourning clothes... spacious rooms and secluded halls, soft mats, couches and benches" as vital parts of the fabric of "li".
Confucius envisioned proper government being guided by the principles of "li". Some Confucians proposed the perfectibility of all human beings with learning "li" as an important part of that process. Overall, Confucians believed governments should place more emphasis on "li" and rely much less on penal punishment when they govern.
Loyalty.
Loyalty (, "zhōng") is particularly relevant for the social class to which most of Confucius' students belonged, because the most important way for an ambitious young scholar to become a prominent official was to enter a ruler's civil service.
Confucius himself did not propose that "might makes right", but rather that a superior should be obeyed because of his moral rectitude. In addition, loyalty does not mean subservience to authority. This is because reciprocity is demanded from the superior as well. As Confucius stated "a prince should employ his minister according to the rules of propriety; ministers should serve their prince with faithfulness (loyalty)".
Similarly, Mencius also said that "when the prince regards his ministers as his hands and feet, his ministers regard their prince as their belly and heart; when he regards them as his dogs and horses, they regard him as another man; when he regards them as the ground or as grass, they regard him as a robber and an enemy". Moreover, Mencius indicated that if the ruler is incompetent, he should be replaced. If the ruler is evil, then the people have the right to overthrow him. A good Confucian is also expected to remonstrate with his superiors when necessary. At the same time, a proper Confucian ruler should also accept his ministers' advice, as this will help him govern the realm better.
In later ages, however, emphasis was often placed more on the obligations of the ruled to the ruler, and less on the ruler's obligations to the ruled. Like filial piety, loyalty was often subverted by the autocratic regimes in China. Nonetheless, throughout the ages, many Confucians continued to fight against unrighteous superiors and rulers. Many of these Confucians suffered and sometimes died because of their conviction and action. During the Ming-Qing era, prominent Confucians such as Wang Yangming promoted individuality and independent thinking as a counterweight to subservience to authority. The famous thinker Huang Zongxi also strongly criticized the autocratic nature of the imperial system and wanted to keep imperial power in check.
Many Confucians also realized that loyalty and filial piety have the potential of coming into conflict with one another. This can be true especially in times of social chaos, such as during the period of the Ming-Qing transition.
Filial piety.
In Confucian philosophy, filial piety (, "xiào") is a virtue of respect for one's parents and ancestors. The Confucian classic Xiao Jing or "Classic of Xiào", thought to be written around the Qin-Han period, has historically been the authoritative source on the Confucian tenet of "xiào" / "filial piety". The book, a conversation between Confucius and his student Zeng Shen (曾參, also known as Zengzi 曾子), is about how to set up a good society using the principle of "xiào" (filial piety). The term can also be applied to general obedience, and is used in religious titles in Christian Churches, like "filial priest" or "filial vicar" for a cleric whose church is subordinate to a larger parish. Filial piety is central to Confucian role ethics.
In more general terms, filial piety means to be good to one's parents; to take care of one's parents; to engage in good conduct not just towards parents but also outside the home so as to bring a good name to one's parents and ancestors; to perform the duties of one's job well so as to obtain the material means to support parents as well as carry out sacrifices to the ancestors; not be rebellious; show love, respect and support; display courtesy; ensure male heirs, uphold fraternity among brothers; wisely advise one's parents, including dissuading them from moral unrighteousness, for blindly following the parents' wishes is not considered to be "xiao"; display sorrow for their sickness and death; and carry out sacrifices after their death.
Filial piety is considered a key virtue in Chinese culture, and it is the main concern of a large number of stories. One of the most famous collections of such stories is "The Twenty-four Filial Exemplars" ("Ershi-si xiao" ). These stories depict how children exercised their filial piety in the past. While China has always had a diversity of religious beliefs, filial piety has been common to almost all of them; historian Hugh D.R. Baker calls respect for the family the only element common to almost all Chinese believers.
Relationships.
Social harmony results in part from every individual knowing his or her place in the natural order, and playing his or her part well. When Duke Jing of Qi asked about government, by which he meant proper administration so as to bring social harmony, Confucius replied:
There is government, when the prince is prince, and the minister is minister; when the father is father, and the son is son. ("Analects" XII, 11, trans. Legge)
Particular duties arise from one's particular situation in relation to others. The individual stands simultaneously in several different relationships with different people: as a junior in relation to parents and elders, and as a senior in relation to younger siblings, students, and others. While juniors are considered in Confucianism to owe their seniors reverence, seniors also have duties of benevolence and concern toward juniors. The same is true with the husband and wife relationship where the husband needs to show benevolence towards his wife and the wife needs to respect the husband in return. This theme of mutuality still exists in East Asian cultures even to this day.
The Five Bonds are: ruler to ruled, father to son, husband to wife, elder brother to younger brother, friend to friend. Specific duties were prescribed to each of the participants in these sets of relationships. Such duties are also extended to the dead, where the living stand as sons to their deceased family. The only relationship where respect for elders isn't stressed was the friend to friend relationship, where mutual equal respect is emphasized instead. In all other relationships, high reverence is usually held for elders.
"Junzi".
The "junzi" (, "jūnzǐ", "lord's son") is a Chinese philosophical term often translated as "gentleman" or "superior person" and employed by Confucius in his works to describe the ideal man. In the "I Ching" it is used by the Duke of Wen.
In Confucianism, the sage or wise is the ideal personality; however, it is very hard to become one of them. Confucius created the model of "junzi", gentleman, which can be achieved by any individual. Later, Zhu Xi defined "junzi" as second only to the sage. There are many characteristics of the "junzi": he can live in poverty, he does more and speaks less, he is loyal, obedient and knowledgeable. The "junzi" disciplines himself. "Ren" is fundamental to become a "junzi".
As the potential leader of a nation, a son of the ruler is raised to have a superior ethical and moral position while gaining inner peace through his virtue. To Confucius, the "junzi" sustained the functions of government and social stratification through his ethical values. Despite its literal meaning, any righteous man willing to improve himself can become a "junzi".
On the contrary, the "xiaoren" (小人, "xiăorén", "small or petty person") does not grasp the value of virtues and seeks only immediate gains. The petty person is egotistic and does not consider the consequences of his action in the overall scheme of things. Should the ruler be surrounded by "xiaoren" as opposed to "junzi", his governance and his people will suffer due to their small-mindness. Examples of such "xiaoren" individuals can range from those who continually indulge in sensual and emotional pleasures all day to the politician who is interested merely in power and fame; neither sincerely aims for the long-term benefit of others.
The "junzi" enforces his rule over his subjects by acting virtuously himself. It is thought that his pure virtue would lead others to follow his example. The ultimate goal is that the government behaves much like a family, the "junzi" being a beacon of filial piety.
Rectification of names.
Confucius believed that social disorder often stemmed from failure to perceive, understand, and deal with reality. Fundamentally, then, social disorder can stem from the failure to call things by their proper names, and his solution to this was "zhèngmíng" (). He gave an explanation of "zhengming" to one of his disciples.
Zi-lu said, "The vassal of Wei has been waiting for you, in order with you to administer the government. What will you consider the first thing to be done?"<br>
The Master replied, "What is necessary to rectify names."<br>
"So! indeed!" said Zi-lu. "You are wide off the mark! Why must there be such rectification?"<br>
The Master said, "How uncultivated you are, Yu! The superior man unz cannot care about the everything, just as he cannot go to check all himself!<br>
        If names be not correct, language is not in accordance with the truth of things.<br>
        If language be not in accordance with the truth of things, affairs cannot be carried on to success.<br>
        When affairs cannot be carried on to success, proprieties and music do not flourish.<br>
        When proprieties and music do not flourish, punishments will not be properly awarded.<br>
        When punishments are not properly awarded, the people do not know how to move hand or foot.<br>
Therefore a superior man considers it necessary that the names he uses may be spoken appropriately, and also that what he speaks may be carried out appropriately. What the superior man requires is just that in his words there may be nothing incorrect."<br>
Xun Zi chapter (22) "On the Rectification of Names" claims the ancient sage-kings chose names () that directly corresponded with actualities (), but later generations confused terminology, coined new nomenclature, and thus could no longer distinguish right from wrong. Since social harmony is of utmost importance, without the proper rectification of names, society would essentially crumble and "undertakings oul not completed." 
History.
Confucianism can be traced even before the birth of its namesake, Confucius (an Anglicization of his actual name, Kong Qiu), to the general culture of the Zhou Dynasty, which emphasized politeness and consideration, though generally with more of a spiritual bent. Thought to be a real historic figure, Confucius was born on September 28, 551 BC. He reportedly grew up in a time of instability in the region that would someday be known as China, and failed in his ambitions to become a high minister of the national government. But he did become known for his attempts to analyze and codify rules of society and behavior. The system of "virtue" he proposed was one of respect for others, including for their position in society, focusing on this as a system of principles, not mysticism.
The Analects that are generally attributed to Confucius actually appear to have been compiled after his death, by followers one or two generations removed, perhaps during the Warring States period (476 BC-221 BC), though no copies exist older than 50 BC, with some scholars saying the document may have been compiled as recently as 140 BC.
Confucianism went through a number of phases of being repressed or unpopular, as in the earlier part of the Han dynasty, or being tolerated, even accepted, with the later Han years being an example of this. It is not until the 12th century AD, though, that it has become such an accepted part of the state that the Analects themselves are integrated into civil service tests.
In fact, this success came via Neo-Confucianism, an attempt to reform the philosophy, which had been influenced by Taoism and Buddhism and was seen as moving toward mysticism and superstition. This movement started as early as the 8th century AD, and was dominant by the 12th. While still influenced by Taoism and Buddhism, it worked to restore Confucianism to what were seen as its secular roots.
The influence of Confucianism increased after the conquest of the region by the Mongol empire, whose Khans were convinced to adopt its philosophy for their own government.
Organisation and liturgy.
Since the 2000s, some intellectuals and students in China have become increasingly identified with Confucianism. In 2003 the Confucian intellectual Kang Xiaoguang published a manifesto in which he made four suggestions: Confucian education should enter official education at any level, from elementary to high school; the state should establish Confucianism as the state religion by law; Confucian religion should enter the daily life of ordinary people through standardization and development of doctrines, rituals, organisations, churches and activity sites; the Confucian religion should be spread through non-governmental organisations. Another modern proponent of the institutionalisation of Confucianism in a state church is Jiang Qing.
In 2005 the Center for the Study of Confucian Religion was established, and "guoxue" education started to be implemented in public schools. Being well received by the population, even Confucian preachers started to appear on television since 2006. The most enthusiast New Confucians proclaim the uniqueness and superiority of Confucian Chinese culture, and have generated some popular sentiment against Western cultural influences in China.
The idea of a "Confucian Church" as the state religion of China has roots in the thought of Kang Youwei, an exponent of the early New Confucian search for a regeneration of the social relevance of Confucianism, at a time when it was de-institutionalised with the collapse of the Qing dynasty and the Chinese empire. Kang modeled his ideal "Confucian Church" after European national Christian churches, as a hierarchical and centralised institution, closely bound to the state, with local church branches, devoted to the worship and the spread of the teachings of Confucius.
In contemporary China, the Confucian revival has developed into different, yet interwoven, directions: the proliferation of Confucian schools or academies ("shuyuan" 书院), the resurgence of Confucian rites ("chuantong liyi"), and the birth of new forms of Confucian activity on the popular level, such as the Confucian communities ("shequ ruxue" 社区儒学). Some scholars also consider the reconstruction of lineage churches and their ancestral temples, as well as cults and temples of natural and national gods within broader Chinese traditional religion, as part of the revival of Confucianism.
Other forms of revival are folk religious or salvationist religious groups with a specifically Confucian focus, or Confucian churches, for example the "Yidan xuetang" (一耽学堂) based in Beijing, the "Mengmutang" (孟母堂) of Shanghai, the Way of the Gods according to the Confucian Tradition or phoenix churches, the Confucian Fellowship (儒教道坛 "Rújiào Dàotán") in northern Fujian which has spread rapidly over the years after its foundation, and ancestral temples of the Kong (Confucius) kin operating as well as Confucian-teaching churches.
Also, the Hong Kong Confucian Academy has expanded its activities to the mainland, with the construction of statues of Confucius, Confucian hospitals, restoration of temples and sponsorship of other activities. In 2009 Zhou Beichen founded another institution that inherits the idea of Kang Youwei's Confucian Church, the Holy Hall of Confucius (孔圣堂 "Kǒngshèngtáng") in Shenzhen affiliated with the Federation of Confucian Culture of Qufu City, the first of a nationwide movement of congregations and civil organisations that was unified in 2015 by the Holy Confucian Church (孔圣会 "Kǒngshènghuì"). The first spiritual leader of the Holy Church is the renowned scholar Jiang Qing.
Chinese folk religion's temples and kinship ancestral shrines on special occasions may choose Confucian liturgy (that is called 儒 "rú", or sometimes 正统 "zhèngtǒng", meaning "orthoprax" ritual style) led by Confucian priests (礼生 "lǐshēng") to worship the gods enshrined, instead of Taoist or popular ritual. "Confucian businessmen" ("rushang", also "learned businessman"), is a recently recovered term that defines people of the entrepreneurial or economic elite that recognise their social responsibility and therefore apply Confucian culture to their business.
Governance.
To govern by virtue, let us compare it to the North Star: it stays in its place, while the myriad stars wait upon it. ("Analects" 2.1)
A key Confucian concept is that in order to govern others one must first govern oneself according to the universal order. When actual, the king's personal virtue ("de") spreads beneficent influence throughout the kingdom. This idea is developed further in the Great Learning, and is tightly linked with the Taoist concept of wu wei (): the less the king does, the more gets done. By being the "calm center" around which the kingdom turns, the king allows everything to function smoothly and avoids having to tamper with the individual parts of the whole.
This idea may be traced back to the ancient shamanic beliefs of the king being the axle between the sky, human beings, and the Earth, reflected in the Chinese idea of the Mandate of Heaven.
Meritocracy.
In teaching, there should be no distinction of classes. ("Analects" 15.39)
Although Confucius claimed that he never invented anything but was only transmitting ancient knowledge ("Analects" 7.1), he did produce a number of new ideas. Many European and American admirers such as Voltaire and H. G. Creel point to the revolutionary idea of replacing nobility of blood with nobility of virtue. "Jūnzǐ" (君子, lit. "lord's child"), which originally signified the younger, non-inheriting, offspring of a noble, became, in Confucius' work, an epithet having much the same meaning and evolution as the English "gentleman".
A virtuous plebeian who cultivates his qualities can be a "gentleman", while a shameless son of the king is only a "small man". That he admitted students of different classes as disciples is a clear demonstration that he fought against the feudal structures that defined pre-imperial Chinese society.
Another new idea, that of meritocracy, led to the introduction of the imperial examination system in China. This system allowed anyone who passed an examination to become a government officer, a position which would bring wealth and honour to the whole family. The Chinese imperial examination system started in the Sui dynasty. Over the following centuries the system grew until finally almost anyone who wished to become an official had to prove his worth by passing written government examinations. The practice of meritocracy still exists today in the Chinese cultural sphere, including China, Taiwan, Singapore and so forth.
Influence.
In 17th-century Europe.
The works of Confucius were translated into European languages through the agency of Jesuit scholars stationed in China. Matteo Ricci was among the very earliest to report on the thoughts of Confucius, and father Prospero Intorcetta wrote about the life and works of Confucius in Latin in 1687.
Translations of Confucian texts influenced European thinkers of the period, particularly among the Deists and other philosophical groups of the Enlightenment who were interested by the integration of the system of morality of Confucius into Western civilization.
Confucianism influenced Gottfried Leibniz, who was attracted to the philosophy because of its perceived similarity to his own. It is postulated that certain elements of Leibniz's philosophy, such as "simple substance" and "preestablished harmony", were borrowed from his interactions with Confucianism. The French philosopher Voltaire was also influenced by Confucius, seeing the concept of Confucian rationalism as an alternative to Christian dogma. He praised Confucian ethics and politics, portraying the sociopolitical hierarchy of China as a model for Europe.
On Islamic thought.
From the late 17th century onwards a whole body of literature known as the Han Kitab developed amongst the Hui Muslims of China who infused Islamic thought with Confucianism. Especially the works of Liu Zhi such as "Tiānfāng Diǎnlǐ"（天方典禮）sought to harmonize Islam with not only Confucianism but also with Daoism and is considered to be one of the crowning achievements of the Chinese Islamic culture.
In modern times.
Important military and political figures in modern Chinese history continued to be influenced by Confucianism, like the Muslim warlord Ma Fuxiang. The New Life Movement in the early 20th century was also influenced by Confucianism.
Referred to variously as the Confucian hypothesis and as a debated component of the more all-encompassing Asian Development Model, there exists among political scientists and economists a theory that Confucianism plays a large latent role in the ostensibly non-Confucian cultures of modern-day East Asia, in the form of the rigorous work ethic it endowed those cultures with. These scholars have held that, if not for Confucianism's influence on these cultures, many of the people of the East Asia region would not have been able to modernize and industrialize as quickly as Singapore, Malaysia, Hong Kong, Taiwan, Japan, South Korea and even China has done.
For example, the impact of the Vietnam War on Vietnam was devastating, however over the last few decades Vietnam has been re-developing in a very fast pace. Most scholars attribute the origins of this idea to futurologist Herman Kahn's "World Economic Development: 1979 and Beyond".
Other studies, for example Cristobal Kay's "Why East Asia Overtook Latin America: Agrarian Reform, Industrialization, and Development", have attributed the Asian growth to other factors, for example the character of agrarian reforms, "state-craft" (state capacity), and interaction between agriculture and industry.
On Chinese martial arts.
After Confucianism had become the official 'state religion' in China, its influence penetrated all walks of life and all streams of thought in Chinese society for the generations to come. This did not exclude martial arts culture. Though in his own day, Confucius had rejected the practice of Martial Arts (with the exception of Archery), he did serve under rulers who used military power extensively to achieve their goals. In later centuries, Confucianism heavily influenced many educated martial artists of great influence, such as Sun Lutang, especially from the 19th century onwards, when empty-handed martial arts in China became more widespread and had begun to more readily absorb philosophical influences from Confucianism, Buddhism and Daoism. Some argue therefore that despite Confucius' disdain with martial culture, his teachings became of much relevance to it.
Criticism.
For many years since the era of Confucius, various critiques of Confucianism have arisen, including Laozi's philosophy and Mozi's critique. Lu Xun also criticised Confucianism heavily for shaping Chinese people into the condition they had reached by the late Qing Dynasty: his criticisms are well portrayed in two of his works, "A Madman's Diary" and "The True Story of Ah Q".
In modern times, waves of critique along with vilification against Confucianism arose. The Taiping Rebellion, New Culture Movement and Cultural Revolution are some upsurges of those waves in China. Taiping rebels described many sages in Confucianism as well as gods in Taoism and Buddhism as mere legends. Marxists during the Cultural Revolution described Confucius as the general representative of the class of slave owners. Numerous opinions and interpretations of Confucianism (of which many are actually opposed by Confucianism) were invented.
In South Korea, there has long been criticism of Confucianism. Many Koreans believe Confucianism has not contributed to the modernization of Korea. For example, South Korean writer Kim Kyong-il wrote an essay entitled "Confucius Must Die For the Nation to Live" (공자가 죽어야 나라가 산다, "gongjaga jug-eoya naraga sanda"). Kim said that filial piety is one-sided and blind, and if it continues social problems will continue as government keeps forcing Confucian filial obligations onto families.
Women in Confucian thought.
Confucianism "largely defined the mainstream discourse on gender in China from the Han dynasty onward." The often strict, obligatory gender roles based on Confucian teachings became a cornerstone of the family, and thus, societal stability. Starting from the Han period onward, Confucians in general began to gradually teach that a virtuous woman was supposed to follow the lead of the males in her family, especially the father before her marriage and the husband after she marries. In the later dynasties, more emphasis was placed on women to uphold the virtue of chastity when they lost their husbands. Chaste widows were revered as heroes during the Ming and Qing periods. This "cult of chastity" accordingly, "condemned many widows to poverty and loneliness by placing a social stigma on remarriage by women."
Confucianism was usually characterized by Western scholarship, up until the mid-1990s, as a sexist, patriarchal ideology that was responsible for the severe subjugation and oppression of women in pre-modern China. However, recent reexaminations of Chinese gender roles suggest that some women can flourish within Confucianism. During the Han dynasty period, the important Confucian text Lessons for Women (Nüjie), was written by Ban Zhao (45–114 CE): by a woman, for women.
She wrote the "Nüjie" ostensibly for her daughters, instructing them on how to live proper Confucian lives as wives and mothers. Although this is a relatively rare instance of a female Confucian voice, Ban Zhao almost entirely accepts the prevailing views concerning women's proper roles; they should be silent, hard-working, and compliant. She stresses the complementarity and equal importance of the male and female roles according to yin-yang theory, but she clearly accepts the dominance of the yang-male. Her only departure from the standard male versions of this orthodoxy is that she insists on the necessity of educating girls and women. We should not underestimate the significance of this point, as education was the bottom line qualification for being a junzi or "noble person,"...her example suggests that the Confucian prescription for a meaningful life as a woman was apparently not stifling for all women. Even some women of the literate elite, for whom Confucianism was quite explicitly the norm, were able to flourish by living their lives according to that model.
Joseph A. Adler has also indicated that even with the Neo-Confucians who have the reputation of discriminating against women, the actual situation was in fact quite complicated. As he writes, "Neo-Confucian writings do not necessarily reflect either the prevailing social practices or the scholars' own attitudes and practices in regard to actual women." There had been a difference between textual teaching and the actual social practice by the Confucians and society in general throughout all of China's dynasties.
Matthew Sommers has also indicated that during the Qing dynasty, the imperial government began to realize the utopian nature of enforcing the "cult of chastity." As a result, by the late Qing period, Qing officials became more tolerant and allowed practices such as widow remarrying to stand. Finally, some Confucian texts like the "Chunqiu Fanlu" 春秋繁露 also has passages which suggest a more equal relationship between a husband and his wife. All of these things add to the complexity of the issue of women in Confucian teaching.
In 2009, for the first time women (and ethnic minorities and people living overseas) were officially recognized as being descendants of Confucius. These additions more than tripled the number of officially recognized descendants of Confucius.
Catholic controversy over Chinese rites.
Ever since Europeans first encountered Confucianism, the issue of how Confucianism should be classified has been subject to debate. In the 16th and the 17th centuries, the earliest European arrivals in China, the Christian Jesuits, considered Confucianism to be an ethical system, not a religion, and one that was compatible with Christianity. The Jesuits, including Matteo Ricci, saw Chinese rituals as "civil rituals" that could co-exist alongside the spiritual rituals of Catholicism.
By the early 18th century, this initial portrayal was rejected by the Dominicans and Franciscans, creating a dispute among Catholics in East Asia that was known as the "Rites Controversy". The Dominicans and Franciscans argued that Chinese ancestral worship was a form of idolatry that was contradictory to the tenets of Christianity. This view was reinforced by Pope Benedict XIV, who ordered a ban on Chinese rituals.
Confucianism is definitively pantheistic, nontheistic and humanistic, in that it is not based on the belief in the supernatural or in a personal god that doesn't impact reality. On spirituality, Confucius said to Chi Lu, one of his students: "You are not yet able to serve men, how can you serve spirits?" Attributes such as ancestor worship, ritual, and sacrifice were advocated by Confucius as necessary for social harmony; these attributes can be traced to the traditional Chinese folk religion.
Scholars recognize that classification ultimately depends on how one defines religion. Using stricter definitions of religion, Confucianism has been described as a moral science or philosophy. But using a broader definition, such as Frederick Streng's characterization of religion as "a means of ultimate transformation", Confucianism could be described as a "sociopolitical doctrine having religious qualities." With the latter definition, Confucianism is religious, even if non-theistic, in the sense that it "performs some of the basic psycho-social functions of full-fledged religions".

</doc>
<doc id="5822" url="https://en.wikipedia.org/wiki?curid=5822" title="Chinese philosophy">
Chinese philosophy

Chinese philosophy originates in the Spring and Autumn and Warring States eras, during a period known as the "Hundred Schools of Thought", which was characterized by significant intellectual and cultural developments. Although much of Chinese philosophy begins in the Warring States period, elements of Chinese philosophy have existed for several thousand years; some can be found in the Yi Jing (the "Book of Changes"), an ancient compendium of divination, which dates back to at least 672 BCE. It was during the Warring States era that what Sima Tan termed the major philosophical schools of China, Confucianism, Legalism, and Daoism, arose, along with philosophies that later fell into obscurity, like Agriculturalism, Mohism, Chinese Naturalism, and the Logicians.
Early beliefs.
Early Shang Dynasty thought was based upon cycles. This notion stems from what the people of the Shang Dynasty could observe around them: day and night cycled, the seasons progressed again and again, and even the moon waxed and waned until it waxed again. Thus, this notion, which remained relevant throughout Chinese history, reflects the order of nature. In juxtaposition, it also marks a fundamental distinction from western philosophy, in which the dominant view of time is a linear progression. During the Shang, fate could be manipulated by great deities, commonly translated as gods. Ancestor worship was present and universally recognized. There was also human and animal sacrifice.
When the Shang were overthrown by the Zhou, a new political, religious and philosophical concept was introduced called the "Mandate of Heaven". This mandate was said to be taken when rulers became unworthy of their position and provided a shrewd justification for Zhou rule. During this period, archaeological evidence points to an increase in literacy and a partial shift away from the faith placed in Shangdi (the Supreme Being in traditional Chinese religion), with ancestor worship becoming commonplace and a more worldly orientation coming to the fore.
Overview.
Confucianism developed during the Spring and Autumn Period from the teachings of the Chinese philosopher Confucius (551–479 BCE), who considered himself a retransmitter of Zhou values. His philosophy concerns the fields of ethics and politics, emphasizing personal and governmental morality, correctness of social relationships, justice, traditionalism, and sincerity. The Analects stress the importance of ritual, but also the importance of 'ren', which loosely translates as 'human-heartedness, Confucianism, along with Legalism, is responsible for creating the world’s first meritocracy, which holds that one's status should be determined by education and character rather than ancestry, wealth, or friendship. Confucianism was and continues to be a major influence in Chinese culture, the state of China and the surrounding areas of Southeast Asia.
Before the Han dynasty the largest rivals to Confucianism were Chinese Legalism, and Mohism. Confucianism largely became the dominant philosophical school of China during the early Han Dynasty following the replacement of its contemporary, the more Taoistic Huang-Lao. Legalism as a coherent philosophy disappeared largely due to its relationship with the unpopular authoritarian rule of Qin Shi Huang, however, many of its ideas and institutions would continue to influence Chinese philosophy until the end of Imperial rule during the Xinhai Revolution.
Mohism, though initially popular due to its emphasis on brotherly love versus harsh Qin Legalism, fell out of favour during the Han Dynasty due to the efforts of Confucians in establishing their views as political orthodoxy. The Six Dynasties era saw the rise of the Xuanxue philosophical school and the maturation of Chinese Buddhism, which had entered China from India during the Late Han Dynasties. By the time of the Tang dynasty five-hundred years after Buddhism's arrival into China, it had transformed into a thoroughly Chinese religious philosophy dominated by the school of Zen Buddhism. Neo-Confucianism became highly popular during the Song dynasty and Ming Dynasty due in large part to the eventual combination of Confucian and Zen Philosophy.
During the 19th and 20th centuries, Chinese philosophy integrated concepts from Western philosophy. Anti-Qing Dynasty revolutionaries, involved in the Xinhai Revolution, saw Western philosophy as an alternative to traditional philosophical schools; students in the May Fourth Movement called for completely abolishing the old imperial institutions and practices of China. During this era, Chinese scholars attempted to incorporate Western philosophical ideologies such as democracy, Marxism, socialism, liberalism, republicanism, anarchism and nationalism into Chinese philosophy. The most notable examples are Sun Yat-Sen's Three Principles of the People ideology and Mao Zedong's Maoism, a variant of Marxism–Leninism. In the modern People's Republic of China, the official ideology is Deng Xiaoping's "market economy socialism".
Although the People's Republic of China has been historically hostile to the philosophy of ancient China, the influences of past are still deeply ingrained in the Chinese culture. In the post-Chinese economic reform era, modern Chinese philosophy has reappeared in forms such as the "New Confucianism". As in Japan, philosophy in China has become a melting pot of ideas. It accepts new concepts, while attempting also to accord old beliefs their due. Chinese philosophy still carries profound influence amongst the people of East Asia, and even Southeast Asia.
Ancient Philosophy.
Spring and Autumn Period.
Around 500 BCE, after the Zhou state weakened and China moved into the Spring and Autumn Period, the classic period of Chinese philosophy began (it is an interesting fact that this date nearly coincides with the emergence of the first Greek philosophers). This is known as the Hundred Schools of Thought (諸子百家; "zhūzǐ bǎijiā"; "various scholars, hundred schools"). This period is considered the golden age of Chinese philosophy. Of the many schools founded at this time and during the subsequent Warring States period, the four most influential ones were Confucianism, Daoism (often spelled "Taoism"), Mohism and Legalism.
Confucianism.
Confucianism is a philosophical school developed from the teachings of Confucius collected in the "The Analects", and of Mencius in "The Mencius" and Xunzi in "The Xunzi". It is a system of moral, social, political, and religious thought that has had tremendous influence on Chinese history, thought, and culture down to the 21st century. Some Westerners have considered it to have been the "state religion" of imperial China. Its influence also spread to Korea and Japan.
The major Confucian concepts include "rén" 仁 (humanity or humaneness), "zhèngmíng" 正名 (rectification of names; e.g. a ruler who rules unjustly is no longer a ruler and may be dethroned), "zhōng" 忠 (loyalty), "xiào" 孝 (filial piety), and "li" 禮 (ritual). Confucius taught both positive and negative versions of the Golden Rule. The concepts Yin and Yang represent two opposing forces that are permanently in conflict with each other, leading to perpetual contradiction and change. The Confucian idea of "Rid of the two ends, take the middle" is a Chinese equivalent of Hegel's idea of "thesis, antithesis, and synthesis", which is a way of reconciling opposites, arriving at some middle ground combining the best of both.
Daoism.
Daoism is a philosophy and later also developed into a religion based on the texts the "Tao Te Ching" (Dào Dé Jīng; ascribed to Laozi) and the "Zhuangzi" (partly ascribed to Zhuangzi). The character "Dao" 道 ("Dao") literally means "path" or "way". However, in Daoism it refers more often to a meta-physical term that describes a force that encompasses the entire universe but which cannot be described nor felt. All major Chinese philosophical schools have investigated the correct "Way" to go about a moral life, but in Taoism it takes on the most abstract meanings, leading this school to be named after it. It advocated nonaction ("wu wei"), the strength of softness, spontaneity, and relativism. Although it serves as a rival to Confucianism, a school of active morality, this rivalry is compromised and given perspective by the idiom "practise Confucianism on the outside, Taoism on the inside."
Most of Taoism's focus is on what is perceived to be the undeniable fact that human attempts to make the world better actually make the world worse. Therefore, it is better to strive for harmony, minimising potentially harmful interference with nature or in human affairs.
Warring States period.
Legalism.
Philosopher Han Fei synthesized together earlier the methods of his predecessors, which famous historian Sima Tan posthumously termed Legalism. With an essential principle like "when the epoch changed, the ways changed", late pre-Han Dynasty reformers emphasized rule by law.
In Han Fei's philosophy, a ruler should govern his subjects by the following trinity:
What has been termed by some as the intrastate Realpolitik of the Warring States period was highly progressive, and extremely critical of the Confucian and Mohist schools. But that of the Qin Dynasty would be blamed for creating a totalitarian society, thereby experiencing decline. Its main motto is: "Set clear strict laws, or deliver harsh punishment". In Han Fei's philosophy the ruler possessed authority regarding reward and penalty, enacted through law. Shang Yang and Han Fei promoted absolute adherence to the law, regardless of the circumstances or the person. Ministers were only to be rewarded if their words were accurate to the results of their proposals. Legalism, in accordance with Shang Yang's interpretation, could encourage the state to be a militaristic autarky.
Naturalists.
The School of Naturalists or the School of Yin-yang (陰陽家/阴阳家; "Yīnyángjiā"; "Yin-yang-chia"; "School of Yin-Yang") was a Warring States era philosophy that synthesized the concepts of yin-yang and the Five Elements; Zou Yan is considered the founder of this school. His theory attempted to explain the universe in terms of basic forces in nature: the complementary agents of yin (dark, cold, female, negative) and yang (light, hot, male, positive) and the Five Elements or Five Phases (water, fire, wood, metal, and earth). In its early days, this theory was most strongly associated with the states of Yan and Qi. In later periods, these epistemological theories came to hold significance in both philosophy and popular belief. This school was absorbed into Taoism's alchemic and magical dimensions as well as into the Chinese medical framework. The earliest surviving recordings of this are in the Ma Wang Dui texts and Huang Di Nei Jing.
Mohism.
Mohism (Moism), founded by Mozi (墨子), promotes universal love with the aim of mutual benefit. Everyone must love each other equally and impartially to avoid conflict and war. Mozi was strongly against Confucian ritual, instead emphasizing pragmatic survival through farming, fortification, and statecraft. Tradition is inconsistent, and human beings need an extra-traditional guide to identify which traditions are acceptable. The moral guide must then promote and encourage social behaviors that maximize general benefit. As motivation for his theory, Mozi brought in the "Will of Heaven", but rather than being religious his philosophy parallels utilitarianism.
Logicians.
The logicians (School of Names) were concerned with logic, paradoxes, names and actuality (similar to Confucian rectification of names). The logician Hui Shi was a friendly rival to Zhuangzi, arguing against Taoism in a light-hearted and humorous manner. Another logician, Gongsun Long, told the famous When a White Horse is Not a Horse dialogue. This school did not thrive because the Chinese regarded sophistry and dialectic as impractical.
Agriculturalists.
Agriculturalism was an early agrarian social and political philosophy that advocated peasant utopian communalism and egalitarianism. The philosophy is founded on the notion that human society originates with the development of agriculture, and societies are based upon "people's natural prospensity to farm."
The Agriculturalists believed that the ideal government, modeled after the semi-mythical governance of Shennong, is led by a benevolent king, one who works alongside the people in tilling the fields. The Agriculturalist king is not paid by the government through its treasuries; his livelihood is derived from the profits he earns working in the fields, not his leadership. Unlike the Confucians, the Agriculturalists did not believe in the division of labour, arguing instead that the economic policies of a country need to be based upon an egalitarian self sufficiency. The Agriculturalists supported the fixing of prices, in which all similar goods, regardless of differences in quality and demand, are set at exactly the same, unchanging price.
Early Imperial era philosophy.
History.
Qin and Han Dynasties.
The short founder Qin Dynasty, where Legalism was the official philosophy, quashed Mohist and Confucianist schools. Legalism remained influential during the early Han Dynasty under the Taoist-Realist ideology Huang-Lao until Emperor Wu of Han adopted Confucianism as official doctrine. Confucianism and Taoism became the determining forces of Chinese thought until the introduction of Buddhism.
Confucianism was particularly strong during the Han Dynasty, whose greatest thinker was Dong Zhongshu, who integrated Confucianism with the thoughts of the Zhongshu School and the theory of the Five Elements. He also was a promoter of the New Text school, which considered Confucius as a divine figure and a spiritual ruler of China, who foresaw and started the evolution of the world towards the Universal Peace. In contrast, there was an Old Text school that advocated the use of Confucian works written in ancient language (from this comes the denomination "Old Text") that were so much more reliable. In particular, they refuted the assumption of Confucius as a godlike figure and considered him as the greatest sage, but simply a human and mortal
Six Dynasties.
The 3rd and 4th centuries saw the rise of the "Xuanxue" (mysterious learning), also called "Neo-Taoism". The most important philosophers of this movement were Wang Bi, Xiang Xiu and Guo Xiang. The main question of this school was whether Being came before Not-Being (in Chinese, "ming" and "wuming"). A peculiar feature of these Taoist thinkers, like the Seven Sages of the Bamboo Grove, was the concept of "feng liu" (lit. wind and flow), a sort of romantic spirit which encouraged following the natural and instinctive impulse.
Buddhism arrived in China around the 1st century AD, but it was not until the Northern and Southern, Sui and Tang Dynasties that it gained considerable influence and acknowledgement. At the beginning, it was considered a sort of Taoist sect. Mahayana Buddhism was far more successful in China than its rival Hinayana, and both Indian schools and local Chinese sects arose from the 5th century. Two chiefly important monk philosophers were Sengzhao and Daosheng. But probably the most influential and original of these schools was the Chan sect, which had an even stronger impact in Japan as the Zen sect.
In the mid-Tang Buddhism reached its peak, and reportedly there were 4,600 monasteries, 40,000 hermitages and 260,500 monks and nuns. The power of the Buddhist clergy was so great and the wealth of the monasteries so impressive, that it instigated criticism from Confucian scholars, who considered Buddhism as a foreign religion. In 845 Emperor Wuzong ordered the Great Anti-Buddhist Persecution, confiscating the riches and returning monks and nuns to lay life. From then on, Buddhism lost much of its influence.
Schools of thought.
Xuanxue.
Xuanxue was a philosophical school that combined elements of Confucianism and Taoism to reinterpret the "Yijing," "Daodejing," and "Zhuangzi." The most important philosophers of this movement were Wang Bi, Xiang Xiu and Guo Xiang. The main question of this school was whether Being came before Not-Being (in Chinese, "ming" and "wuming"). A peculiar feature of these Taoist thinkers, like the Seven Sages of the Bamboo Grove, was the concept of "feng liu" (lit. wind and flow), a sort of romantic spirit which encouraged following the natural and instinctive impulse.
Zen.
Buddhism is a religion, a practical philosophy, and arguably a psychology, focusing on the teachings of Gautama Buddha, who lived on the Indian subcontinent most likely from the mid-6th to the early 5th century BCE. When used in a generic sense, a Buddha is generally considered to be someone who discovers the true nature of reality.
Buddhism until the 4th century A.D had it little impact on China but in the 4th century its teachings hybridized with those of Taoism. Buddhism brought to China the idea of many hells, where sinners went, but the deceased sinners souls could be saved by pious acts. Since Chinese traditional thought focused more on ethics rather than metaphysics, the merging of Buddhist and Taoist concepts developed several schools distinct from the originating Indian schools. The most prominent examples with philosophical merit are Sanlun, Tiantai, Huayan, and Chán (a.k.a. Zen). They investigate consciousness, levels of truth, whether reality is ultimately empty, and how enlightenment is to be achieved. Buddhism has a spiritual aspect that compliments the action of Neo-Confucianism, with prominent Neo-Confucians advocating certain forms of meditation.
Mid to Late Imperial era philosophy.
History.
Neo-Confucianism was a revived version of old Confucian principles that appeared around the Song dynasty, with Buddhist, Taoist, and Legalist features. The first philosophers, such as Shao Yong, Zhou Dunyi and Chang Zai, were cosmologists and worked on the Yi Jing. The Cheng brothers, Cheng Yi and Cheng Hao, are considered the founders of the two main schools of thought of Neo-Confucianism: the School of Principle the first, the School of Mind the latter. The School of Principle gained supremacy during the Song dynasty with the philosophical system elaborated by Zhu Xi, which became mainstream and officially adopted by the government for the Imperial examinations under the Yuan Dynasty. The School of Mind was developed by Lu Jiuyuan, Zhu Xi's main rival, but was soon forgotten. Only during the Ming Dynasty was the School of Mind revived by Wang Shouren, whose influence is equal to that of Zhu Xi. This school was particularly important in Japan.
During the Qing Dynasty many philosophers objected against Neo-Confucianism and there was a return to the Han Dynasty Confucianism, and also the reprise of the controversy between Old Text and New Text. In this period also started the penetration of Western culture, but most Chinese thought that the Westerners were maybe more advanced in technology and warfare, but that China had primacy in moral and intellectual fields.
Schools of thought.
Neo-Confucianism.
Despite Confucianism losing popularity to Taoism and Buddhism, Neo-Confucianism combined those ideas into a more metaphysical framework. Its concepts include "li" (principle, akin to Plato's forms), "qi" (vital or material force), "taiji" (the Great Ultimate), and "xin" (mind). Song Dynasty philosopher Zhou Dunyi (1017–1073) is seen commonly seen as the first true "pioneer" of Neo-Confucianism, using Daoist metaphysics as a framework for his ethical philosophy. Neo-Confucianism developed both as a renaissance of traditional Confucian ideas, and as a reaction to the ideas of Buddhism and religious Daoism. Although the Neo-Confucianists denounced Buddhist metaphysics, Neo-Confucianism did borrow Daoist and Buddhist terminology and concepts.
Neo-Confucianist philosophers like Zhu Xi and Wang Yangming are seen as the most important figures of Neo-Confucianism.
Modern era.
During the Industrial and Modern Ages, Chinese philosophy had also begun to integrate concepts of Western philosophy, as steps toward modernization. Notably, Chinese philosophy never developed the concept of rights, let alone human rights, so that classical Chinese lacked words for them. In 1864, W.A.P. Martin had to invent the word "quanli" to translate the Western concept of "rights" in the process of translating Henry Wheaton's "Elements of International Law" into classical Chinese.
By the time of the Xinhai Revolution in 1911, there were many calls, such as the May Fourth Movement, to completely abolish the old imperial institutions and practices of China. There have been attempts to incorporate democracy, republicanism, and industrialism into Chinese philosophy, notably by Sun Yat-Sen ("Sūn Yìxiān", in one Mandarin form of the name) at the beginning of the 20th century. Mao Zedong ("Máo Zédōng") added Marxism, Stalinism, Chinese Marxist Philosophy and other communist thought.
When the Communist Party of China took over power, previous schools of thought, excepting notably Legalism, were denounced as backward, and later even purged during the Cultural Revolution. Their influence on Chinese thought, however, remains. The current government of the People's Republic of China is trying to encourage a form of market socialism.
Since the radical movement of the Cultural Revolution, the Chinese government has become much more tolerant with the practice of traditional beliefs. The 1978 Constitution of the People's Republic of China guarantees "freedom of religion" with a number of restrictions. Spiritual and philosophical institutions have been allowed to be established or re-established, as long they are not perceived to be a threat to the power of the CPC. (However, it should be noted that those organizations are heavily monitored by the state.) The influences of the past are still deeply ingrained in the Chinese culture. As in Japan, philosophy in China has become a melting pot of ideas. It accepts new concepts, while attempting also to accord old beliefs their due.
New Confucianism.
New Confucianism is an intellectual movement of Confucianism that began in the early 20th century in Republican China, and revived in post-Mao era contemporary China. It is deeply influenced by, but not identical with, the Neo-Confucianism of the Song and Ming dynasties.
Concepts within Chinese philosophy.
Although the individual philosophical schools differ considerably, they nevertheless share a common vocabulary and set of concerns.
Among the terms commonly found in Chinese philosophy are:
Among the commonalities of Chinese philosophies are:

</doc>
<doc id="5823" url="https://en.wikipedia.org/wiki?curid=5823" title="Confucius">
Confucius

Confucius (孔夫子 Kǒng fūzǐ, literally "Master Kong", , ; September 28, 551 – 479 BC) was a Chinese teacher, editor, politician, and philosopher of the Spring and Autumn period of Chinese history.
The philosophy of Confucius emphasized personal and governmental morality, correctness of social relationships, justice and sincerity. His followers competed successfully with many other schools during the Hundred Schools of Thought era only to be suppressed in favor of the Legalists during the Qin Dynasty. Following the victory of Han over Chu after the collapse of Qin, Confucius's thoughts received official sanction and were further developed into a system known as Confucianism.
Confucius is traditionally credited with having authored or edited many of the Chinese classic texts including all of the Five Classics, but modern scholars are cautious of attributing specific assertions to Confucius himself. Aphorisms concerning his teachings were compiled in the "Analects", but only many years after his death.
Confucius's principles had a basis in common Chinese tradition and belief. He championed strong family loyalty, ancestor worship, respect of elders by their children and of husbands by their wives. He also recommended family as a basis for ideal government. He espoused the well-known principle "Do not do to others what you do not want done to yourself", the Golden Rule.
Names.
Confucius' family and personal name respectively was Kong Qiu (孔丘 "Kǒng Qiū"). His courtesy name was Zhongni (仲尼 "Zhòngní"). In Chinese, he is most often known as Kongzi (孔子 Kǒng Zǐ, literally "Master Kong"). He is also known by the honorific Kong Fuzi (孔夫子 "Kǒng Fūzǐ", literally "Grand Master Kong"). In the Wade–Giles system of romanization, the honorific name is rendered as "K'ung Fu-tzu". The Latinized name "Confucius" is derived from "Kong Fuzi", and was first coined by 16th-century Jesuit missionaries to China, most probably by Matteo Ricci.
Within the "Analects", he is often referred to simply as "the Master" (子 Zǐ). In 1 AD, Confucius was given his first posthumous name, the "Laudably Declarable Lord Ni" (褒成宣尼公). In 1530, he was declared the "Extremely Sage Departed Teacher" (至聖先師). He is also known separately as the "Great Sage" (至聖), "First Teacher" (先師), and "Model Teacher for Ten Thousand Ages" (萬世師表).
Family background.
According to tradition, three generations before Confucius' time, his ancestors had migrated from the Song state to the Lu state. Confucius was a descendant of the Shang dynasty Kings through the Dukes of Song.
Biography.
Early life.
It is generally thought that Confucius was born on September 28, 551 BC. His birthplace was in Zou, Lu state (near present-day Qufu, Shandong Province). His father Kong He (孔紇), also known as Shuliang He (叔梁紇), was an officer in the Lu military. Kong He died when Confucius was three years old, and Confucius was raised by his mother Yan Zhengzai (顏徵在) in poverty. His mother would later die at less than 40 years of age. At age 19 he married his wife, surnamed Qiguan (亓官), and a year later the couple had their first child, Kong Li (孔鯉) who was later known as "Boyu", which means "Top Fish" in Chinese. Qiguan and Confucius would later have two daughters together, one of whom is thought to have died early in her life as a child.
Confucius was educated at schools for commoners, where he studied and learned the Six Arts.
Confucius was born into the class of "shi" (士), between the aristocracy and the common people. He is said to have worked in various government jobs during his early 20s, and also worked as a bookkeeper and a caretaker of sheep and horses, which he used the proceeds from to give his mother a proper burial. When his mother died, Confucius (aged 23) is said to have mourned for three years as was the tradition.
Political career.
The Lu state was headed by a ruling ducal house. Under the duke were three aristocratic families, whose heads bore the title of viscount and held hereditary positions in the Lu bureaucracy. The Ji family held the position "Minister over the Masses", who was also the "Prime Minister"; the Meng family held the position "Minister of Works"; and the Shu family held the position "Minister of War". In the winter of 505 BC, Yang Hu—a retainer of the Ji family—rose up in rebellion and seized power from the Ji family. However, by the summer of 501 BC, the three hereditary families had succeeded in expelling Yang Hu from Lu. By then, Confucius had built up a considerable reputation through his teachings, while the families came to see the value of proper conduct and righteousness, so they could achieve loyalty to a legitimate government. Thus, that year (501 BC), Confucius came to be appointed to the minor position of governor of a town. Eventually, he rose to the position of Minister of Crime.
Confucius desired to return the authority of the state to the duke by dismantling the fortifications of the city-strongholds belonging to the three families. This way, he could establish a centralized government. However, Confucius relied solely on diplomacy as he had no military authority himself. In 500 BC, Hou Fan—the governor of Hou—revolted against his lord of the Shu family. Although the Meng and Shu families unsuccessfully besieged Hou, a loyalist official rose up with the people of Hou and forced Hou Fan to flee to the Qi state. The situation may have been in favor for Confucius as this likely made it possible for Confucius and his disciples to convince the aristocratic families to dismantle the fortifications of their cities. Eventually, after a year and a half, Confucius and his disciples succeeded in convincing the Shu family to raze the walls of Hou, the Ji family in razing the walls of Bi, and the Meng family in razing the walls of Cheng. First, the Shu family led an army towards their city Hou and tore down its walls in 498 BC. Soon thereafter, Gongshan Furao—a retainer of the Ji family—revolted and took control of the forces at Bi. He immediately launched an attack and entered the capital Lu.
Earlier, Gongshan had approached Confucius to join him, which Confucius considered. Even though he disapproved the use of a violent revolution, the Ji family dominated the Lu state force for generations and had exiled the previous duke. Although he wanted the opportunity to put his principles in practice, Confucius gave up on this idea in the end. Creel (1949) states that, unlike the rebel Yang Hu before him, Gongshan may have sought to destroy the three hereditary families and restore the power of the duke. However, Dubs (1946) is of the view that Gongshan was encouraged by Viscount Ji Huan to invade the Lu capital in an attempt to avoid dismantling the Bi fortified walls. Whatever the situation may have been, Gongshan was considered an upright man who continued to defend the state of Lu, even after he was forced to flee.
During the revolt by Gongshan, Zhong You (仲由) had managed to keep the duke and the three viscounts together at the court. Zhong You was one of the disciples of Confucius and Confucius had arranged for him to be given the position of governor by the Ji family. When Confucius heard of the raid, he requested that Viscount Ji Huan allow the duke and his court to retreat to a stronghold on his palace grounds. Thereafter, the heads of the three families and the duke retreated to the Ji's palace complex and ascended the Wuzi Terrace. Confucius ordered two officers to lead an assault against the rebels. At least one of the two officers was a retainer of the Ji family, but they were unable to refuse the orders while in the presence of the duke, viscounts, and court. The rebels were pursued and defeated at Gu. Immediately after the revolt was defeated, the Ji family razed the Bi city walls to the ground.
The attackers retreated after realizing that they would have to become rebels against the state and against their own lord. Through Confucius' actions, the Bi officials had inadvertently revolted against their own lord, thus forcing Viscount Ji Huan's hand in having to dismantle the walls of Bi (as it could have harbored such rebels) or confess to instigating the event by going against proper conduct and righteousness as an official. Dubs (1949) suggests that the incident brought to light Confucius' foresight, practical political ability and his insight into human character.
When it was time to dismantle the city walls of the Meng family, the governor was reluctant to have his city walls torn down and convinced the head of the Meng family not to do so. The "Zuo Zhuan" recalls that the governor advised against razing the walls to the ground as he said that it made Cheng vulnerable to the Qi state and cause the destruction of the Meng family. Even though Viscount Meng Yi gave his word not to interfere with an attempt, he went back on his earlier promise to dismantle the walls.
Later in 498 BC, Duke Ding personally went with an army to lay siege to Cheng in an attempt to raze its walls to the ground, but he did not succeed. Thus, Confucius could not achieve the idealistic reforms that he wanted including restoration of the legitimate rule of the duke. He had made powerful enemies within the state, especially with Viscount Ji Huan, due to his successes so far. According to accounts in the "Zuo Zhuan" and "Shiji", Confucius departed his homeland in 497 BC after his support for the failed attempt of dismantling the fortified city walls of the powerful Ji, Meng, and Shu families. He left the state of Lu without resigning, remaining in self-exile and unable to return as long as Viscount Ji Huan was alive.
Exile.
The "Shiji" states that the neighboring Qi state was worried that Lu was becoming too powerful while Confucius was involved in the government of the Lu state. According to this account, Qi decided to sabotage Lu's reforms by sending 100 good horses and 80 beautiful dancing girls to the Duke of Lu. The Duke indulged himself in pleasure and did not attend to official duties for three days. Confucius was deeply disappointed and resolved to leave Lu and seek better opportunities, yet to leave at once would expose the misbehavior of the Duke and therefore bring public humiliation to the ruler Confucius was serving. Confucius therefore waited for the Duke to make a lesser mistake. Soon after, the Duke neglected to send to Confucius a portion of the sacrificial meat that was his due according to custom, and Confucius seized upon this pretext to leave both his post and the Lu state.
After Confucius' resignation, he began a long journey or set of journeys around the small kingdoms of north-east and central China, traditionally including the states of Wei, Song, Chen, and Cai. At the courts of these states, he expounded his political beliefs but did not see them implemented.
Return home.
According to the "Zuo Zhuan", Confucius returned home to his native Lu when he was 68, after he was invited to do so by Ji Kangzi, the chief minister of Lu. The "Analects" depict him spending his last years teaching 72 or 77 disciples and transmitting the old wisdom via a set of texts called the Five Classics.
During his return, Confucius sometimes acted as an advisor to several government officials in Lu, including Ji Kangzi, on matters including governance and crime.
Philosophy.
Although Confucianism is often followed in a religious manner by the Chinese, many argue that its values are secular and therefore it isn't a religion, but more akin to a secular morality. Proponents argue that despite the secular nature of Confucianism's teachings, it is based on a worldview that is religious. Confucianism discusses elements of the afterlife and views concerning Heaven, but it is relatively unconcerned with some spiritual matters often considered essential to religious thought, such as the nature of souls. However, Confucius is said to have believed in astrology saying: "Heaven sends down its good or evil symbols and wise men act accordingly".
In the "Analects", Confucius presents himself as a "transmitter who invented nothing". He puts the greatest emphasis on the importance of study, and it is the Chinese character for study () that opens the text. Far from trying to build a systematic or formalist theory, he wanted his disciples to master and internalize the old classics, so that their deep thought and thorough study would allow them to relate the moral problems of the present to past political events (as recorded in the "Annals") or the past expressions of commoners' feelings and noblemen's reflections (as in the poems of the "Book of Odes").
Ethics.
One of the deepest teachings of Confucius may have been the superiority of personal exemplification over explicit rules of behavior. His moral teachings emphasized self-cultivation, emulation of moral exemplars, and the attainment of skilled judgment rather than knowledge of rules. Confucian ethics may be considered a type of virtue ethics. His teachings rarely rely on reasoned argument and ethical ideals and methods are conveyed more indirectly, through allusion, innuendo, and even tautology. His teachings require examination and context in order to be understood. A good example is found in this famous anecdote:
By not asking about the horses, Confucius demonstrates that the sage values human beings over property; readers are led to reflect on whether their response would follow Confucius' and to pursue self-improvement if it would not have. Confucius, as an exemplar of human excellence, serves as the ultimate model, rather than a deity or a universally true set of abstract principles. For these reasons, according to many commentators, Confucius' teachings may be considered a Chinese example of humanism.
One of his teachings was a variant of the Golden Rule sometimes called the "Silver Rule" owing to its negative form:
Often overlooked in Confucian ethics are the virtues to the self: sincerity and the cultivation of knowledge. Virtuous action towards others begins with virtuous and sincere thought, which begins with knowledge. A virtuous disposition without knowledge is susceptible to corruption and virtuous action without sincerity is not true righteousness. Cultivating knowledge and sincerity is also important for one's own sake; the superior person loves learning for the sake of learning and righteousness for the sake of righteousness.
The Confucian theory of ethics as exemplified in "Lǐ" () is based on three important conceptual aspects of life: (a) ceremonies associated with sacrifice to ancestors and deities of various types, (b) social and political institutions, and (c) the etiquette of daily behavior. It was believed by some that "lǐ" originated from the heavens, but Confucius stressed the development of "lǐ" through the actions of sage leaders in human history. His discussions of "lǐ" seem to redefine the term to refer to all actions committed by a person to build the ideal society, rather than those simply conforming with canonical standards of ceremony.
In the early Confucian tradition, "lǐ" was doing the proper thing at the proper time, balancing between maintaining existing norms to perpetuate an ethical social fabric, and violating them in order to accomplish ethical good. Training in the "lǐ" of past sages cultivates in people virtues that include ethical judgment about when "lǐ" must be adapted in light of situational contexts.
In Confucianism, the concept of "li" is closely related to "yì" (), which is based upon the idea of reciprocity. "Yì" can be translated as righteousness, though it may simply mean what is ethically best to do in a certain context. The term contrasts with action done out of self-interest. While pursuing one's own self-interest is not necessarily bad, one would be a better, more righteous person if one's life was based upon following a path designed to enhance the greater good. Thus an outcome of "yì" is doing the right thing for the right reason.
Just as action according to "Lǐ" should be adapted to conform to the aspiration of adhering to "yì", so "yì" is linked to the core value of "rén" ()."Rén" consists of 5 basic virtues: seriousness, generosity, sincerity, diligence and kindness. "Rén" is the virtue of perfectly fulfilling one's responsibilities toward others, most often translated as "benevolence" or "humaneness"; translator Arthur Waley calls it "Goodness" (with a capital "G"), and other translations that have been put forth include "authoritativeness" and "selflessness." Confucius' moral system was based upon empathy and understanding others, rather than divinely ordained rules. To develop one's spontaneous responses of "rén" so that these could guide action intuitively was even better than living by the rules of "yì". Confucius asserts that virtue is a means between extremes. For example, the properly generous person gives the right amount—not too much and not too little.
Politics.
Confucius' political thought is based upon his ethical thought. He argued that the best government is one that rules through "rites" ("lǐ") and people's natural morality, and "not" by using bribery and coercion. He explained that this is one of the most important analects: "If the people be led by laws, and uniformity sought to be given them by punishments, they will try to avoid the punishment, but have no sense of shame. If they be led by virtue, and uniformity sought to be given them by the rules of propriety, they will have the sense of the shame, and moreover will become good." (Translated by James Legge) in the Great Learning (). This "sense of shame" is an internalisation of duty, where the punishment precedes the evil action, instead of following it in the form of laws as in Legalism.
Confucius looked nostalgically upon earlier days, and urged the Chinese, particularly those with political power, to model themselves on earlier examples. In times of division, chaos, and endless wars between feudal states, he wanted to restore the Mandate of Heaven () that could unify the "world" (, "all under Heaven") and bestow peace and prosperity on the people. Because his vision of personal and social perfections was framed as a revival of the ordered society of earlier times, Confucius is often considered a great proponent of conservatism, but a closer look at what he proposes often shows that he used (and perhaps twisted) past institutions and rites to push a new political agenda of his own: a revival of a unified royal state, whose rulers would succeed to power on the basis of their moral merits instead of lineage. These would be rulers devoted to their people, striving for personal and social perfection, and such a ruler would spread his own virtues to the people instead of imposing proper behavior with laws and rules.
Confucius did not believe in the concept of "democracy", which is itself an Athenian concept unknown in ancient China, but could be interpreted by Confucius' principles recommending against individuals electing their own political leaders to govern them, or that anyone is capable of self-government. He expressed fears that the masses lacked the intellect to make decisions for themselves, and that, in his view, since not everyone is created equal, not everyone has a right of self-government.
While he supported the idea of government ruling by a virtuous king, his ideas contained a number of elements to limit the power of rulers. He argued for representing truth in language, and honesty was of paramount importance. Even in facial expression, truth must always be represented. Confucius believed that if a ruler were to lead correctly, by action, that orders would be deemed unnecessary in that others will follow the proper actions of their ruler. In discussing the relationship between a king and his subject (or a father and his son), he underlined the need to give due respect to superiors. This demanded that the subordinates must give advice to their superiors if the superiors were considered to be taking the course of action that was wrong. Confucius believed in ruling by example, if you lead correctly, orders by force or punishment isn't necessary.
Disciples.
There is not much known of Confucius' disciples and a little over half of them had their surnames recorded in the "Zuo Zhuan". The "Analects" records 22 names that are most likely Confucius' disciples, while the "Mencius" records 24 names, although it is quite certain that there have been many more disciples whose name were not recorded. Most of Confucius' disciples were from the Lu state, while others were from neighboring states. For example, Zigong was from the Wey state and Sima Niu was from the Song state. Confucius' favorite disciple was Yan Hui, most probably one of the most impoverished of them all. Sima Niu, in contrast to Yan Hui, was from a hereditary noble family hailing from the Song state. Under Confucius' teachings, the disciples became well-learned in the principles and methods of government. He often engaged in discussion and debate with his students and gave high importance to their studies in history, poetry, and ritual. Confucius advocated loyalty to principle rather than to individual acumen, in which reform was to be achieved by persuasion rather than violence. Even though Confucius denounced them for their practices, the aristocracy was likely attracted to the idea of having trustworthy officials who were studied in morals as the circumstances of the time made it desirable. In fact, the disciple Zilu even died defending his ruler in Wei.
Yang Hu, who was a subordinate of the Ji family, had dominated the Lu government from 505 to 502 and even attempted a coup, which narrowly failed. As a likely consequence, it was after that that the first disciples of Confucius were appointed to government positions. A few of Confucius' disciples went on to attain official positions of some importance, some of which were arranged by Confucius. By the time Confucius was 50 years old, the Ji family had consolidated their power in the Lu state over the ruling ducal house. Even though the Ji family had practices with which Confucius disagreed and disapproved, they nonetheless gave Confucius' disciples many opportunities for employment. Confucius continued to remind his disciples to stay true to their principles and renounced those who did not, all the while being openly critical of the Ji family.
Legacy.
Confucius' teachings were later turned into an elaborate set of rules and practices by his numerous disciples and followers, who organized his teachings into the Analects. Confucius' disciples and his only grandson, Zisi, continued his philosophical school after his death. These efforts spread Confucian ideals to students who then became officials in many of the royal courts in China, thereby giving Confucianism the first wide-scale test of its dogma.
Two of Confucius' most famous later followers emphasized radically different aspects of his teachings. In the centuries after his death, Mencius () and Xun Zi () both composed important teachings elaborating in different ways on the fundamental ideas associated with Confucius. Mencius (4th century BC) articulated the innate goodness in human beings as a source of the ethical intuitions that guide people towards "rén", "yì", and "lǐ", while Xun Zi (3rd century BC) underscored the realistic and materialistic aspects of Confucian thought, stressing that morality was inculcated in society through tradition and in individuals through training. In time, their writings, together with the "Analects" and other core texts came to constitute the philosophical corpus of Confucianism.
This realignment in Confucian thought was parallel to the development of Legalism, which saw filial piety as self-interest and not a useful tool for a ruler to create an effective state. A disagreement between these two political philosophies came to a head in 223 BC when the Qin state conquered all of China. Li Si, Prime Minister of the Qin Dynasty convinced Qin Shi Huang to abandon the Confucians' recommendation of awarding fiefs akin to the Zhou Dynasty before them which he saw as being against to the Legalist idea of centralizing the state around the ruler. When the Confucian advisers pressed their point, Li Si had many Confucian scholars killed and their books burned—considered a huge blow to the philosophy and Chinese scholarship.
Under the succeeding Han Dynasty and Tang dynasty, Confucian ideas gained even more widespread prominence. Under Wudi, the works of Confucius were made the official imperial philosophy and required reading for civil service examinations in 140 BC which was continued nearly unbroken until the end of the 19th Century. As Moism lost support by the time of the Han, the main philosophical contenders were Legalism, which Confucian thought somewhat absorbed, the teachings of Laozi, whose focus on more spiritual ideas kept it from direct conflict with Confucianism, and the new Buddhist religion, which gained acceptance during the Southern and Northern Dynasties era. Both Confucian ideas and Confucian-trained officials were relied upon in the Ming Dynasty and even the Yuan Dynasty, although Kublai Khan distrusted handing over provincial control to them.
During the Song dynasty, the scholar Zhu Xi (AD 1130–1200) added ideas from Daoism and Buddhism into Confucianism. In his life, Zhu Xi was largely ignored, but not long after his death his ideas became the new orthodox view of what Confucian texts actually meant. Modern historians view Zhu Xi as having created something rather different, and call his way of thinking "Neo-Confucianism". Neo-Confucianism held sway in China, Japan, Korea and Vietnam until the 19th century.
The works of Confucius were translated into European languages through the agency of Jesuit scholars stationed in China. Matteo Ricci started to report on the thoughts of Confucius, and father Prospero Intorcetta published the life and works of Confucius into Latin in 1687. It is thought that such works had considerable importance on European thinkers of the period, particularly among the Deists and other philosophical groups of the Enlightenment who were interested by the integration of the system of morality of Confucius into Western civilization.
In the modern era Confucian movements, such as New Confucianism, still exist but during the Cultural Revolution, Confucianism was frequently attacked by leading figures in the Communist Party of China. This was partially a continuation of the condemnations of Confucianism by intellectuals and activists in the early 20th Century as a cause of the ethnocentric close-mindedness and refusal of the Qing Dynasty to modernize that led to the tragedies that befell China in the 19th Century.
Confucius' works are studied by scholars in many other Asian countries, particularly those in the Chinese cultural sphere, such as Korea, Japan and Vietnam. Many of those countries still hold the traditional memorial ceremony every year.
The Ahmadiyya Muslim Community believes Confucius was a Divine Prophet of God, as were Lao-Tzu and other eminent Chinese personages.
In modern times, Asteroid 7853, "Confucius", was named after the Chinese thinker.
Visual portraits.
No contemporary painting or sculpture of Confucius survives, and it was only during the Han Dynasty that he was portrayed visually. Carvings often depict his legendary meeting with Laozi. Since that time there have been many portraits of Confucius as the ideal philosopher.
In former times, it was customary to have a portrait in Confucius Temples; however, during the reign of Hongwu Emperor (Taizu) of the Ming dynasty it was decided that the only proper portrait of Confucius should be in the temple in his home town, Qufu in Shandong. In other temples, Confucius is represented by a memorial tablet. In 2006, the China Confucius Foundation commissioned a standard portrait of Confucius based on the Tang dynasty portrait by Wu Daozi.
Death and legacy.
Burdened by the loss of both his son and his favorite disciples, he died at the age of 71 or 72. He died from natural causes. Confucius was buried in Kong Lin cemetery which lies in the historical part of Qufu in the Shandong Province. The original tomb erected there in memory of Confucius on the bank of the Sishui River had the shape of an axe. In addition, it has a raised brick platform at the front of the memorial for offerings such as sandalwood incense and fruit.
Memorials of Confucius.
Soon after Confucius' death, Qufu, his home town became a place of devotion and remembrance. The Han dynasty "Records of the Grand Historian" records that it had already become a place of pilgrimage for ministers. It is still a major destination for cultural tourism, and many people visit his grave and the surrounding temples. In pan-China cultures, there are many temples where representations of the Buddha, Laozi and Confucius are found together. There are also many temples dedicated to him, which have been used for Confucianist ceremonies.
The Chinese have a tradition of holding spectacular memorial ceremonies of Confucius () every year, using ceremonies that supposedly derived from Zhou Li () as recorded by Confucius, on the date of Confucius' birth. This tradition was interrupted for several decades in mainland China, where the official stance of the Communist Party and the State was that Confucius and Confucianism represented reactionary feudalist beliefs which held that the subservience of the people to the aristocracy is a part of the natural order. All such ceremonies and rites were therefore banned. Only after the 1990s did the ceremony resume. As it is now considered a veneration of Chinese history and tradition, even Communist Party members may be found in attendance.
In Taiwan, where the Nationalist Party (Kuomintang) strongly promoted Confucian beliefs in ethics and behavior, the tradition of the memorial ceremony of Confucius (祭孔) is supported by the government and has continued without interruption. While not a national holiday, it does appear on all printed calendars, much as Father's Day does in the West.
Descendants.
Confucius' descendants were repeatedly identified and honored by successive imperial governments with titles of nobility and official posts. They were honored with the rank of a marquis thirty-five times since Gaozu of the Han Dynasty, and they were promoted to the rank of duke forty-two times from the Tang dynasty to the Qing Dynasty. Emperor Xuanzong of Tang first bestowed the title of "Duke Wenxuan" on Kong Suizhi of the 35th generation. In 1055, Emperor Renzong of Song first bestowed the title of "Duke Yansheng" on Kong Zongyuan of the 46th generation.
During the Southern Song dynasty the Duke Yansheng Kong Duanyou fled south with the Song Emperor to Quzhou in Zhejiang, while the newly established Jin dynasty (1115–1234) in the north appointed Kong Duanyou's brother Kong Duancao who remained in Qufu as Duke Yansheng. From that time up until the Yuan dynasty, there were two Duke Yanshengs, once in the north in Qufu and the other in the south at Quzhou. During the Yuan dynasty, the Emperor Kublai Khan invited the southern Duke Yansheng Kong Zhu to returned to Qufu. Kong Zhu refused, and gave up the title, so the northern branch of the family kept the title of Duke Yansheng. The southern branch still remained in Quzhou where they lived to this day. Confucius's descendants in Quzhou alone number 30,000.
Despite repeated dynastic change in China, the title of Duke Yansheng was bestowed upon successive generations of descendants until it was abolished by the Nationalist Government in 1935. The last holder of the title, Kung Te-cheng of the 77th generation, was appointed Sacrificial Official to Confucius. Kung Te-cheng died in October 2008, and his son, Kung Wei-yi, the 78th lineal descendant, had died in 1989. Kung Te-cheng's grandson, Kung Tsui-chang, the 79th lineal descendant, was born in 1975; his great-grandson, Kung Yu-jen, the 80th lineal descendant, was born in Taipei on January 1, 2006. Te-cheng's sister, Kong Demao, lives in mainland China and has written a book about her experiences growing up at the family estate in Qufu. Another sister, Kong Deqi, died as a young woman. Many descendants of Confucius still live in Qufu today.
Confucius' family, the Kongs, have the longest recorded extant pedigree in the world today. The father-to-son family tree, now in its 83rd generation, has been recorded since the death of Confucius. According to the Confucius Genealogy Compilation Committee, he has 2 million known and registered descendants, and there are an estimated 3 million in all. Of these, several tens of thousands live outside of China. In the 14th century, a Kong descendant went to Korea, where an estimated 34,000 descendants of Confucius live today. One of the main lineages fled from the Kong ancestral home in Qufu during the Chinese Civil War in the 1940s, and eventually settled in Taiwan. There are also branches of the Kong family who have converted to Islam after marrying Muslim women, in Dachuan in Gansu province in the 1800s, and in 1715 in Xuanwei city in Yunnan province. Kong Dejun (孔德軍) is a prominent Islamic scholar and Arabist from Qinghai province and a 77th generation descendant of Confucius.
Because of the huge interest in the Confucius family tree, there was a project in China to test the DNA of known family members of the collateral branches in mainland China. Among other things, this would allow scientists to identify a common Y chromosome in male descendants of Confucius. If the descent were truly unbroken, father-to-son, since Confucius' lifetime, the males in the family would all have the same Y chromosome as their direct male ancestor, with slight mutations due to the passage of time. The aim of the genetic test was the help members of collateral branches in China who lost their genealogical records to prove their descent. However, in 2009, many of the collateral branches decided not to agree to DNA testing. Bryan Sykes, professor of genetics at Oxford University, understands this decision: "The Confucius family tree has an enormous cultural significance," he said. "It's not just a scientific question." The DNA testing was originally proposed to add new members, many of whose family record books were lost during 20th-century upheavals, to the Confucian family tree. The main branch of the family which fled to Taiwan was never involved in the proposed DNA test at all.
In 2013 a DNA test performed on multiple different families who claimed descent from Confucius found that they shared the same Y chromosome as reported by Fudan University.
The fifth and most recent edition of the Confucius genealogy was printed by the Confucius Genealogy Compilation Committee (CGCC). It was unveiled in a ceremony at Qufu on September 24, 2009. Women are now included for the first time.
There is also a "Sacrificial Official to Mencius" for a descendant of Mencius, a "Sacrificial Official to Zengzi" for a descendant of Zengzi, and a "Sacrificial Official to Yan Hui" for a descendant of Yan Hui.

</doc>
<doc id="5826" url="https://en.wikipedia.org/wiki?curid=5826" title="Complex number">
Complex number

A complex number is a number that can be expressed in the form , where and are real numbers and is the imaginary unit, that satisfies the equation . In this expression, is the "real part" and is the "imaginary part" of the complex number.
Complex numbers extend the concept of the one-dimensional number line to the two-dimensional complex plane by using the horizontal axis for the real part and the vertical axis for the imaginary part. The complex number can be identified with the point in the complex plane. A complex number whose real part is zero is said to be purely imaginary, whereas a complex number whose imaginary part is zero is a real number. In this way, the complex numbers contain the ordinary real numbers while extending them in order to solve problems that cannot be solved with real numbers alone.
As well as their use within mathematics, complex numbers have practical applications in many fields, including physics, chemistry, biology, economics, electrical engineering, and statistics. The Italian mathematician Gerolamo Cardano is the first known to have introduced complex numbers. He called them "fictitious" during his attempts to find solutions to cubic equations in the 16th century.
Overview.
Complex numbers allow for solutions to certain equations that have no solutions in real numbers. For example, the equation
has no real solution, since the square of a real number cannot be negative. Complex numbers provide a solution to this problem. The idea is to extend the real numbers with the imaginary unit where , so that solutions to equations like the preceding one can be found. In this case the solutions are and , as can be verified using the fact that :
According to the fundamental theorem of algebra, all polynomial equations with real or complex coefficients in a single variable have a solution in complex numbers.
Definition.
A complex number is a number of the form , where and are real numbers and is the "imaginary unit", satisfying . For example, is a complex number.
The real number is called the "real part" of the complex number ; the real number is called the "imaginary part" of . By this convention the imaginary part does not include the imaginary unit: hence , not , is the imaginary part. The real part of a complex number is denoted by or ; the imaginary part of a complex number is denoted by or . For example,
Hence, in terms of its real and imaginary parts, a complex number is equal to formula_5. This expression is sometimes known as the Cartesian form of .
A real number can be regarded as a complex number whose imaginary part is 0. A purely imaginary number is a complex number whose real part is zero. It is common to write for and for . Moreover, when the imaginary part is negative, it is common to write with instead of , for example instead of .
The set of all complex numbers is denoted by , formula_6 or formula_7.
Notation.
Some authors write instead of , particularly when "b" is a radical. In some disciplines, in particular electromagnetism and electrical engineering, is used instead of , since is frequently used for electric current. In these cases complex numbers are written as or .
Complex plane.
A complex number can be viewed as a point or position vector in a two-dimensional Cartesian coordinate system called the complex plane or Argand diagram (see and ), named after Jean-Robert Argand. The numbers are conventionally plotted using the real part as the horizontal component, and imaginary part as vertical (see Figure 1). These two values used to identify a given complex number are therefore called its "Cartesian", "rectangular", or "algebraic form".
A position vector may also be defined in terms of its magnitude and direction relative to the origin. These are emphasized in a complex number's "polar form". Using the polar form of the complex number in calculations may lead to a more intuitive interpretation of mathematical results. Notably, the operations of addition and multiplication take on a very natural geometric character when complex numbers are viewed as position vectors: addition corresponds to vector addition while multiplication corresponds to multiplying their magnitudes and adding their arguments (i.e. the angles they make with the "x" axis). Viewed in this way the multiplication of a complex number by corresponds to rotating the position vector counterclockwise by a quarter turn (90°) about the origin: = = .
History in brief.
The solution in radicals (without trigonometric functions) of a general cubic equation contains the square roots of negative numbers when all three roots are real numbers, a situation that cannot be rectified by factoring aided by the rational root test if the cubic is irreducible (the so-called casus irreducibilis). This conundrum led Italian mathematician Gerolamo Cardano to conceive of complex numbers in around 1545, though his understanding was rudimentary.
Work on the problem of general polynomials ultimately led to the fundamental theorem of algebra, which shows that with complex numbers, a solution exists to every polynomial equation of degree one or higher. Complex numbers thus form an algebraically closed field, where any polynomial equation has a root.
Many mathematicians contributed to the full development of complex numbers. The rules for addition, subtraction, multiplication, and division of complex numbers were developed by the Italian mathematician Rafael Bombelli. A more abstract formalism for the complex numbers was further developed by the Irish mathematician William Rowan Hamilton, who extended this abstraction to the theory of quaternions.
Relations.
Equality.
Two complex numbers are equal if and only if both their real and imaginary parts are equal. In symbols:
Ordering.
Because complex numbers are naturally thought of as existing on a two-dimensional plane, there is no natural linear ordering on the set of complex numbers.
There is no linear ordering on the complex numbers that is compatible with addition and multiplication. Formally, we say that the complex numbers cannot have the structure of an ordered field. This is because any square in an ordered field is at least , but .
Elementary operations.
Conjugate.
The "complex conjugate" of the complex number is defined to be . It is denoted formula_9 or .
Formally, for any complex number "z":
Geometrically, formula_9 is the "reflection" of about the real axis. Conjugating twice gives the original complex number: formula_12.
The real and imaginary parts of a complex number can be extracted using the conjugate:
Moreover, a complex number is real if and only if it equals its conjugate.
Conjugation distributes over the standard arithmetic operations:
Addition and subtraction.
Complex numbers are added by adding the real and imaginary parts of the summands. That is to say:
Similarly, subtraction is defined by
Using the visualization of complex numbers in the complex plane, the addition has the following geometric interpretation: the sum of two complex numbers "A" and "B", interpreted as points of the complex plane, is the point "X" obtained by building a parallelogram three of whose vertices are "O", "A" and "B". Equivalently, "X" is the point such that the triangles with vertices "O", "A", "B", and "X", "B", "A", are congruent.
Multiplication and division.
The multiplication of two complex numbers is defined by the following formula:
In particular, the square of the imaginary unit is −1:
The preceding definition of multiplication of general complex numbers follows naturally from this fundamental property of the imaginary unit. Indeed, if is treated as a number so that means times , the above multiplication rule is identical to the usual rule for multiplying two sums of two terms.
The division of two complex numbers is defined in terms of complex multiplication, which is described above, and real division. When at least one of and is non-zero, we have
Division can be defined in this way because of the following observation:
As shown earlier, is the complex conjugate of the denominator . At least one of the real part and the imaginary part of the denominator must be nonzero for division to be defined. This is called "rationalization" of the denominator (although the denominator in the final expression might be an irrational real number).
Reciprocal.
The reciprocal of a nonzero complex number is given by
This formula can be used to compute the multiplicative inverse of a complex number if it is given in rectangular coordinates. Inversive geometry, a branch of geometry studying reflections more general than ones about a line, can also be expressed in terms of complex numbers. In the network analysis of electrical circuits, the complex conjugate is used in finding the equivalent impedance when the maximum power transfer theorem is used.
Square root.
The square roots of (with ) are formula_30, where
and
where sgn is the signum function. This can be seen by squaring formula_30 to obtain . Here formula_34 is called the modulus of , and the square root sign indicates the square root with non-negative real part, called the principal square root; also formula_35, where formula_36.
Polar form.
Absolute value and argument.
An alternative way of defining a point "P" in the complex plane, other than using the "x"- and "y"-coordinates, is to use the distance of the point from "O", the point whose coordinates are (the origin), together with the angle subtended between the positive real axis and the line segment "OP" in a counterclockwise direction. This idea leads to the polar form of complex numbers.
The "absolute value" (or "modulus" or "magnitude") of a complex number is
If is a real number (i.e., ), then . In general, by Pythagoras' theorem, is the distance of the point "P" representing the complex number to the origin. The square of the absolute value is
where formula_9 is the complex conjugate of formula_40.
The "argument" of (in many applications referred to as the "phase") is the angle of the radius "OP" with the positive real axis, and is written as formula_41. As with the modulus, the argument can be found from the rectangular form formula_42:
The value of is expressed in radians in this article. It can increase by any integer multiple of and still give the same angle. Hence, the arg function is sometimes considered as multivalued. Normally, as given above, the principal value in the interval is chosen. Values in the range are obtained by adding if the value is negative. The polar angle for the complex number 0 is indeterminate, but arbitrary choice of the angle 0 is common.
The value of equals the result of atan2: formula_44.
Together, and give another way of representing complex numbers, the "polar form", as the combination of modulus and argument fully specify the position of a point on the plane. Recovering the original rectangular co-ordinates from the polar form is done by the formula called "trigonometric form"
Using Euler's formula this can be written as
Using the cis function, this is sometimes abbreviated to
In angle notation, often used in electronics to represent a phasor with amplitude and phase , it is written as
Multiplication and division in polar form.
Formulas for multiplication, division and exponentiation are simpler in polar form than the corresponding formulas in Cartesian coordinates. Given two complex numbers and , because of the well-known trigonometric identities 
we may derive
In other words, the absolute values are multiplied and the arguments are added to yield the polar form of the product. For example, multiplying by corresponds to a quarter-turn counter-clockwise, which gives back . The picture at the right illustrates the multiplication of
Since the real and imaginary part of are equal, the argument of that number is 45 degrees, or π/4 (in radian). On the other hand, it is also the sum of the angles at the origin of the red and blue triangles are arctan(1/3) and arctan(1/2), respectively. Thus, the formula
holds. As the arctan function can be approximated highly efficiently, formulas like this—known as Machin-like formulas—are used for high-precision approximations of π.
Similarly, division is given by
Exponentiation.
Euler's formula.
Euler's formula states that, for any real number ,
where is the base of the natural logarithm. This can be proved through induction by observing that
and so on, and by considering the Taylor series expansions of , and :
The rearrangement of terms is justified because each series is absolutely convergent.
Natural logarithm.
Euler's formula allows us to observe that, for any complex number
where "r" is a non-negative real number, one possible value for "z"'s natural logarithm is
Because cos and sin are periodic functions, the natural logarithm may be considered a multi-valued function, with:
Integer and fractional exponents.
We may use the identity
to define complex exponentiation, which is likewise multi-valued:
When "n" is an integer, this simplifies to de Moivre's formula:
The th roots of are given by
for any integer satisfying . Here is the usual (positive) th root of the positive real number . While the th root of a positive real number is chosen to be the "positive" real number satisfying there is no natural way of distinguishing one particular complex th root of a complex number. Therefore, the th root of is considered as a multivalued function (in ), as opposed to a usual function , for which is a uniquely defined number. Formulas such as
(which holds for positive real numbers), do in general not hold for complex numbers.
Properties.
Field structure.
The set C of complex numbers is a field. Briefly, this means that the following facts hold: first, any two complex numbers can be added and multiplied to yield another complex number. Second, for any complex number , its additive inverse is also a complex number; and third, every nonzero complex number has a reciprocal complex number. Moreover, these operations satisfy a number of laws, for example the law of commutativity of addition and multiplication for any two complex numbers and :
These two laws and the other requirements on a field can be proven by the formulas given above, using the fact that the real numbers themselves form a field.
Unlike the reals, C is not an ordered field, that is to say, it is not possible to define a relation that is compatible with the addition and multiplication. In fact, in any ordered field, the square of any element is necessarily positive, so precludes the existence of an ordering on C.
When the underlying field for a mathematical topic or construct is the field of complex numbers, the topic's name is usually modified to reflect that fact. For example: complex analysis, complex matrix, complex polynomial, and complex Lie algebra.
Solutions of polynomial equations.
Given any complex numbers (called coefficients) , the equation
has at least one complex solution "z", provided that at least one of the higher coefficients is nonzero. This is the statement of the "fundamental theorem of algebra". Because of this fact, C is called an algebraically closed field. This property does not hold for the field of rational numbers Q (the polynomial does not have a rational root, since is not a rational number) nor the real numbers R (the polynomial does not have a real root for , since the square of is positive for any real number ).
There are various proofs of this theorem, either by analytic methods such as Liouville's theorem, or topological ones such as the winding number, or a proof combining Galois theory and the fact that any real polynomial of "odd" degree has at least one real root.
Because of this fact, theorems that hold "for any algebraically closed field", apply to C. For example, any non-empty complex square matrix has at least one (complex) eigenvalue.
Algebraic characterization.
The field C has the following three properties: first, it has characteristic 0. This means that for any number of summands (all of which equal one). Second, its transcendence degree over Q, the prime field of C, is the cardinality of the continuum. Third, it is algebraically closed (see above). It can be shown that any field having these properties is isomorphic (as a field) to C. For example, the algebraic closure of Q also satisfies these three properties, so these two fields are isomorphic. Also, C is isomorphic to the field of complex Puiseux series. However, specifying an isomorphism requires the axiom of choice. Another consequence of this algebraic characterization is that C contains many proper subfields that are isomorphic to C.
Characterization as a topological field.
The preceding characterization of C describes only the algebraic aspects of C. That is to say, the properties of nearness and continuity, which matter in areas such as analysis and topology, are not dealt with. The following description of C as a topological field (that is, a field that is equipped with a topology, which allows the notion of convergence) does take into account the topological properties. C contains a subset (namely the set of positive real numbers) of nonzero elements satisfying the following three conditions:
Moreover, C has a nontrivial involutive automorphism (namely the complex conjugation), such that is in for any nonzero in C.
Any field with these properties can be endowed with a topology by taking the sets as a base, where ranges over the field and ranges over . With this topology is isomorphic as a "topological" field to C.
The only connected locally compact topological fields are R and C. This gives another characterization of C as a topological field, since C can be distinguished from R because the nonzero complex numbers are connected, while the nonzero real numbers are not.
Formal construction.
Formal development.
Above, complex numbers have been defined by introducing , the imaginary unit, as a symbol. More rigorously, the set of complex numbers can be defined as the set of ordered pairs of real numbers. In this notation, the above formulas for addition and multiplication read
It is then just a matter of notation to express as .
Though this low-level construction does accurately describe the structure of the complex numbers, the following equivalent definition reveals the algebraic nature of more immediately. This characterization relies on the notion of fields and polynomials. A field is a set endowed with addition, subtraction, multiplication and division operations that behave as is familiar from, say, rational numbers. For example, the distributive law
must hold for any three elements , and of a field. The set of real numbers does form a field. A polynomial with real coefficients is an expression of the form
where the are real numbers. The usual addition and multiplication of polynomials endows the set of all such polynomials with a ring structure. This ring is called polynomial ring.
The quotient ring can be shown to be a field.
This extension field contains two square roots of , namely (the cosets of) and , respectively. (The cosets of) and form a basis of as a real vector space, which means that each element of the extension field can be uniquely written as a linear combination in these two elements. Equivalently, elements of the extension field can be written as ordered pairs of real numbers. Moreover, the above formulas for addition etc. correspond to the ones yielded by this abstract algebraic approach—the two definitions of the field are said to be isomorphic (as fields). Together with the above-mentioned fact that is algebraically closed, this also shows that is an algebraic closure of .
Matrix representation of complex numbers.
Complex numbers can also be represented by matrices that have the following form:
Here the entries and are real numbers. The sum and product of two such matrices is again of this form, and the sum and product of complex numbers corresponds to the sum and product of such matrices. The geometric description of the multiplication of complex numbers can also be expressed in terms of rotation matrices by using this correspondence between complex numbers and such matrices. Moreover, the square of the absolute value of a complex number expressed as a matrix is equal to the determinant of that matrix:
The conjugate formula_77 corresponds to the transpose of the matrix.
Though this representation of complex numbers with matrices is the most common, many other representations arise from matrices "other than" formula_78 that square to the negative of the identity matrix. See the article on 2 × 2 real matrices for other representations of complex numbers.
Complex analysis.
The study of functions of a complex variable is known as complex analysis and has enormous practical use in applied mathematics as well as in other branches of mathematics. Often, the most natural proofs for statements in real analysis or even number theory employ techniques from complex analysis (see prime number theorem for an example). Unlike real functions, which are commonly represented as two-dimensional graphs, complex functions have four-dimensional graphs and may usefully be illustrated by color-coding a three-dimensional graph to suggest four dimensions, or by animating the complex function's dynamic transformation of the complex plane.
Complex exponential and related functions.
The notions of convergent series and continuous functions in (real) analysis have natural analogs in complex analysis. A sequence of complex numbers is said to converge if and only if its real and imaginary parts do. This is equivalent to the (ε, δ)-definition of limits, where the absolute value of real numbers is replaced by the one of complex numbers. From a more abstract point of view, C, endowed with the metric
is a complete metric space, which notably includes the triangle inequality
for any two complex numbers and .
Like in real analysis, this notion of convergence is used to construct a number of elementary functions: the "exponential function" , also written , is defined as the infinite series
and the series defining the real trigonometric functions sine and cosine, as well as hyperbolic functions such as sinh also carry over to complex arguments without change. "Euler's identity" states:
for any real number "φ", in particular
Unlike in the situation of real numbers, there is an infinitude of complex solutions of the equation
for any complex number . It can be shown that any such solution —called complex logarithm of —satisfies
where arg is the argument defined above, and ln the (real) natural logarithm. As arg is a multivalued function, unique only up to a multiple of 2"π", log is also multivalued. The principal value of log is often taken by restricting the imaginary part to the interval .
Complex exponentiation is defined as
Consequently, they are in general multi-valued. For , for some natural number , this recovers the non-uniqueness of th roots mentioned above.
Complex numbers, unlike real numbers, do not in general satisfy the unmodified power and logarithm identities, particularly when naïvely treated as single-valued functions; see failure of power and logarithm identities. For example, they do not satisfy
Both sides of the equation are multivalued by the definition of complex exponentiation given here, and the values on the left are a subset of those on the right.
Holomorphic functions.
A function "f" : C → C is called holomorphic if it satisfies the Cauchy–Riemann equations. For example, any R-linear map C → C can be written in the form
with complex coefficients and . This map is holomorphic if and only if . The second summand formula_89 is real-differentiable, but does not satisfy the Cauchy–Riemann equations.
Complex analysis shows some features not apparent in real analysis. For example, any two holomorphic functions and that agree on an arbitrarily small open subset of C necessarily agree everywhere. Meromorphic functions, functions that can locally be written as with a holomorphic function , still share some of the features of holomorphic functions. Other functions have essential singularities, such as at .
Applications.
Complex numbers have essential concrete applications in a variety of scientific and related areas such as signal processing, control theory, electromagnetism, fluid dynamics, quantum mechanics, cartography, and vibration analysis. Some applications of complex numbers are:
Control theory.
In control theory, systems are often transformed from the time domain to the frequency domain using the Laplace transform. The system's poles and zeros are then analyzed in the "complex plane". The root locus, Nyquist plot, and Nichols plot techniques all make use of the complex plane.
In the root locus method, it is especially important whether the poles and zeros are in the left or right half planes, i.e. have real part greater than or less than zero. If a linear, time-invariant (LTI) system has poles that are
If a system has zeros in the right half plane, it is a nonminimum phase system.
Improper integrals.
In applied fields, complex numbers are often used to compute certain real-valued improper integrals, by means of complex-valued functions. Several methods exist to do this; see methods of contour integration.
Fluid dynamics.
In fluid dynamics, complex functions are used to describe potential flow in two dimensions.
Dynamic equations.
In differential equations, it is common to first find all complex roots of the characteristic equation of a linear differential equation or equation system and then attempt to solve the system in terms of base functions of the form . Likewise, in difference equations, the complex roots of the characteristic equation of the difference equation system are used, to attempt to solve the system in terms of base functions of the form .
Electromagnetism and electrical engineering.
In electrical engineering, the Fourier transform is used to analyze varying voltages and currents. The treatment of resistors, capacitors, and inductors can then be unified by introducing imaginary, frequency-dependent resistances for the latter two and combining all three in a single complex number called the impedance. This approach is called phasor calculus.
In electrical engineering, the imaginary unit is denoted by , to avoid confusion with , which is generally in use to denote electric current, or, more particularly, , which is generally in use to denote instantaneous electric current.
Since the voltage in an AC circuit is oscillating, it can be represented as
To obtain the measurable quantity, the real part is taken:
The complex-valued signal formula_92 is called the analytic representation of the real-valued, measurable signal formula_93.
Signal analysis.
Complex numbers are used in signal analysis and other fields for a convenient description for periodically varying signals. For given real functions representing actual physical quantities, often in terms of sines and cosines, corresponding complex functions are considered of which the real parts are the original quantities. For a sine wave of a given frequency, the absolute value of the corresponding is the amplitude and the argument is the phase.
If Fourier analysis is employed to write a given real-valued signal as a sum of periodic functions, these periodic functions are often written as complex valued functions of the form
and
where ω represents the angular frequency and the complex number "A" encodes the phase and amplitude as explained above.
This use is also extended into digital signal processing and digital image processing, which utilize digital versions of Fourier analysis (and wavelet analysis) to transmit, compress, restore, and otherwise process digital audio signals, still images, and video signals.
Another example, relevant to the two side bands of amplitude modulation of AM radio, is:
Quantum mechanics.
The complex number field is intrinsic to the mathematical formulations of quantum mechanics, where complex Hilbert spaces provide the context for one such formulation that is convenient and perhaps most standard. The original foundation formulas of quantum mechanics—the Schrödinger equation and Heisenberg's matrix mechanics—make use of complex numbers.
Relativity.
In special and general relativity, some formulas for the metric on spacetime become simpler if one takes the time component of the spacetime continuum to be imaginary. (This approach is no longer standard in classical relativity, but is used in an essential way in quantum field theory.) Complex numbers are essential to spinors, which are a generalization of the tensors used in relativity.
Geometry.
Fractals.
Certain fractals are plotted in the complex plane, e.g. the Mandelbrot set and Julia sets.
Triangles.
Every triangle has a unique Steiner inellipse—an ellipse inside the triangle and tangent to the midpoints of the three sides of the triangle. The foci of a triangle's Steiner inellipse can be found as follows, according to Marden's theorem: Denote the triangle's vertices in the complex plane as , , and . Write the cubic equation formula_97, take its derivative, and equate the (quadratic) derivative to zero. Marden's Theorem says that the solutions of this equation are the complex numbers denoting the locations of the two foci of the Steiner inellipse.
Algebraic number theory.
As mentioned above, any nonconstant polynomial equation (in complex coefficients) has a solution in C. A fortiori, the same is true if the equation has rational coefficients. The roots of such equations are called algebraic numbers – they are a principal object of study in algebraic number theory. Compared to , the algebraic closure of Q, which also contains all algebraic numbers, C has the advantage of being easily understandable in geometric terms. In this way, algebraic methods can be used to study geometric questions and vice versa. With algebraic methods, more specifically applying the machinery of field theory to the number field containing roots of unity, it can be shown that it is not possible to construct a regular nonagon using only compass and straightedge – a purely geometric problem.
Another example are Gaussian integers, that is, numbers of the form , where and are integers, which can be used to classify sums of squares.
Analytic number theory.
Analytic number theory studies numbers, often integers or rationals, by taking advantage of the fact that they can be regarded as complex numbers, in which analytic methods can be used. This is done by encoding number-theoretic information in complex-valued functions. For example, the Riemann zeta function is related to the distribution of prime numbers.
History.
The earliest fleeting reference to square roots of negative numbers can perhaps be said to occur in the work of the Greek mathematician Hero of Alexandria in the 1st century AD, where in his "Stereometrica" he considers, apparently in error, the volume of an impossible frustum of a pyramid to arrive at the term formula_98 in his calculations, although negative quantities were not conceived of in Hellenistic mathematics and Heron merely replaced it by its positive (formula_99).
The impetus to study complex numbers proper first arose in the 16th century when algebraic solutions for the roots of cubic and quartic polynomials were discovered by Italian mathematicians (see Niccolò Fontana Tartaglia, Gerolamo Cardano). It was soon realized that these formulas, even if one was only interested in real solutions, sometimes required the manipulation of square roots of negative numbers. As an example, Tartaglia's formula for a cubic equation of the form formula_100 gives the solution to the equation as
At first glance this looks like nonsense. However formal calculations with complex numbers show that the equation has solutions , formula_102 and formula_103. Substituting these in turn for formula_104 in Tartaglia's cubic formula and simplifying, one gets 0, 1 and −1 as the solutions of . Of course this particular equation can be solved at sight but it does illustrate that when general formulas are used to solve cubic equations with real roots then, as later mathematicians showed rigorously, the use of complex numbers is unavoidable. Rafael Bombelli was the first to explicitly address these seemingly paradoxical solutions of cubic equations and developed the rules for complex arithmetic trying to resolve these issues.
The term "imaginary" for these quantities was coined by René Descartes in 1637, although he was at pains to stress their imaginary nature
A further source of confusion was that the equation formula_105 seemed to be capriciously inconsistent with the algebraic identity formula_106, which is valid for non-negative real numbers and , and which was also used in complex number calculations with one of , positive and the other negative. The incorrect use of this identity (and the related identity formula_107) in the case when both and are negative even bedeviled Euler. This difficulty eventually led to the convention of using the special symbol in place of to guard against this mistake. Even so, Euler considered it natural to introduce students to complex numbers much earlier than we do today. In his elementary algebra text book, Elements of Algebra, he introduces these numbers almost at once and then uses them in a natural way throughout.
In the 18th century complex numbers gained wider use, as it was noticed that formal manipulation of complex expressions could be used to simplify calculations involving trigonometric functions. For instance, in 1730 Abraham de Moivre noted that the complicated identities relating trigonometric functions of an integer multiple of an angle to powers of trigonometric functions of that angle could be simply re-expressed by the following well-known formula which bears his name, de Moivre's formula:
In 1748 Leonhard Euler went further and obtained Euler's formula of complex analysis:
by formally manipulating complex power series and observed that this formula could be used to reduce any trigonometric identity to much simpler exponential identities.
The idea of a complex number as a point in the complex plane (above) was first described by Caspar Wessel in 1799, although it had been anticipated as early as 1685 in Wallis's "De Algebra tractatus".
Wessel's memoir appeared in the Proceedings of the Copenhagen Academy but went largely unnoticed. In 1806 Jean-Robert Argand independently issued a pamphlet on complex numbers and provided a rigorous proof of the fundamental theorem of algebra. Gauss had earlier published an essentially topological proof of the theorem in 1797 but expressed his doubts at the time about "the true metaphysics of the square root of −1". It was not until 1831 that he overcame these doubts and published his treatise on complex numbers as points in the plane, largely establishing modern notation and terminology. The English mathematician G. H. Hardy remarked that Gauss was the first mathematician to use complex numbers in 'a really confident and scientific way' although mathematicians such as Niels Henrik Abel and Carl Gustav Jacob Jacobi were necessarily using them routinely before Gauss published his 1831 treatise. Augustin Louis Cauchy and Bernhard Riemann together brought the fundamental ideas of complex analysis to a high state of completion, commencing around 1825 in Cauchy's case.
The common terms used in the theory are chiefly due to the founders. Argand called formula_110 the "direction factor", and formula_111 the "modulus"; Cauchy (1828) called formula_112 the "reduced form" (l'expression réduite) and apparently introduced the term "argument"; Gauss used for formula_113, introduced the term "complex number" for , and called the "norm". The expression "direction coefficient", often used for formula_112, is due to Hankel (1867), and "absolute value," for "modulus," is due to Weierstrass.
Later classical writers on the general theory include Richard Dedekind, Otto Hölder, Felix Klein, Henri Poincaré, Hermann Schwarz, Karl Weierstrass and many others.
Generalizations and related notions.
The process of extending the field R of reals to C is known as Cayley–Dickson construction. It can be carried further to higher dimensions, yielding the quaternions H and octonions O which (as a real vector space) are of dimension 4 and 8, respectively.
However, just as applying the construction to reals loses the property of ordering, more properties familiar from real and complex numbers vanish with increasing dimension. The quaternions are only a skew field, i.e. for some : for two quaternions, the multiplication of octonions fails (in addition to not being commutative) to be associative: for some : .
Reals, complex numbers, quaternions and octonions are all normed division algebras over R. However, by Hurwitz's theorem they are the only ones. The next step in the Cayley–Dickson construction, the sedenions, in fact fails to have this structure.
The Cayley–Dickson construction is closely related to the regular representation of C, thought of as an R-algebra (an R-vector space with a multiplication), with respect to the basis . This means the following: the R-linear map
for some fixed complex number can be represented by a matrix (once a basis has been chosen). With respect to the basis , this matrix is
i.e., the one mentioned in the section on matrix representation of complex numbers above. While this is a linear representation of C in the 2 × 2 real matrices, it is not the only one. Any matrix
has the property that its square is the negative of the identity matrix: . Then
is also isomorphic to the field C, and gives an alternative complex structure on R. This is generalized by the notion of a linear complex structure.
Hypercomplex numbers also generalize R, C, H, and O. For example, this notion contains the split-complex numbers, which are elements of the ring (as opposed to ). In this ring, the equation has four solutions.
The field R is the completion of Q, the field of rational numbers, with respect to the usual absolute value metric. Other choices of metrics on Q lead to the fields Q of "p"-adic numbers (for any prime number "p"), which are thereby analogous to R. There are no other nontrivial ways of completing Q than R and Q, by Ostrowski's theorem. The algebraic closures formula_119 of Q still carry a norm, but (unlike C) are not complete with respect to it. The completion formula_120 of formula_119 turns out to be algebraically closed. This field is called "p"-adic complex numbers by analogy.
The fields R and Q and their finite field extensions, including C, are local fields.

</doc>
<doc id="5828" url="https://en.wikipedia.org/wiki?curid=5828" title="Cryptozoology">
Cryptozoology

Cryptozoology is a pseudoscience involving the search for creatures whose existence has not been proven due to lack of evidence. The animals cryptozoologists study are referred to as "cryptids" by cryptozoologists. This includes living examples of creatures that are otherwise considered extinct, such as non-avian dinosaurs; animals whose existence lacks physical evidence but which appear in folklore, such as Bigfoot and chupacabras; and wild animals dramatically outside their normal geographic ranges, such as phantom cats.
Cryptozoology is not a recognized branch of zoology nor a discipline of science. It is an example of pseudoscience because it relies heavily upon anecdotal evidence, stories, and alleged sightings.
While cryptozoology takes a pseudoscientific approach to creatures and beings from the folklore record, the academic study of folklore is folkloristics.
Definitions, etymology, and terminology.
In his survey of the history of cryptozoology, academic Lorenzo Rossi says that defining the term "cryptozoology" has been controversial:
Since its first appearance in the literature … both the word "cryptozoology" and its meaning have been the subject of heated discussions, so that hitherto a commonly accepted definition has yet to be found, and several authors have proposed a very personal vision of the discipline.
Attempts at defining the field were first made from 1982 to 1998 in "Cryptozoology", a peer-reviewed journal for the field. (Rossi himself defines "cryptozoology" as "a term that defines a branch of zoology thetis generally considered a pseudoscience devoted to the study of animal species whose existence is not supported by empirical evidence, but rather hypothesized via indirect or uncertain information, including oral traditions, eyewitness accounts, and inconclusive evidence".)
French-Belgian zoologist Bernard Heuvelmans, known as the "Father of Cryptozoology", coined the term "cryptology" from three elements, all from ancient Greek: "Kryptos" 'hidden', "Zoon" 'animal', and "Logós" 'discussion', yielding 'science of hidden animals'. Heuvelmans later proposed the term "crypto-zoology"—with hyphen— for the study of more 'supernatural' entities by cryptozoologists. However, this division never took hold in among cryptozoologists and academic Lorenzo Rossi says the 'supernatural' element "became the most common interpretation of cryptozoology among the general public, mostly thanks to the publication of commercial successful books on supernatural zoology …" and "hardly any further attempts were made to promote a more scientific approach to this field".
Usage of the term "cryptid" in cryptozoology (specifically among members of International Society of Cryptozoology) was proposed by J.E. Wall in 1983 to replace the problematic terms "hidden animal" and "monster". The term is now universally used among cryptozoologists. The term is not used in zoology.
Overview.
Heuvelmans "On the Track of Unknown Animals" (1955) traces the scholarly origins of the discipline to Anthonie Cornelis Oudemans and his 1892 study, "The Great Sea Serpent". Heuvelmans argued that cryptozoology should be undertaken with scientific rigor, but with an open-minded, interdisciplinary approach. He also stressed that attention should be given to local, urban and folkloric sources regarding such creatures, arguing that while often layered in unlikely and fantastic elements, folktales can have grains of truth and important information regarding undiscovered organisms. Phantom cats (an example of living animals supposedly found outside their normal ranges) are a common subject of cryptozoological interest, largely due to the relative likelihood of existence in comparison to more fantastical cryptids lacking nearly any conclusive evidence of existence, such as Mothman.
Another notable book on the subject is Willy Ley's "Exotic Zoology" (1959). Ley, best known for his writings on rocketry and related topics, was also trained in paleontology, and wrote a number of books about animals. Ley's collection "Exotic Zoology" is of some interest to cryptozoology, as he discusses the Yeti and sea serpents, as well as relict dinosaurs. The book entertains the possibility that some legendary creatures (like the sirrush, the unicorn, or the cyclops) might be based on actual animals, through misinterpretation of the animals and/or their remains. Also notable is the work of British zoologist and cryptozoologist Karl Shuker, who has published 12 books and countless articles on numerous cryptozoological subjects since the mid-1980s. Loren Coleman, a modern popularizer of cryptozoology, has chronicled the history and personalities of cryptozoology in his books.
Hallmarks.
Many species appear in cryptozoological literature, including mythical and folkloric animals, such as Bigfoot and lake monsters (including the Loch Ness Monster), which have appeared commonly as cultural references, and within TV, movies, and other media. A few extant species such as the okapi and mountain gorilla are also commonly used by cryptozoologists as examples of animals they say were previously thought to be cryptids, but are now known to exist.
The 2003 discovery of the fossil remains of "Homo floresiensis" was cited by paleontologist Henry Gee, editor of the journal "Nature", as possible evidence that humanoid cryptids like the Orang Pendek and yeti were "founded on grains of truth." "Cryptozoology," Gee said, "the study of such fabulous creatures, can come in from the cold." While cryptozoologists are often unable to properly follow the scientific method due to the nature of their work, the vast majority still reject supernatural explanations for cryptid sightings, preferring to keep explanations as plausible as possible without ruling out the cryptid's existence.
Reception and criticism.
Cryptozoology has been criticised because of its reliance on anecdotal information and because some cryptozoologists do not follow the scientific method, devoting a substantial portion of their efforts to investigations of animals that most scientists believe are unlikely to have existed.
In a 2011 forward for "The American Biology Teacher", then National Association of Biology Teachers president Dan Ward uses cryptozoology as an example of "technological pseudoscience" that may confuse students about the scientific method. Ward says that "Cryptozoology … is not valid science or even science at all. It is monster hunting." Historian of science Brian Regal includes an entry for cryptozoology in his "Pseudoscience: A Critical Encyclopedia" (2009). Regal says that "as an intellectual endeavor, cryptozoology has been studied as much as cryptozoologists have sought hidden animals".
In a 1992 issue of "Folklore", folklorist Véronique Campion-Vincent says:
Campion-Vincent says that "four currents can be distinguished in the study of mysterious animal appearances": "Forteans" ("compiler of anomalies" such as via publications like the "Fortean Times"), "occultists" (which she describes as related to "Forteans"), "folklorists", and "cryptozoologists". Regarding cryptozoologists, Campion-Vincent says that "this movement seems to deserve the appellation of parascience, like parapsychology: the same corpus is reviewed; many scientists participate, but for those who have an official status of university professor or researcher, the participation is a private hobby".
In her "Encyclopedia of American Folklore", academic Linda Watts says that "folklore concerning unreal animals or beings, sometimes called monsters, is a popular field of inquiry" and describes cryptozoology as an example of "American narrative traditions" that "feature many monsters".
Cryptozoologists contend that because species once considered superstition, hoaxes, delusions, or misidentifications were later accepted as legitimate by the scientific community, descriptions and reports of folkloric creatures should be taken seriously.
According to Mike Dash, few scientists doubt there are thousands of unknown animals, particularly invertebrates, awaiting discovery; however, cryptozoologists are largely uninterested in researching and cataloging newly discovered species of ants or beetles, instead focusing their efforts towards "more elusive" creatures that have often defied decades of work aimed at confirming their existence. The majority of mainstream criticism of cryptozoology is thus directed towards the search for megafaunal cryptids such as Bigfoot, the Yeti, and the Loch Ness Monster, which appear often in popular culture, but for which there is little or no scientific support. Some scientists argue that megafaunal cryptids are unlikely to exist undetected in great enough numbers to maintain a breeding population and are unlikely to be able to survive in their reported habitats owing to issues of climate and food supply.
Another criticism is that actual discoveries of new species have rarely, if ever, been predicted by cryptozoologists. Critics note that while other researchers have stumbled upon real animals, cryptozoologists have focused on finding legendary creatures with no success.

</doc>
