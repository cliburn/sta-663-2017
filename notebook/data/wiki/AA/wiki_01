<doc id="627" url="https://en.wikipedia.org/wiki?curid=627" title="Agriculture">
Agriculture

Agriculture is the cultivation of animals, plants, fungi, and other life forms for food, fiber, biofuel, medicinal and other products used to sustain and enhance human life. Agriculture was the key development in the rise of sedentary human civilization, whereby farming of domesticated species created food surpluses that nurtured the development of civilization. The study of agriculture is known as agricultural science. The history of agriculture dates back thousands of years, and its development has been driven and defined by greatly different climates, cultures, and technologies. In the civilized world, industrial agriculture based on large-scale monoculture farming has become the dominant agricultural methodology.
Modern agronomy, plant breeding, agrochemicals such as pesticides and fertilizers, and technological developments have in many cases sharply increased yields from cultivation, but at the same time have caused widespread ecological damage and negative human health effects. Selective breeding and modern practices in animal husbandry have similarly increased the output of meat, but have raised concerns about animal welfare and the health effects of the antibiotics, growth hormones, and other chemicals commonly used in industrial meat production. Genetically modified organisms are an increasing component of agriculture, although they are banned in several countries. Agricultural food production and water management are increasingly becoming global issues that are fostering debate on a number of fronts. Significant degradation of land and water resources, including the depletion of aquifers, has been observed in recent decades, and the effects of global warming on agriculture and of agriculture on global warming are still not fully understood.
The major agricultural products can be broadly grouped into foods, fibers, fuels, and raw materials. Specific foods include cereals (grains), vegetables, fruits, oils, meats and spices. Fibers include cotton, wool, hemp, silk and flax. Raw materials include lumber and bamboo. Other useful materials are produced by plants, such as resins, dyes, drugs, perfumes, biofuels and ornamental products such as cut flowers and nursery plants. Over one third of the world's workers are employed in agriculture, second only to the services' sector, although the percentages of agricultural workers in developed countries has decreased significantly over the past several centuries.
Etymology and terminology.
The word "agriculture" is a late Middle English adaptation of Latin "agricultūra", from "ager", "field", and "cultūra", "cultivation" or "growing". Agriculture usually refers to human activities, although it is also observed in certain species of ant, termite and ambrosia beetle. To practice agriculture means to use natural resources to "produce commodities which maintain life, including food, fiber, forest products, horticultural crops, and their related services." This definition includes arable farming or agronomy, and horticulture, all terms for the growing of plants, animal husbandry and forestry. A distinction is sometimes made between forestry and agriculture, based on the former's longer management rotations, extensive versus intensive management practices and development mainly by nature, rather than by man. Even then, it is acknowledged that there is a large amount of knowledge transfer and overlap between silviculture (the management of forests) and agriculture. In traditional farming, the two are often combined even on small landholdings, leading to the term agroforestry.
Agriculture and civilization.
Civilization was the product of the Agricultural Neolithic Revolution. In the course of history, civilization coincided in space with fertile areas (The Fertile Crescent) and most intensive state formation took place in circumscribed agricultural lands (Carneiro's circumscription theory). The Great Wall of China and the Roman limes demarcated the same northern frontier of the basic (cereal) agriculture. This cereal belt nourished the belt of great civilizations formed in the Axial Age and connected by the famous Silk Road.
Ancient Egyptians, whose agriculture depended exclusively on Nile, deified the River, worshiped, and exalted in a great hymn. The Chinese imperial court issued numerous edicts, stating: "Agriculture is the foundation of this Empire." Egyptian, Mesopotamian, Chinese, and Inca Emperors themselves plowed ceremonial fields in order to show personal example to everyone. "Thus the most exalted men in human history—the "Beloved of the Gods", the "Son of Sun", the "Son of Heaven", and Inca—although ceremonially but nonetheless personally tilled the earth."
Ancient strategists, Chinese Guan Zhong and Shang Yang and Indian Kautilya, drew doctrines linking agriculture with military power. Agriculture defined the limits on how large and for how long an army could be mobilized. Shang Yang called agriculture and war the "One". In the vast human pantheon of agricultural deities there are several deities who combined the functions of agriculture and war. Great granaries were the inevitable feature of great empires.
As the Neolithic Agricultural Revolution produced civilization, the modern Agricultural Revolution, begun in Britain (British Agricultural Revolution), made possible the Industrial civilization. The first precondition for industry was greater yields by less manpower, resulting in greater percentage of manpower available for non-agricultural sectors. The most industrial world appeared in the most fertile cereal regions of the world. Today's "Industrial North" (Global North) originally was the belt of civilizations formed in the Axial Age. 
The link between the basic (cereal) agriculture and military power survived in the Industrial Age too. All modern great powers were first of all great cereal powers, as had been the greatest of their predecessors. The cereal domestic product closely correlates with the Gross Domestic Product and it was argued that the cereal product is the basis of the GDP. The Cold War was waged between two cereal superpowers.The outcome of the Cold War corresponds to the cereal factor too—the United States produced its "agricultural miracle", while the USSR suffered a progressive cereal crisis. In 1976, French sociologist Emmanuel Todd, impressed by the magnitude of Soviet grain purchases, predicted the collapse of the USSR within ten years.
Contemporary agriculture.
In the past century, agriculture has been characterized by increased productivity, the substitution of synthetic fertilizers and pesticides for labor, water pollution, and farm subsidies. In recent years there has been a backlash against the external environmental effects of conventional agriculture, resulting in the organic and sustainable agriculture movements. One of the major forces behind this movement has been the European Union, which first certified organic food in 1991 and began reform of its Common Agricultural Policy (CAP) in 2005 to phase out commodity-linked farm subsidies, also known as decoupling. The growth of organic farming has renewed research in alternative technologies such as integrated pest management and selective breeding. Recent mainstream technological developments include genetically modified food.
In 2007, higher incentives for farmers to grow non-food biofuel crops combined with other factors, such as overdevelopment of former farm lands, rising transportation costs, climate change, growing consumer demand in China and India, and population growth, caused food shortages in Asia, the Middle East, Africa, and Mexico, as well as rising food prices around the globe. As of December 2007, 37 countries faced food crises, and 20 had imposed some sort of food-price controls. Some of these shortages resulted in food riots and even deadly stampedes. The International Fund for Agricultural Development posits that an increase in smallholder agriculture may be part of the solution to concerns about food prices and overall food security. They in part base this on the experience of Vietnam, which went from a food importer to large food exporter and saw a significant drop in poverty, due mainly to the development of smallholder agriculture in the country.
Disease and land degradation are two of the major concerns in agriculture today. For example, an epidemic of stem rust on wheat caused by the Ug99 lineage is currently spreading across Africa and into Asia and is causing major concerns due to crop losses of 70% or more under some conditions. Approximately 40% of the world's agricultural land is seriously degraded. In Africa, if current trends of soil degradation continue, the continent might be able to feed just 25% of its population by 2025, according to UNU's Ghana-based Institute for Natural Resources in Africa.
Agrarian structure is a long-term structure in the Braudelian understanding of the concept. On a larger scale the agrarian structure is more dependent on the regional, social, cultural and historical factors than on the state’s undertaken activities. Like in Poland, where despite running an intense agrarian policy for many years, the agrarian structure in 2002 has much in common with that found in 1921 soon after the partitions period.
In 2009, the agricultural output of China was the largest in the world, followed by the European Union, India and the United States, according to the International Monetary Fund ("see below"). Economists measure the total factor productivity of agriculture and by this measure agriculture in the United States is roughly 1.7 times more productive than it was in 1948.
Workforce.
, the International Labour Organization states that approximately one billion people, or over 1/3 of the available work force, are employed in the global agricultural sector. Agriculture constitutes approximately 70% of the global employment of children, and in many countries employs the largest percentage of women of any industry. The service sector only overtook the agricultural sector as the largest global employer in 2007. Between 1997 and 2007, the percentage of people employed in agriculture fell by over four percentage points, a trend that is expected to continue. The number of people employed in agriculture varies widely on a per-country basis, ranging from less than 2% in countries like the US and Canada to over 80% in many African nations. In developed countries, these figures are significantly lower than in previous centuries. During the 16th century in Europe, for example, between 55 and 75 percent of the population was engaged in agriculture, depending on the country. By the 19th century in Europe, this had dropped to between 35 and 65 percent. In the same countries today, the figure is less than 10%.
Safety.
Agriculture, specifically farming, remains a hazardous industry, and farmers worldwide remain at high risk of work-related injuries, lung disease, noise-induced hearing loss, skin diseases, as well as certain cancers related to chemical use and prolonged sun exposure. On industrialized farms, injuries frequently involve the use of agricultural machinery, and a common cause of fatal agricultural injuries in developed countries is tractor rollovers. Pesticides and other chemicals used in farming can also be hazardous to worker health, and workers exposed to pesticides may experience illness or have children with birth defects. As an industry in which families commonly share in work and live on the farm itself, entire families can be at risk for injuries, illness, and death. Common causes of fatal injuries among young farm workers include drowning, machinery and motor vehicle-related accidents.
The International Labour Organization considers agriculture "one of the most hazardous of all economic sectors." It estimates that the annual work-related death toll among agricultural employees is at least 170,000, twice the average rate of other jobs. In addition, incidences of death, injury and illness related to agricultural activities often go unreported. The organization has developed the Safety and Health in Agriculture Convention, 2001, which covers the range of risks in the agriculture occupation, the prevention of these risks and the role that individuals and organizations engaged in agriculture should play.
Agricultural production systems.
Crop cultivation systems.
Cropping systems vary among farms depending on the available resources and constraints; geography and climate of the farm; government policy; economic, social and political pressures; and the philosophy and culture of the farmer.
Shifting cultivation (or slash and burn) is a system in which forests are burnt, releasing nutrients to support cultivation of annual and then perennial crops for a period of several years. Then the plot is left fallow to regrow forest, and the farmer moves to a new plot, returning after many more years (10–20). This fallow period is shortened if population density grows, requiring the input of nutrients (fertilizer or manure) and some manual pest control. Annual cultivation is the next phase of intensity in which there is no fallow period. This requires even greater nutrient and pest control inputs.
Further industrialization led to the use of monocultures, when one cultivar is planted on a large acreage. Because of the low biodiversity, nutrient use is uniform and pests tend to build up, necessitating the greater use of pesticides and fertilizers. Multiple cropping, in which several crops are grown sequentially in one year, and intercropping, when several crops are grown at the same time, are other kinds of annual cropping systems known as polycultures.
In subtropical and arid environments, the timing and extent of agriculture may be limited by rainfall, either not allowing multiple annual crops in a year, or requiring irrigation. In all of these environments perennial crops are grown (coffee, chocolate) and systems are practiced such as agroforestry. In temperate environments, where ecosystems were predominantly grassland or prairie, highly productive annual farming is the dominant agricultural system.
Crop statistics.
Important categories of crops include cereals and pseudocereals, pulses (legumes), forage, and fruits and vegetables. Specific crops are cultivated in distinct growing regions throughout the world. In millions of metric tons, based on FAO estimate.
Livestock production systems.
Animals, including horses, mules, oxen, water buffalo, camels, llamas, alpacas, donkeys, and dogs, are often used to help cultivate fields, harvest crops, wrangle other animals, and transport farm products to buyers. Animal husbandry not only refers to the breeding and raising of animals for meat or to harvest animal products (like milk, eggs, or wool) on a continual basis, but also to the breeding and care of species for work and companionship.
Livestock production systems can be defined based on feed source, as grassland-based, mixed, and landless. , 30% of Earth's ice- and water-free area was used for producing livestock, with the sector employing approximately 1.3 billion people. Between the 1960s and the 2000s, there was a significant increase in livestock production, both by numbers and by carcass weight, especially among beef, pigs and chickens, the latter of which had production increased by almost a factor of 10. Non-meat animals, such as milk cows and egg-producing chickens, also showed significant production increases. Global cattle, sheep and goat populations are expected to continue to increase sharply through 2050. Aquaculture or fish farming, the production of fish for human consumption in confined operations, is one of the fastest growing sectors of food production, growing at an average of 9% a year between 1975 and 2007.
During the second half of the 20th century, producers using selective breeding focused on creating livestock breeds and crossbreeds that increased production, while mostly disregarding the need to preserve genetic diversity. This trend has led to a significant decrease in genetic diversity and resources among livestock breeds, leading to a corresponding decrease in disease resistance and local adaptations previously found among traditional breeds.
Grassland based livestock production relies upon plant material such as shrubland, rangeland, and pastures for feeding ruminant animals. Outside nutrient inputs may be used, however manure is returned directly to the grassland as a major nutrient source. This system is particularly important in areas where crop production is not feasible because of climate or soil, representing 30–40 million pastoralists. Mixed production systems use grassland, fodder crops and grain feed crops as feed for ruminant and monogastric (one stomach; mainly chickens and pigs) livestock. Manure is typically recycled in mixed systems as a fertilizer for crops.
Landless systems rely upon feed from outside the farm, representing the de-linking of crop and livestock production found more prevalently in Organisation for Economic Co-operation and Development(OECD) member countries. Synthetic fertilizers are more heavily relied upon for crop production and manure utilization becomes a challenge as well as a source for pollution. Industrialized countries use these operations to produce much of the global supplies of poultry and pork. Scientists estimate that 75% of the growth in livestock production between 2003 and 2030 will be in confined animal feeding operations, sometimes called factory farming. Much of this growth is happening in developing countries in Asia, with much smaller amounts of growth in Africa. Some of the practices used in commercial livestock production, including the usage of growth hormones, are controversial.
Production practices.
Farming is the practice of agriculture by specialized labor in an area primarily devoted to agricultural processes, in service of a dislocated population usually in a city.
Tillage is the practice of plowing soil to prepare for planting or for nutrient incorporation or for pest control. Tillage varies in intensity from conventional to no-till. It may improve productivity by warming the soil, incorporating fertilizer and controlling weeds, but also renders soil more prone to erosion, triggers the decomposition of organic matter releasing CO, and reduces the abundance and diversity of soil organisms.
Pest control includes the management of weeds, insects, mites, and diseases. Chemical (pesticides), biological (biocontrol), mechanical (tillage), and cultural practices are used. Cultural practices include crop rotation, culling, cover crops, intercropping, composting, avoidance, and resistance. Integrated pest management attempts to use all of these methods to keep pest populations below the number which would cause economic loss, and recommends pesticides as a last resort.
Nutrient management includes both the source of nutrient inputs for crop and livestock production, and the method of utilization of manure produced by livestock. Nutrient inputs can be chemical inorganic fertilizers, manure, green manure, compost and mined minerals. Crop nutrient use may also be managed using cultural techniques such as crop rotation or a fallow period. Manure is used either by holding livestock where the feed crop is growing, such as in managed intensive rotational grazing, or by spreading either dry or liquid formulations of manure on cropland or pastures.
Water management is needed where rainfall is insufficient or variable, which occurs to some degree in most regions of the world. Some farmers use irrigation to supplement rainfall. In other areas such as the Great Plains in the U.S. and Canada, farmers use a fallow year to conserve soil moisture to use for growing a crop in the following year. Agriculture represents 70% of freshwater use worldwide.
According to a report by the International Food Policy Research Institute, agricultural technologies will have the greatest impact on food production if adopted in combination with each other; using a model that assessed how eleven technologies could impact agricultural productivity, food security and trade by 2050, the International Food Policy Research Institute found that the number of people at risk from hunger could be reduced by as much as 40% and food prices could be reduced by almost half.
"Payment for ecosystem services (PES) can further incentivise efforts to green the agriculture sector. This is an approach that verifies values and rewards the benefits of ecosystem services provided by green agricultural practices." "Innovative PES measures could include reforestation payments made by cities to upstream communities in rural areas of shared watersheds for improved quantities and quality of fresh water for municipal users. Ecoservice payments by farmers to upstream forest stewards for properly managing the flow of soil nutrients, and methods to monetise the carbon sequestration and emission reduction credit benefits of green agriculture practices in order to compensate farmers for their efforts to restore and build SOM and employ other practices."
Crop alteration and biotechnology.
Crop alteration has been practiced by humankind for thousands of years, since the beginning of civilization. Altering crops through breeding practices changes the genetic make-up of a plant to develop crops with more beneficial characteristics for humans, for example, larger fruits or seeds, drought-tolerance, or resistance to pests. Significant advances in plant breeding ensued after the work of geneticist Gregor Mendel. His work on dominant and recessive alleles, although initially largely ignored for almost 50 years, gave plant breeders a better understanding of genetics and breeding techniques. Crop breeding includes techniques such as plant selection with desirable traits, self-pollination and cross-pollination, and molecular techniques that genetically modify the organism.
Domestication of plants has, over the centuries increased yield, improved disease resistance and drought tolerance, eased harvest and improved the taste and nutritional value of crop plants. Careful selection and breeding have had enormous effects on the characteristics of crop plants. Plant selection and breeding in the 1920s and 1930s improved pasture (grasses and clover) in New Zealand. Extensive X-ray and ultraviolet induced mutagenesis efforts (i.e. primitive genetic engineering) during the 1950s produced the modern commercial varieties of grains such as wheat, corn (maize) and barley.
The Green Revolution popularized the use of conventional hybridization to sharply increase yield by creating "high-yielding varieties". For example, average yields of corn (maize) in the USA have increased from around 2.5 tons per hectare (t/ha) (40 bushels per acre) in 1900 to about 9.4 t/ha (150 bushels per acre) in 2001. Similarly, worldwide average wheat yields have increased from less than 1 t/ha in 1900 to more than 2.5 t/ha in 1990. South American average wheat yields are around 2 t/ha, African under 1 t/ha, and Egypt and Arabia up to 3.5 to 4 t/ha with irrigation. In contrast, the average wheat yield in countries such as France is over 8 t/ha. Variations in yields are due mainly to variation in climate, genetics, and the level of intensive farming techniques (use of fertilizers, chemical pest control, growth control to avoid lodging).
Genetic engineering.
Genetically modified organisms (GMO) are organisms whose genetic material has been altered by genetic engineering techniques generally known as recombinant DNA technology. Genetic engineering has expanded the genes available to breeders to utilize in creating desired germlines for new crops. Increased durability, nutritional content, insect and virus resistance and herbicide tolerance are a few of the attributes bred into crops through genetic engineering. For some, GMO crops cause food safety and food labeling concerns. Numerous countries have placed restrictions on the production, import or use of GMO foods and crops, which have been put in place due to concerns over potential health issues, declining agricultural diversity and contamination of non-GMO crops. Currently a global treaty, the Biosafety Protocol, regulates the trade of GMOs. There is ongoing discussion regarding the labeling of foods made from GMOs, and while the EU currently requires all GMO foods to be labeled, the US does not.
Herbicide-resistant seed has a gene implanted into its genome that allows the plants to tolerate exposure to herbicides, including glyphosates. These seeds allow the farmer to grow a crop that can be sprayed with herbicides to control weeds without harming the resistant crop. Herbicide-tolerant crops are used by farmers worldwide. With the increasing use of herbicide-tolerant crops, comes an increase in the use of glyphosate-based herbicide sprays. In some areas glyphosate resistant weeds have developed, causing farmers to switch to other herbicides. Some studies also link widespread glyphosate usage to iron deficiencies in some crops, which is both a crop production and a nutritional quality concern, with potential economic and health implications.
Other GMO crops used by growers include insect-resistant crops, which have a gene from the soil bacterium "Bacillus thuringiensis" (Bt), which produces a toxin specific to insects. These crops protect plants from damage by insects. Some believe that similar or better pest-resistance traits can be acquired through traditional breeding practices, and resistance to various pests can be gained through hybridization or cross-pollination with wild species. In some cases, wild species are the primary source of resistance traits; some tomato cultivars that have gained resistance to at least 19 diseases did so through crossing with wild populations of tomatoes.
Environmental impact.
Agriculture, as implemented through the method of farming, imposes external costs upon society through pesticides, nutrient runoff, excessive water usage, loss of natural environment and assorted other problems. A 2000 assessment of agriculture in the UK determined total external costs for 1996 of £2,343 million, or £208 per hectare. A 2005 analysis of these costs in the USA concluded that cropland imposes approximately $5 to 16 billion ($30 to $96 per hectare), while livestock production imposes $714 million. Both studies, which focused solely on the fiscal impacts, concluded that more should be done to internalize external costs. Neither included subsidies in their analysis, but they noted that subsidies also influence the cost of agriculture to society. In 2010, the International Resource Panel of the United Nations Environment Programme published a report assessing the environmental impacts of consumption and production. The study found that agriculture and food consumption are two of the most important drivers of environmental pressures, particularly habitat change, climate change, water use and toxic emissions. The 2011 UNEP Green Economy report states that "gricultural operations, excluding land use changes, produce approximately 13 per cent of anthropogenic global GHG emissions. This includes GHGs emitted by the use of inorganic fertilisers agro-chemical pesticides and herbicides; (GHG emissions resulting from production of these inputs are included in industrial emissions); and fossil fuel-energy inputs. "On average we find that the total amount of fresh residues from agricultural and forestry production for second- generation biofuel production amounts to 3.8 billion tonnes per year between 2011 and 2050 (with an average annual growth rate of 11 per cent throughout the period analysed, accounting for higher growth during early years, 48 per cent for 2011-2020 and an average 2 per cent annual expansion after 2020)."
Livestock issues.
A senior UN official and co-author of a UN report detailing this problem, Henning Steinfeld, said "Livestock are one of the most significant contributors to today's most serious environmental problems". Livestock production occupies 70% of all land used for agriculture, or 30% of the land surface of the planet. It is one of the largest sources of greenhouse gases, responsible for 18% of the world's greenhouse gas emissions as measured in CO equivalents. By comparison, all transportation emits 13.5% of the CO. It produces 65% of human-related nitrous oxide (which has 296 times the global warming potential of CO) and 37% of all human-induced methane (which is 23 times as warming as CO.) It also generates 64% of the ammonia emission. Livestock expansion is cited as a key factor driving deforestation; in the Amazon basin 70% of previously forested area is now occupied by pastures and the remainder used for feedcrops. Through deforestation and land degradation, livestock is also driving reductions in biodiversity. Furthermore, the UNEP states that "methane emissions from global livestock are projected to increase by 60 per cent by 2030 under current practices and consumption patterns."
Land and water issues.
Land transformation, the use of land to yield goods and services, is the most substantial way humans alter the Earth's ecosystems, and is considered the driving force in the loss of biodiversity. Estimates of the amount of land transformed by humans vary from 39 to 50%. Land degradation, the long-term decline in ecosystem function and productivity, is estimated to be occurring on 24% of land worldwide, with cropland overrepresented. The UN-FAO report cites land management as the driving factor behind degradation and reports that 1.5 billion people rely upon the degrading land. Degradation can be deforestation, desertification, soil erosion, mineral depletion, or chemical degradation (acidification and salinization).
Eutrophication, excessive nutrients in aquatic ecosystems resulting in algal blooms and anoxia, leads to fish kills, loss of biodiversity, and renders water unfit for drinking and other industrial uses. Excessive fertilization and manure application to cropland, as well as high livestock stocking densities cause nutrient (mainly nitrogen and phosphorus) runoff and leaching from agricultural land. These nutrients are major nonpoint pollutants contributing to eutrophication of aquatic ecosystems.
Agriculture accounts for 70% of withdrawals of freshwater resources. Agriculture is a major draw on water from aquifers, and currently draws from those underground water sources at an unsustainable rate. It is long known that aquifers in areas as diverse as northern China, the Upper Ganges and the western US are being depleted, and new research extends these problems to aquifers in Iran, Mexico and Saudi Arabia. Increasing pressure is being placed on water resources by industry and urban areas, meaning that water scarcity is increasing and agriculture is facing the challenge of producing more food for the world's growing population with reduced water resources. Agricultural water usage can also cause major environmental problems, including the destruction of natural wetlands, the spread of water-borne diseases, and land degradation through salinization and waterlogging, when irrigation is performed incorrectly.
Pesticides.
Pesticide use has increased since 1950 to 2.5 million tons annually worldwide, yet crop loss from pests has remained relatively constant. The World Health Organization estimated in 1992 that 3 million pesticide poisonings occur annually, causing 220,000 deaths. Pesticides select for pesticide resistance in the pest population, leading to a condition termed the 'pesticide treadmill' in which pest resistance warrants the development of a new pesticide.
An alternative argument is that the way to 'save the environment' and prevent famine is by using pesticides and intensive high yield farming, a view exemplified by a quote heading the Center for Global Food Issues website: 'Growing more per acre leaves more land for nature'. However, critics argue that a trade-off between the environment and a need for food is not inevitable, and that pesticides simply replace good agronomic practices such as crop rotation. The UNEP introduces the Push–pull agricultural pest management technique which involves intercropping that uses plant aromas to repel or push away pests while pulling in or attracting the right insects. "The implementation of push-pull in eastern Africa has significantly increased maize yields and the combined cultivation of N-fixing forage crops has enriched the soil and has also provided farmers with feed for livestock. With increased livestock operations, the farmers are able to produce meat, milk and other dairy products and they use the manure as organic fertiliser that returns nutrients to the fields."
Climate change.
Climate change has the potential to affect agriculture through changes in temperature, rainfall (timing and quantity), CO, solar radiation and the interaction of these elements. Extreme events, such as droughts and floods, are forecast to increase as climate change takes hold. Agriculture is among sectors most vulnerable to the impacts of climate change; water supply for example, will be critical to sustain agricultural production and provide the increase in food output required to sustain the world's growing population. Fluctuations in the flow of rivers are likely to increase in the twenty-first century. Based on the experience of countries in the Nile river basin (Ethiopia, Kenya and Sudan) and other developing countries, depletion of water resources during seasons crucial for agriculture can lead to a decline in yield by up to 50%. Transformational approaches will be needed to manage natural resources in the future. For example, policies, practices and tools promoting climate-smart agriculture will be important, as will better use of scientific information on climate for assessing risks and vulnerability. Planners and policy-makers will need to help create suitable policies that encourage funding for such agricultural transformation.
Agriculture in its many forms can both mitigate or worsen global warming. Some of the increase in CO in the atmosphere comes from the decomposition of organic matter in the soil, and much of the methane emitted into the atmosphere is caused by the decomposition of organic matter in wet soils such as rice paddies, as well as the normal digestive activities of farm animals. Further, wet or anaerobic soils also lose nitrogen through denitrification, releasing the greenhouse gases nitric oxide and nitrous oxide. Changes in management can reduce the release of these greenhouse gases, and soil can further be used to sequester some of the CO in the atmosphere. Informed by the UNEP, "griculture also produces about 58 per cent of global nitrous oxide emissions and about 47 per cent of global methane emissions. Cattle and rice farms release methane, fertilized fields release nitrous oxide, and the cutting down of rainforests to grow crops or raise livestock releases carbon dioxide. Both of these gases have a far greater global warming potential per tonne than CO2 (298 times and 25 times respectively)."
There are several factors within the field of agriculture that contribute to the large amount of CO2 emissions. The diversity of the sources ranges from the production of farming tools to the transport of harvested produce. Approximately 8% of the national carbon footprint is due to agricultural sources. Of that, 75% is of the carbon emissions released from the production of crop assisting chemicals. Factories producing insecticides, herbicides, fungicides, and fertilizers are a major culprit of the greenhouse gas. Productivity on the farm itself and the use of machinery is another source of the carbon emission. Almost all the industrial machines used in modern farming are powered by fossil fuels. These instruments are burning fossil fuels from the beginning of the process to the end. Tractors are the root of this source. The tractor is going to burn fuel and release CO2 just to run. The amount of emissions from the machinery increase with the attachment of different units and need for more power. During the soil preparation stage tillers and plows will be used to disrupt the soil. During growth watering pumps and sprayers are used to keep the crops hydrated. And when the crops are ready for picking a forage or combine harvester is used. These types of machinery all require additional energy which leads to increased carbon dioxide emissions from the basic tractors. The final major contribution to CO2 emissions in agriculture is in the final transport of produce. Local farming suffered a decline over the past century due to large amounts of farm subsidies. The majority of crops are shipped hundreds of miles to various processing plants before ending up in the grocery store. These shipments are made using fossil fuel burning modes of transportation. Inevitably these transport adds to carbon dioxide emissions.
Sustainability.
Some major organizations are hailing farming within agroecosystems as the way forward for mainstream agriculture. Current farming methods have resulted in over-stretched water resources, high levels of erosion and reduced soil fertility. According to a report by the International Water Management Institute and UNEP, there is not enough water to continue farming using current practices; therefore how critical water, land, and ecosystem resources are used to boost crop yields must be reconsidered. The report suggested assigning value to ecosystems, recognizing environmental and livelihood tradeoffs, and balancing the rights of a variety of users and interests. Inequities that result when such measures are adopted would need to be addressed, such as the reallocation of water from poor to rich, the clearing of land to make way for
more productive farmland, or the preservation of a wetland system that limits fishing rights.
Technological advancements help provide farmers with tools and resources to make farming more sustainable. New technologies have given rise to innovations like conservation tillage, a farming process which helps prevent land loss to erosion, water pollution and enhances carbon sequestration.
According to a report by the International Food Policy Research Institute (IFPRI), agricultural technologies will have the greatest impact on food production if adopted in combination with each other; using a model that assessed how eleven technologies could impact agricultural productivity, food security and trade by 2050, IFPRI found that the number of people at risk from hunger could be reduced by as much as 40% and food prices could be reduced by almost half.
Agricultural economics.
Agricultural economics refers to economics as it relates to the "production, distribution and consumption of gricultura goods and services". Combining agricultural production with general theories of marketing and business as a discipline of study began in the late 1800s, and grew significantly through the 20th century. Although the study of agricultural economics is relatively recent, major trends in agriculture have significantly affected national and international economies throughout history, ranging from tenant farmers and sharecropping in the post-American Civil War Southern United States to the European feudal system of manorialism. In the United States, and elsewhere, food costs attributed to food processing, distribution, and agricultural marketing, sometimes referred to as the value chain, have risen while the costs attributed to farming have declined. This is related to the greater efficiency of farming, combined with the increased level of value addition (e.g. more highly processed products) provided by the supply chain. Market concentration has increased in the sector as well, and although the total effect of the increased market concentration is likely increased efficiency, the changes redistribute economic surplus from producers (farmers) and consumers, and may have negative implications for rural communities.
National government policies can significantly change the economic marketplace for agricultural products, in the form of taxation, subsidies, tariffs and other measures. Since at least the 1960s, a combination of import/export restrictions, exchange rate policies and subsidies have affected farmers in both the developing and developed world. In the 1980s, it was clear that non-subsidized farmers in developing countries were experiencing adverse effects from national policies that created artificially low global prices for farm products. Between the mid-1980s and the early 2000s, several international agreements were put into place that limited agricultural tariffs, subsidies and other trade restrictions.
However, , there was still a significant amount of policy-driven distortion in global agricultural product prices. The three agricultural products with the greatest amount of trade distortion were sugar, milk and rice, mainly due to taxation. Among the oilseeds, sesame had the greatest amount of taxation, but overall, feed grains and oilseeds had much lower levels of taxation than livestock products. Since the 1980s, policy-driven distortions have seen a greater decrease among livestock products than crops during the worldwide reforms in agricultural policy. Despite this progress, certain crops, such as cotton, still see subsidies in developed countries artificially deflating global prices, causing hardship in developing countries with non-subsidized farmers. Unprocessed commodities (i.e. corn, soybeans, cows) are generally graded to indicate quality. The quality affects the price the producer receives. Commodities are generally reported by production quantities, such as volume, number or weight.
Energy and agriculture.
Since the 1940s, agricultural productivity has increased dramatically, due largely to the increased use of energy-intensive mechanization, fertilizers and pesticides. The vast majority of this energy input comes from fossil fuel sources. Between the 1960–65 measuring cycle and the cycle from 1986 to 1990, the Green Revolution transformed agriculture around the globe, with world grain production increasing significantly (between 70% and 390% for wheat and 60% to 150% for rice, depending on geographic area) as world population doubled. Modern agriculture's heavy reliance on petrochemicals and mechanization has raised concerns that oil shortages could increase costs and reduce agricultural output, causing food shortages.
Modern or industrialized agriculture is dependent on fossil fuels in two fundamental ways: 1. direct consumption on the farm and 2. indirect consumption to manufacture inputs used on the farm. Direct consumption includes the use of lubricants and fuels to operate farm vehicles and machinery; and use of gasoline, liquid propane, and electricity to power dryers, pumps, lights, heaters, and coolers. American farms directly consumed about 1.2 exajoules (1.1 quadrillion BTU) in 2002, or just over 1% of the nation's total energy.
Indirect consumption is mainly oil and natural gas used to manufacture fertilizers and pesticides, which accounted for 0.6 exajoules (0.6 quadrillion BTU) in 2002. The natural gas and coal consumed by the production of nitrogen fertilizer can account for over half of the agricultural energy usage. China utilizes mostly coal in the production of nitrogen fertilizer, while most of Europe uses large amounts of natural gas and small amounts of coal. According to a 2010 report published by The Royal Society, agriculture is increasingly dependent on the direct and indirect input of fossil fuels. Overall, the fuels used in agriculture vary based on several factors, including crop, production system and location. The energy used to manufacture farm machinery is also a form of indirect agricultural energy consumption. Together, direct and indirect consumption by US farms accounts for about 2% of the nation's energy use. Direct and indirect energy consumption by U.S. farms peaked in 1979, and has gradually declined over the past 30 years. Food systems encompass not just agricultural production, but also off-farm processing, packaging, transporting, marketing, consumption, and disposal of food and food-related items. Agriculture accounts for less than one-fifth of food system energy use in the US.
Mitigation of effects of petroleum shortages.
In the event of a petroleum shortage (see peak oil for global concerns), organic agriculture can be more attractive than conventional practices that use petroleum-based pesticides, herbicides, or fertilizers. Some studies using modern organic-farming methods have reported yields as high as those available from conventional farming. In the aftermath of the fall of the Soviet Union, with shortages of conventional petroleum-based inputs, Cuba made use of mostly organic practices, including biopesticides, plant-based pesticides and sustainable cropping practices, to feed its populace. However, organic farming may be more labor-intensive and would require a shift of the workforce from urban to rural areas. The reconditioning of soil to restore nutrients lost during the use of monoculture agriculture techniques also takes time.
It has been suggested that rural communities might obtain fuel from the biochar and synfuel process, which uses agricultural "waste" to provide charcoal fertilizer, some fuel "and" food, instead of the normal food vs. fuel debate. As the synfuel would be used on-site, the process would be more efficient and might just provide enough fuel for a new organic-agriculture fusion.
It has been suggested that some transgenic plants may some day be developed which would allow for maintaining or increasing yields while requiring fewer fossil-fuel-derived inputs than conventional crops. The possibility of success of these programs is questioned by ecologists and economists concerned with unsustainable GMO practices such as terminator seeds. While there has been some research on sustainability using GMO crops, at least one prominent multi-year attempt by Monsanto Company has been unsuccessful, though during the same period traditional breeding techniques yielded a more sustainable variety of the same crop.
Policy.
Agricultural policy is the set of government decisions and actions relating to domestic agriculture and imports of foreign agricultural products. Governments usually implement agricultural policies with the goal of achieving a specific outcome in the domestic agricultural product markets. Some overarching themes include risk management and adjustment (including policies related to climate change, food safety and natural disasters), economic stability (including policies related to taxes), natural resources and environmental sustainability (especially water policy), research and development, and market access for domestic commodities (including relations with global organizations and agreements with other countries). Agricultural policy can also touch on food quality, ensuring that the food supply is of a consistent and known quality, food security, ensuring that the food supply meets the population's needs, and conservation. Policy programs can range from financial programs, such as subsidies, to encouraging producers to enroll in voluntary quality assurance programs.
There are many influences on the creation of agricultural policy, including consumers, agribusiness, trade lobbies and other groups. Agribusiness interests hold a large amount of influence over policy making, in the form of lobbying and campaign contributions. Political action groups, including those interested in environmental issues and labor unions, also provide influence, as do lobbying organizations representing individual agricultural commodities. The Food and Agriculture Organization of the United Nations (FAO) leads international efforts to defeat hunger and provides a forum for the negotiation of global agricultural regulations and agreements. Dr. Samuel Jutzi, director of FAO's animal production and health division, states that lobbying by large corporations has stopped reforms that would improve human health and the environment. For example, proposals in 2010 for a voluntary code of conduct for the livestock industry that would have provided incentives for improving standards for health, and environmental regulations, such as the number of animals an area of land can support without long-term damage, were successfully defeated due to large food company pressure.

</doc>
<doc id="628" url="https://en.wikipedia.org/wiki?curid=628" title="Aldous Huxley">
Aldous Huxley

Aldous Leonard Huxley (26 July 1894 – 22 November 1963) was an English writer, novelist, philosopher and prominent member of the Huxley family. He graduated from Balliol College, Oxford with a first in English literature.
He was best known for his novels including "Brave New World", set in a dystopian London, and for non-fiction books, such as "The Doors of Perception", which recalls experiences when taking a psychedelic drug, and a wide-ranging output of essays. Early in his career Huxley edited the magazine "Oxford Poetry", and published short stories and poetry. Mid career and later, he published travel writing, film stories, and scripts. He spent the later part of his life in the U.S., living in Los Angeles from 1937 until his death. In 1962, a year before his death, he was elected Companion of Literature by the Royal Society of Literature.
Huxley was a humanist, pacifist, and satirist. He later became interested in spiritual subjects such as parapsychology and philosophical mysticism, in particular Universalism. By the end of his life, Huxley was widely acknowledged as one of the pre-eminent intellectuals of his time. He was nominated for the Nobel Prize in Literature in seven different years.
Early life.
Aldous Huxley was born in Godalming, Surrey, England, in 1894. He was the third son of the writer and schoolmaster Leonard Huxley and his first wife, Julia Arnold, who founded Prior's Field School. Julia was the niece of poet and critic Matthew Arnold and the sister of Mrs. Humphrey Ward. Aldous was the grandson of Thomas Henry Huxley, the zoologist, agnostic, and controversialist ("Darwin's Bulldog"). His brother Julian Huxley and half-brother Andrew Huxley also became outstanding biologists. Aldous had another brother, Noel Trevelyan Huxley (1891–1914), who committed suicide after a period of clinical depression.
Huxley's education began in his father's well-equipped botanical laboratory, after which he enrolled at Hillside School, Malvern. He was taught there by his own mother for several years until she became terminally ill. After Hillside, he went on to Eton College. Huxley's mother died in 1908 when he was 14. In 1911, he contracted an eye disease (keratitis punctata) which "left i practically blind for two to three years". In October 1913, Huxley went up to Balliol College, Oxford, where he read English Literature. In January 1916 he volunteered to join the British Army in the Great War, but was rejected on health grounds, being half-blind in one eye. His eyesight later partly recovered. In 1916 he edited "Oxford Poetry" and in June of that year graduated BA with First Class honours. His brother Julian wrote:
Following his years at Balliol, Huxley, being financially indebted to his father, decided to find employment. From April to July 1917, he was in charge of ordering supplies at the Air Ministry for the Royal Air Force. He taught French for a year at Eton, where Eric Blair (later to become George Orwell) and Steven Runciman were among his pupils. He was mainly remembered as being an incompetent schoolmaster unable to keep order in class. Nevertheless, Blair and others spoke highly of his brilliant command of language.
Significantly, Huxley also worked for a time during the 1920s at Brunner and Mond, a high-tech chemical plant in Billingham, North East England. According to the introduction to the latest edition of his great science fiction novel "Brave New World" (1932) the experience he had there of "an ordered universe in a world of planless incoherence" was an important source for the novel.
Career.
Huxley completed his first (unpublished) novel at the age of 17 and began writing seriously in his early 20s, establishing himself as a successful writer and social satirist. His first published novels were social satires, "Crome Yellow" (1921), "Antic Hay" (1923), "Those Barren Leaves" (1925), and "Point Counter Point" (1928). "Brave New World" was Huxley's fifth novel and first dystopian work. In the 1920s he was also a contributor to "Vanity Fair" and British "Vogue" magazines.
Bloomsbury Set.
During World War I, Huxley spent much of his time at Garsington Manor near Oxford, home of Lady Ottoline Morrell, working as a farm labourer. There he met several Bloomsbury figures, including Bertrand Russell, Alfred North Whitehead, and Clive Bell. Later, in "Crome Yellow" (1921) he caricatured the Garsington lifestyle. Jobs were very scarce, but in 1919 John Middleton Murry was reorganising the "Athenaeum" and invited Huxley to join the staff. He accepted immediately, and quickly married the Belgian refugee Maria Nys, also at Garsington. They lived with their young son in Italy part of the time during the 1920s, where Huxley would visit his friend D. H. Lawrence. Following Lawrence's death in 1930, Huxley edited Lawrence's letters (1932).
Works of this period included important novels on the dehumanising aspects of scientific progress, most famously "Brave New World", and on pacifist themes (for example, "Eyeless in Gaza"). In "Brave New World", set in a dystopian London, Huxley portrays a society operating on the principles of mass production and Pavlovian conditioning. Huxley was strongly influenced by F. Matthias Alexander and included him as a character in "Eyeless in Gaza".
Starting from this period, Huxley began to write and edit non-fiction works on pacifist issues, including "Ends and Means", "An Encyclopedia of Pacifism", and "Pacifism and Philosophy", and was an active member of the Peace Pledge Union.
United States.
In 1937, Huxley moved to Hollywood with his wife Maria, son Matthew, and friend Gerald Heard. He lived in the US, mainly in southern California, until his death, but also for a time in Taos, New Mexico, where he wrote "Ends and Means" (published in 1937). The book contains illuminating tracts on war, religion, nationalism and ethics.
Heard introduced Huxley to Vedanta (Upanishad-centered philosophy), meditation, and vegetarianism through the principle of ahimsa. In 1938, Huxley befriended Jiddu Krishnamurti, whose teachings he greatly admired. He also became a Vedantist in the circle of Hindu Swami Prabhavananda, and introduced Christopher Isherwood to this circle. Not long after, Huxley wrote his book on widely held spiritual values and ideas, "The Perennial Philosophy", which discussed the teachings of renowned mystics of the world. Huxley's book affirmed a sensibility that insists there are realities beyond the generally accepted "five senses" and that there is genuine meaning for humans beyond both sensual satisfactions and sentimentalities.
Huxley became a close friend of Remsen Bird, president of Occidental College. He spent much time at the college, which is in the Eagle Rock neighbourhood of Los Angeles. The college appears as "Tarzana College" in his satirical novel "After Many a Summer" (1939). The novel won Huxley a British literary award, the 1939 James Tait Black Memorial Prize for fiction. Huxley also incorporated Bird into the novel.
During this period, Huxley earned a substantial income as a Hollywood screenwriter; Christopher Isherwood, in his autobiography "My Guru and His Disciple", states that Huxley earned more than $3,000 per week (an enormous sum in those days) as a screenwriter, and that he used much of it to transport Jewish and left-wing writer and artist refugees from Hitler's Germany to the U.S. In March 1938, his friend Anita Loos, a novelist and screenwriter, put him in touch with Metro-Goldwyn-Mayer who hired Huxley for "Madame Curie", which was originally to star Greta Garbo and be directed by George Cukor. (Eventually, the film was completed by MGM in 1943 with a different director and cast.) Huxley received screen credit for "Pride and Prejudice" (1940) and was paid for his work on a number of other films, including "Jane Eyre" (1944). Huxley was commissioned by Walt Disney in 1945 to write a script based on "Alice's Adventures in Wonderland" and the biography of the story's author, Lewis Carroll. The script was not used, however.
Huxley wrote an introduction to the posthumous publication of J. D. Unwin's 1940 book "Hopousia or The Sexual and Economic Foundations of a New Society".
On 21 October 1949, Huxley wrote to George Orwell, author of "Nineteen Eighty-Four", congratulating him on "how fine and how profoundly important the book is". In his letter to Orwell, he predicted:
Huxley had deeply felt apprehensions about the future the developed world might make for itself. From these, he made some warnings in his writings and talks. In a 1958 televised interview conducted by journalist Mike Wallace, Huxley outlined several major concerns: the difficulties and dangers of world overpopulation; the tendency toward distinctly hierarchical social organisation; the crucial importance of evaluating the use of technology in mass societies susceptible to wily persuasion; the tendency to promote modern politicians, to a naive public, as well-marketed commodities.
Post World War II.
After World War II, Huxley applied for United States citizenship. His application was continuously deferred on the grounds that he would not say he would take up arms to defend the U.S. He claimed a philosophical, rather than a religious objection, and therefore was not exempt under the McCarran Act. He withdrew his application. Nevertheless, he remained in the country; and in 1959 he turned down an offer of a Knight Bachelor by the Macmillan government.
Association with Vedanta.
Beginning in 1939 and continuing until his death in 1963, Huxley had an extensive association with the Vedanta Society of Southern California, founded and headed by Swami Prabhavananda. Together with Gerald Heard, Christopher Isherwood, and other followers he was initiated by the Swami and was taught meditation and spiritual practices.
In 1944, Huxley wrote the introduction to the "Bhagavad Gita: The Song of God", translated by Swami Prabhavanada and Christopher Isherwood, which was published by The Vedanta Society of Southern California.
From 1941 until 1960, Huxley contributed 48 articles to "Vedanta and the West", published by the society. He also served on the editorial board with Isherwood, Heard, and playwright John van Druten from 1951 through 1962.
Huxley also occasionally lectured at the Hollywood and Santa Barbara Vedanta temples. Two of those lectures have been released on CD: "Knowledge and Understanding" and "Who Are We?" from 1955. Nonetheless, Huxley's agnosticism, together with his speculative propensity, made it difficult for him to fully embrace any form of institutionalized religion. In spring of 1953, Huxley had his first, supervised, experience with psychedelic drugs (in this case, mescaline). After the publication of "The Doors of Perception", in which he recounted this experience, Huxley and Swami Prabhavanada disagreed about the meaning and importance of the psychedelic drug experience, which may have caused the relationship to cool, but Huxley continued to write articles for the society's journal, lecture at the temple, and attend social functions.
Eyesight.
There are differing accounts about the details of the quality of Huxley's eyesight at specific points in his life. About 1939, Huxley encountered the Bates method for better eyesight, and a teacher, Margaret Darst Corbett, who was able to teach the method to him. In 1940, Huxley relocated from Hollywood to a "ranchito" in the high desert hamlet of Llano, California, in northernmost Los Angeles County. Huxley then said that his sight improved dramatically with the Bates Method and the extreme and pure natural lighting of the southwestern American desert. He reported that, for the first time in more than 25 years, he was able to read without glasses and without strain. He even tried driving a car along the dirt road beside the ranch. He wrote a book about his successes with the Bates Method, "The Art of Seeing", which was published in 1942 (U.S.), 1943 (UK). The book contained some generally disputed theories, and its publication created a growing degree of popular controversy about Huxley's eyesight.
It was, and is, widely believed that Huxley was nearly blind since the illness in his teens, despite the partial recovery that had enabled him to study at Oxford. For example, some ten years after publication of "The Art of Seeing", in 1952, Bennett Cerf was present when Huxley spoke at a Hollywood banquet, wearing no glasses and apparently reading his paper from the lectern without difficulty: "Then suddenly he faltered — and the disturbing truth became obvious. He wasn't reading his address at all. He had learned it by heart. To refresh his memory he brought the paper closer and closer to his eyes. When it was only an inch or so away he still couldn't read it, and had to fish for a magnifying glass in his pocket to make the typing visible to him. It was an agonising moment."
On the other hand, Huxley's second wife, Laura Archera Huxley, would later emphasise in her biographical account, "This Timeless Moment": "One of the great achievements of his life: that of having regained his sight." After revealing a letter she wrote to the "Los Angeles Times" disclaiming the label of Huxley as a "poor fellow who can hardly see" by Walter C. Alvarez, she tempered this: "Although I feel it was an injustice to treat Aldous as though he were blind, it is true there were many indications of his impaired vision. For instance, although Aldous did not wear glasses, he would quite often use a magnifying lens." Laura Huxley proceeded to elaborate a few nuances of inconsistency peculiar to Huxley's vision. Her account, in this respect, is discernibly congruent with the following sample of Huxley's own words from "The Art of Seeing": "The most characteristic fact about the functioning of the total organism, or any part of the organism, is that it is not constant, but highly variable." Nevertheless, the topic of Huxley's eyesight continues to endure similar, significant controversy, regardless of how trivial a subject matter it might initially appear.
American popular science author Steven Johnson, in his book "Mind Wide Open", quotes Huxley about his difficulties with visual encoding: "I am and, for as long as I can remember, I have always been a poor visualizer. Words, even the pregnant words of poets, do not evoke pictures in my mind. No hypnagogic visions greet me on the verge of sleep. When I recall something, the memory does not present itself to me as a vividly seen event or object. By an effort of the will, I can evoke a not very vivid image of what happened yesterday afternoon..."
Personal life.
Huxley married Maria Nys (10 September 1899 – 12 February 1955), a Belgian he met at Garsington, Oxfordshire, in 1919. They had one child, Matthew Huxley (19 April 1920 – 10 February 2005), who had a career as an author, anthropologist, and prominent epidemiologist.
In 1956, Huxley married Laura Archera (1911–2007), also an author. She wrote "This Timeless Moment", a biography of Huxley. Laura felt inspired to illuminate the story of their marriage through Mary Ann Braubach's 2010 documentary, "Huxley on Huxley".
In 1960, Huxley was diagnosed with laryngeal cancer and, in the years that followed, with his health deteriorating, he wrote the Utopian novel "Island", and gave lectures on "Human Potentialities" both at the University of California's San Francisco Medical Center and at the Esalen Institute. These lectures were fundamental to the beginning of the Human Potential Movement.
Huxley was a close friend of Jiddu Krishnamurti and Rosalind Rajagopal and was involved in the creation of the Happy Valley School (now Besant Hill School of Happy Valley) in Ojai, California.
The most substantial collection of Huxley's few remaining papers (following the destruction of most in a fire) is at the Library of the University of California, Los Angeles. Some are also at the Stanford University Libraries.
On 9 April 1962, Huxley was informed he was elected Companion of Literature by the Royal Society of Literature, the senior literary organisation in Britain, and he accepted the title via letter on 28 April 1962. The correspondence between Huxley and the society are kept at the Cambridge University Library. The society invited Huxley to appear at a banquet and give a lecture at Somerset House, London in June 1963. Huxley wrote a draft of the speech he intended to give at the society; however, his deteriorating health meant he would not be able to attend.
Death.
On his deathbed, unable to speak due to advanced laryngeal cancer, Huxley made a written request to his wife Laura for "LSD, 100 µg, intramuscular". According to her account of his death, in "This Timeless Moment", she obliged with an injection at 11:20 a.m. and a second dose an hour later; Huxley died aged 69, at 5:20 p.m. (Los Angeles time), on 22 November 1963.
Media coverage of Huxley's passing — as with that of the author C. S. Lewis – was overshadowed by the assassination of U.S. President John F. Kennedy on the same day. This coincidence served as the basis for Peter Kreeft's book "Between Heaven and Hell: A Dialog Somewhere Beyond Death with John F. Kennedy, C. S. Lewis, & Aldous Huxley", which imagines a conversation among the three men taking place in Purgatory following their deaths.
Huxley's memorial service took place in London in December 1963 which was led by his older brother Julian, and his ashes were interred in the family grave at the Watts Cemetery, home of the Watts Mortuary Chapel in Compton, a village near Guildford, Surrey, England.
Huxley had been a long-time friend of famous Russian composer Igor Stravinsky, who later dedicated his last orchestral composition to Huxley. Stravinsky began "Variations" in Santa Fé, New Mexico in July 1963, and completed the composition in Hollywood on 28 October 1964. It was first performed in Chicago on 17 April 1965, by the Chicago Symphony Orchestra conducted by Robert Craft.

</doc>
<doc id="630" url="https://en.wikipedia.org/wiki?curid=630" title="Ada">
Ada

Ada may refer to:

</doc>
<doc id="632" url="https://en.wikipedia.org/wiki?curid=632" title="Aberdeen (disambiguation)">
Aberdeen (disambiguation)

Aberdeen is a city in Scotland, United Kingdom.
Aberdeen may also refer to:

</doc>
<doc id="633" url="https://en.wikipedia.org/wiki?curid=633" title="Algae">
Algae

Algae ( or ; singular "alga" ) is an informal term for a large, diverse group of eukaryotes that are not necessarily closely related and are thus polyphyletic. Included organisms range from unicellular genera, such as "Chlorella" and the diatoms, to multicellular forms, such as the giant kelp, a large brown alga that may grow up to 50 meters in length. Most are aquatic and autotrophic and lack many of the distinct cell and tissue types, such as stomata, xylem and phloem, that are found in land plants. The largest and most complex marine algae are called seaweeds, while the most complex freshwater forms are the Charophyta, a division of green algae that includes, for example, "Spirogyra" and the stoneworts.
There is no generally accepted definition of algae. One definition is that algae "have chlorophyll as their primary photosynthetic pigment and lack a sterile covering of cells around their reproductive cells". Some authors exclude all prokaryotes and thus do not consider cyanobacteria (blue-green algae) as algae.
Algae constitute a polyphyletic group since they do not include a common ancestor, and although their plastids seem to have a single origin, from cyanobacteria, they were acquired in different ways. Green algae are examples of algae that have primary chloroplasts derived from endosymbiotic cyanobacteria. Diatoms and brown algae are examples of algae with secondary chloroplasts derived from an endosymbiotic red alga.
Algae exhibit a wide range of reproductive strategies, from simple asexual cell division to complex forms of sexual reproduction.
Algae lack the various structures that characterize land plants, such as the phyllids (leaf-like structures) of bryophytes, rhizoids in nonvascular plants, and the roots, leaves, and other organs that are found in tracheophytes (vascular plants). Most are phototrophic, although some groups contain members that are mixotrophic, deriving energy both from photosynthesis and uptake of organic carbon either by osmotrophy, myzotrophy, or phagotrophy. Some unicellular species of green algae, many golden algae, euglenids, dinoflagellates and other algae have become heterotrophs (also called colorless or apochlorotic algae), sometimes parasitic, relying entirely on external energy sources and have limited or no photosynthetic apparatus. Some other heterotrophic organisms, like the apicomplexans, are also derived from cells whose ancestors possessed plastids, but are not traditionally considered as algae. Algae have photosynthetic machinery ultimately derived from cyanobacteria that produce oxygen as a by-product of photosynthesis, unlike other photosynthetic bacteria such as purple and green sulfur bacteria. Fossilized filamentous algae from the Vindhya basin have been dated back to 1.6 to 1.7 billion years ago.
Etymology and study.
The singular "alga" is the Latin word for "seaweed" and retains that meaning in English. The etymology is obscure. Although some speculate that it is related to Latin "algēre", "be cold", there is no known reason to associate seaweed with temperature. A more likely source is "alliga", "binding, entwining."
The Ancient Greek word for seaweed was "φῦκος" (fūkos or phykos), which could mean either the seaweed (probably red algae) or a red dye derived from it. The Latinization, "fūcus", meant primarily the cosmetic rouge. The etymology is uncertain, but a strong candidate has long been some word related to the Biblical "פוך" (pūk), "paint" (if not that word itself), a cosmetic eye-shadow used by the ancient Egyptians and other inhabitants of the eastern Mediterranean. It could be any color: black, red, green, blue.
Accordingly, the modern study of marine and freshwater algae is called either phycology or algology, depending on whether the Greek or Latin root is used. The name "Fucus" appears in a number of taxa.
Classification.
Most algae contain chloroplasts that are similar in structure to cyanobacteria. Chloroplasts contain circular DNA like that in cyanobacteria and presumably represent reduced endosymbiotic cyanobacteria. However, the exact origin of the chloroplasts is different among separate lineages of algae, reflecting their acquisition during different endosymbiotic events. The table below describes the composition of the three major groups of algae. Their lineage relationships are shown in the figure in the upper right. Many of these groups contain some members that are no longer photosynthetic. Some retain plastids, but not chloroplasts, while others have lost plastids entirely.
Phylogeny based on plastid not nucleocytoplasmic genealogy:
Linnaeus, in Species Plantarum (1753), the starting point for modern botanical nomenclature, recognized 14 genera of algae, of which only 4 are currently considered among algae. In "Systema Naturae", Linnaeus described the genera "Volvox" and "Corallina", among the animals.
In 1768, Samuel Gottlieb Gmelin (1744–1774) published the "Historia Fucorum", the first work dedicated to marine algae and the first book on marine biology to use the then new binomial nomenclature of Linnaeus. It included elaborate illustrations of seaweed and marine algae on folded leaves.
W.H.Harvey (1811—1866) and Lamouroux (1813) were the first to divide macroscopic algae into four divisions based on their pigmentation. This is the first use of a biochemical criterion in plant systematics. Harvey's four divisions are: red algae (Rhodospermae), brown algae (Melanospermae), green algae (Chlorospermae) and Diatomaceae.
At this time, microscopic algae were discovered and reported by a different group of workers (e.g., O. F. Müller and Ehrenberg) studying the Infusoria (microscopic organisms). Unlike macroalgae, which were clearly viewed as plants, microalgae were frequently considered animals because they are often motile. Even the non-motile (coccoid) microalgae were sometimes merely seen as stages of the life cycle of plants, macroalgae or animals.
Although used as a taxonomic category in some pre-Darwinian classifications, e.g., Linnaeus (1753), de Jussieu (1789), Horaninow (1843), Agassiz (1859), Wilson & Cassin (1864), in further classifications, the "algae" are seen as an artificial, polyphyletic group.
Throughout the 20th century, most classifications treated the following groups as divisions or classes of algae: cyanophytes, rhodophytes, chrysophytes, xanthophytes, bacillariophytes, phaeophytes, pyrrhophytes (cryptophytes and dinophytes), euglenophytes and chlorophytes. Later, many new groups were discovered (e.g., Bolidophyceae), and others were splintered from older groups: charophytes and glaucophytes (from chlorophytes), many heterokontophytes (e.g., synurophytes from chrysophytes, or eustigmatophytes from xanthophytes), haptophytes (from chrysophytes) and chlorarachniophytes (from xanthophytes).
With the abandonment of plant-animal dichotomous classification, most groups of algae (sometimes all) were included in Protista, later also abandoned in favour of Eukaryota. However, as a legacy of the older plant life scheme, some groups that were also treated as protozoans in the past still have duplicated classifications (see ambiregnal protists).
Some parasitic algae (e.g., the green algae "Prototheca" and "Helicosporidium", parasites of metazoans, or "Cephaleuros", parasites of plants) were originally classified as fungi, sporozoans or protistans of incertae sedis, while others (e.g., the green algae "Phyllosiphon" and "Rhodochytrium", parasites of plants, or the red algae "Pterocladiophila" and "Gelidiocolax mammillatus", parasites of other red algae, or the dinoflagellates "Oodinium", parasites of fish) had their relationship with algae conjectured early. In other cases, some groups were originally characterized as parasitic algae (e.g., "Chlorochytrium"), but later were seen as endophytic algae. Furthermore, groups like the apicomplexans are also parasites derived from ancestors that possessed plastids, but are not included in any group traditionally seen as algae.
Relationship to land plants.
The first land plants probably evolved from shallow freshwater charophyte algae much like "Chara" almost 500 million years ago. These probably had an isomorphic alternation of generations and were probably filamentous. Fossils of isolated land plant spores suggest land plants may have been around as long as 475 million years ago.
Morphology.
A range of algal morphologies are exhibited, and convergence of features in unrelated groups is common. The only groups to exhibit three-dimensional multicellular thalli are the reds and browns, and some chlorophytes. Apical growth is constrained to subsets of these groups: the florideophyte reds, various browns, and the charophytes. The form of charophytes is quite different from those of reds and browns, because they have distinct nodes, separated by internode 'stems'; whorls of branches reminiscent of the horsetails occur at the nodes. Conceptacles are another polyphyletic trait; they appear in the coralline algae and the Hildenbrandiales, as well as the browns.
Most of the simpler algae are unicellular flagellates or amoeboids, but colonial and non-motile forms have developed independently among several of the groups. Some of the more common organizational levels, more than one of which may occur in the life cycle of a species, are
In three lines, even higher levels of organization have been reached, with full tissue differentiation. These are the brown algae,—some of which may reach 50 m in length (kelps)—the red algae, and the green algae. The most complex forms are found among the green algae (see Charales and Charophyta), in a lineage that eventually led to the higher land plants. The point where these non-algal plants begin and algae stop is usually taken to be the presence of reproductive organs with protective cell layers, a characteristic not found in the other alga groups.
Physiology.
Many algae, particularly members of the Characeae, have served as model experimental organisms to understand the mechanisms of the water permeability of membranes, osmoregulation, turgor regulation, salt tolerance, cytoplasmic streaming, and the generation of action potentials.
Phytohormones are found not only in higher plants, but in algae too.
Symbiotic algae.
Some species of algae form symbiotic relationships with other organisms. In these symbioses, the algae supply photosynthates (organic substances) to the host organism providing protection to the algal cells. The host organism derives some or all of its energy requirements from the algae. Examples are as follows.
Lichens.
"Lichens" are defined by the International Association for Lichenology to be "an association of a fungus and a photosynthetic symbiont resulting in a stable vegetative body having a specific structure." The fungi, or mycobionts, are mainly from the Ascomycota with a few from the Basidiomycota. They are not found alone in nature; but when they began to associate is not known. One mycobiont associates with the same phycobiont species, rarely two, from the green algae, except that alternatively the mycobiont may associate with a species of cyanobacteria (hence "photobiont" is the more accurate term). A photobiont may be associated with many different mycobionts or may live independently; accordingly, lichens are named and classified as fungal species. The association is termed a morphogenesis because the lichen has a form and capabilities not possessed by the symbiont species alone (they can be experimentally isolated). It is possible that the photobiont triggers otherwise latent genes in the mycobiont.
Coral reefs.
 Coral reefs are accumulated from the calcareous exoskeletons of marine invertebrates of the order Scleractinia (stony corals). These animals metabolize sugar and oxygen to obtain energy for their cell-building processes, including secretion of the exoskeleton, with water and carbon dioxide as byproducts. Dinoflagellates (algal protists) are often endosymbionts in the cells of the coral-forming marine invertebrates, where they accelerate host-cell metabolism by generating immediately available sugar and oxygen through photosynthesis using incident light and the carbon dioxide produced by the host. Reef-building stony corals (hermatypic corals) require endosymbiotic algae from the genus "Symbiodinium" to be in a healthy condition. The loss of "Symbiodinium" from the host is known as coral bleaching, a condition which leads to the deterioration of a reef.
Sea sponges.
Green algae live close to the surface of some sponges, for example, breadcrumb sponge ("Halichondria panicea"). The alga is thus protected from predators; the sponge is provided with oxygen and sugars which can account for 50 to 80% of sponge growth in some species.
Life-cycle.
Rhodophyta, Chlorophyta and Heterokontophyta, the three main algal divisions, have life-cycles which show considerable variation and complexity. In general, there is an asexual phase where the seaweed's cells are diploid, a sexual phase where the cells are haploid followed by fusion of the male and female gametes. Asexual reproduction permits efficient population increases, but less variation is possible. Commonly, in sexual reproduction of unicellular and colonial algae, two specialized sexually compatible haploid gametes make physical contact and fuse to form a zygote. To ensure a successful mating, the development and release of gametes is highly synchronized and regulated; pheromones may play a key role in these processes. Sexual reproduction allows for more variation and provides the benefit of efficient recombinational repair of DNA damages during meiosis, a key stage of the sexual cycle. However, sexual reproduction is more costly than asexual reproduction. Meiosis has been shown to occur in many different species of algae.
Numbers.
The "Algal Collection of the US National Herbarium" (located in the National Museum of Natural History) consists of approximately 320,500 dried specimens, which, although not exhaustive (no exhaustive collection exists), gives an idea of the order of magnitude of the number of algal species (that number remains unknown). Estimates vary widely. For example, according to one standard textbook, in the British Isles the "UK Biodiversity Steering Group Report" estimated there to be 20000 algal species in the UK. Another checklist reports only about 5000 species. Regarding the difference of about 15000 species, the text concludes: "It will require many detailed field surveys before it is possible to provide a reliable estimate of the total number of species ..."
Regional and group estimates have been made as well:
and so on, but lacking any scientific basis or reliable sources, these numbers have no more credibility than the British ones mentioned above. Most estimates also omit microscopic algae, such as phytoplankton.
The most recent estimate suggests 72,500 algal species worldwide.
Distribution.
The distribution of algal species has been fairly well studied since the founding of phytogeography in the mid-19th century AD. Algae spread mainly by the dispersal of spores analogously to the dispersal of Plantae by seeds and spores. This dispersal can be accomplished by air, water, or other organisms. Due to this, spores can be found in a variety of environments: fresh and marine waters, air, soil, and in or on other organisms. Whether a spore is to grow into an organism depends on the combination of the species and the environmental conditions of where the spore lands.
The spores of fresh-water algae are dispersed mainly by running water and wind, as well as by living carriers. However, not all bodies of water can carry all species of algae, as the chemical composition of certain water bodies will limit the algae that can survive within it. Marine spores are often spread by ocean currents. Ocean water presents many vastly-different habitats based on temperature and nutrient-availability, resulting in phytogeographic zones, regions and provinces.
To some degree, the distribution of algae is subject to floristic discontinuities caused by geographical features, such as Antarctica, long distances of ocean or general land masses. It is therefore possible to identify species occurring by locality, such as "Pacific Algae" or "North Sea Algae". When they occur out of their localities, it is usually possible to hypothesize a transport mechanism, such as the hulls of ships. For example, "Ulva reticulata" and "Ulva fasciata" travelled from the mainland to Hawaii in this manner.
Mapping is possible for select species only: "there are many valid examples of confined distribution patterns." For example, "Clathromorphum" is an arctic genus and is not mapped far south of there. On the other hand, scientists regard the overall data as insufficient due to the "difficulties of undertaking such studies."
Ecology.
Algae are prominent in bodies of water, common in terrestrial environments and are found in unusual environments, such as on snow and on ice. Seaweeds grow mostly in shallow marine waters, under ; however, some have been recorded to a depth of .
The various sorts of algae play significant roles in aquatic ecology. Microscopic forms that live suspended in the water column (phytoplankton) provide the food base for most marine food chains. In very high densities (algal blooms) these algae may discolor the water and outcompete, poison, or asphyxiate other life forms.
Algae are variously sensitive to different factors, which has made them useful as biological indicators in the Ballantine Scale and its modification .
On the basis of their habitat, algae can be categorized as: aquatic (planktonic, benthic, marine, freshwater), terrestrial, aerial (subareial), lithophytic, halophytic (or euryhaline), psammon, thermophilic, cryophilic, epibiont (epiphytic, epizoic), endosymbiont (endophytic, endozoic), parasitic, calcifilic or lichenic (phycobiont).
Cultural associations.
In Classical Chinese, the word is used both for "algae" and (in the modest tradition of the imperial scholars) for "literary talent". The third island in Kunming Lake beside the Summer Palace in Beijing is known as the Zaojian Tang Dao which thus simultaneously means "Island of the Algae-Viewing Hall" and "Island of the Hall for Reflecting on Literary Talent".
Uses.
Agar.
Agar, a gelatinous substance derived from red algae, has a number of commercial uses. It is a good medium on which to grow bacteria and fungi as most microorganisms cannot digest agar.
Alginates.
Alginic acid, or alginate, is extracted from brown algae. Its uses range from gelling agents in food, to medical dressings. Alginic acid also has been used in the field of biotechnology as a biocompatible medium for cell encapsulation and cell immobilization. Molecular cuisine is also a user of the substance for its gelling properties, by which it becomes a delivery vehicle for flavours.
Between 100,000 and 170,000 wet tons of "Macrocystis" are harvested annually in New Mexico for alginate extraction and abalone feed.
Energy source.
To be competitive and independent from fluctuating support from (local) policy on the long run, biofuels should equal or beat the cost level of fossil fuels. Here, algae based fuels hold great promise, directly related to the potential to produce more biomass per unit area in a year than any other form of biomass. The break-even point for algae-based biofuels is estimated to occur by 2025.
Fertilizer.
For centuries, seaweed has been used as a fertilizer; George Owen of Henllys writing in the 16th century referring to drift weed in South Wales:This kind of ore they often gather and lay on great heapes, where it heteth and rotteth, and will have a strong and loathsome smell; when being so rotten they cast on the land, as they do their muck, and thereof springeth good corn, especially barley ... After spring-tydes or great rigs of the sea, they fetch it in sacks on horse backes, and carie the same three, four, or five miles, and cast it on the lande, which doth very much better the ground for corn and grass.
Today, algae are used by humans in many ways; for example, as fertilizers, soil conditioners and livestock feed. Aquatic and microscopic species are cultured in clear tanks or ponds and are either harvested or used to treat effluents pumped through the ponds. Algaculture on a large scale is an important type of aquaculture in some places. Maerl is commonly used as a soil conditioner.
Nutrition.
Naturally growing seaweeds are an important source of food, especially in Asia. They provide many vitamins including: A, B, B, B, niacin and C, and are rich in iodine, potassium, iron, magnesium and calcium. In addition commercially cultivated microalgae, including both algae and cyanobacteria, are marketed as nutritional supplements, such as Spirulina, Chlorella and the Vitamin-C supplement, Dunaliella, high in beta-carotene.
Algae are national foods of many nations: China consumes more than 70 species, including "fat choy", a cyanobacterium considered a vegetable; Japan, over 20 species; Ireland, dulse; Chile, cochayuyo. Laver is used to make "laver bread" in Wales where it is known as "bara lawr"; in Korea, gim; in Japan, nori and aonori. It is also used along the west coast of North America from California to British Columbia, in Hawaii and by the Māori of New Zealand. Sea lettuce and badderlocks are a salad ingredient in Scotland, Ireland, Greenland and Iceland.
The oils from some algae have high levels of unsaturated fatty acids. For example, "Parietochloris incisa" is very high in arachidonic acid, where it reaches up to 47% of the triglyceride pool. Some varieties of algae favored by vegetarianism and veganism contain the long-chain, essential omega-3 fatty acids, docosahexaenoic acid (DHA) and eicosapentaenoic acid (EPA). Fish oil contains the omega-3 fatty acids, but the original source is algae (microalgae in particular), which are eaten by marine life such as copepods and are passed up the food chain. Algae have emerged in recent years as a popular source of omega-3 fatty acids for vegetarians who cannot get long-chain EPA and DHA from other vegetarian sources such as flaxseed oil, which only contains the short-chain alpha-linolenic acid (ALA).
Agricultural Research Service scientists found that 60–90% of nitrogen runoff and 70–100% of phosphorus runoff can be captured from manure effluents using a horizontal algae scrubber, also called an algal turf scrubber (ATS). Scientists developed the ATS, which consists of shallow, 100-foot raceways of nylon netting where algae colonies can form, and studied its efficacy for three years. They found that algae can readily be used to reduce the nutrient runoff from agricultural fields and increase the quality of water flowing into rivers, streams, and oceans. Researchers collected and dried the nutrient-rich algae from the ATS and studied its potential as an organic fertilizer. They found that cucumber and corn seedlings grew just as well using ATS organic fertilizer as they did with commercial fertilizers. Algae scrubbers, using bubbling upflow or vertical waterfall versions, are now also being used to filter aquariums and ponds.
Bioremediation.
The alga "Stichococcus bacillaris", has been seen to colonize silicone resins used at archaeological sites; biodegrading the synthetic substance.
Pigments.
The natural pigments (carotenoids and chlorophylls) produced by algae can be used as an alternative to chemical dyes and coloring agents.
The presence of some individual alga pigments, together with specific pigment concentrations ratios, are taxon-specific: analysis of their concentrations with various analytical methods, particularly high-performance liquid chromatography (HPLC), can therefore offer deep insight into the taxonomic composition and relative abundance of natural alga populations in sea water samples.
Stabilizing substances.
Carrageenan, from the red alga "Chondrus crispus", is used as a stabilizer in milk products.

</doc>
<doc id="634" url="https://en.wikipedia.org/wiki?curid=634" title="Analysis of variance">
Analysis of variance

Analysis of variance (ANOVA) is a collection of statistical models used to analyze the differences among group means and their associated procedures (such as "variation" among and between groups), developed by statistician and evolutionary biologist Ronald Fisher. In the ANOVA setting, the observed variance in a particular variable is partitioned into components attributable to different sources of variation. In its simplest form, ANOVA provides a statistical test of whether or not the means of several groups are equal, and therefore generalizes the "t"-test to more than two groups. ANOVAs are useful for comparing (testing) three or more means (groups or variables) for statistical significance. It is conceptually similar to multiple two-sample t-tests, but is less conservative (results in less type I error) and is therefore suited to a wide range of practical problems.
History.
While the analysis of variance reached fruition in the 20th century, antecedents extend centuries into the past according to Stigler. These include hypothesis testing, the partitioning of sums of squares, experimental techniques and the additive model. Laplace was performing hypothesis testing in the 1770s. The development of least-squares methods by Laplace and Gauss circa 1800 provided an improved method of combining observations (over the existing practices of astronomy and geodesy). It also initiated much study of the contributions to sums of squares. Laplace soon knew how to estimate a variance from a residual (rather than a total) sum of squares. By 1827 Laplace was using least squares methods to address ANOVA problems regarding measurements of atmospheric tides. Before 1800 astronomers had isolated observational errors resulting 
from reaction times (the "personal equation") and had developed methods of reducing the errors. The experimental methods used in the study of the personal equation were later accepted by the emerging field of psychology which developed strong (full factorial) experimental methods to which randomization and blinding were soon added. An eloquent non-mathematical explanation of the additive effects model was
available in 1885.
Ronald Fisher introduced the term variance and proposed its formal analysis in a 1918 article "The Correlation Between Relatives on the Supposition of Mendelian Inheritance". His first application of the analysis of variance was published in 1921. Analysis of variance became widely known after being included in Fisher's 1925 book "Statistical Methods for Research Workers".
Randomization models were developed by several researchers. The first was published in Polish by Neyman in 1923.
One of the attributes of ANOVA which ensured its early popularity was computational elegance. The structure of the additive model allows solution for the additive coefficients by simple algebra rather than by matrix calculations. In the era of mechanical calculators this simplicity was critical. The determination of statistical significance also required access to tables of the F function which were supplied by early statistics texts.
Motivating example.
The analysis of variance can be used as an exploratory tool to explain observations. A dog show provides an example. A dog show is not a random sampling of the breed: it is typically limited to dogs that are adult, pure-bred, and exemplary. A histogram of dog weights from a show might plausibly be rather complex, like the yellow-orange distribution shown in the illustrations. Suppose we wanted to predict the weight of a dog based on a certain set of characteristics of each dog. Before we could do that, we would need to "explain" the distribution of weights by dividing the dog population into groups based on those characteristics. A successful grouping will split dogs such that (a) each group has a low variance of dog weights (meaning the group is relatively homogeneous) and (b) the mean of each group is distinct (if two groups have the same mean, then it isn't reasonable to conclude that the groups are, in fact, separate in any meaningful way).
In the illustrations to the right, each group is identified as "X", "X", etc. In the first illustration, we divide the dogs according to the product (interaction) of two binary groupings: young vs old, and short-haired vs long-haired (thus, group 1 is young, short-haired dogs, group 2 is young, long-haired dogs, etc.). Since the distributions of dog weight within each of the groups (shown in blue) has a large variance, and since the means are very close across groups, grouping dogs by these characteristics does not produce an effective way to explain the variation in dog weights: knowing which group a dog is in does not allow us to make any reasonable statements as to what that dog's weight is likely to be. Thus, this grouping fails to "fit" the distribution we are trying to explain (yellow-orange).
An attempt to explain the weight distribution by grouping dogs as (pet vs working breed) and (less athletic vs more athletic) would probably be somewhat more successful (fair fit). The heaviest show dogs are likely to be big strong working breeds, while breeds kept as pets tend to be smaller and thus lighter. As shown by the second illustration, the distributions have variances that are considerably smaller than in the first case, and the means are more reasonably distinguishable. However, the significant overlap of distributions, for example, means that we cannot reliably say that "X" and "X" are truly distinct (i.e., it is perhaps reasonably likely that splitting dogs according to the flip of a coin—by pure chance—might produce distributions that look similar).
An attempt to explain weight by breed is likely to produce a very good fit. All Chihuahuas are light and all St Bernards are heavy. The difference in weights between Setters and Pointers does not justify separate breeds. The analysis of variance provides the formal tools to justify these intuitive judgments. A common use of the method is the analysis of experimental data or the development of models. The method has some advantages over correlation: not all of the data must be numeric and one result of the method is a judgment in the confidence in an explanatory relationship.
Background and terminology.
ANOVA is a particular form of statistical hypothesis testing heavily used in the analysis of experimental data. A test result (calculated from the null hypothesis and the sample) is called statistically significant if it is deemed unlikely to have occurred by chance, "assuming the truth of the null hypothesis". A statistically significant result, when a probability (p-value) is less than a threshold (significance level), justifies the rejection of the null hypothesis, but only if the a priori probability of the null hypothesis is not high.
In the typical application of ANOVA, the null hypothesis is that all groups are simply random samples of the same population. For example, when studying the effect of different treatments on similar samples of patients, the null hypothesis would be that all treatments have the same effect (perhaps none). Rejecting the null hypothesis would imply that different treatments result in altered effects.
By construction, hypothesis testing limits the rate of Type I errors (false positives) to a significance level. Experimenters also wish to limit Type II errors (false negatives). 
The rate of Type II errors depends largely on sample size (the rate will increase for small numbers of samples), significance 
level (when the standard of proof is high, the chances of overlooking 
a discovery are also high) and effect size (a smaller effect size is more prone to Type II error).
The terminology of ANOVA is largely from the statistical 
design of experiments. The experimenter adjusts factors and 
measures responses in an attempt to determine an effect. Factors are 
assigned to experimental units by a combination of randomization and 
blocking to ensure the validity of the results. Blinding keeps the
weighing impartial. Responses show a variability that is partially 
the result of the effect and is partially random error.
ANOVA is the synthesis of several ideas and it is used for multiple 
purposes. As a consequence, it is difficult to define concisely or precisely.
"Classical ANOVA for balanced data does three things at once:
In short, ANOVA is a statistical tool used in several ways to develop and confirm an explanation for the observed data.
Additionally:
As a result:
ANOVA "has long enjoyed the status of being the most used (some would 
say abused) statistical technique in psychological research."
ANOVA "is probably the most useful technique in the field of 
statistical inference."
ANOVA is difficult to teach, particularly for complex experiments, with split-plot designs being notorious. In some cases the proper 
application of the method is best determined by problem pattern recognition 
followed by the consultation of a classic authoritative test.
Design-of-experiments terms.
(Condensed from the NIST Engineering Statistics handbook: Section 5.7. A 
Glossary of DOE Terminology.)
Classes of models.
There are three classes of models used in the analysis of variance, and these are outlined here.
Fixed-effects models.
The fixed-effects model (class I) of analysis of variance applies to situations in which the experimenter applies one or more treatments to the subjects of the experiment to see whether the response variable values change. This allows the experimenter to estimate the ranges of response variable values that the treatment would generate in the population as a whole.
Random-effects models.
Random effects model (class II) is used when the treatments are not fixed. This occurs when the various factor levels are sampled from a larger population. Because the levels themselves are random variables, some assumptions and the method of contrasting the treatments (a multi-variable generalization of simple differences) differ from the fixed-effects model.
Mixed-effects models.
A mixed-effects model (class III) contains experimental factors of both fixed and random-effects types, with appropriately different interpretations and analysis for the two types.
Example:
Teaching experiments could be performed by a university department 
to find a good introductory textbook, with each text considered a 
treatment. The fixed-effects model would compare a list of candidate 
texts. The random-effects model would determine whether important 
differences exist among a list of randomly selected texts. The 
mixed-effects model would compare the (fixed) incumbent texts to 
randomly selected alternatives.
Defining fixed and random effects has proven elusive, with competing 
definitions arguably leading toward a linguistic quagmire.
Assumptions of ANOVA.
The analysis of variance has been studied from several approaches, the most common of which uses a linear model that relates the response to the treatments and blocks. Note that the model is linear in parameters but may be nonlinear across factor levels. Interpretation is easy when data is balanced across factors but much deeper understanding is needed for unbalanced data.
Textbook analysis using a normal distribution.
The analysis of variance can be presented in terms of a linear model, which makes the following assumptions about the probability distribution of the responses:
The separate assumptions of the textbook model imply that the errors are independently, identically, and normally distributed for fixed effects models, that is, that the errors (formula_1) are independent and
Randomization-based analysis.
In a randomized controlled experiment, the treatments are randomly assigned to experimental units, following the experimental protocol. This randomization is objective and declared before the experiment is carried out. The objective random-assignment is used to test the significance of the null hypothesis, following the ideas of C. S. Peirce and Ronald Fisher. This design-based analysis was discussed and developed by Francis J. Anscombe at Rothamsted Experimental Station and by Oscar Kempthorne at Iowa State University. Kempthorne and his students make an assumption of "unit treatment additivity", which is discussed in the books of Kempthorne and David R. Cox.
Unit-treatment additivity.
In its simplest form, the assumption of unit-treatment additivity states that the observed response formula_3 from experimental unit formula_4 when receiving treatment formula_5 can be written as the sum of the unit's response formula_6 and the treatment-effect formula_7, that is 
The assumption of unit-treatment additivity implies that, for every treatment formula_5, the formula_5th treatment has exactly the same effect formula_11 on every experiment unit.
The assumption of unit treatment additivity usually cannot be directly falsified, according to Cox and Kempthorne. However, many "consequences" of treatment-unit additivity can be falsified. For a randomized experiment, the assumption of unit-treatment additivity "implies" that the variance is constant for all treatments. Therefore, by contraposition, a necessary condition for unit-treatment additivity is that the variance is constant.
The use of unit treatment additivity and randomization is similar to the design-based inference that is standard in finite-population survey sampling.
Derived linear model.
Kempthorne uses the randomization-distribution and the assumption of "unit treatment additivity" to produce a "derived linear model", very similar to the textbook model discussed previously. The test statistics of this derived linear model are closely approximated by the test statistics of an appropriate normal linear model, according to approximation theorems and simulation studies. However, there are differences. For example, the randomization-based analysis results in a small but (strictly) negative correlation between the observations. In the randomization-based analysis, there is "no assumption" of a "normal" distribution and certainly "no assumption" of "independence". On the contrary, "the observations are dependent"!
The randomization-based analysis has the disadvantage that its exposition involves tedious algebra and extensive time. Since the randomization-based analysis is complicated and is closely approximated by the approach using a normal linear model, most teachers emphasize the normal linear model approach. Few statisticians object to model-based analysis of balanced randomized experiments.
Statistical models for observational data.
However, when applied to data from non-randomized experiments or observational studies, model-based analysis lacks the warrant of randomization. For observational data, the derivation of confidence intervals must use "subjective" models, as emphasized by Ronald Fisher and his followers. In practice, the estimates of treatment-effects from observational studies generally are often inconsistent. In practice, "statistical models" and observational data are useful for suggesting hypotheses that should be treated very cautiously by the public.
Summary of assumptions.
The normal-model based ANOVA analysis assumes the independence, normality and 
homogeneity of the variances of the residuals. The 
randomization-based analysis assumes only the homogeneity of the 
variances of the residuals (as a consequence of unit-treatment 
additivity) and uses the randomization procedure of the experiment. 
Both these analyses require homoscedasticity, as an assumption for the normal-model analysis and as a consequence of randomization and additivity for the randomization-based analysis.
However, studies of processes that 
change variances rather than means (called dispersion effects) have 
been successfully conducted using ANOVA. There are
"no" necessary assumptions for ANOVA in its full generality, but the
F-test used for ANOVA hypothesis testing has assumptions and practical 
limitations which are of continuing interest.
Problems which do not satisfy the assumptions of ANOVA can often be transformed to satisfy the assumptions. 
The property of unit-treatment additivity is not invariant under a "change of scale", so statisticians often use transformations to achieve unit-treatment additivity. If the response variable is expected to follow a parametric family of probability distributions, then the statistician may specify (in the protocol for the experiment or observational study) that the responses be transformed to stabilize the variance. Also, a statistician may specify that logarithmic transforms be applied to the responses, which are believed to follow a multiplicative model.
According to Cauchy's functional equation theorem, the logarithm is the only continuous transformation that transforms real multiplication to addition.
Characteristics of ANOVA.
ANOVA is used in the analysis of comparative experiments, those in 
which only the difference in outcomes is of interest. The statistical
significance of the experiment is determined by a ratio of two 
variances. This ratio is independent of several possible alterations
to the experimental observations: Adding a constant to all 
observations does not alter significance. Multiplying all 
observations by a constant does not alter significance. So ANOVA 
statistical significance result is independent of constant bias and 
scaling errors as well as the units used in expressing observations. 
In the era of mechanical calculation it was common to 
subtract a constant from all observations (when equivalent to 
dropping leading digits) to simplify data entry. This is an example of data
coding.
Logic of ANOVA.
The calculations of ANOVA can be characterized as computing a number
of means and variances, dividing two variances and comparing the ratio 
to a handbook value to determine statistical significance. Calculating 
a treatment effect is then trivial, "the effect of any treatment is 
estimated by taking the difference between the mean of the 
observations which receive the treatment and the general mean."
Partitioning of the sum of squares.
ANOVA uses traditional standardized terminology. The definitional 
equation of sample variance is
formula_12, where the 
divisor is called the degrees of freedom (DF), the summation is called 
the sum of squares (SS), the result is called the mean square (MS) and 
the squared terms are deviations from the sample mean. ANOVA 
estimates 3 sample variances: a total variance based on all the 
observation deviations from the grand mean, an error variance based on 
all the observation deviations from their appropriate 
treatment means and a treatment variance. The treatment variance is
based on the deviations of treatment means from the grand mean, the 
result being multiplied by the number of observations in each 
treatment to account for the difference between the variance of 
observations and the variance of means.
The fundamental technique is a partitioning of the total sum of squares "SS" into components related to the effects used in the model. For example, the model for a simplified ANOVA with one type of treatment at different levels.
The number of degrees of freedom "DF" can be partitioned in a similar way: one of these components (that for error) specifies a chi-squared distribution which describes the associated sum of squares, while the same is true for "treatments" if there is no treatment effect.
See also Lack-of-fit sum of squares.
The F-test.
The F-test is used for comparing the factors of the total deviation. For example, in one-way, or single-factor ANOVA, statistical significance is tested for by comparing the F test statistic
where "MS" is mean square, formula_17 = number of treatments and 
formula_18 = total number of cases
to the F-distribution with formula_19, formula_20 degrees of freedom. Using the F-distribution is a natural candidate because the test statistic is the ratio of two scaled sums of squares each of which follows a scaled chi-squared distribution.
The expected value of F is formula_21 (where n is the treatment sample size)
which is 1 for no treatment effect. As values of F increase above 1, the evidence is increasingly inconsistent with the null hypothesis. Two apparent experimental methods of increasing F are increasing the sample size and reducing the error variance by tight experimental controls.
There are two methods of concluding the ANOVA hypothesis test, both of which produce the same result:
The ANOVA F-test is known to be nearly optimal in the sense of minimizing false negative errors for a fixed rate of false positive errors (i.e. maximizing power for a fixed significance level). For example, to test the hypothesis that various medical treatments have exactly the same effect, the F-test's p-values closely approximate the permutation test's p-values: The approximation is particularly close when the design is balanced. Such permutation tests characterize tests with maximum power against all alternative hypotheses, as observed by Rosenbaum. The ANOVA F–test (of the null-hypothesis that all treatments have exactly the same effect) is recommended as a practical test, because of its robustness against many alternative distributions.
Extended logic.
ANOVA consists of separable parts; partitioning sources of variance 
and hypothesis testing can be used individually. ANOVA is used to 
support other statistical tools. Regression is first used to fit more 
complex models to data, then ANOVA is used to compare models with the 
objective of selecting simple(r) models that adequately describe the 
data. "Such models could be fit without any reference to ANOVA, but 
ANOVA tools could then be used to make some sense of the fitted models, 
and to test hypotheses about batches of coefficients." 
"e think of the analysis of variance as a way of understanding and structuring 
multilevel models—not as an alternative to regression but as a tool 
for summarizing complex high-dimensional inferences ..."
ANOVA for a single factor.
The simplest experiment suitable for ANOVA analysis is the completely 
randomized experiment with a single factor. More complex experiments 
with a single factor involve constraints on randomization and include 
completely randomized blocks and Latin squares (and variants: 
Graeco-Latin squares, etc.). The more complex experiments share many 
of the complexities of multiple factors. A relatively complete 
discussion of the analysis (models, data summaries, ANOVA table) of 
the completely randomized experiment is 
available.
ANOVA for multiple factors.
ANOVA generalizes to the study of the effects of multiple factors. 
When the experiment includes observations at all combinations of 
levels of each factor, it is termed factorial. 
Factorial experiments 
are more efficient than a series of single factor experiments and the 
efficiency grows as the number of factors increases. Consequently, factorial designs are heavily used.
The use of ANOVA to study the effects of multiple factors has a complication. In a 3-way ANOVA with factors x, y and z, the ANOVA model includes terms for the main effects (x, y, z) and terms for interactions (xy, xz, yz, xyz). 
All terms require hypothesis tests. The proliferation of interaction terms increases the risk that some hypothesis test will produce a false positive by chance. Fortunately, experience says that high order interactions are rare. 
The ability to detect interactions is a major advantage of multiple 
factor ANOVA. Testing one factor at a time hides interactions, but 
produces apparently inconsistent experimental results.
Caution is advised when encountering interactions; Test 
interaction terms first and expand the analysis beyond ANOVA if 
interactions are found. Texts vary in their recommendations regarding 
the continuation of the ANOVA procedure after encountering an 
interaction. Interactions complicate the interpretation of 
experimental data. Neither the calculations of significance nor the 
estimated treatment effects can be taken at face value. "A 
significant interaction will often mask the significance of main effects." Graphical methods are recommended
to enhance understanding. Regression is often useful. A lengthy discussion of interactions is available in Cox (1958). Some interactions can be removed (by transformations) while others cannot.
A variety of techniques are used with multiple factor ANOVA to reduce expense. One technique used in factorial designs is to minimize replication (possibly no replication with support of analytical trickery) and to combine groups when effects are found to be statistically (or practically) insignificant. An experiment with many insignificant factors may collapse into one with a few factors supported by many replications.
Worked numeric examples.
Several fully worked numerical examples are available. A 
simple case uses one-way (a single factor) analysis. A more complex case uses two-way (two-factor) analysis.
Associated analysis.
Some analysis is required in support of the "design" of the experiment while other analysis is performed after changes in the factors are formally found to produce statistically significant changes in the responses. Because experimentation is iterative, the results of one experiment alter plans for following experiments.
Preparatory analysis.
The number of experimental units.
In the design of an experiment, the number of experimental units is planned to satisfy the goals of the experiment. Experimentation is often sequential.
Early experiments are often designed to provide mean-unbiased estimates of treatment effects and of experimental error. Later experiments are often designed to test a hypothesis that a treatment effect has an important magnitude; in this case, the number of experimental units is chosen so that the experiment is within budget and has adequate power, among other goals.
Reporting sample size analysis is generally required in psychology. "Provide information on sample size and the process that led to sample size decisions." The analysis, which is written in the experimental protocol before the experiment is conducted, is examined in grant applications and administrative review boards.
Besides the power analysis, there are less formal methods for selecting the number of experimental units. These include graphical methods based on limiting
the probability of false negative errors, graphical methods based on an expected variation increase (above the residuals) and methods based on achieving a desired confident interval.
Power analysis.
Power analysis is often applied in the context of ANOVA in order to assess the probability of successfully rejecting the null hypothesis if we assume a certain ANOVA design, effect size in the population, sample size and significance level. Power analysis can assist in study design by determining what sample size would be required in order to have a reasonable chance of rejecting the null hypothesis when the alternative hypothesis is true.
Effect size.
Several standardized measures of effect have been proposed for ANOVA to summarize the strength of the association between a predictor(s) and the dependent variable (e.g., η, ω, or ƒ) or the overall standardized difference (Ψ) of the complete model. Standardized effect-size estimates facilitate comparison of findings across studies and disciplines. However, while standardized effect sizes are commonly used in much of the professional literature, a non-standardized measure of effect size that has immediately "meaningful" units may be preferable for reporting purposes.
Follow-up analysis.
It is always appropriate to carefully consider outliers. They have a disproportionate impact on statistical conclusions and are often the result of errors.
Model confirmation.
It is prudent to verify that the assumptions of ANOVA have been met. Residuals are examined or analyzed to confirm homoscedasticity and gross normality. Residuals should have the appearance of (zero mean normal distribution) noise when plotted as a function of anything including time and 
modeled data values. Trends hint at interactions among factors or among observations. One rule of thumb: "If the largest standard deviation is less than twice the smallest standard deviation, we can use methods based on the assumption of equal standard deviations and our results 
will still be approximately correct."
Follow-up tests.
A statistically significant effect in ANOVA is often followed up with one or more different follow-up tests. This can be done in order to assess which groups are different from which other groups or to test various other focused hypotheses. Follow-up tests are often distinguished in terms of whether they are planned (a priori) or post hoc. Planned tests are determined before looking at the data and post hoc tests are performed after looking at the data.
Often one of the "treatments" is none, so the treatment group can act as a control. Dunnett's test (a modification of the t-test) tests whether each of the other treatment groups has the same 
mean as the control.
Post hoc tests such as Tukey's range test most commonly compare every group mean with every other group mean and typically incorporate some method of controlling for Type I errors. Comparisons, which are most commonly planned, can be either simple or compound. Simple comparisons compare one group mean with one other group mean. Compound comparisons typically compare two sets of groups means where one set has two or more groups (e.g., compare average group means of group A, B and C with group D). Comparisons can also look at tests of trend, such as linear and quadratic relationships, when the independent variable involves ordered levels.
Following ANOVA with pair-wise multiple-comparison tests has been criticized on several grounds. There are many such tests (10 in one table) and recommendations regarding their use are vague or conflicting.
Study designs and ANOVAs.
There are several types of ANOVA. Many statisticians base ANOVA on the design of the experiment, especially on the protocol that specifies the random assignment of treatments to subjects; the protocol's description of the assignment mechanism should include a specification of the structure of the treatments and of any blocking. It is also common to apply ANOVA to observational data using an appropriate statistical model.
Some popular designs use the following types of ANOVA:
ANOVA cautions.
Balanced experiments (those with an equal sample size for each treatment) are relatively easy to interpret; Unbalanced 
experiments offer more complexity. For single factor (one way) ANOVA, the adjustment for unbalanced data is easy, but the unbalanced analysis lacks both robustness and power. For more complex designs the lack of balance leads to further complications. "The orthogonality property of main effects and interactions present in balanced data does not carry over to the unbalanced case. This means that the usual analysis of variance techniques do not apply. 
Consequently, the analysis of unbalanced factorials is much more difficult than that for balanced designs." In the general case, "The analysis of variance can also be applied to unbalanced data, but then the sums of squares, mean squares, and F-ratios will depend on the order in which the sources of variation 
are considered." The simplest techniques for handling unbalanced data restore balance by either throwing out data or by synthesizing missing data. More complex techniques use regression.
ANOVA is (in part) a significance test. The American Psychological Association holds the view that simply reporting significance is insufficient and that reporting confidence bounds is preferred.
While ANOVA is conservative (in maintaining a significance level) against multiple comparisons in one dimension, it is not conservative against comparisons in multiple dimensions.
Generalizations.
ANOVA is considered to be a special case of linear regression which in turn is a special case of the general linear model. All consider the observations to be the sum of a model (fit) and a residual (error) to be minimized.
The Kruskal–Wallis test and the Friedman test are nonparametric tests, which do not rely on an assumption of normality.

</doc>
<doc id="639" url="https://en.wikipedia.org/wiki?curid=639" title="Alkane">
Alkane

In organic chemistry, an alkane, or paraffin (a historical name that also has other meanings), is a saturated hydrocarbon. Alkanes consist only of hydrogen and carbon atoms and all bonds are single bonds. Alkanes (technically, always acyclic or open-chain compounds) have the general chemical formula . For example, methane is CH, in which n=1 (n being the number of carbon atoms). Alkanes belong to a homologous series of organic compounds in which the members differ by a molecular mass of 14.03u (mass of a methanediyl group, —CH—, one carbon atom of mass 12.01u, and two hydrogen atoms of mass 1.01u each). There are two main commercial sources: petroleum (crude oil) and natural gas.
Each carbon atom has 4 bonds (either C-H or C-C bonds), and each hydrogen atom is joined to a carbon atom (H-C bonds). A series of linked carbon atoms is known as the carbon skeleton or carbon backbone. The number of carbon atoms is used to define the size of the alkane e.g., C-alkane.
An alkyl group, generally abbreviated with the symbol R, is a functional group or side-chain that, like an alkane, consists solely of single-bonded carbon and hydrogen atoms, for example a methyl or ethyl group.
The simplest possible alkane (the parent molecule) is methane, CH. There is no limit to the number of carbon atoms that can be linked together, the only limitation being that the molecule is acyclic, is saturated, and is a hydrocarbon. Waxes include examples of larger alkanes where the number of carbons in the carbon backbone is greater than about 17, above which the compounds are solids at standard ambient temperature and pressure (SATP).
Alkanes are not very reactive and have little biological activity. All alkanes are colourless and odourless. Alkanes can be viewed as a molecular tree upon which can be hung the more biologically active/reactive portions (functional groups) of the molecule.
Structure classification.
Saturated hydrocarbons are hydrocarbons having only single covalent bonds between their carbons. They can be:
According to the definition by IUPAC, the former two are alkanes, whereas the third group is called cycloalkanes. Saturated hydrocarbons can also combine any of the linear, cyclic (e.g., polycyclic) and branching structures; the general formula is , where "k" is the number of independent loops. Alkanes are the acyclic (loopless) ones, corresponding to "k" = 0.
Isomerism.
Alkanes with more than three carbon atoms can be arranged in various different ways, forming structural isomers. The simplest isomer of an alkane is the one in which the carbon atoms are arranged in a single chain with no branches. This isomer is sometimes called the "n"-isomer ("n" for "normal", although it is not necessarily the most common). However the chain of carbon atoms may also be branched at one or more points. The number of possible isomers increases rapidly with the number of carbon atoms. For example:
Branched alkanes can be chiral. For example, 3-methylhexane and its higher homologues are chiral due to their stereogenic center at carbon atom number 3. In addition to the alkane isomers, the chain of carbon atoms may form one or more loops. Such compounds are called cycloalkanes.
Nomenclature.
The IUPAC nomenclature (systematic way of naming compounds) for alkanes is based on identifying hydrocarbon chains. Unbranched, saturated hydrocarbon chains are named systematically with a Greek numerical prefix denoting the number of carbons and the suffix "-ane".
In 1866, August Wilhelm von Hofmann suggested systematizing nomenclature by using the whole sequence of vowels a, e, i, o and u to create suffixes -ane, -ene, -ine (or -yne), -one, -une, for the hydrocarbons CH, CH, CH, CH, CH. Now, the first three name hydrocarbons with single, double and triple bonds; "-one" represents a ketone; "-ol" represents an alcohol or OH group; "-oxy-" means an ether and refers to oxygen between two carbons, so that methoxy-methane is the IUPAC name for dimethyl ether.
It is difficult or impossible to find compounds with more than one IUPAC name. This is because shorter chains attached to longer chains are prefixes and the convention includes brackets. Numbers in the name, referring to which carbon a group is attached to, should be as low as possible, so that 1- is implied and usually omitted from names of organic compounds with only one side-group. Symmetric compounds will have two ways of arriving at the same name.
Linear alkanes.
Straight-chain alkanes are sometimes indicated by the prefix "n-" (for "normal") where a non-linear isomer exists. Although this is not strictly necessary, the usage is still common in cases where there is an important difference in properties between the straight-chain and branched-chain isomers, e.g., "n"-hexane or 2- or 3-methylpentane. Alternative names for this group are: linear paraffins or n-paraffins.
The members of the series (in terms of number of carbon atoms) are named as follows:
The first four names were derived from methanol, ether, propionic acid and butyric acid, respectively. Alkanes with five or more carbon atoms are named by adding the suffix -ane to the appropriate numerical multiplier prefix with elision of any terminal vowel ("-a" or "-o") from the basic numerical term. Hence, pentane, CH; hexane, CH; heptane, CH; octane, CH; etc. The prefix is generally Greek, however alkanes with a carbon atom count ending in nine, for example nonane, use the Latin prefix non-. For a more complete list, see List of alkanes.
Branched alkanes.
Simple branched alkanes often have a common name using a prefix to distinguish them from linear alkanes, for example "n"-pentane, isopentane, and neopentane.
IUPAC naming conventions can be used to produce a systematic name.
The key steps in the naming of more complicated branched alkanes are as follows:
Cyclic alkanes.
So-called cyclic alkanes are, in the technical sense, not a subset of alkanes, but are cycloalkanes instead. They are hydrocarbons just like alkanes, but contain one or more rings.
Simple cycloalkanes have a prefix "cyclo-" to distinguish them from alkanes. Cycloalkanes are named as per their acyclic counterparts with respect to the number of carbon atoms, e.g., cyclopentane (CH) is a cycloalkane with 5 carbon atoms just like pentane (CH), but they are joined up in a five-membered ring. In a similar manner, propane and cyclopropane, butane and cyclobutane, etc.
Substituted cycloalkanes are named similarly to substituted alkanes — the cycloalkane ring is stated, and the substituents are according to their position on the ring, with the numbering decided by Cahn-Ingold-Prelog rules.
Trivial/common names.
The trivial (non-systematic) name for alkanes is "paraffins". Together, alkanes are known as the "paraffin series". Trivial names for compounds are usually historical artifacts. They were coined before the development of systematic names, and have been retained due to familiar usage in industry. Cycloalkanes are also called naphthenes.
It is almost certain that the term "paraffin" stems from the petrochemical industry. Branched-chain alkanes are called "isoparaffins". The use of the term "paraffin" is a general term and often does not distinguish between pure compounds and mixtures of isomers, i.e., compounds with the same chemical formula, e.g., pentane and isopentane.
The following trivial names are retained in the IUPAC system:
Physical properties.
All alkanes are colourless and odourless.
Boiling point.
Alkanes experience inter-molecular van der Waals forces. Stronger inter-molecular van der Waals forces give rise to greater boiling points of alkanes.
There are two determinants for the strength of the van der Waals forces:
Under standard conditions, from CH to CH alkanes are gaseous; from CH to CH they are liquids; and after CH they are solids. As the boiling point of alkanes is primarily determined by weight, it should not be a surprise that the boiling point has almost a linear relationship with the size (molecular weight) of the molecule. As a rule of thumb, the boiling point rises 20–30 °C for each carbon added to the chain; this rule applies to other homologous series.
A straight-chain alkane will have a boiling point higher than a branched-chain alkane due to the greater surface area in contact, thus the greater van der Waals forces, between adjacent molecules. For example, compare isobutane (2-methylpropane) and n-butane (butane), which boil at −12 and 0 °C, and 2,2-dimethylbutane and 2,3-dimethylbutane which boil at 50 and 58 °C, respectively. For the latter case, two molecules 2,3-dimethylbutane can "lock" into each other better than the cross-shaped 2,2-dimethylbutane, hence the greater van der Waals forces.
On the other hand, cycloalkanes tend to have higher boiling points than their linear counterparts due to the locked conformations of the molecules, which give a plane of intermolecular contact.
Melting points.
The melting points of the alkanes follow a similar trend to boiling points for the same reason as outlined above. That is, (all other things being equal) the larger the molecule the higher the melting point. There is one significant difference between boiling points and melting points. Solids have more rigid and fixed structure than liquids. This rigid structure requires energy to break down. Thus the better put together solid structures will require more energy to break apart. For alkanes, this can be seen from the graph above (i.e., the blue line). The odd-numbered alkanes have a lower trend in melting points than even numbered alkanes. This is because even numbered alkanes pack well in the solid phase, forming a well-organized structure, which requires more energy to break apart. The odd-number alkanes pack less well and so the "looser" organized solid packing structure requires less energy to break apart.
The melting points of branched-chain alkanes can be either higher or lower than those of the corresponding straight-chain alkanes, again depending on the ability of the alkane in question to pack well in the solid phase: This is particularly true for isoalkanes (2-methyl isomers), which often have melting points higher than those of the linear analogues.
Conductivity and solubility.
Alkanes do not conduct electricity, nor are they substantially polarized by an electric field. For this reason they do not form hydrogen bonds and are insoluble in polar solvents such as water. Since the hydrogen bonds between individual water molecules are aligned away from an alkane molecule, the coexistence of an alkane and water leads to an increase in molecular order (a reduction in entropy). As there is no significant bonding between water molecules and alkane molecules, the second law of thermodynamics suggests that this reduction in entropy should be minimized by minimizing the contact between alkane and water: Alkanes are said to be hydrophobic in that they repel water.
Their solubility in nonpolar solvents is relatively good, a property that is called lipophilicity. Different alkanes are, for example, miscible in all proportions among themselves.
The density of the alkanes usually increases with the number of carbon atoms, but remains less than that of water. Hence, alkanes form the upper layer in an alkane-water mixture.
Molecular geometry.
The molecular structure of the alkanes directly affects their physical and chemical characteristics. It is derived from the electron configuration of carbon, which has four valence electrons. The carbon atoms in alkanes are always sp hybridized, that is to say that the valence electrons are said to be in four equivalent orbitals derived from the combination of the 2s orbital and the three 2p orbitals. These orbitals, which have identical energies, are arranged spatially in the form of a tetrahedron, the angle of cos(−⅓) ≈ 109.47° between them.
Bond lengths and bond angles.
An alkane molecule has only C – H and C – C single bonds. The former result from the overlap of a sp-orbital of carbon with the 1s-orbital of a hydrogen; the latter by the overlap of two sp-orbitals on different carbon atoms. The bond lengths amount to 1.09×10 m for a C – H bond and 1.54×10 m for a C – C bond.
The spatial arrangement of the bonds is similar to that of the four sp-orbitals—they are tetrahedrally arranged, with an angle of 109.47° between them. Structural formulae that represent the bonds as being at right angles to one another, while both common and useful, do not correspond with the reality.
Conformation.
The structural formula and the bond angles are not usually sufficient to completely describe the geometry of a molecule. There is a further degree of freedom for each carbon – carbon bond: the torsion angle between the atoms or groups bound to the atoms at each end of the bond. The spatial arrangement described by the torsion angles of the molecule is known as its conformation.
Ethane forms the simplest case for studying the conformation of alkanes, as there is only one C – C bond. If one looks down the axis of the C – C bond, one will see the so-called Newman projection. The hydrogen atoms on both the front and rear carbon atoms have an angle of 120° between them, resulting from the projection of the base of the tetrahedron onto a flat plane. However, the torsion angle between a given hydrogen atom attached to the front carbon and a given hydrogen atom attached to the rear carbon can vary freely between 0° and 360°. This is a consequence of the free rotation about a carbon – carbon single bond. Despite this apparent freedom, only two limiting conformations are important: eclipsed conformation and staggered conformation.
The two conformations, also known as rotamers, differ in energy: The staggered conformation is 12.6 kJ/mol lower in energy (more stable) than the eclipsed conformation (the least stable).
This difference in energy between the two conformations, known as the torsion energy, is low compared to the thermal energy of an ethane molecule at ambient temperature. There is constant rotation about the C-C bond. The time taken for an ethane molecule to pass from one staggered conformation to the next, equivalent to the rotation of one CH-group by 120° relative to the other, is of the order of 10 seconds.
The case of higher alkanes is more complex but based on similar principles, with the antiperiplanar conformation always being the most favored around each carbon-carbon bond. For this reason, alkanes are usually shown in a zigzag arrangement in diagrams or in models. The actual structure will always differ somewhat from these idealized forms, as the differences in energy between the conformations are small compared to the thermal energy of the molecules: Alkane molecules have no fixed structural form, whatever the models may suggest.
Spectroscopic properties.
Virtually all organic compounds contain carbon – carbon and carbon – hydrogen bonds, and so show some of the features of alkanes in their spectra. Alkanes are notable for having no other groups, and therefore for the "absence" of other characteristic spectroscopic features of different functional group like -OH, -CHO, -COOH etc.
Infrared spectroscopy.
The carbon–hydrogen stretching mode gives a strong absorption between 2850 and 2960 cm, while the carbon–carbon stretching mode absorbs between 800 and 1300 cm. The carbon–hydrogen bending modes depend on the nature of the group: methyl groups show bands at 1450 cm and 1375 cm, while methylene groups show bands at 1465 cm and 1450 cm. Carbon chains with more than four carbon atoms show a weak absorption at around 725 cm.
NMR spectroscopy.
The proton resonances of alkanes are usually found at δ = 0.5 – 1.5. The carbon-13 resonances depend on the number of hydrogen atoms attached to the carbon: δ = 8 – 30 (primary, methyl, -CH), 15 – 55 (secondary, methylene, -CH-), 20 – 60 (tertiary, methyne, C-H) and quaternary. The carbon-13 resonance of quaternary carbon atoms is characteristically weak, due to the lack of Nuclear Overhauser effect and the long relaxation time, and can be missed in weak samples, or samples that have not been run for a sufficiently long time.
Mass spectrometry.
Alkanes have a high ionization energy, and the molecular ion is usually weak. The fragmentation pattern can be difficult to interpret, but, in the case of branched chain alkanes, the carbon chain is preferentially cleaved at tertiary or quaternary carbons due to the relative stability of the resulting free radicals. The fragment resulting from the loss of a single methyl group (M−15) is often absent, and other fragment are often spaced by intervals of fourteen mass units, corresponding to sequential loss of CH-groups.
Chemical properties.
Alkanes are only weakly reactive with ionic and other polar substances. The acid dissociation constant (pK) values of all alkanes are above 60, hence they are practically inert to acids and bases (see: carbon acids). This inertness is the source of the term "paraffins" (with the meaning here of "lacking affinity"). In crude oil the alkane molecules have remained chemically unchanged for millions of years.
However redox reactions of alkanes, in particular with oxygen and the halogens, are possible as the carbon atoms are in a strongly reduced condition; in the case of methane, the lowest possible oxidation state for carbon (−4) is reached. Reaction with oxygen ("if" present in sufficient quantity to satisfy the reaction stoichiometry) leads to combustion without any smoke, producing carbon dioxide and water. Free radical halogenation reactions occur with halogens, leading to the production of haloalkanes. In addition, alkanes have been shown to interact with, and bind to, certain transition metal complexes in (See: carbon-hydrogen bond activation).
Free radicals, molecules with unpaired electrons, play a large role in most reactions of alkanes, such as cracking and reformation where long-chain alkanes are converted into shorter-chain alkanes and straight-chain alkanes into branched-chain isomers.
In highly branched alkanes, the bond angle may differ significantly from the optimal value (109.5°) in order to allow the different groups sufficient space. This causes a tension in the molecule, known as steric hindrance, and can substantially increase the reactivity.
Reactions with oxygen (combustion reaction).
All alkanes react with oxygen in a combustion reaction, although they become increasingly difficult to ignite as the number of carbon atoms increases. The general equation for complete combustion is:
or CH + ((3n+1)/2)O → ("n"+1)HO + "n"CO
In the absence of sufficient oxygen, carbon monoxide or even soot can be formed, as shown below:
For example, methane:
See the alkane heat of formation table for detailed data.
The standard enthalpy change of combustion, Δ"H", for alkanes increases by about 650 kJ/mol per CH group. Branched-chain alkanes have lower values of Δ"H" than straight-chain alkanes of the same number of carbon atoms, and so can be seen to be somewhat more stable.
Reactions with halogens.
Alkanes react with halogens in a so-called "free radical halogenation" reaction. The hydrogen atoms of the alkane are progressively replaced by halogen atoms. Free-radicals are the reactive species that participate in the reaction, which usually leads to a mixture of products. The reaction is highly exothermic, and can lead to an explosion.
These reactions are an important industrial route to halogenated hydrocarbons. There are three steps:
Experiments have shown that all halogenation produces a mixture of all possible isomers, indicating that all hydrogen atoms are susceptible to reaction. The mixture produced, however, is not a statistical mixture: Secondary and tertiary hydrogen atoms are preferentially replaced due to the greater stability of secondary and tertiary free-radicals. An example can be seen in the monobromination of propane: n the Figure below, the Statistical Distribution should be 25% and 75
Cracking.
Cracking breaks larger molecules into smaller ones. This can be done with a thermal or catalytic method. The thermal cracking process follows a homolytic mechanism with formation of free-radicals. The catalytic cracking process involves the presence of acid catalysts (usually solid acids such as silica-alumina and zeolites), which promote a heterolytic (asymmetric) breakage of bonds yielding pairs of ions of opposite charges, usually a carbocation and the very unstable hydride anion. Carbon-localized free-radicals and cations are both highly unstable and undergo processes of chain rearrangement, C-C scission in position beta (i.e., cracking) and intra- and intermolecular hydrogen transfer or hydride transfer. In both types of processes, the corresponding reactive intermediates (radicals, ions) are permanently regenerated, and thus they proceed by a self-propagating chain mechanism. The chain of reactions is eventually terminated by radical or ion recombination.
Isomerization and reformation.
Dragan and his colleague were the first to report about isomerization in alkanes. Isomerization and reformation are processes in which straight-chain alkanes are heated in the presence of a platinum catalyst. In isomerization, the alkanes become branched-chain isomers. In other words, it does not lose any carbons or hydrogens, keeping the same molecular weight. In reformation, the alkanes become cycloalkanes or aromatic hydrocarbons, giving off hydrogen as a by-product. Both of these processes raise the octane number of the substance. Butane is the most common alkane that is put under the process of isomerization, as it makes many branched alkanes with high octane numbers.
Other reactions.
Alkanes will react with steam in the presence of a nickel catalyst to give hydrogen. Alkanes can be chlorosulfonated and nitrated, although both reactions require special conditions. The fermentation of alkanes to carboxylic acids is of some technical importance. In the Reed reaction, sulfur dioxide, chlorine and light convert hydrocarbons to sulfonyl chlorides. Nucleophilic Abstraction can be used to separate an alkane from a metal. Alkyl groups can be transferred from one compound to another by transmetalation reactions.
Occurrence.
Occurrence of alkanes in the Universe.
Alkanes form a small portion of the atmospheres of the outer gas planets such as Jupiter (0.1% methane, 0.0002% ethane), Saturn (0.2% methane, 0.0005% ethane), Uranus (1.99% methane, 0.00025% ethane) and Neptune (1.5% methane, 1.5 ppm ethane). Titan (1.6% methane), a satellite of Saturn, was examined by the "Huygens" probe, which indicated that Titan's atmosphere periodically rains liquid methane onto the moon's surface. Also on Titan the Cassini mission has imaged seasonal methane/ethane lakes near the polar regions of Titan. Methane and ethane have also been detected in the tail of the comet Hyakutake. Chemical analysis showed that the abundances of ethane and methane were roughly equal, which is thought to imply that its ices formed in interstellar space, away from the Sun, which would have evaporated these volatile molecules. Alkanes have also been detected in meteorites such as carbonaceous chondrites.
Occurrence of alkanes on Earth.
Traces of methane gas (about 0.0002% or 1745 ppb) occur in the Earth's atmosphere, produced primarily by methanogenic microorganisms, such as Archaea in the gut of ruminants.
The most important commercial sources for alkanes are natural gas and oil. Natural gas contains primarily methane and ethane, with some propane and butane: oil is a mixture of liquid alkanes and other hydrocarbons. These hydrocarbons were formed when marine animals and plants (zooplankton and phytoplankton) died and sank to the bottom of ancient seas and were covered with sediments in an anoxic environment and converted over many millions of years at high temperatures and high pressure to their current form. Natural gas resulted thereby for example from the following reaction:
These hydrocarbon deposits, collected in porous rocks trapped beneath impermeable cap rocks, comprise commercial oil fields. They have formed over millions of years and once exhausted cannot be readily replaced. The depletion of these hydrocarbons reserves is the basis for what is known as the energy crisis.
Methane is also present in what is called biogas, produced by animals and decaying matter, which is a possible renewable energy source.
Alkanes have a low solubility in water, so the content in the oceans is negligible; however, at high pressures and low temperatures (such as at the bottom of the oceans), methane can co-crystallize with water to form a solid methane clathrate (methane hydrate). Although this cannot be commercially exploited at the present time, the amount of combustible energy of the known methane clathrate fields exceeds the energy content of all the natural gas and oil deposits put together. Methane extracted from methane clathrate is therefore a candidate for future fuels.
Biological occurrence.
Acyclic alkanes occur in nature in various ways.
Certain types of bacteria can metabolize alkanes: they prefer even-numbered carbon chains as they are easier to degrade than odd-numbered chains.
On the other hand, certain archaea, the methanogens, produce large quantities of methane by the metabolism of carbon dioxide or other oxidized organic compounds. The energy is released by the oxidation of hydrogen:
Methanogens are also the producers of marsh gas in wetlands, and release about two billion tonnes of methane per year—the atmospheric content of this gas is produced nearly exclusively by them. The methane output of cattle and other herbivores, which can release up to 150 liters per day, and of termites, is also due to methanogens. They also produce this simplest of all alkanes in the intestines of humans. Methanogenic archaea are, hence, at the end of the carbon cycle, with carbon being released back into the atmosphere after having been fixed by photosynthesis. It is probable that our current deposits of natural gas were formed in a similar way.
Alkanes also play a role, if a minor role, in the biology of the three eukaryotic groups of organisms: fungi, plants and animals. Some specialized yeasts, e.g., "Candida tropicale", "Pichia" sp., "Rhodotorula" sp., can use alkanes as a source of carbon or energy. The fungus "Amorphotheca resinae" prefers the longer-chain alkanes in aviation fuel, and can cause serious problems for aircraft in tropical regions.
In plants, the solid long-chain alkanes are found in the plant cuticle and epicuticular wax of many species, but are only rarely major constituents. They protect the plant against water loss, prevent the leaching of important minerals by the rain, and protect against bacteria, fungi, and harmful insects. The carbon chains in plant alkanes are usually odd-numbered, between twenty-seven and thirty-three carbon atoms in length and are made by the plants by decarboxylation of even-numbered fatty acids. The exact composition of the layer of wax is not only species-dependent, but changes also with the season and such environmental factors as lighting conditions, temperature or humidity.
More volatile short-chain alkanes are also produced by and found in plant tissues. The Jeffrey pine is noted for producing exceptionally high levels of n-heptane in its resin, for which reason its distillate was designated as the zero point for one octane rating. Floral scents have also long been known to contain volatile alkane components, and n-nonane is a significant component in the scent of some roses. Emission of gaseous and volatile alkanes such as ethane, pentane, and hexane by plants has also been documented at low levels, though they are not generally considered to be a major component of biogenic air pollution.
Edible vegetable oils also typically contain small fractions of biogenic alkanes with a wide spectrum of carbon numbers, mainly 8 to 35, usually peaking in the low to upper 20s, with concentrations up to dozens of milligrams per kilogram (parts per million by weight) and sometimes over a hundred for the total alkane fraction.
Alkanes are found in animal products, although they are less important than unsaturated hydrocarbons. One example is the shark liver oil, which is approximately 14% pristane (2,6,10,14-tetramethylpentadecane, CH). They are important as pheromones, chemical messenger materials, on which insects depend for communication. In some species, e.g. the support beetle "Xylotrechus colonus", pentacosane (CH), 3-methylpentaicosane (CH) and 9-methylpentaicosane (CH) are transferred by body contact. With others like the tsetse fly "Glossina morsitans morsitans", the pheromone contains the four alkanes 2-methylheptadecane (CH), 17,21-dimethylheptatriacontane (CH), 15,19-dimethylheptatriacontane (CH) and 15,19,23-trimethylheptatriacontane (CH), and acts by smell over longer distances. Waggle-dancing honey bees produce and release two alkanes, tricosane and pentacosane.
Ecological relations.
One example, in which both plant and animal alkanes play a role, is the ecological relationship between the sand bee ("Andrena nigroaenea") and the early spider orchid ("Ophrys sphegodes"); the latter is dependent for pollination on the former. Sand bees use pheromones in order to identify a mate; in the case of "A. nigroaenea", the females emit a mixture of tricosane (CH), pentacosane (CH) and heptacosane (CH) in the ratio 3:3:1, and males are attracted by specifically this odor. The orchid takes advantage of this mating arrangement to get the male bee to collect and disseminate its pollen; parts of its flower not only resemble the appearance of sand bees, but also produce large quantities of the three alkanes in the same ratio as female sand bees. As a result, numerous males are lured to the blooms and attempt to copulate with their imaginary partner: although this endeavor is not crowned with success for the bee, it allows the orchid to transfer its pollen,
which will be dispersed after the departure of the frustrated male to different blooms.
Production.
Petroleum refining.
As stated earlier, the most important source of alkanes is natural gas and crude oil. Alkanes are separated in an oil refinery by fractional distillation and processed into many different products.
Fischer-Tropsch.
The Fischer-Tropsch process is a method to synthesize liquid hydrocarbons, including alkanes, from carbon monoxide and hydrogen. This method is used to produce substitutes for petroleum distillates.
Laboratory preparation.
There is usually little need for alkanes to be synthesized in the laboratory, since they are usually commercially available. Also, alkanes are generally non-reactive chemically or biologically, and do not undergo functional group interconversions cleanly. When alkanes are produced in the laboratory, it is often a side-product of a reaction. For example, the use of "n"-butyllithium as a strong base gives the conjugate acid, n-butane as a side-product:
However, at times it may be desirable to make a portion of a molecule into an alkane like functionality (alkyl group) using the above or similar methods. For example, an ethyl group is an alkyl group; when this is attached to a hydroxy group, it gives ethanol, which is not an alkane. To do so, the best-known methods are hydrogenation of alkenes:
Alkanes or alkyl groups can also be prepared directly from alkyl halides in the Corey-House-Posner-Whitesides reaction. The Barton-McCombie deoxygenation removes hydroxyl groups from alcohols e.g.
and the Clemmensen reduction removes carbonyl groups from aldehydes and ketones to form alkanes or alkyl-substituted compounds e.g.:
Applications.
The applications of a certain alkane can be determined quite well according to the number of carbon atoms. The first four alkanes are used mainly for heating and cooking purposes, and in some countries for electricity generation. Methane and ethane are the main components of natural gas; they are normally stored as gases under pressure. It is, however, easier to transport them as liquids: This requires both compression and cooling of the gas.
Propane and butane can be liquefied at fairly low pressures, and are well known as liquified petroleum gas (LPG). Propane, for example, is used in the propane gas burner and as a fuel for cars, butane in disposable cigarette lighters. The two alkanes are used as propellants in aerosol sprays.
From pentane to octane the alkanes are reasonably volatile liquids. They are used as fuels in internal combustion engines, as they vaporise easily on entry into the combustion chamber without forming droplets, which would impair the uniformity of the combustion. Branched-chain alkanes are preferred as they are much less prone to premature ignition, which causes knocking, than their straight-chain homologues. This propensity to premature ignition is measured by the octane rating of the fuel, where 2,2,4-trimethylpentane ("isooctane") has an arbitrary value of 100, and heptane has a value of zero. Apart from their use as fuels, the middle alkanes are also good solvents for nonpolar substances.
Alkanes from nonane to, for instance, hexadecane (an alkane with sixteen carbon atoms) are liquids of higher viscosity, less and less suitable for use in gasoline. They form instead the major part of diesel and aviation fuel. Diesel fuels are characterized by their cetane number, cetane being an old name for hexadecane. However, the higher melting points of these alkanes can cause problems at low temperatures and in polar regions, where the fuel becomes too thick to flow correctly.
Alkanes from hexadecane upwards form the most important components of fuel oil and lubricating oil. In the latter function, they work at the same time as anti-corrosive agents, as their hydrophobic nature means that water cannot reach the metal surface. Many solid alkanes find use as paraffin wax, for example, in candles. This should not be confused however with true wax, which consists primarily of esters.
Alkanes with a chain length of approximately 35 or more carbon atoms are found in bitumen, used, for example, in road surfacing. However, the higher alkanes have little value and are usually split into lower alkanes by cracking.
Some synthetic polymers such as polyethylene and polypropylene are alkanes with chains containing hundreds of thousands of carbon atoms. These materials are used in innumerable applications, and billions of kilograms of these materials are made and used each year.
Environmental transformations.
When released in the environment, alkanes don't undergo rapid biodegradation, because they have no functional groups (like hydroxyl or carbonyl) that are needed by most organisms in order to metabolize the compound.
However, some bacteria can metabolize some alkanes (especially those linear and short), by oxidizing the terminal carbon atom. The product is an alcohol, that could be next oxidized to an aldehyde, and finally to a carboxylic acid. The resulting fatty acid could be metabolized through the fatty acid degradation pathway.
Hazards.
Methane is explosive when mixed with air (1 – 8% CH). Other lower alkanes can also form explosive mixtures with air. The lighter liquid alkanes are highly flammable, although this risk decreases with the length of the carbon chain. Pentane, hexane, heptane, and octane are classed as "dangerous for the environment" and "harmful".
Considerations for detection /risk control:

</doc>
<doc id="640" url="https://en.wikipedia.org/wiki?curid=640" title="Appellate procedure in the United States">
Appellate procedure in the United States

In United States appellate procedure, an appeal is a petition for review of a case that has been decided by a court of law. The petition is made to a higher court for the purpose of overturning the lower court's decision.
The specific procedures for appealing, including even whether there is a right of appeal from a particular type of decision, can vary greatly from state to state. The right to file an appeal can also vary from state to state; for example, the New Jersey Constitution vests judicial power in a Supreme Court, a Superior Court, and other courts of limited jurisdiction, with an appellate court being part of the Superior Court.
The nature of an appeal can vary greatly depending on the type of case and the rules of the court in the jurisdiction where the case was prosecuted. There are many types of standard of review for appeals, such as "de novo" and abuse of discretion.
An appellate court is a court that hears cases on appeal from another court. Depending on the particular legal rules that apply to each circumstance, a party to a court case who is unhappy with the result might be able to challenge that result in an appellate court on specific grounds. These grounds typically could include errors of law, fact, procedure or due process.
In different jurisdictions, appellate courts are also called appeals courts, courts of appeals, superior courts, or supreme courts.
Access to appellant status.
A party who files an appeal is called an "appellant", "plaintiff in error", "petitioner" or "pursuer", and a party on the other side is called a "appellee". A "cross-appeal" is an appeal brought by the respondent. For example, suppose at trial the judge found for the plaintiff and ordered the defendant to pay $50,000. If the defendant files an appeal arguing that he should not have to pay any money, then the plaintiff might file a cross-appeal arguing that the defendant should have to pay $200,000 instead of $50,000.
The appellant is the party who, having lost part or all their claim in a lower court decision, is appealing to a higher court to have their case reconsidered. This is usually done on the basis that the lower court judge erred in the application of law, but it may also be possible to appeal on the basis of court misconduct, or that a finding of fact was entirely unreasonable to make on the evidence.
The appellant in the new case can be either the plaintiff (or claimant), defendant, third-party intervenor, or respondent (appellee) from the lower case, depending on who was the losing party. The winning party from the lower court, however, is now the respondent. In unusual cases the appellant can be the victor in the court below, but still appeal.
An appellee is the party to an appeal in which the lower court judgment was in its favor. The appellee is required to respond to the petition, oral arguments, and legal briefs of the appellant. In general, the appellee takes the procedural posture that the lower court's decision should be affirmed.
Ability to appeal.
An appeal "as of right" is one that is guaranteed by statute or some underlying constitutional or legal principle. The appellate court cannot refuse to listen to the appeal. An appeal "by leave" or "permission" requires the appellant to obtain leave to appeal; in such a situation either or both of the lower court and the court may have the discretion to grant or refuse the appellant's demand to appeal the lower court's decision. In the Supreme Court, review in most cases is available only if the Court exercises its discretion and grants a writ of certiorari.
In tort, equity, or other civil matters either party to a previous case may file an appeal. In criminal matters, however, the state or prosecution generally has no appeal "as of right". And due to the double jeopardy principle, the state or prosecution may never appeal a jury or bench verdict of acquittal. But in some jurisdictions, the state or prosecution may appeal "as of right" from a trial court's dismissal of an indictment in whole or in part or from a trial court's granting of a defendant's suppression motion. Likewise, in some jurisdictions, the state or prosecution may appeal an issue of law "by leave" from the trial court or the appellate court. The ability of the prosecution to appeal a decision in favor of a defendant varies significantly internationally. All parties must present grounds to appeal, or it will not be heard.
By convention in some law reports, the appellant is named first. This can mean that where it is the defendant who appeals, the name of the case in the law reports reverses (in some cases twice) as the appeals work their way up the court hierarchy. This is not always true, however. In the federal courts, the parties' names always stay in the same order as the lower court when an appeal is taken to the circuit courts of appeals, and are re-ordered only if the appeal reaches the Supreme Court.
Direct or collateral: Appealing criminal convictions.
Many jurisdictions recognize two types of appeals, particularly in the "criminal context. The first is the traditional "direct" appeal in which the appellant files an appeal with the next higher court of review. The second is the collateral appeal or post-conviction petition, in which the petitioner-appellant files the appeal in a court of first instance—usually the court that tried the case.
The key distinguishing factor between direct and collateral appeals is that the former occurs in state courts, and the latter in federal courts.
Relief in post-conviction is rare and is most often found in capital or violent felony cases. The typical scenario involves an incarcerated defendant locating DNA evidence demonstrating the defendant's actual innocence.
Appellate review.
"Appellate review" is the general term for the process by which courts with appellate jurisdiction take jurisdiction of matters decided by lower courts. It is distinguished from judicial review, which refers to the court's overriding constitutional or statutory right to determine if a legislative act or administrative decision is defective for jurisdictional or other reasons (which may vary by jurisdiction).
In most jurisdictions the normal and preferred way of seeking appellate review is by filing an appeal of the final judgment. Generally, an appeal of the judgment will also allow appeal of all other orders or rulings made by the trial court in the course of the case. This is because such orders cannot be appealed "as of right". However, certain critical interlocutory court orders, such as the denial of a request for an interim injunction, or an order holding a person in contempt of court, can be appealed immediately although the case may otherwise not have been fully disposed of.
There are two distinct forms of appellate review, "direct" and "collateral". For example, a criminal defendant may be convicted in state court, and lose on "direct appeal" to higher state appellate courts, and if unsuccessful, mount a "collateral" action such as filing for a writ of habeas corpus in the federal courts. Generally speaking, "irect appeal statutes afford defendants the opportunity to challenge the merits of a judgment and allege errors of law or fact. ... ollateral revie, on the other hand, provide an independent and civil inquiry into the validity of a conviction and sentence, and as such are generally limited to challenges to constitutional, jurisdictional, or other fundamental violations that occurred at trial." "Graham v. Borgen", 483 F 3d. 475 (7th Cir. 2007) (no. 04-4103) (slip op. at 7) (citation omitted).
In Anglo-American common law courts, appellate review of lower court decisions may also be obtained by filing a petition for review by prerogative writ in certain cases. There is no corresponding right to a writ in any pure or continental civil law legal systems, though some mixed systems such as Quebec recognize these prerogative writs.
Direct Appeal.
After exhausting the first appeal as of right, defendants usually petition the highest state court to review the decision. This appeal is known as a direct appeal. The highest state court, generally known as the Supreme Court, exercises discretion over whether it will review the case. On direct appeal, a prisoner challenges the grounds of the conviction based on an error that occurred at trial or some other stage in the adjudicative process.
Preservation Issues.
An appellant's claim(s) must usually be preserved at trial. This means that the defendant had to object to the error when it occurred in the trial. Because constitutional claims are of great magnitude, appellate courts might be more lenient to review the claim even if it was not preserved. For example Connecticut applies the following standard to review unpreserved claims: 1.the record is adequate to review the alleged claim of error; 2. the claim is of constitutional magnitude alleging the violation of a fundamental right; 3. the alleged constitutional violation clearly exists and clearly deprived the defendant of a fair trial; 4. if subject to harmless error analysis, the state has failed to demonstrate harmlessness of the alleged constitutional violation beyond a reasonable doubt.
State Post Conviction Relief: Collateral Appeal.
All States have a post-conviction relief process. Similar to federal post-conviction relief, an appellant can petition the court to correct alleged fundamental errors that were not corrected on direct review. Typical claims might include ineffective assistance of counsel and actual innocence based on new evidence. These proceedings are normally separate from the direct appeal, however some states allow for collateral relief to be sought on direct appeal. After direct appeal, the conviction is considered final. An appeal from the post conviction court proceeds just as a direct appeal. That is, it goes to the intermediate appellate court, followed by the highest court. If the petition is granted the appellant could be released from incarceration, the sentence could be modified, or a new trial could be ordered.
Notice of appeal.
A "notice of appeal" is a form or document that in many cases is required to begin an appeal. The form is completed by the appellant or by the appellant's legal representative. The nature of this form can vary greatly from country to country and from court to court within a country.
The specific rules of the legal system will dictate exactly how the appeal is officially begun. For example, the appellant might have to file the notice of appeal with the appellate court, or with the court from which the appeal is taken, or both.
Some courts have samples of a notice of appeal on the court's own web site. In New Jersey, for example, the Administrative Office of the Court has promulgated a form of notice of appeal for use by appellants, though using this exact form is not mandatory and the failure to use it is not a jurisdictional defect provided that all pertinent information is set forth in whatever form of notice of appeal is used.
The deadline for beginning an appeal can often be very short: traditionally, it is measured in days, not months. This can vary from country to country, as well as within a country, depending on the specific rules in force. In the U.S. federal court system, criminal defendants must file a notice of appeal within 10 days of the entry of either the judgment or the order being appealed, or the right to appeal is forfeited.
Appellate procedure.
Generally speaking the appellate court examines the record of evidence presented in the trial court and the law that the lower court applied and decides whether that decision was legally sound or not. The appellate court will typically be deferential to the lower court's findings of fact (such as whether a defendant committed a particular act), unless clearly erroneous, and so will focus on the court's application of the law to those facts (such as whether the act found by the court to have occurred fits a legal definition at issue).
If the appellate court finds no defect, it "affirms" the judgment. If the appellate court does find a legal defect in the decision "below" (i.e., in the lower court), it may "modify" the ruling to correct the defect, or it may nullify ("reverse" or "vacate") the whole decision or any part of it. It may, in addition, send the case back ("remand" or "remit") to the lower court for further proceedings to remedy the defect.
In some cases, an appellate court may review a lower court decision "de novo" (or completely), challenging even the lower court's findings of fact. This might be the proper standard of review, for example, if the lower court resolved the case by granting a pre-trial motion to dismiss or motion for summary judgment which is usually based only upon written submissions to the trial court and not on any trial testimony.
Another situation is where appeal is by way of "re-hearing". Certain jurisdictions permit certain appeals to cause the trial to be heard afresh in the appellate court.
Sometimes, the appellate court finds a defect in the procedure the parties used in filing the appeal and dismisses the appeal without considering its merits, which has the same effect as affirming the judgment below. (This would happen, for example, if the appellant waited too long, under the appellate court's rules, to file the appeal.)
Generally, there is no trial in an appellate court, only consideration of the record of the evidence presented to the trial court and all the pre-trial and trial court proceedings are reviewed—unless the appeal is by way of re-hearing, new evidence will usually only be considered on appeal in "very" rare instances, for example if that material evidence was unavailable to a party for some very significant reason such as prosecutorial misconduct.
In some systems, an appellate court will only consider the written decision of the lower court, together with any written evidence that was before that court and is relevant to the appeal. In other systems, the appellate court will normally consider the record of the lower court. In those cases the record will first be certified by the lower court.
The appellant has the opportunity to present arguments for the granting of the appeal and the appellee (or respondent) can present arguments against it. Arguments of the parties to the appeal are presented through their appellate lawyers, if represented, or "pro se" if the party has not engaged legal representation. Those arguments are presented in written briefs and sometimes in oral argument to the court at a hearing. At such hearings each party is allowed a brief presentation at which the appellate judges ask questions based on their review of the record below and the submitted briefs.
In an adversarial system, appellate courts do not have the power to review lower court decisions unless a party appeals it. Therefore, if a lower court has ruled in an improper manner, or against legal precedent, that judgment will stand if not appealed – even if it might have been overturned on appeal.
The United States legal system generally recognizes two types of appeals: a trial "de novo" or an appeal on the record.
A trial de novo is usually available for review of informal proceedings conducted by some minor judicial tribunals in proceedings that do not provide all the procedural attributes of a formal judicial trial. If unchallenged, these decisions have the power to settle more minor legal disputes once and for all. If a party is dissatisfied with the finding of such a tribunal, one generally has the power to request a trial "de novo" by a court of record. In such a proceeding, all issues and evidence may be developed newly, as though never heard before, and one is not restricted to the evidence heard in the lower proceeding. Sometimes, however, the decision of the lower proceeding is itself admissible as evidence, thus helping to curb frivolous appeals.
In some cases, an application for "trial de novo" effectively erases the prior trial as if it had never taken place. The Supreme Court of Virginia has stated that '"This Court has repeatedly held that the effect of an appeal to circuit court is to "annul the judgment of the inferior tribunal as completely as if there had been no previous trial."' The only exception to this is that if a defendant appeals a conviction for a crime having multiple levels of offenses, where they are convicted on a lesser offense, the appeal is of the lesser offense; the conviction represents an acquittal of the more serious offenses. " trial on the same charges in the circuit court does not violate double jeopardy principles, . . . subject only to the limitation that conviction in h district court for an offense lesser included in the one charged constitutes an acquittal of the greater offense,
permitting trial de novo in the circuit court only for the lesser-included offense."
In an appeal on the record from a decision in a judicial proceeding, both appellant and respondent are bound to base their arguments wholly on the proceedings and body of evidence as they were presented in the lower tribunal. Each seeks to prove to the higher court that the result they desired was the just result. Precedent and case law figure prominently in the arguments. In order for the appeal to succeed, the appellant must prove that the lower court committed reversible error, that is, an impermissible action by the court acted to cause a result that was unjust, and which would not have resulted had the court acted properly. Some examples of reversible error would be erroneously instructing the jury on the law applicable to the case, permitting seriously improper argument by an attorney, admitting or excluding evidence improperly, acting outside the court's jurisdiction, injecting bias into the proceeding or appearing to do so, juror misconduct, etc. The failure to formally object at the time, to what one views as improper action in the lower court, may result in the affirmance of the lower court's judgment on the grounds that one did not "preserve the issue for appeal" by objecting.
In cases where a judge rather than a jury decided issues of fact, an appellate court will apply an "abuse of discretion" standard of review. Under this standard, the appellate court gives deference to the lower court's view of the evidence, and reverses its decision only if it were a clear abuse of discretion. This is usually defined as a decision outside the bounds of reasonableness. On the other hand, the appellate court normally gives less deference to a lower court's decision on issues of law, and may reverse if it finds that the lower court applied the wrong legal standard.
In some cases, an appellant may successfully argue that the law under which the lower decision was rendered was unconstitutional or otherwise invalid, or may convince the higher court to order a new trial on the basis that evidence earlier sought was concealed or only recently discovered. In the case of new evidence, there must be a high probability that its presence or absence would have made a material difference in the trial. Another issue suitable for appeal in criminal cases is effective assistance of counsel. If a defendant has been convicted and can prove that his lawyer did not adequately handle his case and that there is a reasonable probability that the result of the trial would have been different had the lawyer given competent representation, he is entitled to a new trial.
A lawyer traditionally starts an oral argument to any appellate court with the words "May it please the court."
After an appeal is heard, the "mandate" is a formal notice of a decision by a court of appeal; this notice is transmitted to the trial court and, when filed by the clerk of the trial court, constitutes the final judgment on the case, unless the appeal court has directed further proceedings in the trial court. The mandate is distinguished from the appeal court's opinion, which sets out the legal reasoning for its decision. In some jurisdictions the mandate is known as the "remittitur".
Results.
The result of an appeal can be:
There can be multiple outcomes, so that the reviewing court can affirm some rulings, reverse others and remand the case all at the same time. Remand is not required where there is nothing left to do in the case. "Generally speaking, an appellate court's judgment provides 'the final directive of the appeals courts as to the matter appealed, setting out with specificity the court's determination that the action appealed from should be affirmed, reversed, remanded or modified'".
Some reviewing courts who have discretionary review may send a case back without comment other than "review improvidently granted". In other words, after looking at the case, they chose not to say anything. The result for the case of "review improvidently granted" is effectively the same as affirmed, but without that extra higher court stamp of approval.

</doc>
<doc id="642" url="https://en.wikipedia.org/wiki?curid=642" title="Answer">
Answer

Generally, an answer is a reply to a question. It can be solution, a retaliation or a response to it.
In law, an answer was originally a solemn assertion in opposition to someone or something, and thus generally any counter-statement or defense, a reply to a question or response, or objection, or a correct solution of a problem.
In the common law, an answer is the first pleading by a defendant, usually filed and served upon the plaintiff within a certain strict time limit after a civil complaint or criminal information or indictment has been served upon the defendant. It may have been preceded by an "optional" "pre-answer" motion to dismiss or demurrer; if such a motion is unsuccessful, the defendant "must" file an answer to the complaint or risk an adverse default judgment.
In a criminal case, there is usually an arraignment or some other kind of appearance before the defendant comes to court. The pleading in the criminal case, which is entered on the record in open court, is usually either guilty or not guilty. Generally speaking in private, civil cases there is no plea entered of guilt or innocence. There is only a judgment that grants money damages or some other kind of equitable remedy such as restitution or a permanent injunction. Criminal cases may lead to fines or other punishment, such as imprisonment.
The famous Latin "Responsa Prudentium" ("answers of the learned ones") were the accumulated views of many successive generations of Roman lawyers, a body of legal opinion which gradually became authoritative.
In music an "answer" (also known as countersubject) is the technical name in counterpoint for the repetition or modification by one part or instrument of a theme proposed by another.

</doc>
<doc id="643" url="https://en.wikipedia.org/wiki?curid=643" title="Appellate court">
Appellate court

An appellate court, commonly called an appeals court, court of appeals (American English), appeal court (British English), court of second instance or second instance court, is any court of law that is empowered to hear an appeal of a trial court or other lower tribunal. In most jurisdictions, the court system is divided into at least three levels: the trial court, which initially hears cases and reviews evidence and testimony to determine the facts of the case; at least one intermediate appellate court; and a supreme court (or court of last resort) which primarily reviews the decisions of the intermediate courts. A jurisdiction's supreme court is that jurisdiction's highest appellate court. Appellate courts nationwide can operate by varying rules.
The authority of appellate courts to review decisions of lower courts varies widely from one jurisdiction to another. In some places, the appellate court has limited powers of review. "Generally speaking, an appellate court's judgment provides 'the final directive of the appeals courts as to the matter appealed, setting out with specificity the court's determination that the action appealed from should be affirmed, reversed, remanded or modified'".
United States.
In the United States, both state and federal appellate courts are usually restricted to examining whether the lower court made the correct legal determinations, rather than hearing direct evidence and determining what the facts of the case were. Furthermore, U.S. appellate courts are usually restricted to hearing appeals based on matters that were originally brought up before the trial court. Hence, such an appellate court will not consider an appellant's argument if it is based on a theory that is raised for the first time in the appeal. 
In most U.S. states, and in U.S. federal courts, parties before the court are allowed one appeal as of right. This means that a party who is unsatisfied with the outcome of a trial may bring an appeal to contest that outcome. However, appeals may be costly, and the appellate court must find an error on the part of the court below that justifies upsetting the verdict. Therefore, only a small proportion of trial court decisions result in appeals. Some appellate courts, particularly supreme courts, have the power of discretionary review, meaning that they can decide whether they will hear an appeal brought in a particular case.
Institutional titles.
Many U.S. jurisdictions title their appellate court a court of appeal or court of appeals. Historically, others have titled their appellate court a court of errors (or court of errors and appeals), on the premise that it was intended to correct errors made by lower courts. Examples of such courts include the New Jersey Court of Errors and Appeals (which existed from 1844 to 1947), the Connecticut Supreme Court of Errors (which has been renamed the Connecticut Supreme Court), the Kentucky Court of Errors (renamed the Kentucky Supreme Court), and the Mississippi High Court of Errors and Appeals (since renamed the Supreme Court of Mississippi). In some jurisdictions, courts able to hear appeals are known as an appellate division.
The phrase "court of appeals" most often refers to intermediate appellate courts. However, the New York system is different: the "New York Court of Appeals" is the highest appellate court; and the phrase "New York Supreme Court" applies to the trial court of general jurisdiction.
Depending on the system, certain courts may serve as both trial courts and appellate courts, hearing appeals of decisions made by courts with more limited jurisdiction. Some jurisdictions have specialized appellate courts, such as the Texas Court of Criminal Appeals, which only hears appeals raised in criminal cases, and the United States Court of Appeals for the Federal Circuit, which has general jurisdiction but derives most of its caseload from patent cases, on one hand, and appeals from the Court of Federal Claims on the other.
New Zealand.
The Court of Appeal of New Zealand, located in Wellington, is New Zealand's principal intermediate appellate court. In practice, most appeals are resolved at this intermediate appellate level, rather than in the Supreme Court.

</doc>
<doc id="649" url="https://en.wikipedia.org/wiki?curid=649" title="Arraignment">
Arraignment

Arraignment is a formal reading of a criminal charging document in the presence of the defendant to inform the defendant of the charges against them. In response to arraignment, the accused is expected to enter a plea. Acceptable pleas vary among jurisdictions, but they generally include "guilty", "not guilty", and the peremptory pleas (or pleas in bar) setting out reasons why a trial cannot proceed. Pleas of "nolo contendere" (no contest) and the ""Alford" plea" are allowed in some circumstances.
By country.
Australia.
In Australia, arraignment is the first of eleven stages in a criminal trial, and involves the clerk of the court reading out the indictment. The judge will testify during the indictment process.
Canada.
In every province in Canada except British Columbia, a defendant is arraigned on the day of their trial. In British Columbia, arraignment takes places in one of the first few court appearances by the defendant or their lawyer. The defendant is asked whether he or she pleads guilty or not guilty to each charge.
France.
In France, the general rule is that one cannot remain in police custody for more than 24 hours from the time of the arrest. However, police custody can last another 24 hours in specific circumstances, especially if the offence is punishable by at least one year's imprisonment, or if the investigation is deemed to require the extra time, and can last up to 96 hours in certain cases involving terrorism, drug trafficking or organised crime. The police needs to have the consent of the prosecutor (in the vast majority of cases, the prosecutor will consent).
Germany.
In Germany, if one has been arrested and taken into custody by the police one must be brought before a judge as soon as possible and at the latest on the day after the arrest.
New Zealand.
At the first appearance, the accused is read the charges and asked for a plea. The available pleas are, guilty, not guilty, and no plea. No plea allows the defendant to get legal advice on the plea, which must be made on the second appearance.
South Africa.
In South Africa, arraignment is defined as the calling upon the accused to appear, the informing of the accused of the crime charged against him, the demanding of the accused whether he be guilty or not guilty, and the entering of his plea. His plea having been entered he is said to stand arraigned.
United Kingdom.
In England, Wales, and Northern Ireland, arraignment is the first of eleven stages in a criminal trial, and involves the clerk of the court reading out the indictment.
In England and Wales, the police cannot legally detain anyone for more than 24 hours without charging them unless an officer with the rank of superintendent (or above) authorises detention for a further 12 hours (36 hours total), or a judge (who will be a magistrate) authorises detention by the police before charge for up to a maximum of 96 hours, but for terrorism-related offences people can be held by the police for up to 28 days before charge. If they are not released after being charged, they should be brought before a court as soon as practicable.
United States.
Under the United States Federal Rules of Criminal Procedure, "arraignment shall . onsist of a open . reading the indictment . to the defendant . and call on him to plead thereto. He/she shall be given a copy of the indictment . before he/she is called upon to plead."
In federal courts, arraignment takes place in two stages. The first is called the initial arraignment and must take place within 48 hours of an individual's arrest, 72 hours if the individual was arrested on the weekend and not able to go before a judge until Monday. During this arraignment the defendant is informed of the pending legal charges and is informed of his or her right to retain counsel. The presiding judge also decides at what amount, if any, to set bail. During the second arraignment, a post-indictment arraignment or PIA, the defendant is allowed to enter a plea.
In New York, most people arrested must be released if they are not arraigned within 24 hours.
In California, arraignments must be conducted without unnecessary delay and, in any event, within 48 hours of arrest, excluding weekends and holidays. Thus, an individual arrested without a warrant, in some cases, may be held for as long as 168 hours (7 days) without arraignment or charge.
Form of the arraignment.
The wording of the arraignment varies from jurisdiction to jurisdiction. However, it generally conforms with the following principles:
Video arraignment.
Video arraignment is the act of conducting the arraignment process using some form of videoconferencing technology. Use of video arraignment system allows the courts to conduct the requisite arraignment process without the need to transport the defendant to the courtroom by using an audio-visual link between the location where the defendant is being held and the courtroom.
Use of the video arraignment process addresses the problems associated with having to transport defendants. The transportation of defendants requires time, puts additional demands on the public safety organizations to provide for the safety of the public, court personnel and for the security of the population held in detention. It also addresses the rising costs of transportation.
Guilty and not-guilty pleas.
If the defendant pleads guilty, an evidentiary hearing usually follows. The court is not required to accept a guilty plea. During the hearing, the judge assesses the offense, the mitigating factors, and the defendant's character, and passes sentence.
If the defendant pleads not guilty, a date is set for a preliminary hearing or a trial.
In the past, a defendant who refused to plead (or "stood mute") was subject to peine forte et dure (Law French for "strong and hard punishment"). Today in common-law jurisdictions, the court enters a plea of not guilty for a defendant who refuses to enter a plea. The rationale for this is the defendant's right to silence.
Pre-trial release.
This is also often the stage at which arguments for or against pre-trial release and bail may be made, depending on the alleged crime and jurisdiction.

</doc>
<doc id="651" url="https://en.wikipedia.org/wiki?curid=651" title="America the Beautiful">
America the Beautiful

"America the Beautiful" is an American patriotic song. The lyrics were written by Katharine Lee Bates, and the music was composed by church organist and choirmaster Samuel A. Ward.
Bates originally wrote the words as a poem, "Pikes Peak", first published in the Fourth of July edition of the church periodical "The Congregationalist" in 1895. At that time, the poem was titled "America" for publication.
Ward had originally written the music, "Materna", for the hymn "O Mother dear, Jerusalem" in 1882, though it was not first published until 1892. Ward's music combined with the Bates poem was first published in 1910 and titled "America the Beautiful".
The song is one of the most popular of the many American patriotic songs.
History.
In 1893, at the age of 33, Bates, an English professor at Wellesley College, had taken a train trip to Colorado Springs, Colorado, to teach a short summer school session at Colorado College. Several of the sights on her trip inspired her, and they found their way into her poem, including the World's Columbian Exposition in Chicago, the "White City" with its promise of the future contained within its alabaster buildings; the wheat fields of America's heartland Kansas, through which her train was riding on July 16; and the majestic view of the Great Plains from high atop Zebulon's Pikes Peak.
On the pinnacle of that mountain, the words of the poem started to come to her, and she wrote them down upon returning to her hotel room at the original Antlers Hotel. The poem was initially published two years later in "The Congregationalist" to commemorate the Fourth of July. It quickly caught the public's fancy. Amended versions were published in 1904 and 1913.
Several existing pieces of music were adapted to the poem. A hymn tune composed by Samuel A. Ward was generally considered the best music as early as 1910 and is still the popular tune today. Just as Bates had been inspired to write her poem, Ward, too, was inspired to compose his tune. The tune came to him while he was on a ferryboat trip from Coney Island back to his home in New York City, after a leisurely summer day in 1882, and he immediately wrote it down. He was so anxious to capture the tune in his head, he asked fellow passenger friend Harry Martin for his shirt cuff to write the tune on. He composed the tune for the old hymn "O Mother Dear, Jerusalem", retitling the work "Materna". Ward's music combined with Bates's poem were first published together in 1910 and titled "America the Beautiful".
Ward died in 1903, not knowing the national stature his music would attain since the music was only first applied to the song in 1904. Bates was more fortunate since the song's popularity was well established by the time of her death in 1929.
At various times in the more than 100 years that have elapsed since the song was written, particularly during the John F. Kennedy administration, there have been efforts to give "America the Beautiful" legal status either as a national hymn or as a national anthem equal to, or in place of, "The Star-Spangled Banner", but so far this has not succeeded. Proponents prefer "America the Beautiful" for various reasons, saying it is easier to sing, more melodic, and more adaptable to new orchestrations while still remaining as easily recognizable as "The Star-Spangled Banner". Some prefer "America the Beautiful" over "The Star-Spangled Banner" due to the latter's war-oriented imagery. Others prefer "The Star-Spangled Banner" for the same reason. While that national dichotomy has stymied any effort at changing the tradition of the national anthem, "America the Beautiful" continues to be held in high esteem by a large number of Americans.
This song was used as the background music of the television broadcast of the Tiangong-1 launch.
The song is often included in songbooks in a wide variety of religious congregations in the United States.
Popular versions.
In 1976, while the United States celebrated its bicentennial, a soulful version popularized by Ray Charles peaked at number 98 on the US R&B Charts, and is included on the soundtrack for the movie "The Sandlot".
Three different renditions of the song have entered the Hot Country Songs charts. The first was by Charlie Rich, which went to number 22 in 1976. A second, by Mickey Newbury, peaked at number 82 in 1980. An all-star version of "America the Beautiful" performed by country singers Trace Adkins, Sherrié Austin, Billy Dean, Vince Gill, Carolyn Dawn Johnson, Toby Keith, Brenda Lee, Lonestar, Lyle Lovett, Lila McCann, Lorrie Morgan, Jamie O'Neal, The Oak Ridge Boys, Collin Raye, Kenny Rogers, Keith Urban and Phil Vassar reached number 58 in July 2001. The song re-entered the chart following the September 11 attacks.
Popularity of the song increased greatly following the September 11 attacks; at some sporting events it was sung in addition to the traditional singing of the national anthem. During the first taping of the "Late Show with David Letterman" following the attacks, CBS newsman Dan Rather cried briefly as he quoted the fourth verse.
For Super Bowl XLVIII, The Coca-Cola Company aired a multilingual version of the song, sung in several different languages. This commercial incited an outcry from quite a few Americans.
Idioms.
"From sea to shining sea", originally used in the charters of some of the English Colonies in North America, is an American idiom meaning from the Atlantic Ocean to the Pacific Ocean (or vice versa). Many songs have used this term, including the American patriotic songs "America, the Beautiful" and "God Bless the USA". In addition to these, it is also featured in Schoolhouse Rock's "Elbow Room". A term similar to this is the official Canadian (Latin) motto "" (From sea to sea).
"Purple mountain majesties" refers to the shade of the Pikes Peak in Colorado Springs, Colorado, which Bates looked at while writing the poem.
Books.
Lynn Sherr's 2001 book "America the Beautiful" discusses the origins of the song and the backgrounds of its authors in depth. The book points out that the poem has the same meter as that of "Auld Lang Syne"; the songs can be sung interchangeably. Additionally, Sherr discusses the evolution of the lyrics, for instance changes in from the original third verse written by Bates. The song appears in Ellen Raskin's "The Westing Game".

</doc>
<doc id="653" url="https://en.wikipedia.org/wiki?curid=653" title="Assistive technology">
Assistive technology

Assistive technology is an umbrella term that includes assistive, adaptive, and rehabilitative devices for people with disabilities and also includes the process used in selecting, locating, and using them. Assistive technology promotes greater independence by enabling people to perform tasks that they were formerly unable to accomplish, or had great difficulty accomplishing, by providing enhancements to, or changing methods of interacting with, the technology needed to accomplish such tasks.
Assistive technology and adaptive technology.
The term adaptive technology is often used as the synonym for assistive technology, however, they are different terms. Assistive technology refers to "any item, piece of equipment, or product system, whether acquired commercially, modified, or customized, that is used to increase, maintain, or improve functional capabilities of individuals with disabilities", while adaptive technology covers items that are specifically designed for persons with disabilities and would seldom be used by non-disabled persons. In other words, "assistive technology is any object or system that increases or maintains the capabilities of people with disabilities," while adaptive technology is "any object or system that is specifically designed for the purpose of increasing or maintaining the capabilities of people with disabilities." Consequently, adaptive technology is a subset of assistive technology. Adaptive technology often refers specifically to electronic and information technology access.
Mobility impairments.
Wheelchairs.
Wheelchairs are devices that can be manually propelled or electrically propelled and that include a seating system and are designed to be a substitute for the normal mobility that most people enjoy. Wheelchairs and other mobility devices allow people to perform mobility related activities of daily living which include feeding, toileting, dressing grooming and bathing. The devices comes in a number of variations where they can be propelled either by hand or by motors where the occupant uses electrical controls to manage motors and seating control actuators through a joystick, sip-and-puff control, or other input devices. Often there are handles behind the seat for someone else to do the pushing or input devices for caregivers. Wheelchairs are used by people for whom walking is difficult or impossible due to illness, injury, or disability. People with both sitting and walking disability often need to use a wheelchair or walker.
Transfer devices.
Patient transfer devices generally allow patients with impaired mobility to be moved by caregivers between beds, wheelchairs, commodes, toilets, chairs, stretchers, shower benches, automobiles, swimming pools, and other patient support systems (i.e., radiology, surgical, or examining tables). The most common devices are Patient lifts (for vertical transfer), Transfer benches, stretcher or convertible chairs (for lateral, supine transfer), sit-to-stand lifts (for moving patients from one seated position to another i.e., from wheelchairs to commodes), air bearing inflatable mattresses (for supine transfer i.e., transfer from a gurney to an operating room table), and sliding boards (usually used for transfer from a bed to a wheelchair). Highly dependent patients who cannot assist their caregiver in moving them often require a Patient lift (a floor or ceiling-suspended sling lift) which though invented in 1955 and in common use since the early 1960's is still considered the state-of-the-art transfer device by OSHA and the American Nursing Association.
Walkers.
A walker or walking frame or Rollator is a tool for disabled people who need additional support to maintain balance or stability while walking. It consists of a frame that is about waist high, approximately twelve inches deep and slightly wider than the user. Walkers are also available in other sizes, such as for children, or for heavy people. Modern walkers are height-adjustable. The front two legs of the walker may or may not have wheels attached depending on the strength and abilities of the person using it. It is also common to see caster wheels or glides on the back legs of a walker with wheels on the front.
Prosthesis.
A prosthesis, prosthetic, or prosthetic limb is a device that replaces a missing body part. It is part of the field of biomechatronics, the science of using mechanical devices with human muscle, skeleton, and nervous systems to assist or enhance motor control lost by trauma, disease, or defect. Prostheses are typically used to replace parts lost by injury (traumatic) or missing from birth (congenital) or to supplement defective body parts. Inside the body, artificial heart valves are in common use with artificial hearts and lungs seeing less common use but under active technology development. Other medical devices and aids that can be considered prosthetics include hearing aids, artificial eyes, palatal obturator, gastric bands, and dentures.
Prostheses are specifically "not" orthoses, although given certain circumstances a prosthesis might end up performing some or all of the same functionary benefits as an orthosis. Prostheses are technically the complete finished item. For instance, a C-Leg knee alone is "not" a prosthesis, but only a prosthetic "component". The complete prosthesis would consist of the attachment system  to the residual limb — usually a "socket", and all the attachment hardware components all the way down to and including the terminal device. Keep this in mind as nomenclature is often interchanged.
The terms "prosthetic" and "orthotic" are adjectives used to describe devices such as a prosthetic knee. The terms "prosthetics" and "orthotics" are used to describe the respective allied health fields. The devices themselves are properly referred to as "prostheses" and "orthoses" in the plural and "prosthesis" and "orthosis" in the singular.
Visual impairments.
Many people with serious visual impairments live independently, using a wide range of tools and techniques. Examples of assistive technology for visually impairment include screen readers, screen magnifiers, Braille embossers, desktop video magnifiers, and voice recorders.
Screen readers.
Screen readers allow the visually impaired to easily access electronic information. These software programs connect to a computer to read the text displayed out loud. There is a variety of platforms and applications available for a variety of costs.
Braille and braille embossers.
Braille is a system of raised dots formed into units called braille cells. A full braille cell is made up of six dots, with two parallel rows of three dots, but other combinations and quantities of dots represent other letters, numbers, punctuation marks, or words. People can then use their fingers to read the code of raised dots.
A braille embosser is, simply put, a printer for braille. Instead of a standard printer adding ink onto a page, the braille embosser imprints the raised dots of braille onto a page. Some braille embossers combine both braille and ink so the documents can be read with either sight or touch.
Desktop video magnifier.
Desktop video magnifiers are electronic devices that use a camera and a display screen to perform digital magnification of printed materials. They enlarge printed pages for those with low vision. A camera connects to a monitor that displays real time images, and the user can control settings such as magnification, focus, contrast, underlining, highlighting, and other screen preferences. They come in a variety of sizes and styles; some are small and portable with handheld cameras, while others are much larger and mounted on a fixed stand.
Screen magnification software.
A screen magnifier is software that interfaces with a computer's graphical output to present enlarged screen content. It allows users to enlarge the texts and graphics on their computer screens for easier viewing. Similar to desktop video magnifiers, this technology assists people with low vision. After the user loads the software into their computer's memory, it serves as a kind of "computer magnifying glass." Wherever the computer cursor moves, it enlarges the area around it. This allows greater computer accessibility for a wide range of visual abilities.
Personal emergency response systems.
Personal emergency response systems (PERS), or Telecare (UK term), are a particular sort of assistive technology that use electronic sensors connected to an alarm system to help caregivers manage risk and help vulnerable people stay independent at home longer. An example would be the systems being put in place for senior people such as fall detectors, thermometers (for hypothermia risk), flooding and unlit gas sensors (for people with mild dementia). Notably, these alerts can be customized to the particular person's risks. When the alert is triggered, a message is sent to a caregiver or contact center who can respond appropriately.
Accessibility software.
In human–computer interaction, computer accessibility (also known as accessible computing) refers to the accessibility of a computer system to all people, regardless of disability or severity of impairment, examples include web accessibility guidelines. Another approach is for the user to present a token to the computer terminal, such as a smart card, that has configuration information to adjust the computer speed, text size, etc. to their particular needs. This is useful where users want to access public computer based terminals in Libraries, ATM, Information kiosks etc. The concept is encompassed by the CEN EN 1332-4 Identification Card Systems - Man-Machine Interface. This development of this standard has been supported in Europe by SNAPI and has been successfully incorporated into the Lasseo specifications, but with limited success due to the lack of interest from public computer terminal suppliers.
Hearing impairments.
The deaf or hard of hearing community has a difficult time to communicate and perceive information as compared to hearing individuals. Thus, these individuals often rely on visual and tactile mediums for receiving and communicating information. The use of assistive technology and devices provides this community with various solutions to their problems by providing higher sound (for those who are hard of hearing), tactile feedback, visual cues and improved technology access. Individuals who are deaf or hard of hearing utilize a variety of assistive technologies that provide them with improved accessibility to information in numerous environments. Most devices either provide amplified sound or alternate ways to access information through vision and/or vibration. These technologies can be grouped into three general categories: Hearing Technology, alerting devices, and communication support.
Hearing aids.
A hearing aid or deaf aid is an electroacoustic device which is designed to amplify sound for the wearer, usually with the aim of making speech more intelligible, and to correct impaired hearing as measured by audiometry. This type of assistive technology helps people with hearing loss participate more fully in their communities by allowing them to hear more clearly. They amplify any and all sound waves through use of a microphone, amplifier, and speaker. There is a wide variety of hearing aids available, including digital, in-the-ear, in-the-canal, behind-the-ear, and on-the-body aids.
Assistive listening devices.
Assistive listening devices include FM, infrared, and loop assistive listening devices. This type of technology allows people with hearing difficulties to focus on a speaker or subject by getting rid of extra background noises and distractions, making places like auditoriums, classrooms, and meetings much easier to participate in. The assistive listening device usually uses a microphone to capture an audio source near to its origin and broadcast it wirelessly over an FM (Frequency Modulation) transmission, IR (Infra Red) transmission, IL (Induction Loop) transmission, or other transmission method. The person who is listening may use an FM/IR/IL Receiver to tune into the signal and listen at his/her preferred volume.
Amplified telephone equipment.
This type of assistive technology allows users to amplify the volume and clarity of their phone calls so that they can easily partake in this medium of communication. There are also options to adjust the frequency and tone of a call to suit their individual hearing needs. Additionally, there is a wide variety of amplified telephones to choose from, with different degrees of amplification. For example, a phone with 26 to 40 decibel is generally sufficient for mild hearing loss, while a phone with 71 to 90 decibel is better for more severe hearing loss.
Augmentative and alternative communication.
Augmentative and alternative communication (AAC) is an umbrella term that encompasses methods of communication for those with impairments or restrictions on the production or comprehension of spoken or written language. AAC systems are extremely diverse and depend on the capabilities of the user. They may be as basic as pictures on a board that the are used to request food, drink, or other care; or they can be advanced speech generating devices, based on speech synthesis, that are capable of storing hundreds of phrases and words.
Cognitive impairments.
Assistive technology for cognition (ATC) is the use of technology (usually high tech) to augment and assistive cognitive processes such as attention, memory, self-regulation, navigation, emotion recognition and management, planning, and sequencing activity. Systematic reviews of the field have found that the number of ATC are growing rapidly, but have focused on memory and planning, that there is emerging evidence for efficacy, that a lot of scope exists to develop new ATC. Examples of ATC include: NeuroPage which prompts users about meetings, Wakamaru, which provides companionship and reminds users to take medicine and calls for help if something is wrong, and telephone Reassurance systems.
Memory aids.
Memory aids are any type of assistive technology that helps a user learn and remember certain information. Many memory aids are used for cognitive impairments such as reading, writing, or organizational difficulties. For example, a Smartpen records handwritten notes by creating both a digital copy and an audio recording of the text. Users simply tap certain parts of their notes and the pen saves it and reads it back to them. From there, the user can also download their notes onto a computer for increased accessibility. Digital voice recorders are also used to record "in the moment" information for fast and easy recall at a later time.
Educational software.
"Main article: Educational software"
Educational software is software that assists people with reading, learning, comprehension, and organizational difficulties. Any accommodation software such as text readers, notetakers, text enlargers, organization tools, word predictions, and talking word processors falls under the category of educational software.
Assistive technology in sport.
Assistive technology in sport is an area of technology design that is growing. Assistive technology is the array of new devices created to enable sports enthusiasts who have disabilities to play. Assistive technology may be used in adaptive sports, where an existing sport is modified to enable players with a disability to participate; or, assistive technology may be used to invent completely new sports with athletes with disabilities exclusively in mind.
An increasing number of people with disabilities are participating in sports, leading to the development of new assistive technology. Assistive technology devices can be simple, or "low-tech", or they may use highly advanced technology, with some even using computers. Assistive technology for sports may also be simple, or advanced. Accordingly, assistive technology can be found in sports ranging from local community recreation to the elite Paralympic Games. More complex assistive technology devices have been developed over time, and as a result, sports for people with disabilities "have changed from being a clinical therapeutic tool to an increasingly competition-oriented activity".
Assistive technology in education.
In the United States there are two major pieces of legislation that govern the use of assistive technology within the school system. The first is Section 504 of the Rehabilitation Act of 1973 and the second being the Individuals with Disabilities Education Act (IDEA) which was first enacted in 1975 under the name The Education for All Handicapped Children Act. In 2004, during the reauthorization period for IDEA, the National Instructional Material Access Center (NIMAC) was created which provided a repository of accessible text including publisher's textbooks to students with a qualifying disability. Files provided are in XML format and used as a starting platform for braille readers, screen readers, and other digital text software. IDEA defines assistive technology as follows: "any item, piece of equipment, or product system, whether acquired commercially off the shelf, modified, or customized, that is used to increase, maintain, or improve functional capabilities of a child with a disability. (B) Exception.--The term does not include a medical device that is surgically implanted, or the replacement of such device." 
Assistive technology in this area is broken down into low, mid, and high tech categories. Low tech encompasses equipment that is often low cost and does not include batteries or requires charging. Examples include adapted paper and pencil grips for writing or masks and color overlays for reading. Mid tech supports used in the school setting include the use of handheld spelling dictionaries and portable word processors used to keyboard writing. High tech supports involve the use of tablet devices and computers with accompanying software. Software supports for writing include the use of auditory feedback while keyboarding, word prediction for spelling, and speech to text. Supports for reading include the use of text to speech (TTS) software and font modification via access to digital text. Limited supports are available for math instruction and mostly consist of grid based software to allow younger students to keyboard equations and auditory feedback of more complex equations using MathML and Daisy.
Computer accessibility.
One of the largest problems that affect people with disabilities is discomfort with prostheses. An experiment performed in Massachusetts utilized 20 people with various sensors attached to their arms. The subjects tried different arm exercises, and the sensors recorded their movements. All of the data helped engineers develop new engineering concepts for prosthetics.
Assistive technology may attempt to improve the ergonomics of the devices themselves such as Dvorak and other alternative keyboard layouts, which offer more ergonomic layouts of the keys.
Assistive technology devices have been created to enable people with disabilities to use modern touch screen mobile computers such as the iPad, iPhone and iPod touch. The Pererro is a plug and play adapter for iOS devices which uses the built in Apple VoiceOver feature in combination with a basic switch. This brings touch screen technology to those who were previously unable to use it. Apple, with the release of iOS 7 had introduced the ability to navigate apps using switch control. Switch access could be activated either through an external bluetooth connected switch, single touch of the screen, or use of right and left head turns using the device's camera. Additional accessibility features include the use of Assistive Touch which allows a user to access multi-touch gestures through pre-programmed onscreen buttons.
For users with physical disabilities a large variety of switches are available and customizable to the user's needs varying in size, shape, or amount of pressure required for activation. Switch access may be placed near any area of the body which has consistent and reliable mobility and less subject to fatigue. Common sites include the hands, head, and feet. Eye gaze and head mouse systems can also be used as an alternative mouse navigation. A user may utilize single or multiple switch sites and the process often involves a scanning through items on a screen and activating the switch once the desired object is highlighted.
Home automation.
The form of home automation called assistive domotics focuses on making it possible for elderly and disabled people to live independently. Home automation is becoming a viable option for the elderly and disabled who would prefer to stay in their own homes rather than move to a healthcare facility. This field uses much of the same technology and equipment as home automation for security, entertainment, and energy conservation but tailors it towards elderly and disabled users. For example, automated prompts and reminders utilize motion sensors and pre-recorded audio messages; an automated prompt in the kitchen may remind the resident to turn off the oven, and one by the front door may remind the resident to lock the door.
Impacts of assistive technology.
Overall, assistive technology aims to allow people with disabilities to "participate more fully in all aspects of life (home, school, and community)" and increases their opportunities for "education, social interactions, and potential for meaningful employment." It creates greater independence and control for disabled individuals. For example, in one study of 1,342 infants, toddlers and preschoolers, all with some kind of developmental, physical, sensory, or cognitive disability, the use of assistive technology created improvements in child development. These included improvements in "cognitive, social, communication, literacy, motor, adaptive, and increases in engagement in learning activities."

</doc>
<doc id="655" url="https://en.wikipedia.org/wiki?curid=655" title="Abacus">
Abacus

The abacus ("plural" abaci or abacuses), also called a counting frame, is a calculating tool that was in use centuries before the adoption of the written modern numeral system and is still widely used by merchants, traders and clerks in Asia, Africa, and elsewhere. Today, abaci are often constructed as a bamboo frame with beads sliding on wires, but originally they were beans or stones moved in grooves in sand or on tablets of wood, stone, or metal.
Etymology.
The use of the word "abacus" dates before 1387 AD, when a Middle English work borrowed the word from Latin to describe a sandboard abacus. The Latin word came from Greek ἄβαξ "abax" which means something without base, and improperly, any piece of rectangular board or plank. 
Alternatively, without reference to ancient texts on etymology, it has been suggested that it means "a square tablet strewn with dust", or "drawing-board covered with dust (for the use of mathematics)" (the exact shape of the Latin perhaps reflects the genitive form of the Greek word, ἄβακoς "abakos"). Whereas the table strewn with dust definition is popular, there are those that do not place credence in this at all and in fact state that it is not proven. Greek ἄβαξ itself is probably a borrowing of a Northwest Semitic, perhaps Phoenician, word akin to Hebrew "ʾābāq" (אבק), "dust" (or in post-Biblical sense meaning "sand used as a writing surface").
The preferred plural of "abacus" is a subject of disagreement, with both "abacuses" and "abaci" in use. The user of an abacus is called an "abacist".
History.
Mesopotamian.
The period 2700–2300 BC saw the first appearance of the Sumerian abacus, a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system.
Some scholars point to a character from the Babylonian cuneiform which may have been derived from a representation of the abacus. It is the belief of Old Babylonian scholars such as Carruccio that Old Babylonians "may have used the abacus for the operations of addition and subtraction; however, this primitive device proved difficult to use for more complex calculations".
Egyptian.
The use of the abacus in Ancient Egypt is mentioned by the Greek historian Herodotus, who writes that the Egyptians manipulated the pebbles from right to left, opposite in direction to the Greek left-to-right method. Archaeologists have found ancient disks of various sizes that are thought to have been used as counters. However, wall depictions of this instrument have not been discovered.
Persian.
During the Achaemenid Empire, around 600 BC the Persians first began to use the abacus. Under the Parthian, Sassanian and Iranian empires, scholars concentrated on exchanging knowledge and inventions with the countries around them – India, China, and the Roman Empire, when it is thought to have been exported to other countries.
Greek.
The earliest archaeological evidence for the use of the Greek abacus dates to the 5th century BC. Also Demosthenes (384 BC–322 BC) talked of the need to use pebbles for calculations too difficult for your head. A play by Alexis from the 4th century BC mentions an abacus and pebbles for accounting, and both Diogenes and Polybius mention men that sometimes stood for more and sometimes for less, like the pebbles on an abacus. The Greek abacus was a table of wood or marble, pre-set with small counters in wood or metal for mathematical calculations. This Greek abacus saw use in Achaemenid Persia, the Etruscan civilization, Ancient Rome and, until the French Revolution, the Western Christian world.
A tablet found on the Greek island Salamis in 1846 AD (the Salamis Tablet), dates back to 300 BC, making it the oldest counting board discovered so far. It is a slab of white marble long, wide, and thick, on which are 5 groups of markings. In the center of the tablet is a set of 5 parallel lines equally divided by a vertical line, capped with a semicircle at the intersection of the bottom-most horizontal line and the single vertical line. Below these lines is a wide space with a horizontal crack dividing it. Below this crack is another group of eleven parallel lines, again divided into two sections by a line perpendicular to them, but with the semicircle at the top of the intersection; the third, sixth and ninth of these lines are marked with a cross where they intersect with the vertical line. Also from this time frame the "Darius Vase" was unearthed in 1851. It was covered with pictures including a "treasurer" holding a wax tablet in one hand while manipulating counters on a table with the other.
Chinese.
The earliest known written documentation of the Chinese abacus dates to the 2nd century BC.
The Chinese abacus, known as the suanpan (, lit. "Counting tray", Mandarin "suàn pán", Cantonese "syun pun"), is typically tall and comes in various widths depending on the operator. It usually has more than seven rods. There are two beads on each rod in the upper deck and five beads each in the bottom for both decimal and hexadecimal computation. The beads are usually rounded and made of a hardwood. The beads are counted by moving them up or down towards the beam. If you move them toward the beam, you count their value. If you move away, you don't count their value. The suanpan can be reset to the starting position instantly by a quick movement along the horizontal axis to spin all the beads away from the horizontal beam at the center.
Suanpans can be used for functions other than counting. Unlike the simple counting board used in elementary schools, very efficient suanpan techniques have been developed to do multiplication, division, addition, subtraction, square root and cube root operations at high speed. There are currently schools teaching students how to use it.
In the long scroll "Along the River During the Qingming Festival" painted by Zhang Zeduan (1085–1145 AD) during the Song dynasty (960–1297 AD), a suanpan is clearly seen lying beside an account book and doctor's prescriptions on the counter of an apothecary's (Feibao).
The similarity of the Roman abacus to the Chinese one suggests that one could have inspired the other, as there is some evidence of a trade relationship between the Roman Empire and China. However, no direct connection can be demonstrated, and the similarity of the abaci may be coincidental, both ultimately arising from counting with five fingers per hand. Where the Roman model (like most modern Korean and Japanese) has 4 plus 1 bead per decimal place, the standard suanpan has 5 plus 2. (Incidentally, this allows use with a hexadecimal numeral system.) Instead of running on wires as in the Chinese, Korean, and Japanese models, the beads of Roman model run in grooves, presumably making arithmetic calculations much slower.
Another possible source of the suanpan is Chinese counting rods, which operated with a decimal system but lacked the concept of zero as a place holder. The zero was probably introduced to the Chinese in the Tang dynasty (618-907 AD) when travel in the Indian Ocean and the Middle East would have provided direct contact with India, allowing them to acquire the concept of zero and the decimal point from Indian merchants and mathematicians.
Roman.
The normal method of calculation in ancient Rome, as in Greece, was by moving counters on a smooth table. Originally pebbles ("calculi") were used. Later, and in medieval Europe, jetons were manufactured. Marked lines indicated units, fives, tens etc. as in the Roman numeral system. This system of 'counter casting' continued into the late Roman empire and in medieval Europe, and persisted in limited use into the nineteenth century. Due to Pope Sylvester II's reintroduction of the abacus with very useful modifications, it became widely used in Europe once again during the 11th century This abacus used beads on wires, unlike the traditional Roman counting boards, which meant the abacus could be used much faster.
Writing in the 1st century BC, Horace refers to the wax abacus, a board covered with a thin layer of black wax on which columns and figures were inscribed using a stylus.
One example of archaeological evidence of the Roman abacus, shown here in reconstruction, dates to the 1st century AD. It has eight long grooves containing up to five beads in each and eight shorter grooves having either one or no beads in each. The groove marked I indicates units, X tens, and so on up to millions. The beads in the shorter grooves denote fives –five units, five tens etc., essentially in a bi-quinary coded decimal system, obviously related to the Roman numerals. The short grooves on the right may have been used for marking Roman "ounces" (i.e. fractions).
Indian.
The "Abhidharmakośabhāṣya" of Vasubandhu (316-396), a Sanskrit work on Buddhist philosophy, says that the second-century CE philosopher Vasumitra said that, "placing a wick (Sanskrit "vartikā") on the number one ("ekāṅka") means it is a one, while placing the wick on the number hundred means it is called a hundred, and on the number one thousand means it is called a thousand". It is unclear exactly what this arrangement may have been, but it could refer to tokens being cast into counting-pits, or placed on numbered squares. Perhaps it was a type of abacus.
Around the 5th century, Indian clerks were already finding new ways of recording the contents of the Abacus. Hindu texts used the term "śūnya" (zero) to indicate the empty column on the abacus.
Japanese.
In Japanese, the abacus is called "soroban" (, lit. "Counting tray"), imported from China in the 14th century. It was probably in use by the working class a century or more before the ruling class started, as the class structure did not allow for devices used by the lower class to be adopted or used by the ruling class. The 1/4 abacus, which is suited to decimal calculation, appeared circa 1930, and became widespread as the Japanese abandoned hexadecimal weight calculation which was still common in China. The abacus is still manufactured in Japan today even with the proliferation, practicality, and affordability of pocket electronic calculators. The use of the soroban is still taught in Japanese primary schools as part of mathematics, primarily as an aid to faster mental calculation. Using visual imagery of a soroban, one can arrive at the answer in the same time as, or even faster than, is possible with a physical instrument.
Korean.
The Chinese abacus migrated from China to Korea around 1400 AD. Koreans call it "jupan" (주판), "supan" (수판) or "jusan-means calculating with an abacus-" (주산).
Native American.
Some sources mention the use of an abacus called a "nepohualtzintzin" in ancient Aztec culture. This Mesoamerican abacus used a 5-digit base-20 system.
The word Nepōhualtzintzin comes from Nahuatl and it is formed by the roots; "Ne" - personal -; "pōhual" or "pōhualli" - the account -; and "tzintzin" - small similar elements. Its complete meaning was taken as: counting with small similar elements by somebody. Its use was taught in the Calmecac to the "temalpouhqueh" , who were students dedicated to take the accounts of skies, from childhood.
The Nepōhualtzintzin was divided in two main parts separated by a bar or intermediate cord. In the left part there were four beads, which in the first row have unitary values (1, 2, 3, and 4), and in the right side there are three beads with values of 5, 10, and 15 respectively. In order to know the value of the respective beads of the upper rows, it is enough to multiply by 20 (by each row), the value of the corresponding account in the first row.
Altogether, there were 13 rows with 7 beads in each one, which made up 91 beads in each Nepōhualtzintzin. This was a basic number to understand, 7 times 13, a close relation conceived between natural phenomena, the underworld and the cycles of the heavens. One Nepōhualtzintzin (91) represented the number of days that a season of the year lasts, two Nepōhualtzitzin (182) is the number of days of the corn's cycle, from its sowing to its harvest, three Nepōhualtzintzin (273) is the number of days of a baby's gestation, and four Nepōhualtzintzin (364) completed a cycle and approximate a year (1 days short). When translated into modern computer arithmetic, the Nepōhualtzintzin amounted to the rank from 10 to the 18 in floating point, which calculated stellar as well as infinitesimal amounts with absolute precision, meant that no round off was allowed.
The rediscovery of the Nepōhualtzintzin was due to the Mexican engineer David Esparza Hidalgo, who in his wanderings throughout Mexico found diverse engravings and paintings of this instrument and reconstructed several of them made in gold, jade, encrustations of shell, etc. There have also been found very old Nepōhualtzintzin attributed to the Olmec culture, and even some bracelets of Mayan origin, as well as a diversity of forms and materials in other cultures.
George I. Sanchez, "Arithmetic in Maya", Austin-Texas, 1961 found another base 5, base 4 abacus in the Yucatán peninsula that also computed calendar data. This was a finger abacus, on one hand 0, 1, 2, 3, and 4 were used; and on the other hand 0, 1, 2 and 3 were used. Note the use of zero at the beginning and end of the two cycles. Sanchez worked with Sylvanus Morley, a noted Mayanist.
The quipu of the Incas was a system of colored knotted cords used to record numerical data, like advanced tally sticks – but not used to perform calculations. Calculations were carried out using a yupana (Quechua for "counting tool"; see figure) which was still in use after the conquest of Peru. The working principle of a yupana is unknown, but in 2001 an explanation of the mathematical basis of these instruments was proposed by Italian mathematician Nicolino De Pasquale. By comparing the form of several yupanas, researchers found that calculations were based using the Fibonacci sequence 1, 1, 2, 3, 5 and powers of 10, 20 and 40 as place values for the different fields in the instrument. Using the Fibonacci sequence would keep the number of grains within any one field at a minimum.
Russian.
The Russian abacus, the "schoty" (счёты), usually has a single slanted deck, with ten beads on each wire (except one wire, usually positioned near the user, with four beads for quarter-ruble fractions). Older models have another 4-bead wire for quarter-kopeks, which were minted until 1916. The Russian abacus is often used vertically, with wires from left to right in the manner of a book. The wires are usually bowed to bulge upward in the center, to keep the beads pinned to either of the two sides. It is cleared when all the beads are moved to the right. During manipulation, beads are moved to the left. For easy viewing, the middle 2 beads on each wire (the 5th and 6th bead) usually are of a different colour from the other eight beads. Likewise, the left bead of the thousands wire (and the million wire, if present) may have a different color.
As a simple, cheap and reliable device, the Russian abacus was in use in all shops and markets throughout the former Soviet Union, and the usage of it was taught in most schools until the 1990s. Even the 1874 invention of mechanical calculator, Odhner arithmometer, had not replaced them in Russia and likewise the mass production of Felix arithmometers since 1924 did not significantly reduce their use in the Soviet Union. The Russian abacus began to lose popularity only after the mass production of microcalculators had started in the Soviet Union in 1974. Today it is regarded as an archaism and replaced by the handheld calculator.
The Russian abacus was brought to France around 1820 by the mathematician Jean-Victor Poncelet, who served in Napoleon's army and had been a prisoner of war in Russia. The abacus had fallen out of use in western Europe in the 16th century with the rise of decimal notation and algorismic methods. To Poncelet's French contemporaries, it was something new. Poncelet used it, not for any applied purpose, but as a teaching and demonstration aid. The Turks and the Armenian people also used abaci similar to the Russian schoty. It was named a "coulba" by the Turks and a "choreb" by the Armenians.
School abacus.
Around the world, abaci have been used in pre-schools and elementary schools as an aid in teaching the numeral system and arithmetic.
In Western countries, a bead frame similar to the Russian abacus but with straight wires and a vertical frame has been common (see image). It is still often seen as a plastic or wooden toy.
The wire frame may be used either with positional notation like other abaci (thus the 10-wire version may represent numbers up to 9,999,999,999), or each bead may represent one unit (so that e.g. 74 can be represented by shifting all beads on 7 wires and 4 beads on the 8th wire, so numbers up to 100 may be represented). In the bead frame shown, the gap between the 5th and 6th wire, corresponding to the color change between the 5th and the 6th bead on each wire, suggests the latter use.
The red-and-white abacus is used in contemporary primary schools for a wide range of number-related lessons. The twenty bead version, referred to by its Dutch name "rekenrek", is often used, sometimes on a string of beads, sometimes on a rigid framework.
Uses by the blind.
An adapted abacus, invented by Tim Cranmer, called a Cranmer abacus is still commonly used by individuals who are blind. A piece of soft fabric or rubber is placed behind the beads so that they do not move inadvertently. This keeps the beads in place while the users feel or manipulate them. They use an abacus to perform the mathematical functions multiplication, division, addition, subtraction, square root and cube root.
Although blind students have benefited from talking calculators, the abacus is still very often taught to these students in early grades, both in public schools and state schools for the blind. The abacus teaches mathematical skills that can never be replaced with talking calculators and is an important learning tool for blind students. Blind students also complete mathematical assignments using a braille-writer and Nemeth code (a type of braille code for mathematics) but large multiplication and long division problems can be long and difficult. The abacus gives blind and visually impaired students a tool to compute mathematical problems that equals the speed and mathematical knowledge required by their sighted peers using pencil and paper. Many blind people find this number machine a very useful tool throughout life.
Binary abacus.
The binary abacus is used to explain how computers manipulate numbers. The abacus shows how numbers, letters, and signs can be stored in a binary system on a computer, or via ASCII. The device consists of a series of beads on parallel wires arranged in three separate rows. The beads represent a switch on the computer in either an 'on' or 'off' position.

</doc>
<doc id="656" url="https://en.wikipedia.org/wiki?curid=656" title="Acid">
Acid

An acid (from the Latin "acidus/acēre" meaning "sour") is a chemical substance whose aqueous solutions are characterized by a sour taste, the ability to turn blue litmus red, and the ability to react with bases and certain metals (like calcium) to form salts. An aqueous solution of an acid has a pH of less than 7 and is colloquially also referred to as 'acid' (as in 'dissolved in acid'), while the strict definition refers only to the solute. An acid usually contains a hydrogen atom bonded to a chemical structure that is still energetically favorable after loss of H (a positive hydrogen ion or proton). A lower pH means a higher acidity, and thus a higher concentration of positive hydrogen ions in the solution. Chemicals or substances having the property of an acid are said to be acidic.
There are three common definitions for acids: the Arrhenius definition, the Brønsted-Lowry definition, and the Lewis definition. The Arrhenius definition defines acids as substances which increase the concentration of hydrogen ions (H), or more accurately, hydronium ions (HO), when dissolved in water. The Brønsted-Lowry definition is an expansion to include solvents other than water: an acid is a substance which can act as a proton donor. By this definition, any compound which can be deprotonated can be considered an acid. Examples include alcohols and amines which contain O-H or N-H fragments. A Lewis acid is a substance that can accept a pair of electrons to form a covalent bond. Examples of Lewis acids include all metal cations, and electron-deficient molecules such as boron trifluoride and aluminium trichloride.
Common examples of acids include hydrochloric acid (a solution of hydrogen chloride which is found in gastric acid in the stomach and activates digestive enzymes), acetic acid (vinegar is a dilute solution of this liquid), sulfuric acid (used in car batteries), and tartaric acid (a solid used in baking). As these examples show, acids can be solutions or pure substances, and can be derived from solids, liquids, or gases. Strong acids and some concentrated weak acids are corrosive, but there are exceptions such as carboranes and boric acid.
Definitions and concepts.
Modern definitions are concerned with the fundamental chemical reactions common to all acids.
Most acids encountered in everyday life are aqueous solutions, or can be dissolved in water, so the Arrhenius and Brønsted-Lowry definitions are the most relevant.
The Brønsted-Lowry definition is the most widely used definition; unless otherwise specified, acid-base reactions are assumed to involve the transfer of a proton (H) from an acid to a base.
Hydronium ions are acids according to all three definitions. Interestingly, although alcohols and amines can be Brønsted-Lowry acids, they can also function as Lewis bases due to the lone pairs of electrons on their oxygen and nitrogen atoms.
Arrhenius acids.
The Swedish chemist Svante Arrhenius attributed the properties of acidity to hydrogen ions (H) or protons in 1884. An Arrhenius acid is a substance that, when added to water, increases the concentration of H ions in the water. Note that chemists often write H("aq") and refer to the hydrogen ion when describing acid-base reactions but the free hydrogen nucleus, a proton, does not exist alone in water, it exists as the hydronium ion, HO. Thus, an Arrhenius acid can also be described as a substance that increases the concentration of hydronium ions when added to water. This definition stems from the equilibrium dissociation of water into hydronium and hydroxide (OH) ions:
In pure water the majority of molecules are HO, but the molecules are constantly dissociating and re-associating, and at any time a small number of the molecules are hydronium and an equal number are hydroxide. Because the numbers are equal, pure water is neutral (not acidic or basic).
An Arrhenius base, on the other hand, is a substance which increases the concentration of hydroxide ions when dissolved in water, hence decreasing the concentration of hydronium.
The constant association and disassociation of HO molecules forms an equilibrium in which any increase in the concentration of hydronium is accompanied by a decrease in the concentration of hydroxide, thus an Arrhenius acid could also be said to be one that decreases hydroxide concentration, with an Arrhenius base increasing it.
In an acidic solution, the concentration of hydronium ions is greater than 10 moles per liter. Since pH is defined as the negative logarithm of the concentration of hydronium ions, acidic solutions thus have a pH of less than 7.
Brønsted-Lowry acids.
While the Arrhenius concept is useful for describing many reactions, it is also quite limited in its scope. In 1923 chemists Johannes Nicolaus Brønsted and Thomas Martin Lowry independently recognized that acid-base reactions involve the transfer of a proton. A Brønsted-Lowry acid (or simply Brønsted acid) is a species that donates a proton to a Brønsted-Lowry base. Brønsted-Lowry acid-base theory has several advantages over Arrhenius theory. Consider the following reactions of acetic acid (CHCOOH), the organic acid that gives vinegar its characteristic taste:
Both theories easily describe the first reaction: CHCOOH acts as an Arrhenius acid because it acts as a source of HO when dissolved in water, and it acts as a Brønsted acid by donating a proton to water. In the second example CHCOOH undergoes the same transformation, in this case donating a proton to ammonia (NH), but cannot be described using the Arrhenius definition of an acid because the reaction does not produce hydronium.
Brønsted-Lowry theory can also be used to describe molecular compounds, whereas Arrhenius acids must be ionic compounds. Hydrogen chloride (HCl) and ammonia combine under several different conditions to form ammonium chloride, NHCl. In aqueous solution HCl behaves as hydrochloric acid and exists as hydronium and chloride ions. The following reactions illustrate the limitations of Arrhenius's definition:
As with the acetic acid reactions, both definitions work for the first example, where water is the solvent and hydronium ion is formed by the HCl solute. The next two reactions do not involve the formation of ions but are still proton transfer reactions. In the second reaction hydrogen chloride and ammonia (dissolved in benzene) react to form solid ammonium chloride in a benzene solvent and in the third gaseous HCl and NH combine to form the solid.
Lewis acids.
A third concept was proposed in 1923 by Gilbert N. Lewis which includes reactions with acid-base characteristics that do not involve a proton transfer. A Lewis acid is a species that accepts a pair of electrons from another species; in other words, it is an electron pair acceptor. Brønsted acid-base reactions are proton transfer reactions while Lewis acid-base reactions are electron pair transfers. All Brønsted acids are also Lewis acids, but not all Lewis acids are Brønsted acids. Contrast how the following reactions are described in terms of acid-base chemistry.
In the first reaction a fluoride ion, F, gives up an electron pair to boron trifluoride to form the product tetrafluoroborate. Fluoride "loses" a pair of valence electrons because the electrons shared in the B—F bond are located in the region of space between the two atomic nuclei and are therefore more distant from the fluoride nucleus than they are in the lone fluoride ion. BF is a Lewis acid because it accepts the electron pair from fluoride. This reaction cannot be described in terms of Brønsted theory because there is no proton transfer. The second reaction can be described using either theory. A proton is transferred from an unspecified Brønsted acid to ammonia, a Brønsted base; alternatively, ammonia acts as a Lewis base and transfers a lone pair of electrons to form a bond with a hydrogen ion. The species that gains the electron pair is the Lewis acid; for example, the oxygen atom in HO gains a pair of electrons when one of the H—O bonds is broken and the electrons shared in the bond become localized on oxygen. Depending on the context, a Lewis acid may also be described as an oxidizer or an electrophile. Organic acids, such as acetic, citric, or oxalic acid, are not Lewis acids.
Dissociation and equilibrium.
Reactions of acids are often generalized in the form HA H + A, where HA represents the acid and A is the conjugate base. This reaction is the protolysis. Acid-base conjugate pairs differ by one proton, and can be interconverted by the addition or removal of a proton (protonation and deprotonation, respectively). Note that the acid can be the charged species and the conjugate base can be neutral in which case the generalized reaction scheme could be written as HA H + A. In solution there exists an equilibrium between the acid and its conjugate base. The equilibrium constant "K" is an expression of the equilibrium concentrations of the molecules or the ions in solution. Brackets indicate concentration, such that means "the concentration of HO". The acid dissociation constant "K" is generally used in the context of acid-base reactions. The numerical value of "K" is equal to the product of the concentrations of the products divided by the concentration of the reactants, where the reactant is the acid (HA) and the products are the conjugate base and H.
The stronger of two acids will have a higher "K" than the weaker acid; the ratio of hydrogen ions to acid will be higher for the stronger acid as the stronger acid has a greater tendency to lose its proton. Because the range of possible values for "K" spans many orders of magnitude, a more manageable constant, p"K" is more frequently used, where p"K" = -log "K". Stronger acids have a smaller p"K" than weaker acids. Experimentally determined p"K" at 25 °C in aqueous solution are often quoted in textbooks and reference material.
Nomenclature.
In the classical naming system, acids are named according to their anions. That ionic suffix is dropped and replaced with a new suffix (and sometimes prefix), according to the table below.
For example, HCl has chloride as its anion, so the -ide suffix makes it take the form hydrochloric acid. In the IUPAC naming system, "aqueous" is simply added to the name of the ionic compound. Thus, for hydrogen chloride, the IUPAC name would be aqueous hydrogen chloride. The prefix "hydro-" is added only if the acid is made up of just hydrogen and one other element.
Classical naming system:
Acid strength.
The strength of an acid refers to its ability or tendency to lose a proton. A strong acid is one that completely dissociates in water; in other words, one mole of a strong acid HA dissolves in water yielding one mole of H and one mole of the conjugate base, A, and none of the protonated acid HA. In contrast, a weak acid only partially dissociates and at equilibrium both the acid and the conjugate base are in solution. Examples of strong acids are hydrochloric acid (HCl), hydroiodic acid (HI), hydrobromic acid (HBr), perchloric acid (HClO), nitric acid (HNO) and sulfuric acid (HSO). In water each of these essentially ionizes 100%. The stronger an acid is, the more easily it loses a proton, H. Two key factors that contribute to the ease of deprotonation are the polarity of the H—A bond and the size of atom A, which determines the strength of the H—A bond. Acid strengths are also often discussed in terms of the stability of the conjugate base.
Stronger acids have a larger "K" and a more negative p"K" than weaker acids.
Sulfonic acids, which are organic oxyacids, are a class of strong acids. A common example is toluenesulfonic acid (tosylic acid). Unlike sulfuric acid itself, sulfonic acids can be solids. In fact, polystyrene functionalized into polystyrene sulfonate is a solid strongly acidic plastic that is filterable.
Superacids are acids stronger than 100% sulfuric acid. Examples of superacids are fluoroantimonic acid, magic acid and perchloric acid. Superacids can permanently protonate water to give ionic, crystalline hydronium "salts". They can also quantitatively stabilize carbocations.
While "K" measures the strength of an acid compound, the strength of an aqueous acid solution is measured by pH, which is an indication of the concentration of hydronium in the solution. The pH of a simple solution of an acid compound in water is determined by the dilution of the compound and the compound's "K".
Chemical characteristics.
Monoprotic acids.
Monoprotic acids are those acids that are able to donate one proton per molecule during the process of dissociation (sometimes called ionization) as shown below (symbolized by HA):
Common examples of monoprotic acids in mineral acids include hydrochloric acid (HCl) and nitric acid (HNO). On the other hand, for organic acids the term mainly indicates the presence of one carboxylic acid group and sometimes these acids are known as monocarboxylic acid. Examples in organic acids include formic acid (HCOOH), acetic acid (CHCOOH) and benzoic acid (CHCOOH).
Polyprotic acids.
Polyprotic acids, also known as polybasic acids, are able to donate more than one proton per acid molecule, in contrast to monoprotic acids that only donate one proton per molecule. Specific types of polyprotic acids have more specific names, such as diprotic acid (two potential protons to donate) and triprotic acid (three potential protons to donate).
A diprotic acid (here symbolized by HA) can undergo one or two dissociations depending on the pH. Each dissociation has its own dissociation constant, K and K.
The first dissociation constant is typically greater than the second; i.e., "K" > "K". For example, sulfuric acid (HSO) can donate one proton to form the bisulfate anion (HSO), for which "K" is very large; then it can donate a second proton to form the sulfate anion (SO), wherein the "K" is intermediate strength. The large "K" for the first dissociation makes sulfuric a strong acid. In a similar manner, the weak unstable carbonic acid (HCO) can lose one proton to form bicarbonate anion (HCO) and lose a second to form carbonate anion (CO). Both "K" values are small, but "K" > "K" .
A triprotic acid (HA) can undergo one, two, or three dissociations and has three dissociation constants, where "K" > "K" > "K".
An inorganic example of a triprotic acid is orthophosphoric acid (HPO), usually just called phosphoric acid. All three protons can be successively lost to yield HPO, then HPO, and finally PO, the orthophosphate ion, usually just called phosphate. Even though the positions of the three protons on the original phosphoric acid molecule are equivalent, the successive "K" values differ since it is energetically less favorable to lose a proton if the conjugate base is more negatively charged. An organic example of a triprotic acid is citric acid, which can successively lose three protons to finally form the citrate ion.
Although the subsequent loss of each hydrogen ion is less favorable, all of the conjugate bases are present in solution. The fractional concentration, "α" (alpha), for each species can be calculated. For example, a generic diprotic acid will generate 3 species in solution: HA, HA, and A. The fractional concentrations can be calculated as below when given either the pH (which can be converted to the 
It is the decrease in pH that signals the brain to breathe faster and deeper, expelling the excess CO and resupplying the cells with O.
Common acids.
Sulfonic acids.
A sulfonic acid has the general formula RS(=O)–OH, where R is an organic radical.
Carboxylic acids.
A carboxylic acid has the general formula R-C(O)OH, where R is an organic radical. The carboxyl group -C(O)OH contains a carbonyl group, C=O, and a hydroxyl group, O-H.
Halogenated carboxylic acids.
Halogenation at alpha position increases acid strength, so that the following acids are all stronger than acetic acid.
Vinylogous carboxylic acids.
Normal carboxylic acids are the direct union of a carbonyl group and a hydroxyl group. In vinylogous carboxylic acids, a carbon-carbon double bond separates the carbonyl and hydroxyl groups.
{{Authority control}}

</doc>
<doc id="657" url="https://en.wikipedia.org/wiki?curid=657" title="Asphalt">
Asphalt

Asphalt ( or , occasionally ), also known as bitumen (, ) is a sticky, black and highly viscous liquid or semi-solid form of petroleum. It may be found in natural deposits or may be a refined product; it is a substance classed as a pitch. Until the 20th century, the term asphaltum was also used. The word is derived from the Ancient Greek ἄσφαλτος "ásphaltos".
The primary use (70%) of asphalt/bitumen is in road construction, where it is used as the glue or binder mixed with aggregate particles to create asphalt concrete. Its other main uses are for bituminous waterproofing products, including production of roofing felt and for sealing flat roofs.
The terms "asphalt" and "bitumen" are often used interchangeably to mean both natural and manufactured forms of the substance. In American English, asphalt (or asphalt cement) is the carefully refined residue from the distillation process of selected crude oils. Outside the United States, the product is often called bitumen. Geologists often prefer the term "bitumen". Common usage often refers to various forms of asphalt/bitumen as "tar", such as at the La Brea Tar Pits. Another archaic term for asphalt/bitumen is "pitch".
Naturally occurring asphalt/bitumen is sometimes specified by the term "crude bitumen". Its viscosity is similar to that of cold molasses while the material obtained from the fractional distillation of crude oil boiling at is sometimes referred to as "refined bitumen". The Canadian province of Alberta has most of the world's reserves of natural bitumen, covering , an area larger than England.
Composition.
The components of asphalt are classified into four classes of compounds: 
The naphthene aromatics and polar aromatics are typically the majority components. Additionally, most natural bitumens contain organosulfur compounds, resulting in an overall sulfur content of up to 4%. Nickel and vanadium are found in the <10 ppm level, as is typical of some petroleum.
The substance is soluble in carbon disulfide. It is commonly modelled as a colloid, with asphaltenes as the dispersed phase and maltenes as the continuous phase. and "it is almost impossible to separate and identify all the different molecules of asphalt, because the number of molecules with different chemical structure is extremely large".
Asphalt/bitumen can sometimes be confused with "coal tar", which is a visually similar black, thermoplastic material produced by the destructive distillation of coal. During the early and mid-20th century when town gas was produced, coal tar was a readily available byproduct and extensively used as the binder for road aggregates. The addition of tar to macadam roads led to the word tarmac, which is now used in common parlance to refer to road-making materials. However, since the 1970s, when natural gas succeeded town gas, asphalt/bitumen has completely overtaken the use of coal tar in these applications. Other examples of this confusion include the La Brea Tar Pits and the Canadian oil sands, both of which actually contain natural bitumen rather than tar. Pitch is another term sometimes used at times to refer to asphalt/bitumen, as in Pitch Lake.
Occurrence.
The great majority of asphalt used commercially is obtained from petroleum. Nonetheless, large amounts of asphalt occur in concentrated form in nature. Naturally occurring deposits of asphalt/bitumen are formed from the remains of ancient, microscopic algae (diatoms) and other once-living things. These remains were deposited in the mud on the bottom of the ocean or lake where the organisms lived. Under the heat (above 50 °C) and pressure of burial deep in the earth, the remains were transformed into materials such as asphalt/bitumen, kerogen, or petroleum.
Natural deposits of asphalt/bitumen include lakes such as the Pitch Lake in Trinidad and Tobago and Lake Bermudez in Venezuela. Natural seeps of asphalt/bitumen occur in the La Brea Tar Pits and in the Dead Sea.
Asphalt/bitumen also occurs in unconsolidated sandstones known as "oil sands" in Alberta, Canada, and the similar "tar sands" in Utah, US.
The Canadian province of Alberta has most of the world's reserves of natural bitumen, in three huge deposits covering , an area larger than England or New York state. These bituminous sands contain of commercially established oil reserves, giving Canada the third largest oil reserves in the world. and produce over of heavy crude oil and synthetic crude oil. Although historically it was used without refining to pave roads, nearly all of the bitumen is now used as raw material for oil refineries in Canada and the United States.
The world's largest deposit of natural bitumen, known as the Athabasca oil sands is located in the McMurray Formation of Northern Alberta. This formation is from the early Cretaceous, and is composed of numerous lenses of oil-bearing sand with up to 20% oil. Isotopic studies attribute the oil deposits to be about 110 million years old. Two smaller but still very large formations occur in the Peace River oil sands and the Cold Lake oil sands, to the west and southeast of the Athabasca oil sands, respectively. Of the Alberta bitumen deposits, only parts of the Athabasca oil sands are shallow enough to be suitable for surface mining. The other 80% has to be produced by oil wells using enhanced oil recovery techniques like steam-assisted gravity drainage.
Much smaller heavy oil or bitumen deposits also occur in the Uinta Basin in Utah, US. The Tar Sand Triangle deposit, for example, is roughly 6% bitumen.
Asphalt/bitumen occurs in hydrothermal veins. An example of this is within the Uinta Basin of Utah, in the US, where there is a swarm of laterally and vertically extensive veins composed of a solid hydrocarbon termed Gilsonite. These veins formed by the polymerization and solidification of hydrocarbons that were mobilized from the deeper oil shales of the Green River Formation during burial and diagenesis.
Asphalt/bitumen is similar to the organic matter in carbonaceous meteorites. However, detailed studies have shown these materials to be distinct. The vast Alberta bitumen resources are believed to have started out as living material from marine plants and animals, mainly algae, that died millions of years ago when an ancient ocean covered Alberta. They were covered by mud, buried deeply over the eons, and gently cooked into oil by geothermal heat at a temperature of . Due to pressure from the rising of the Rocky Mountains in southwestern Alberta, 80 to 55 million years ago, the oil was driven northeast hundreds of kilometres into underground sand deposits left behind by ancient river beds and ocean beaches, thus forming the oil sands.
History.
Ancient times.
The use of asphalt/bitumen for waterproofing and as an adhesive dates at least to the fifth millennium BC in the early Indus valley sites like Mehrgarh, where it was used to line the baskets in which crops were gathered.
In the ancient Middle East, the Sumerians used natural asphalt/bitumen deposits for mortar between bricks and stones, to cement parts of carvings, such as eyes, into place, for ship caulking, and for waterproofing. The Greek historian Herodotus said hot asphalt/bitumen was used as mortar in the walls of Babylon.
A tunnel beneath the river Euphrates at Babylon in the time of Queen Semiramis (ca. 800 BC) was reportedly constructed of burnt bricks covered with asphalt/bitumen as a waterproofing agent.
Asphalt/bitumen was used by ancient Egyptians to embalm mummies. The Persian word for asphalt is "moom", which is related to the English word mummy. The Egyptians' primary source of asphalt/bitumen was the Dead Sea, which the Romans knew as "Palus Asphaltites" (Asphalt Lake).
Approximately 40 AD, Dioscorides described the Dead Sea material as "Judaicum bitumen", and noted other places in the region where it could be found.
The Sidon bitumen is thought to refer to asphalt/bitumen found at Hasbeya. Pliny refers also to asphalt/bitumen being found in Epirus. It was a valuable strategic resource; the object of the first known battle for a hydrocarbon deposit, between the Seleucids and the Nabateans in 312 BC.
In the ancient Far East, natural asphalt/bitumen was slowly boiled to get rid of the higher fractions, leaving a thermoplastic material of higher molecular weight which when layered on objects became quite hard upon cooling. This was used to cover objects that needed waterproofing, such as scabbards and other items. Statuettes of household deities were also cast with this type of material in Japan, and probably also in China.
In North America, archaeological recovery has indicated asphalt/bitumen was sometimes used to adhere stone projectile points to wooden shafts. In Canada, aboriginal people used bitumen seeping out of the banks of the Athabasca and other rivers to waterproof birch bark canoes, and also heated it in smudge pots to ward off mosquitoes in the summer time.
Early use in Europe.
One hundred years after the fall of Constantinople in 1453, Pierre Belon described in his work "Observations" in 1553 that "pissasphalto", a mixture of pitch and bitumen, was used in Dubrovnik for tarring of ships from where it was exported to a market place in Venice where it could be bought by anyone.
An 1838 edition of "Mechanics Magazine" cites an early use of asphalt in France. A pamphlet dated 1621, by "a certain Monsieur d'Eyrinys, states that he had discovered the existence (of asphaltum) in large quantities in the vicinity of Neufchatel", and that he proposed to use it in a variety of ways – "principally in the construction of air-proof granaries, and in protecting, by means of the arches, the water-courses in the city of Paris from the intrusion of dirt and filth", which at that time made the water unusable. "He expatiates also on the excellence of this material for forming level and durable terraces" in palaces, "the notion of forming such terraces in the streets not one likely to cross the brain of a Parisian of that generation". But it was generally neglected in France until the revolution of 1830. Then, in the 1830s, there was a surge of interest, and asphalt became widely used "for pavements, flat roofs, and the lining of cisterns, and in England, some use of it had been made of it for similar purposes". Its rise in Europe was "a sudden phenomenon", after natural deposits were found "in France at Osbann (BasRhin), the Parc (l'Ain) and the Puy-de-la-Poix (Puy-de-Dome)", although it could also be made artificially. One of the earliest uses in France was the laying of about 24,000 square yards of Seyssel asphalt at the Place de la Concorde in 1835.
Photography and art.
Bitumen was used in early photographic technology. In 1826 or 1827, it was used by French scientist Joseph Nicéphore Niépce to make the oldest surviving photograph from nature. The bitumen was thinly coated onto a pewter plate which was then exposed in a camera. Exposure to light hardened the bitumen and made it insoluble, so that when it was subsequently rinsed with a solvent only the sufficiently light-struck areas remained. Many hours of exposure in the camera were required, making bitumen impractical for ordinary photography, but from the 1850s to the 1920s it was in common use as a photoresist in the production of printing plates for various photomechanical printing processes.
Bitumen was the nemesis of many artists during the 19th century. Although widely used for a time, it ultimately proved unstable for use in oil painting, especially when mixed with the most common diluents, such as linseed oil, varnish and turpentine. Unless thoroughly diluted, bitumen never fully solidifies and will in time corrupt the other pigments with which it comes into contact. The use of bitumen as a glaze to set in shadow or mixed with other colors to render a darker tone resulted in the eventual deterioration of many paintings, for instance those of Delacroix. Perhaps the most famous example of the destructiveness of bitumen is Théodore Géricault's Raft of the Medusa (1818–1819), where his use of bitumen caused the brilliant colors to degenerate into dark greens and blacks and the paint and canvas to buckle.
Early use in the United Kingdom.
Among the earlier uses of asphalt/bitumen in the United Kingdom was for etching. William Salmon's "Polygraphice" (1673) provides a recipe for varnish used in etching, consisting of three ounces of virgin wax, two ounces of mastic, and one ounce of asphaltum. By the fifth edition in 1685, he had included more asphaltum recipes from other sources.
The first British patent for the use of asphalt/bitumen was 'Cassell's patent asphalte or bitumen' in 1834. Then on 25 November 1837, Richard Tappin Claridge patented the use of Seyssel asphalt (patent #7849), for use in asphalte pavement, having seen it employed in France and Belgium when visiting with Frederick Walter Simms, who worked with him on the introduction of asphalt to Britain. Dr T. Lamb Phipson writes that his father, Samuel Ryland Phipson, a friend of Claridge, was also "instrumental in introducing the asphalte pavement (in 1836)". Indeed, mastic pavements had been previously employed at Vauxhall by a competitor of Claridge, but without success.
In 1838, Claridge obtained patents in Scotland on 27 March, and Ireland on 23 April, and in 1851 extensions were sought for all three patents, by the trustees of a company previously formed by Claridge. This was "Claridge's Patent Asphalte Company", formed in 1838 for the purpose of introducing to Britain "Asphalte in its natural state from the mine at Pyrimont Seysell in France", and "laid one of the first asphalt pavements in Whitehall". Trials were made of the pavement in 1838 on the footway in Whitehall, the stable at Knightsbridge Barracks, "and subsequently on the space at the bottom of the steps leading from Waterloo Place to St. James Park". "The formation in 1838 of Claridge's Patent Asphalte Company (with a distinguished list of aristocratic patrons, and Marc and Isambard Brunel as, respectively, a trustee and consulting engineer), gave an enormous impetus to the development of a British asphalt industry". "By the end of 1838, at least two other companies, Robinson's and the Bastenne company, were in production", with asphalt being laid as paving at Brighton, Herne Bay, Canterbury, Kensington, the Strand, and a large floor area in Bunhill-row, while meantime Claridge's Whitehall paving "continue(d) in good order".
In 1838, there was a flurry of entrepreneurial activity involving asphalt/bitumen, which had uses beyond paving. For example, asphalt could also used for flooring, damp proofing in buildings, and for waterproofing of various types of pools and baths, with these latter themselves proliferating in the 19th century. On the London stockmarket, there were various claims as to the exclusivity of asphalt quality from France, Germany and England. And numerous patents were granted in France, with similar numbers of patent applications being denied in England due to their similarity to each other. In England, "Claridge's was the type most used in the 1840s and 50s"
In 1914, Claridge's Company entered into a joint venture to produce tar-bound macadam, with materials manufactured through a subsidiary company called Clarmac Roads Ltd. Two products resulted, namely "Clarmac", and "Clarphalte", with the former being manufactured by Clarmac Roads and the latter by Claridge's Patent Asphalte Co., although "Clarmac" was more widely used. However, the First World War impacted financially on the Clarmac Company, which entered into liquidation in 1915. The failure of Clarmac Roads Ltd had a flow-on effect to Claridge's Company, which was itself compulsorily wound up, ceasing operations in 1917, having invested a substantial amount of funds into the new venture, both at the outset, and in a subsequent attempt to save the Clarmac Company.
Early use in the US.
The first use of asphalt/bitumen in the New World was by indigenous peoples. On the west coast, as early as the 13th century, the Tongva, Luiseño and Chumash peoples collected the naturally occurring asphalt/bitumen that seeped to the surface above underlying petroleum deposits. All three used the substance as an adhesive. It is found on many different artifacts of tools and ceremonial items. For example, it was used on rattles to adhere gourds or turtle shells to rattle handles. It was also used in decorations. Small round shell beads were often set in asphaltum to provide decorations. It was used as a sealant on baskets to make them watertight for carrying water. Asphaltum was used also to seal the planks on ocean-going canoes.
Roads in the US have been paved with materials that include asphalt/bitumen since at least 1870, when a street in front of the Newark, NJ City Hall was paved. In many cases, these early pavings were made from naturally occurring "bituminous rock", such as at Ritchie Mines in Macfarlan in Ritchie County, West Virginia from 1852 to 1873. In 1876, asphalt-based paving was used to pave Pennsylvania Avenue in Washington, DC, in time for the celebration of the national centennial. Asphalt/bitumen was also used for flooring, paving and waterproofing of baths and swimming pools during the early 20th century, following similar trends in Europe.
Early use in Canada.
Canada has the world's largest deposit of natural bitumen in the Athabasca oil sands and Canadian First Nations along the Athabasca River had long used it to waterproof their canoes. In 1719, a Cree Indian named Wa-Pa-Su brought a sample for trade to Henry Kelsey of the Hudson’s Bay Company, who was the first recorded European to see it. However, it wasn't until 1787 that fur trader and explorer Alexander MacKenzie saw the Athabasca oil sands and said, "At about 24 miles from the fork (of the Athabasca and Clearwater Rivers) are some bituminous fountains into which a pole of 20 feet long may be inserted without the least resistance."
The value of the deposit was obvious from the start, but the means of extracting the bitumen were not. The nearest town, Fort McMurray, Alberta was a small fur trading post, other markets were far away, and transportation costs were too high to ship the raw bituminous sand for paving. In 1915, Sidney Ells of the Federal Mines Branch experimented with separation techniques and used the bitumen to pave 600 feet of road in Edmonton, Alberta. Other roads in Alberta were paved with oil sands, but it was generally not economic. During the 1920s Dr. Karl A. Clark of the Alberta Research Council patented a hot water oil separation process and entrepreneur Robert C. Fitzsimmons built the Bitumount oil separation plant, which between 1925 and 1958 produced up to per day of bitumen using Dr. Clark's method. Most of the bitumen was used for waterproofing roofs, but other uses included fuels, lubrication oils, printers ink, medicines, rust and acid-proof paints, fireproof roofing, street paving, patent leather, and fence post preservatives. Eventually Fitzsimmons ran out of money and the plant was taken over by the Alberta government. Today the Bitumount plant is a Provincial Historic Site.
Modern use.
Rolled asphalt concrete.
The largest use of asphalt/bitumen is for making asphalt concrete for road surfaces and accounts for approximately 85% of the asphalt consumed in the United States. Asphalt concrete pavement mixes are typically composed of 5% asphalt/bitumen cement and 95% aggregates (stone, sand, and gravel). Due to its highly viscous nature, asphalt/bitumen cement must be heated so it can be mixed with the aggregates at the asphalt mixing facility. The temperature required varies depending upon characteristics of the asphalt/bitumen and the aggregates, but warm-mix asphalt technologies allow producers to reduce the temperature required. There are about 4,000 asphalt concrete mixing plants in the U.S., and a similar number in Europe.
When maintenance is performed on asphalt pavements, such as milling to remove a worn or damaged surface, the removed material can be returned to a facility for processing into new pavement mixtures. The asphalt/bitumen in the removed material can be reactivated and put back to use in new pavement mixes. With some 95% of paved roads being constructed of or surfaced with asphalt, a substantial amount of asphalt pavement material is reclaimed each year. According to industry surveys conducted annually by the Federal Highway Administration and the National Asphalt Pavement Association, more than 99% of the asphalt removed each year from road surfaces during widening and resurfacing projects is reused as part of new pavements, roadbeds, shoulders and embankments.
Asphalt concrete paving is widely used in airports around the world. Due to the sturdiness and ability to be repaired quickly, it is widely used for runways dedicated to aircraft landing and taking off.
Mastic asphalt.
Mastic asphalt is a type of asphalt which differs from dense graded asphalt (asphalt concrete) in that it has a higher asphalt/bitumen (binder) content, usually around 7–10% of the whole aggregate mix, as opposed to rolled asphalt concrete, which has only around 5% added asphalt/bitumen. This thermoplastic substance is widely used in the building industry for waterproofing flat roofs and tanking underground. Mastic asphalt is heated to a temperature of and is spread in layers to form an impervious barrier about thick.
Asphalt emulsion.
A number of technologies allow asphalt/bitumen to be mixed at much lower temperatures. These involve mixing with petroleum solvents to form "cutbacks" with reduced melting point, or mixtures with water to turn the asphalt/bitumen into an emulsion. Asphalt emulsions contain up to 70% asphalt/bitumen and typically less than 1.5% chemical additives. There are two main types of emulsions with different affinity for aggregates, cationic and anionic. Asphalt emulsions are used in a wide variety of applications. Chipseal involves spraying the road surface with asphalt emulsion followed by a layer of crushed rock, gravel or crushed slag. Slurry seal involves the creation of a mixture of asphalt emulsion and fine crushed aggregate that is spread on the surface of a road. Cold-mixed asphalt can also be made from asphalt emulsion to create pavements similar to hot-mixed asphalt, several inches in depth and asphalt emulsions are also blended into recycled hot-mix asphalt to create low-cost pavements.
Synthetic crude oil.
Synthetic crude oil, also known as syncrude, is the output from a bitumen upgrader facility used in connection with oil sand production in Canada. Bituminous sands are mined using enormous (100 ton capacity) power shovels and loaded into even larger (400 ton capacity) dump trucks for movement to an upgrading facility. The process used to extract the bitumen from the sand is a hot water process originally developed by Dr. Karl Clark of the University of Alberta during the 1920s. After extraction from the sand, the bitumen is fed into a bitumen upgrader which converts it into a light crude oil equivalent. This synthetic substance is fluid enough to be transferred through conventional oil pipelines and can be fed into conventional oil refineries without any further treatment. By 2015 Canadian bitumen upgraders were producing over per day of synthetic crude oil, of which 75% was exported to oil refineries in the United States.
In Alberta, five bitumen upgraders produce synthetic crude oil and a variety of other products: The Suncor Energy upgrader near Fort McMurray, Alberta produces synthetic crude oil plus diesel fuel; the Syncrude Canada, Canadian Natural Resources, and Nexen upgraders near Fort McMurray produce synthetic crude oil; and the Shell Scotford Upgrader near Edmonton produces synthetic crude oil plus an intermediate feedstock for the nearby Shell Oil Refinery. A sixth upgrader, under construction in 2015 near Redwater, Alberta, will upgrade half of its crude bitumen directly to diesel fuel, with the remainder of the output being sold as feedstock to nearby oil refineries and petrochemical plants.
Non-upgraded crude bitumen.
Canadian bitumen does not differ substantially from oils such as Venezuelan extra-heavy and Mexican heavy oil in chemical composition, and the real difficulty is moving the extremely viscous bitumen through oil pipelines to the refinery. Many modern oil refineries are extremely sophisticated and can process non-upgraded bitumen directly into products such as gasoline, diesel fuel, and refined asphalt without any preprocessing. This is particularly common in areas such as the US Gulf coast, where refineries were designed to process Venezuelan and Mexican oil, and in areas such as the US Midwest where refineries were rebuilt to process heavy oil as domestic light oil production declined. Given the choice, such heavy oil refineries usually prefer to buy bitumen rather than synthetic oil because the cost is lower, and in some cases because they prefer to produce more diesel fuel and less gasoline. By 2015 Canadian production and exports of non-upgraded bitumen exceeded that of synthetic crude oil at over per day, of which about 65% was exported to the United States.
Because of the difficulty of moving crude bitumen through pipelines, non-upgraded bitumen is usually diluted with natural-gas condensate in a form called dilbit or with synthetic crude oil, called synbit. However, to meet international competition, much non-upgraded bitumen is now sold as a blend of multiple grades of bitumen, conventional crude oil, synthetic crude oil, and condensate in a standardized benchmark product such as Western Canadian Select. This sour, heavy crude oil blend is designed to have uniform refining characteristics to compete with internationally marketed heavy oils such as Mexican Mayan or Arabian Dubai Crude.
Other uses.
Roofing shingles account for most of the remaining asphalt/bitumen consumption. Other uses include cattle sprays, fence-post treatments, and waterproofing for fabrics. Asphalt/bitumen is used to make Japan black, a lacquer known especially for its use on iron and steel, and it is also used in paint and marker inks by some graffiti supply companies to increase the weather resistance and permanence of the paint or ink, and to make the color much darker. Asphalt/bitumen is also used to seal some alkaline batteries during the manufacturing process.
Production.
About 40,000,000 tons were produced in 1984. It is obtained as the "heavy" (i.e., difficult to distill) fraction. Material with a boiling point greater than around 500 °C is considered asphalt. Vacuum distillation separates it from the other components in crude oil (such as naphtha, gasoline and diesel). The resulting material is typically further treated to extract small but valuable amounts of lubricants and to adjust the properties of the material to suit applications. In a de-asphalting unit, the crude asphalt is treated with either propane or butane in a supercritical phase to extract the lighter molecules, which are then separated. Further processing is possible by "blowing" the product: namely reacting it with oxygen. This step makes the product harder and more viscous.
Asphalt/bitumen is typically stored and transported at temperatures around . Sometimes diesel oil or kerosene are mixed in before shipping to retain liquidity; upon delivery, these lighter materials are separated out of the mixture. This mixture is often called "bitumen feedstock", or BFS. Some dump trucks route the hot engine exhaust through pipes in the dump body to keep the material warm. The backs of tippers carrying asphalt/bitumen, as well as some handling equipment, are also commonly sprayed with a releasing agent before filling to aid release. Diesel oil is no longer used as a release agent due to environmental concerns.
From oil sands.
Naturally occurring crude asphalt/bitumen impregnated in sedimentary rock is the prime feed stock for petroleum production from "Oil sands", currently under development in Alberta, Canada. Canada has most of the world's supply of natural asphalt/bitumen, covering 140,000 square kilometres (an area larger than England), giving it the second-largest proven oil reserves in the world. The Athabasca oil sands is the largest asphalt/bitumen deposit in Canada and the only one accessible to surface mining, although recent technological breakthroughs have resulted in deeper deposits becoming producible by "in situ" methods. Because of oil price increases after 2003, producing bitumen became highly profitable, but as a result of the decline after 2014 it became uneconomic to build new plants again. By 2014, Canadian crude asphalt/bitumen production averaged about per day and was projected to rise to per day by 2020. The total amount of crude asphalt/bitumen in Alberta which could be extracted is estimated to be about , which at a rate of would last about 200 years.
Alternatives and bioasphalt.
Although uncompetitive economically, asphalt/bitumen can be made from nonpetroleum-based renewable resources such as sugar, molasses and rice, corn and potato starches. Asphalt/bitumen can also be made from waste material by fractional distillation of used motor oil, which is sometimes otherwise disposed of by burning or dumping into landfills. Use of motor oil may cause premature cracking in colder climates, resulting in roads that need to be repaved more frequently.
Nonpetroleum-based asphalt/bitumen binders can be made light-colored. Lighter-colored roads absorb less heat from solar radiation, and have less surface heat than darker surfaces, reducing their contribution to the urban heat island effect. Parking lots that use asphalt alternatives are called green parking lots.
Natural bitumen.
Selenizza is a naturally occurring solid hydrocarbon bitumen found in the native asphalt deposit of Selenice, in Albania, the only European asphalt mine still in use. The rock asphalt is found in the form of veins, filling cracks in a more or less horizontal direction. The bitumen content varies from 83% to 92% (soluble in carbon disulphide), with a penetration value near to zero and a softening point (ring & ball) around 120 °C. The insoluble matter, consisting mainly of silica ore, ranges from 8% to 17%.
The Albanian bitumen extraction has a long history and was practiced in an organized way by the Romans. After centuries of silence, the first mentions of Albanian bitumen appeared only in 1868, when the Frenchman Coquand published the first geological description of the deposits of Albanian bitumen. In 1875, the exploitation rights were granted to the Ottoman government and in 1912, they were transferred to the Italian company Simsa. Since 1945, the mine was exploited by the Albanian government and from 2001 to date, the management passed to a French company, which organized the mining process for the manufacture of the natural bitumen on an industrial scale.
Today the mine is predominantly exploited in an open pit quarry but several of the many underground mines (deep and extending over several km) still remain viable. The bitumen Selenizza is produced primarily in granular form, after melting the asphalt pieces selected in the mine.
Selenizza is mainly used as an additive in the road construction sector. It is mixed with traditional bitumen to improve both the viscoelastic properties and the resistance to ageing. It may be blended with the hot bitumen in tanks, but its granular form allows it to be fed in the mixer or in the recycling ring of normal asphalt plants. Other typical applications include the production of mastic asphalts for sidewalks, bridges, car-parks and urban roads as well as drilling fluid additives for the oil and gas industry. Selenizza is available in powder or in granular material of various particle sizes and is packaged in big bags or in thermal fusible polyethylene bags.
A Life Cycle Assessment (LCA) study of the natural bitumen Selenizza compared with petroleum bitumen, has shown that the environmental impact of the natural bitumen is about half the impact of the road bitumen produced in oil refineries in terms of carbon dioxide emission.
Occupational safety.
People can be exposed to asphalt in the workplace by breathing in fumes or skin absorption. The National Institute for Occupational Safety and Health (NIOSH) has set a Recommended exposure limit (REL) of 5 mg/m over a 15-minute period. Asphalt is basically an inert material that must be heated or diluted to a point where it becomes workable for the production of materials for paving, roofing, and other applications. In examining the potential health hazards associated with asphalt, the International Agency for Research on Cancer (IARC) determined that it is the application parameters, predominantly temperature, that effect occupational exposure and the potential bioavailable carcinogenic hazard/risk of the asphalt emissions. In particular, temperatures greater than 199 °C (390 °F), were shown to produce a greater exposure risk than when asphalt was heated to lower temperatures, such as those typically used in asphalt pavement mix production and placement.
Etymology.
The word "asphalt" is derived from the late Middle English, in turn from French "asphalte", based on Late Latin "asphalton", "asphaltum", which is the latinisation of the Greek ἄσφαλτος ("ásphaltos", "ásphalton"), a word meaning "asphalt/bitumen/pitch", which perhaps derives from ἀ-, "without" and σφάλλω ("sfallō"), "make fall". Note that in French, the term "asphalte" is used for naturally occurring bitumen-soaked limestone deposits, and for specialised manufactured products with fewer voids or greater bitumen content than the "asphaltic concrete" used to pave roads. It is a significant fact that the first use of asphalt by the ancients was in the nature of a cement for securing or joining together various objects, and it thus seems likely that the name itself was expressive of this application. Specifically Herodotus mentioned that bitumen was brought to Babylon to build its gigantic fortification wall. From the Greek, the word passed into late Latin, and thence into French ("asphalte") and English ("asphaltum" and "asphalt").
The expression "bitumen" originated in the Sanskrit, where we find the words "jatu", meaning "pitch," and "jatu-krit", meaning "pitch creating", "pitch producing" (referring to coniferous or resinous trees). The Latin equivalent is claimed by some to be originally "gwitu-men" (pertaining to pitch), and by others, "pixtumens" (exuding or bubbling pitch), which was subsequently shortened to "bitumen", thence passing via French into English. From the same root is derived the Anglo Saxon word "cwidu" (mastix), the German word "Kitt" (cement or mastic) and the old Norse word "kvada".
Neither of the terms asphalt or bitumen should be confused with tar or coal tars.
Modern usage.
In British English, the word 'asphalt' is used to refer to a mixture of mineral aggregate and asphalt/bitumen (also called tarmac in common parlance). When bitumen is mixed with clay it is usually called asphaltum. The earlier word 'asphaltum' is now archaic and not commonly used. In American English, 'asphalt' is equivalent to the British 'bitumen'. However, 'asphalt' is also commonly used as a shortened form of 'asphalt concrete' (therefore equivalent to the British 'asphalt' or 'tarmac'). In Australian English, bitumen is often used as the generic term for road surfaces. In Canadian English, the word bitumen is used to refer to the vast Canadian deposits of extremely heavy crude oil, while asphalt is used for the oil refinery product used to pave roads and manufacture roof shingles and various waterproofing products. Diluted bitumen (diluted with naphtha to make it flow in pipelines) is known as dilbit in the Canadian petroleum industry, while bitumen "upgraded" to synthetic crude oil is known as syncrude and syncrude blended with bitumen as synbit.
Bitumen is still the preferred geological term for naturally occurring deposits of the solid or semi-solid form of petroleum. Bituminous rock is a form of sandstone impregnated with bitumen. The tar sands of Alberta, Canada are a similar material.

</doc>
<doc id="659" url="https://en.wikipedia.org/wiki?curid=659" title="American National Standards Institute">
American National Standards Institute

The American National Standards Institute (ANSI, ) is a private non-profit organization that oversees the development of voluntary consensus standards for products, services, processes, systems, and personnel in the United States. The organization also coordinates U.S. standards with international standards so that American products can be used worldwide. 
ANSI accredits standards that are developed by representatives of other standards organizations, government agencies, consumer groups, companies, and others. These standards ensure that the characteristics and performance of products are consistent, that people use the same definitions and terms, and that products are tested the same way. ANSI also accredits organizations that carry out product or personnel certification in accordance with requirements defined in international standards.
The organization's headquarters are in Washington, DC. ANSI's operations office is located in New York City. The ANSI annual operating budget is funded by the sale of publications, membership dues and fees, accreditation services, fee-based programs, and international standards programs.
History.
ANSI was originally formed in 1918, when five engineering societies and three government agencies founded the American Engineering Standards Committee (AESC). In 1928, the AESC became the American Standards Association (ASA). In 1966, the ASA was reorganized and became United States of America Standards Institute (USASI). The present name was adopted in 1969.
Prior to 1918, these five founding engineering societies:
had been members of the United Engineering Society (UES).
At the behest of the AIEE, they invited the U.S. government Departments of War, Navy (combined in 1947 to become the Department of Defense or DOD) and Commerce to join in founding a national standards organization.
According to Paul G. Agnew, the first permanent secretary and head of staff in 1919, AESC started as an ambitious program and little else. Staff for the first year consisted of one executive, Clifford B. LePage, who was on loan from a founding member, ASME. An annual budget of $7,500 was provided by the founding bodies.
In 1931, the organization (renamed ASA in 1928) became affiliated with the U.S. National Committee of the International Electrotechnical Commission (IEC), which had been formed in 1904 to develop electrical and electronics standards.
Members.
ANSI's membership comprises government agencies, organizations, corporations, academic and international bodies, and individuals. In total, the Institute represents the interests of more than 125,000 companies and 3.5 million professionals.
Process.
Though ANSI itself does not develop standards, the Institute oversees the development and use of standards by accrediting the procedures of standards developing organizations. ANSI accreditation signifies that the procedures used by standards developing organizations meet the Institute's requirements for openness, balance, consensus, and due process.
ANSI also designates specific standards as American National Standards, or ANS, when the Institute determines that the standards were developed in an environment that is equitable, accessible and responsive to the requirements of various stakeholders.
Voluntary consensus standards quicken the market acceptance of products while making clear how to improve the safety of those products for the protection of consumers. There are approximately 9,500 American National Standards that carry the ANSI designation.
The American National Standards process involves:
International activities.
In addition to facilitating the formation of standards in the United States, ANSI promotes the use of U.S. standards internationally, advocates U.S. policy and technical positions in international and regional standards organizations, and encourages the adoption of international standards as national standards where appropriate.
The Institute is the official U.S. representative to the two major international standards organizations, the International Organization for Standardization (ISO), as a founding member, and the International Electrotechnical Commission (IEC), via the U.S. National Committee (USNC). ANSI participates in almost the entire technical program of both the ISO and the IEC, and administers many key committees and subgroups. In many instances, U.S. standards are taken forward to ISO and IEC, through ANSI or the USNC, where they are adopted in whole or in part as international standards.
Standards panels.
The Institute administers nine standards panels:
Each of the panels works to identify, coordinate, and harmonize voluntary standards relevant to these areas.
In 2009, ANSI and the National Institute of Standards and Technology (NIST) formed the Nuclear Energy Standards Coordination Collaborative (NESCC). NESCC is a joint initiative to identify and respond to the current need for standards in the nuclear industry.

</doc>
<doc id="661" url="https://en.wikipedia.org/wiki?curid=661" title="Argument (disambiguation)">
Argument (disambiguation)

In philosophy and logic, an argument is an attempt to persuade someone of something, or give evidence or reasons for accepting a particular conclusion.
Argument may also refer to: 

</doc>
<doc id="662" url="https://en.wikipedia.org/wiki?curid=662" title="Apollo 11">
Apollo 11

Apollo 11 was the first spaceflight that landed humans on the Moon. Americans Neil Armstrong and Buzz Aldrin landed on July 20, 1969, at 20:18 UTC ( years ago). Armstrong became the first to step onto the lunar surface six hours later on July 21 at 02:56 UTC. Armstrong spent about two and a half hours outside the spacecraft, and together with Aldrin collected of lunar material for return to Earth. The third member of the mission, Michael Collins, piloted the command spacecraft alone in lunar orbit until Armstrong and Aldrin returned to it just under a day later for the trip back to Earth.
Launched by a Saturn V rocket from Kennedy Space Center in Merritt Island, Florida, on July 16, Apollo 11 was the fifth manned mission of NASA's Apollo program. The Apollo spacecraft had three parts: a Command Module (CM) with a cabin for the three astronauts, and the only part that landed back on Earth; a Service Module (SM), which supported the Command Module with propulsion, electrical power, oxygen, and water; and a Lunar Module (LM) for landing on the Moon (which itself was composed of two parts). After being sent toward the Moon by the Saturn V's upper stage, the astronauts separated the spacecraft from it and traveled for three days until they entered into lunar orbit. Armstrong and Aldrin then moved into the Lunar Module and landed in the Sea of Tranquility. They stayed a total of about 21½ hours on the lunar surface. After lifting off in the upper part of the Lunar Module and rejoining Collins in the Command Module, they returned to Earth and landed in the Pacific Ocean on July 24.
Broadcast on live TV to a world-wide audience, Armstrong stepped onto the lunar surface and described the event as "one small step for man, one giant leap for mankind." Apollo 11 effectively ended the Space Race and fulfilled a national goal proposed in 1961 by the U.S. President John F. Kennedy in a speech before the U.S. Congress: "before this decade is out, of landing a man on the Moon and returning him safely to the Earth."
Framework.
Crew.
Apollo 11 was the second all-veteran multi-person crew (the first being Apollo 10) in human spaceflight history. A previous solo veteran flight had been made on Soyuz 1 in 1967 by Soviet cosmonaut Vladimir Komarov.
Collins was originally slated to be the Command Module Pilot (CMP) on Apollo 8 but was removed when he required surgery on his back and was replaced by Jim Lovell, his backup for that flight. After Collins was medically cleared, he took what would have been Lovell's spot on Apollo 11; as a veteran of Apollo 8, Lovell was transferred to Apollo 11's backup crew, but promoted to backup commander.
Backup crew.
In early 1969, Anders accepted a job with the National Space Council effective August 1969 and announced that he would retire as an astronaut on that date. At that point Ken Mattingly was moved from the support crew into parallel training with Anders as backup Command Module Pilot in case Apollo 11 was delayed past its intended July launch (at which point Anders would be unavailable if needed) and would later join Lovell's crew and ultimately be assigned as the original Apollo 13 CMP.
Call signs.
After the crew of Apollo 10 named their spacecraft "Charlie Brown" and "Snoopy", assistant manager for public affairs Julian Scheer wrote to Manned Spacecraft Center director George M. Low to suggest the Apollo 11 crew be less flippant in naming their craft. During early mission planning, the names "Snowcone" and "Haystack" were used and put in the news release, but the crew later decided to change them.
The Command Module was named "Columbia" after the "Columbiad", the giant cannon shell "spacecraft" fired by a giant cannon (also from Florida) in Jules Verne's 1865 novel "From the Earth to the Moon". The Lunar Module was named "Eagle" for the national bird of the United States, the bald eagle, which is featured prominently on the mission insignia.
Insignia.
The Apollo 11 mission insignia was designed by Collins, who wanted a symbol for "peaceful lunar landing by the United States". He chose an eagle as the symbol, put an olive branch in its beak, and drew a lunar background with the Earth in the distance. NASA officials said the talons of the eagle looked too "warlike" and after some discussion, the olive branch was moved to the claws. The crew decided the Roman numeral XI would not be understood in some nations and went with "Apollo 11"; they decided not to put their names on the patch, so it would "be representative of "everyone" who had worked toward a lunar landing". All colors are natural, with blue and gold borders around the patch.
When the Eisenhower dollar coin was released in 1971, the patch design provided the eagle for its reverse side. The design was also used for the smaller Susan B. Anthony dollar unveiled in 1979, ten years after the Apollo 11 mission.
Mementos.
Neil Armstrong's personal preference kit carried a piece of wood from the Wright brothers' 1903 airplane's left propeller and a piece of fabric from its wing, along with a diamond-studded astronaut pin originally given to Deke Slayton by the widows of the Apollo 1 crew. This pin had been intended to be flown on Apollo 1 and given to Slayton after the mission but following the disastrous launch pad fire and subsequent funerals, the widows gave the pin to Slayton and Armstrong took it on Apollo 11.
Mission highlights.
Launch and flight to lunar orbit.
In addition to many people crowding highways and beaches near the launch site, millions watched the event on television, with NASA Chief of Public Information Jack King providing commentary. President Richard M. Nixon viewed the proceedings from the Oval Office of the White House.
A Saturn V launched Apollo 11 from Launch Pad 39A, part of the Launch Complex 39 site at the Kennedy Space Center on July 16, 1969 at 13:32:00 UTC (9:32:00 a.m. EDT local time). It entered Earth orbit, at an altitude of by , twelve minutes later. After one and a half orbits, the S-IVB third-stage engine pushed the spacecraft onto its trajectory toward the Moon with the trans-lunar injection (TLI) burn at 16:22:13 UTC. About 30 minutes later the command/service module pair separated from this last remaining Saturn V stage and docked with the Lunar Module still nestled in the Lunar Module Adaptor. After the Lunar Module was extracted, the combined spacecraft headed for the Moon, while the third stage booster flew on a trajectory past the Moon and into orbit around the Sun.
On July 19 at 17:21:50 UTC, Apollo 11 passed behind the Moon and fired its service propulsion engine to enter lunar orbit. In the thirty orbits that followed, the crew saw passing views of their landing site in the southern Sea of Tranquility (Mare Tranquillitatis) about southwest of the crater Sabine D (0.67408N, 23.47297E). The landing site was selected in part because it had been characterized as relatively flat and smooth by the automated "Ranger 8" and "Surveyor 5" landers along with the "Lunar Orbiter" mapping spacecraft and unlikely to present major landing or extravehicular activity (EVA) challenges.
Lunar descent.
On July 20, 1969, the Lunar Module "Eagle" separated from the Command Module "Columbia". Collins, alone aboard "Columbia", inspected "Eagle" as it pirouetted before him to ensure the craft was not damaged.
As the descent began, Armstrong and Aldrin found that they were passing landmarks on the surface four seconds early and reported that they were "long"; they would land miles west of their target point.
Five minutes into the descent burn, and above the surface of the Moon, the LM navigation and guidance computer distracted the crew with the first of several unexpected "1202" and "1201" program alarms. Inside Mission Control Center in Houston, Texas, computer engineer Jack Garman told guidance officer Steve Bales it was safe to continue the descent, and this was relayed to the crew. The program alarms indicated "executive overflows", meaning the guidance computer could not complete all of its tasks in real time and had to postpone some of them.
Landing.
When Armstrong again looked outside, he saw that the computer's landing target was in a boulder-strewn area just north and east of a diameter crater (later determined to be West crater, named for its location in the western part of the originally planned landing ellipse). Armstrong took semi-automatic control and, with Aldrin calling out altitude and velocity data, landed at 20:17:40 UTC on July 20 with about 25 seconds of fuel left.
Apollo 11 landed with less fuel than other missions, and the astronauts encountered a premature low fuel warning. This was later found to be the result of greater propellant 'slosh' than expected, uncovering a fuel sensor. On subsequent missions, extra anti-slosh baffles were added to the tanks to prevent this.
Throughout the descent Aldrin had called out navigation data to Armstrong, who was busy piloting the LM. A few moments before the landing, a light informed Aldrin that at least one of the probes hanging from "Eagle"'s footpads had touched the surface, and he said "Contact light!" Three seconds later, "Eagle" landed and Armstrong said "Shutdown." Aldrin immediately said "Okay, engine stop. ACA – out of detent." Armstrong acknowledged "Out of detent. Auto" and Aldrin continued "Mode control – both auto. Descent engine command override off. Engine arm – off. 413 is in."
Charles Duke, CAPCOM during the landing phase, acknowledged their landing by saying "We copy you down, Eagle."
Armstrong acknowledged Aldrin's completion of the post landing checklist with "Engine arm is off", before responding to Duke with the words, "Houston, Tranquility Base here. The "Eagle" has landed." Armstrong's unrehearsed change of call sign from "Eagle" to "Tranquility Base" emphasized to listeners that landing was complete and successful. Duke mispronounced his reply as he expressed the relief at Mission Control: "Roger, Twan— Tranquility, we copy you on the ground. You got a bunch of guys about to turn blue. We're breathing again. Thanks a lot."
Two and a half hours after landing, before preparations began for the EVA, Aldrin radioed to Earth:
The schedule for the mission called for the astronauts to follow the landing with a five-hour sleep period, since they had been awake since early morning. However, they elected to forgo the sleep period and begin the preparations for the EVA early, thinking that they would be unable to sleep.
Lunar surface operations.
The astronauts planned placement of the Early Apollo Scientific Experiment Package (EASEP) and the U.S. flag by studying their landing site through "Eagle"'s twin triangular windows, which gave them a 60° field of view. Preparation required longer than the two hours scheduled. Armstrong initially had some difficulties squeezing through the hatch with his Portable Life Support System (PLSS). According to veteran Moon-walker John Young, a redesign of the LM to incorporate a smaller hatch had not been followed by a redesign of the PLSS backpack, so some of the highest heart rates recorded from Apollo astronauts occurred during LM egress and ingress.
Several books indicate early mission timelines had Buzz Aldrin rather than Neil Armstrong as the first man on the Moon.
At 02:39 UTC on Monday July 21, 1969, Armstrong opened the hatch, and at 02:51 UTC began his descent to the lunar surface. The Remote Control Unit controls on his chest kept him from seeing his feet. Climbing down the nine-rung ladder, Armstrong pulled a D-ring to deploy the Modular Equipment Stowage Assembly (MESA) folded against "Eagle"'s side and activate the TV camera, and at 02:56:15 UTC he set his left foot on the surface. The first landing used slow-scan television incompatible with commercial TV, so it was displayed on a special monitor and a conventional TV camera viewed this monitor, significantly reducing the quality of the picture. The signal was received at Goldstone in the United States but with better fidelity by Honeysuckle Creek Tracking Station in Australia. Minutes later the feed was switched to the more sensitive Parkes radio telescope in Australia. Despite some technical and weather difficulties, ghostly black and white images of the first lunar EVA were received and broadcast to at least 600 million people on Earth. Although copies of this video in broadcast format were saved and are widely available, recordings of the original slow scan source transmission from the lunar surface were accidentally destroyed during routine magnetic tape re-use at NASA.
While still on the ladder, Armstrong uncovered a plaque mounted on the LM Descent Stage bearing two drawings of Earth (of the Western and Eastern Hemispheres), an inscription, and signatures of the astronauts and President Nixon. The inscription read:
After describing the surface dust as "very fine-grained" and "almost like a powder," six and a half hours after landing, Armstrong stepped off "Eagle"'s footpad and declared, "That's one small step for man, one giant leap for mankind."
Armstrong intended to say "That's one small step for a man", but the word "a" is not audible in the transmission, and thus was not initially reported by most observers of the live broadcast. When later asked about his quote, Armstrong said he believed he said "for a man", and subsequent printed versions of the quote included the "a" in square brackets. One explanation for the absence may be that his accent caused him to slur the words "for a" together; another is the intermittent nature of the audio and video links to Earth, partly because of storms near Parkes Observatory. More recent digital analysis of the tape claims to reveal the "a" may have been spoken but obscured by static.
About seven minutes after stepping onto the Moon's surface, Armstrong collected a contingency soil sample using a sample bag on a stick. He then folded the bag and tucked it into a pocket on his right thigh. This was to guarantee there would be some lunar soil brought back in case an emergency required the astronauts to abandon the EVA and return to the LM.
Twelve minutes after the contingency sample was collected, Aldrin joined Armstrong on the surface, and described the view with the simple phrase, "Magnificent desolation."
In addition to fulfilling President Kennedy's mandate to land a man on the Moon before the end of the 1960s, Apollo 11 was an engineering test of the Apollo system; therefore, Armstrong snapped photos of the LM so engineers would be able to judge its post-landing condition. He removed the TV camera from the MESA and made a panoramic sweep, then mounted it on a tripod from the LM. The TV camera cable remained partly coiled and presented a tripping hazard throughout the EVA.
Armstrong said that moving in the lunar gravity, one-sixth of Earth's, was "even perhaps easier than the simulations ... It's absolutely no trouble to walk around." Aldrin joined him on the surface and tested methods for moving around, including two-footed kangaroo hops. The PLSS backpack created a tendency to tip backwards, but neither astronaut had serious problems maintaining balance. Loping became the preferred method of movement. The astronauts reported that they needed to plan their movements six or seven steps ahead. The fine soil was quite slippery. Aldrin remarked that moving from sunlight into "Eagle"'s shadow produced no temperature change inside the suit, though the helmet was warmer in sunlight, so he felt cooler in shadow.
The astronauts planted a specially designed U.S. flag on the lunar surface, in clear view of the TV camera. Some time later, President Richard Nixon spoke to them through a telephone-radio transmission which Nixon called "the most historic phone call ever made from the White House." Nixon originally had a long speech prepared to read during the phone call, but Frank Borman, who was at the White House as a NASA liaison during Apollo 11, convinced Nixon to keep his words brief, to respect the lunar landing as Kennedy's legacy. Armstrong thanked the President, and gave a brief reflection on the significance of the moment:
Nixon: Hello, Neil and Buzz. I'm talking to you by telephone from the Oval Room at the White House. And this certainly has to be the most historic telephone call ever made. I just can't tell you how proud we all are of what you've done. For every American, this has to be the proudest day of our lives. And for people all over the world, I am sure they too join with Americans in recognizing what an immense feat this is. Because of what you have done, the heavens have become a part of man's world. And as you talk to us from the Sea of Tranquility, it inspires us to redouble our efforts to bring peace and tranquility to Earth. For one priceless moment in the whole history of man, all the people on this Earth are truly one: one in their pride in what you have done, and one in our prayers that you will return safely to Earth.
Armstrong: Thank you Mr. President. It's a great honor and privilege for us to be here, representing not only the United States, but men of peace of all nations, and with interest and curiosity, and men with a vision for the future. It's an honor for us to be able to participate here today.
The MESA failed to provide a stable work platform and was in shadow, slowing work somewhat. As they worked, the moonwalkers kicked up gray dust which soiled the outer part of their suits, the integrated thermal meteoroid garment.
They deployed the EASEP, which included a passive seismograph and a Lunar Ranging Retroreflector (LRRR). Then Armstrong walked from the LM to snap photos at the rim of Little West Crater while Aldrin collected two core tubes. He used the geological hammer to pound in the tubes – the only time the hammer was used on Apollo 11. The astronauts then collected rock samples using scoops and tongs on extension handles. Many of the surface activities took longer than expected, so they had to stop documenting sample collection halfway through the allotted 34 minutes.
Three new minerals were discovered in the rock samples collected by the astronauts: armalcolite, tranquillityite, and pyroxferroite. Armalcolite was named after Armstrong, Aldrin, and Collins.
During this period Mission Control used a coded phrase to warn Armstrong that his metabolic rates were high and that he should slow down. He was moving rapidly from task to task as time ran out. However, as metabolic rates remained generally lower than expected for both astronauts throughout the walk, Mission Control granted the astronauts a 15-minute extension. In a 2010 interview, Armstrong, who had walked a maximum of from the LM, explained that NASA limited the first moonwalk's time and distance because there was no empirical proof of how much cooling water the astronauts' PLSS backpacks would consume to handle their body heat generation while working on the Moon.
Lunar ascent and return.
Aldrin entered "Eagle" first. With some difficulty the astronauts lifted film and two sample boxes containing of lunar surface material to the LM hatch using a flat cable pulley device called the Lunar Equipment Conveyor. Armstrong reminded Aldrin of a bag of memorial items in his suit pocket sleeve, and Aldrin tossed the bag down; Armstrong then jumped to the ladder's third rung and climbed into the LM. After transferring to LM life support, the explorers lightened the ascent stage for return to lunar orbit by tossing out their PLSS backpacks, lunar overshoes, one Hasselblad camera, and other equipment. They then pressurized the LM, and settled down to sleep.
President Nixon's speech writer William Safire had prepared "In Event of Moon Disaster" for the President to read on television in the event the Apollo 11 astronauts were stranded on the Moon. The contingency plan originated in a memo from Safire to Nixon's White House Chief of Staff H. R. Haldeman, in which Safire suggested a protocol the administration might follow in reaction to such a disaster. According to the plan, Mission Control would "close down communications" with the LM, and a clergyman would "commend their souls to the deepest of the deep" in a public ritual likened to burial at sea. The last line of the prepared text contained an allusion to Rupert Brooke's First World War poem, "The Soldier". The plan included presidential telephone calls to the astronauts' wives.
While moving within the cabin, Aldrin accidentally damaged the circuit breaker that would arm the main engine for lift off from the Moon. There was concern this would prevent firing the engine, stranding them on the Moon. Fortunately a felt-tip pen was sufficient to activate the switch. Had this not worked, the Lunar Module circuitry could have been reconfigured to allow firing the ascent engine.
After about seven hours of rest, the crew was awakened by Houston to prepare for the return flight. Two and a half hours later, at 17:54 UTC, they lifted off in "Eagle"'s ascent stage to rejoin Collins aboard "Columbia" in lunar orbit.
After more than 21½ total hours on the lunar surface, they had left behind scientific instruments that included a retroreflector array used for the Lunar Laser Ranging Experiment and a Passive Seismic Experiment Package used to measure moonquakes. They also left an Apollo 1 mission patch, and a memorial bag containing a gold replica of an olive branch as a traditional symbol of peace and a silicon message disk. The disk carries the goodwill statements by Presidents Eisenhower, Kennedy, Johnson, and Nixon and messages from leaders of 73 countries around the world. The disc also carries a listing of the leadership of the US Congress, a listing of members of the four committees of the House and Senate responsible for the NASA legislation, and the names of NASA's past and present top management. (In his 1989 book, "Men from Earth", Aldrin says that the items included Soviet medals commemorating Cosmonauts Vladimir Komarov and Yuri Gagarin.) Also, according to Deke Slayton's book "Moonshot", Armstrong carried with him a special diamond-studded astronaut pin from Slayton.
Film taken from the LM Ascent Stage upon liftoff from the Moon reveals the American flag, planted some from the descent stage, whipping violently in the exhaust of the ascent stage engine. Aldrin looked up in time to witness the flag topple: "The ascent stage of the LM separated ... I was concentrating on the computers, and Neil was studying the attitude indicator, but I looked up long enough to see the flag fall over." Subsequent Apollo missions usually planted the American flags at least from the LM to prevent them being blown over by the ascent engine exhaust.
After rendezvous with "Columbia", "Eagle"s ascent stage was jettisoned into lunar orbit on July 21, 1969, at 23:41 UTC. Just before the Apollo 12 flight, it was noted that "Eagle" was still likely to be orbiting the Moon. Later NASA reports mentioned that "Eagle"'s orbit had decayed, resulting in it impacting in an "uncertain location" on the lunar surface. The location is uncertain because the "Eagle" ascent stage was not tracked after it was jettisoned, and the lunar gravity field is sufficiently non-uniform to make the orbit of the spacecraft unpredictable after a short time. NASA estimated that the orbit had decayed within months and would have impacted on the Moon.
On July 23, the last night before splashdown, the three astronauts made a television broadcast in which Collins commented: ... The Saturn V rocket which put us in orbit is an incredibly complicated piece of machinery, every piece of which worked flawlessly ... We have always had confidence that this equipment will work properly. All this is possible only through the blood, sweat, and tears of a number of a people ... All you see is the three of us, but beneath the surface are thousands and thousands of others, and to all of those, I would like to say, "Thank you very much."
Aldrin added: This has been far more than three men on a mission to the Moon; more, still, than the efforts of a government and industry team; more, even, than the efforts of one nation. We feel that this stands as a symbol of the insatiable curiosity of all mankind to explore the unknown ... Personally, in reflecting on the events of the past several days, a verse from Psalms comes to mind. "When I consider the heavens, the work of Thy fingers, the Moon and the stars, which Thou hast ordained; What is man that Thou art mindful of him?"
Armstrong concluded: The responsibility for this flight lies first with history and with the giants of science who have preceded this effort; next with the American people, who have, through their will, indicated their desire; next with four administrations and their Congresses, for implementing that will; and then, with the agency and industry teams that built our spacecraft, the Saturn, the Columbia, the Eagle, and the little EMU, the spacesuit and backpack that was our small spacecraft out on the lunar surface. We would like to give special thanks to all those Americans who built the spacecraft; who did the construction, design, the tests, and put their hearts and all their abilities into those craft. To those people tonight, we give a special thank you, and to all the other people that are listening and watching tonight, God bless you. Good night from Apollo 11.
On the return to Earth, a bearing at the Guam tracking station failed, potentially preventing communication on the last segment of the Earth return. A regular repair was not possible in the available time but the station director, Charles Force, had his ten-year-old son Greg use his small hands to reach into the housing and pack it with grease. Greg later was thanked by Armstrong.
Splashdown and quarantine.
On July 24, the astronauts returned home aboard the Command Module "Columbia" just before dawn local time (16:51 UTC) at , in the Pacific Ocean east of Wake Island, south of Johnston Atoll, and from the recovery ship, .
At 16:44 UTC the drogue parachutes had been deployed and seven minutes later the Command Module struck the water forcefully. During splashdown, the Command Module landed upside down but was righted within 10 minutes by flotation bags triggered by the astronauts. "Everything's okay. Our checklist is complete. Awaiting swimmers", was Armstrong's last official transmission from the "Columbia". A diver from the Navy helicopter hovering above attached a sea anchor to the Command Module to prevent it from drifting. Additional divers attached flotation collars to stabilize the module and position rafts for astronaut extraction. Though the chance of bringing back pathogens from the lunar surface was considered remote, it was considered a possibility and NASA took great precautions at the recovery site. Divers provided the astronauts with Biological Isolation Garments (BIGs) which were worn until they reached isolation facilities on board the "Hornet". Additionally astronauts were rubbed down with a sodium hypochlorite solution and the Command Module wiped with Betadine to remove any lunar dust that might be present. The raft containing decontamination materials was then intentionally sunk.
A second Sea King helicopter hoisted the astronauts aboard one by one, where a NASA flight surgeon gave each a brief physical check during the trip back to the "Hornet".
After touchdown on the "Hornet", the astronauts exited the helicopter, leaving the flight surgeon and three crewmen. The helicopter was then lowered into hangar bay #2 where the astronauts walked the to the Mobile Quarantine Facility (MQF) where they would begin their 21 days of quarantine. This practice would continue for two more Apollo missions, Apollo 12 and Apollo 14, before the Moon was proven to be barren of life and the quarantine process dropped.
President Richard Nixon was aboard "Hornet" to personally welcome the astronauts back to Earth. He told the astronauts, "As a result of what you've done, the world has never been closer together before." After Nixon departed, the "Hornet" was brought alongside the five-ton Command Module where it was placed aboard by the ship's crane, placed on a dolly and moved next to the MQF. The "Hornet" sailed for Pearl Harbor where the Command Module and MQF were airlifted to the Manned Spacecraft Center.
In accordance with the recently passed Extra-Terrestrial Exposure Law, the astronauts were placed in quarantine for fear that the Moon might contain undiscovered pathogens and that the astronauts might have been exposed to them during their Moon walks. However, after almost three weeks in confinement (first in their trailer and later in the Lunar Receiving Laboratory at the Manned Spacecraft Center), the astronauts were given a clean bill of health. On August 10, 1969, the astronauts exited quarantine.
Celebration.
On August 13, they rode in parades in their honor in New York, Chicago, and Los Angeles. On the same evening in Los Angeles there was an official State Dinner to celebrate the flight, attended by members of Congress, 44 governors, the Chief Justice of the United States, and ambassadors from 83 nations at the Century Plaza Hotel. President Richard Nixon and Vice President Spiro T. Agnew honored each astronaut with a presentation of the Presidential Medal of Freedom. This celebration was the beginning of a 45-day "Giant Leap" tour that brought the astronauts to 25 foreign countries and included visits with prominent leaders such as Queen Elizabeth II of the United Kingdom. Many nations honored the first manned Moon landing with special features in magazines or by issuing Apollo 11 commemorative postage stamps or coins.
On September 16, 1969, the three astronauts spoke before a joint session of Congress on Capitol Hill. They presented two US flags, one to the House of Representatives and the other to the Senate, that had been carried to the surface of the Moon with them.
Moon race.
The Soviet Union was secretly attempting to compete with the US in landing a man on the Moon but had been hampered by repeated failures in development of a launcher comparable to the Saturn V. Meanwhile, they tried to beat the US to return lunar material to the Earth by means of unmanned probes. On July 13, three days before Apollo 11's launch, they launched Luna 15, which reached lunar orbit before Apollo 11. During descent, a malfunction caused Luna 15 to crash in Mare Crisium about two hours before Armstrong and Aldrin took off from the surface. The Jodrell Bank Observatory radio telescope in England was later discovered to have recorded transmissions from Luna 15 during its descent, and this was published in July 2009 on the 40th anniversary of Apollo 11.
Spacecraft location.
The Command Module is displayed at the National Air and Space Museum, Washington, D.C. It is in the central "Milestones of Flight" exhibition hall in front of the Jefferson Drive entrance, sharing the main hall with other pioneering flight vehicles such as the Wright Flyer, the "Spirit of St. Louis", the Bell X-1, the North American X-15, Mercury spacecraft "Friendship 7", and Gemini 4. Armstrong's and Aldrin's space suits are displayed in the museum's "Apollo to the Moon" exhibit. The quarantine trailer, the flotation collar, and the righting spheres are displayed at the Smithsonian's Steven F. Udvar-Hazy Center annex near Washington Dulles International Airport in Virginia.
In 2009 the Lunar Reconnaissance Orbiter (LRO) imaged the various Apollo landing sites on the surface of the Moon, for the first time with sufficient resolution to see the descent stages of the lunar modules, scientific instruments, and foot trails made by the astronauts.
In March 2012 Amazon founder Jeff Bezos located the F-1 engines that launched Apollo 11 into space. The engines were found below the Atlantic Ocean's surface through the use of advanced sonar scanning. His team brought at least one of the five engines to the surface. In July 2013, it was confirmed through serial numbers (2044) that F-1 engine parts brought up from the depths of the Atlantic Ocean were from the Apollo 11 launch.
40th anniversary events.
On July 15, 2009, Life.com released a photo gallery of previously unpublished photos of the astronauts taken by "Life" photographer Ralph Morse prior to the Apollo 11 launch.
From July 16–24, 2009, NASA streamed the original mission audio on its website in real time 40 years to the minute after the events occurred.
In addition, it is in the process of restoring the video footage and has released a preview of key moments.
On July 20, 2009, the crew of Armstrong, Aldrin, and Collins met with U.S. President Barack Obama at the White House. "We expect that there is, as we speak, another generation of kids out there who are looking up at the sky and are going to be the next Armstrong, Collins and Aldrin", Obama said. "We want to make sure that NASA is going to be there for them when they want to take their journey."
The John F. Kennedy Presidential Library and Museum set up a Flash website that rebroadcasts the transmissions of Apollo 11 from launch to landing on the Moon.
A group of British scientists interviewed as part of the anniversary events reflected on the significance of the Moon landing:
It was carried out in a technically brilliant way with risks taken ... that would be inconceivable in the risk-averse world of today ... The Apollo programme is arguably the greatest technical achievement of mankind to date ... nothing since Apollo has come close the excitement that was generated by those astronauts – Armstrong, Aldrin and the 10 others who followed them.
On August 7, 2009, an act of Congress awarded the three astronauts a Congressional Gold Medal, the highest civilian award in the United States. The bill was sponsored by Florida Sen. Bill Nelson and Florida Rep. Alan Grayson.
In July 2010, air-to-ground voice recordings and film footage shot in Mission Control during the Apollo 11 powered descent and landing was re-synchronised and released for the first time.
For young readers
NASA reports
Multimedia

</doc>
<doc id="663" url="https://en.wikipedia.org/wiki?curid=663" title="Apollo 8">
Apollo 8

Apollo 8, the second human spaceflight mission in the United States Apollo space program, was launched on December 21, 1968, and became the first manned spacecraft to leave Earth orbit, reach the Earth's Moon, orbit it and return safely to Earth. The three-astronaut crew — Commander Frank Borman, Command Module Pilot James Lovell, and Lunar Module Pilot William Anders — became the first humans to travel beyond low Earth orbit, the first to see Earth as a whole planet, the first to directly see the far side of the Moon, and then the first to witness Earthrise. The 1968 mission, the third flight of the Saturn V rocket and that rocket's first manned launch, was also the first human spaceflight launch from the Kennedy Space Center, Florida, located adjacent to Cape Canaveral Air Force Station.
The mission was originally planned as Apollo 9, to be performed in early 1969 as the second test of the complete Apollo spacecraft, including the Lunar Module and the Command/Service Module in an elliptical medium Earth orbit. But when the Lunar Module proved unready to make its first test in a lower Earth orbit in December 1968, it was decided in August to fly Apollo 8 in December as a more ambitious lunar orbital flight without the Lunar Module. This meant Borman's crew was scheduled to fly two to three months sooner than originally planned, leaving them a shorter time for training and preparation, thus placing more demands than usual on their time and discipline.
Apollo 8 took three days to travel to the Moon. It orbited ten times over the course of 20 hours, during which the crew made a Christmas Eve television broadcast where they read the first 10 verses from the Book of Genesis. At the time, the broadcast was the most watched TV program ever. Apollo 8's successful mission paved the way for Apollo 11 to fulfill U.S. President John F. Kennedy's goal of landing a man on the Moon before the end of the 1960s. The Apollo 8 astronauts returned to Earth on December 27, 1968, when their spacecraft splashed down in the Northern Pacific Ocean. The crew was named "Time" magazine's "Men of the Year" for 1968 upon their return.
Crew.
Lovell was originally the CMP on the back-up crew, with Michael Collins as the prime crew's CMP. However, Collins was replaced in July 1968, after suffering a cervical disc herniation that required surgery to repair.
This crew was unique among pre-shuttle era missions in that the commander was not the most experienced member of the crew, as Lovell had flown twice before, on Gemini VII and Gemini XII. This was also the first case of the rarity of an astronaut who had commanded a spaceflight mission subsequently flying as a non-commander, as Lovell had previously commanded Gemini XII.
Backup crew.
On a lunar mission, the Command Module Pilot (CMP) was assigned the role of navigator, while the Lunar Module Pilot (LMP) was assigned the role of flight engineer, responsible for monitoring all spacecraft systems, even if the flight didn't include a Lunar Module.
Edwin Aldrin was originally the backup LMP. When Lovell was rotated to the prime crew, no one with experience on CSM-103 (the specific spacecraft used for the mission) was available, so Aldrin was moved to CMP and Fred Haise brought in as backup LMP. Neil Armstrong went on to command Apollo 11, where Aldrin was returned to the LMP position and Collins was assigned as CMP. Haise was rotated out of the crew and onto the backup crew of Apollo 11 as LMP.
Mission control.
The Earth-based mission control teams for Apollo 8 consisted of astronauts assigned to the support crew, as well as non-astronaut flight directors and their staffs. The support crew members were not trained to fly the mission, but were able to stand in for astronauts in meetings and be involved in the minutiae of mission planning, while the prime and backup crews trained. They also served as CAPCOMs during the mission. For Apollo 8, these crew members included astronauts John S. Bull, Vance D. Brand, Gerald P. Carr, and Ken Mattingly. The mission control teams on Earth rotated in three shifts, each led by a flight director. The directors for Apollo 8 included Clifford E. Charlesworth (Green team), Glynn Lunney (Black team), and Milton Windler (Maroon team).
Mission insignia.
The triangular shape of the insignia symbolizes the shape of the Apollo Command Module (CM). It shows a red figure-8 looping around the Earth and Moon representing the mission number as well as the circumlunar nature of the mission. On the red number 8 are the names of the three astronauts.
The initial design of the insignia was developed by Jim Lovell. Lovell reportedly sketched the initial design while riding in the backseat of a T-38 flight from California to Houston, shortly after learning of the re-designation of the flight to become a lunar-orbital mission. The graphic design of the insignia was done by Houston artist and animator William Bradley.
Planning.
Apollo 4 and Apollo 6 had been "A" missions, unmanned tests of the Saturn V launch vehicle using an unmanned Block I production model of the Apollo Command and Service Module in Earth orbit. , scheduled for October 1968, would be a manned Earth-orbit flight of the CSM, completing the objectives for Mission "C".
Further missions depended on the readiness of the Lunar Module. Apollo 8 was planned as the "D" mission, to test the LM in a low Earth orbit in December 1968 by James McDivitt, David Scott and Russell Schweickart, while Borman's crew would fly the "E" mission, a more rigorous LM test in an elliptical medium Earth orbit as Apollo 9, in early 1969.
But production of the LM fell behind schedule, and when Apollo 8's LM arrived at Cape Canaveral in June 1968, significant defects were discovered, leading Grumman, the lead contractor for the LM, to predict that the first mission-ready LM would not be ready until at least February 1969. This would mean delaying the "D" and subsequent missions, endangering the program's goal of a lunar landing before the end of 1969.
George Low, the Manager of the Apollo Spacecraft Program Office, proposed a solution in August to keep the program on track despite the LM delay. Since the Command/Service Module (CSM) would be ready three months before the Lunar Module, a CSM-only mission could be flown in December 1968. Instead of just repeating the "C" mission flight of Apollo 7, this CSM could be sent all the way to the Moon, with the possibility of entering a lunar orbit. The new mission would also allow NASA to test lunar landing procedures that would otherwise have to wait until Apollo 10, the scheduled "F" mission. This also meant that the medium Earth orbit "E" mission could be dispensed with. The net result was that only the "D" mission had to be delayed.
Almost every senior manager at NASA agreed with this new mission, citing both confidence in the hardware and personnel, and the potential for a significant morale boost provided by a circumlunar flight. The only person who needed some convincing was James E. Webb, the NASA administrator. With the rest of his agency in support of the new mission, Webb eventually approved the mission change. The mission was officially changed from a "D" mission to a "C-Prime" lunar-orbit mission, but was still referred to in press releases as an Earth-orbit mission at Webb's direction. No public announcement was made about the change in mission until November 12, three weeks after Apollo 7's successful Earth-orbit mission and less than 40 days before launch.
With the change in mission for Apollo 8, Director of Flight Crew Operations Deke Slayton decided to swap the crews of the D and E missions. This swap also meant a swap of spacecraft, requiring Borman's crew to use CSM-103, while McDivitt's crew would use CSM-104.
On September 9, the crew entered the simulators to begin their preparation for the flight. By the time the mission flew, the crew had spent seven hours training for every actual hour of flight. Although all crew members were trained in all aspects of the mission, it was necessary to specialize. Borman, as commander, was given training on controlling the spacecraft during the re-entry. Lovell was trained on navigating the spacecraft in case communication was lost with the Earth. Anders was placed in charge of checking that the spacecraft was in working order.
Added pressure on the Apollo program to make its 1969 landing goal was provided by the Soviet Union's flight of some living creatures, including Russian tortoises, in a cislunar loop around the Moon on Zond 5 and return to Earth on September 21. There was speculation within NASA and the press that they might be preparing to launch cosmonauts on a similar circumlunar mission before the end of 1968.
The Apollo 8 crew, now living in the crew quarters at Kennedy Space Center, received a visit from Charles Lindbergh and his wife, Anne Morrow Lindbergh, the night before the launch. They talked about how, before his 1927 flight, Lindbergh had used a piece of string to measure the distance from New York City to Paris on a globe and from that calculated the fuel needed for the flight. The total was a tenth of the amount that the Saturn V would burn every second. The next day, the Lindberghs watched the launch of Apollo 8 from a nearby dune.
Saturn V.
The Saturn V rocket used by Apollo 8 was designated SA-503, or the "03rd" model of the Saturn V ("5") Rocket to be used in the Saturn-Apollo ("SA") program. When it was erected in the Vertical Assembly Building on December 20, 1967, it was thought that the rocket would be used for an unmanned Earth-orbit test flight carrying a boilerplate Command/Service Module. Apollo 6 had suffered several major problems during its April 1968 flight, including severe pogo oscillation during its first stage, two second stage engine failures, and a third stage that failed to reignite in orbit. Without assurances that these problems had been rectified, NASA administrators could not justify risking a manned mission until additional unmanned test flights proved that the Saturn V was ready.
Teams from the Marshall Space Flight Center (MSFC) went to work on the problems. Of primary concern was the pogo oscillation, which would not only hamper engine performance, but could exert significant g-forces on a crew. A task force of contractors, NASA agency representatives, and MSFC researchers concluded that the engines vibrated at a frequency similar to the frequency at which the spacecraft itself vibrated, causing a resonance effect that induced oscillations in the rocket. A system using helium gas to absorb some of these vibrations was installed.
Of equal importance was the failure of three engines during flight. Researchers quickly determined that a leaking hydrogen fuel line ruptured when exposed to vacuum, causing a loss of fuel pressure in engine two. When an automatic shutoff attempted to close the liquid hydrogen valve and shut down engine two, it accidentally shut down engine three's liquid oxygen due to a miswired connection. As a result, engine three failed within one second of engine two's shutdown. Further investigation revealed the same problem for the third-stage engine—a faulty igniter line. The team modified the igniter lines and fuel conduits, hoping to avoid similar problems on future launches.
The teams tested their solutions in August 1968 at the Marshall Space Flight Center. A Saturn stage IC was equipped with shock absorbing devices to demonstrate the team's solution to the problem of pogo oscillation, while a Saturn Stage II was retrofitted with modified fuel lines to demonstrate their resistance to leaks and ruptures in vacuum conditions. Once NASA administrators were convinced that the problems were solved, they gave their approval for a manned mission using SA-503.
The Apollo 8 spacecraft was placed on top of the rocket on September 21 and the rocket made the slow 3-mile (5 km) journey to the launch pad on October 9. Testing continued all through December until the day before launch, including various levels of readiness testing from December 5 through 11. Final testing of modifications to address the problems of pogo oscillation, ruptured fuel lines, and bad igniter lines took place on December 18, a mere three days before the scheduled launch.
Mission.
Parameter summary.
As the first manned spacecraft to orbit more than one celestial body, Apollo 8's profile had two different sets of orbital parameters, separated by a translunar injection maneuver.
Apollo lunar missions would begin with a nominal circular Earth parking orbit. Apollo 8 was launched into an initial orbit with an apogee of and a perigee of , with an inclination of 32.51° to the Equator, and an orbital period of 88.19 minutes. Propellant venting increased the apogee by over the 2 hours, 44 minutes and 30 seconds spent in the parking orbit.
This was followed by a Trans-Lunar Injection (TLI) burn of the S-IVB third stage for 318 seconds, accelerating the spacecraft from an orbital velocity of to the injection velocity of , which set a record for the highest speed, relative to Earth, that humans had ever traveled. This speed was slightly less than the Earth's escape velocity of , but put Apollo 8 into an elongated elliptical Earth orbit, to a point where the Moon's gravity would capture it.
The standard lunar orbit for Apollo missions was planned as a nominal circular orbit above the Moon's surface. Initial lunar orbit insertion was an ellipse with a perilune of and an apolune of , at an inclination of 12° from the lunar equator. This was then circularized at by , with an orbital period of 128.7 minutes. The effect of lunar mass concentrations ("masscons") on the orbit was found to be greater than initially predicted; over the course of the twenty-hour mission, the orbit was perturbated to by .
Apollo 8 achieved a maximum distance from Earth of .
Launch and trans-lunar injection.
Apollo 8 launched at 7:51:00 a.m. Eastern Standard Time on December 21, 1968, using the Saturn V's three stages to achieve Earth orbit. The S-IC first stage impacted the Atlantic Ocean at and the S-II second stage at . The S-IVB third stage injected the craft into Earth orbit, but remained attached to later perform the trans-lunar injection (TLI) burn that put the spacecraft on a trajectory to the Moon.
The Titan II launch vehicle used for the Gemini program had been notoriously rough-riding, and technicians promised the astronauts that the Saturn V, which was designed for the Apollo program rather than adapted from a missile, would have a much smoother ride. Lovell and Borman, both Gemini veterans, found this promise did not disappoint. During liftoff, they reported feeling nothing but a dull, muted rumble in the distance.
Once the vehicle reached Earth orbit, both the crew and Houston flight controllers spent the next 2 hours and 38 minutes checking that the spacecraft was in proper working order and ready for TLI. The proper operation of the S-IVB third stage of the rocket was crucial: in the last unmanned test, it had failed to re-ignite for TLI.
During the flight, three fellow astronauts served on the ground as Capsule Communicators (usually referred to as "CAPCOMs") on a rotating schedule. The CAPCOMs were the only people who regularly communicated with the crew. Michael Collins was the first CAPCOM on duty and at 2 hours, 27 minutes and 22 seconds after launch radioed, "Apollo 8. You are Go for TLI." This communication signified that Mission Control had given official permission for Apollo 8 to go to the Moon. Over the next 12 minutes before the TLI burn, the Apollo 8 crew continued to monitor the spacecraft and the S-IVB. The engine ignited on time and performed the TLI burn perfectly.
After the S-IVB had performed its required tasks, it was jettisoned. The crew then rotated the spacecraft to take some photographs of the spent stage and then practiced flying in formation with it. As the crew rotated the spacecraft, they had their first views of the Earth as they moved away from it. This marked the first time humans could view the whole Earth at once. Borman became worried that the S-IVB was staying too close to the Command/Service Module and suggested to Mission Control that the crew perform a separation maneuver. Mission Control first suggested pointing the spacecraft towards Earth and using the Reaction Control System (RCS) thrusters on the Service Module (SM) to add away from the Earth, but Borman did not want to lose sight of the S-IVB. After discussion, the crew and Mission Control decided to burn in this direction, but at instead. These discussions put the crew an hour behind their flight plan.
Five hours after launch, Mission Control sent a command to the S-IVB booster to vent its remaining fuel through its engine bell to change the booster's trajectory. This S-IVB would then pass the Moon and enter into a solar orbit, posing no further hazard to Apollo 8. The S-IVB subsequently went into a solar orbit with an inclination of 23.47° from the plane of the ecliptic, and an orbital period of 340.80 days. After the insertion into trans-Lunar orbit, the Saturn IVB third stage became a object. It will continue to orbit the Sun for many years.
The Apollo 8 crew were the first humans to pass through the Van Allen radiation belts, which extend up to from Earth. Scientists predicted that passing through the belts quickly at the spacecraft's high speed would cause a radiation dosage of no more than a chest X-ray, or 1 milligray (during a year, the average human receives a dose of 2 to 3 mGy). To record the actual radiation dosages, each crew member wore a Personal Radiation Dosimeter that transmitted data to Earth as well as three passive film dosimeters that showed the cumulative radiation experienced by the crew. By the end of the mission, the crew experienced an average radiation dose of 1.6 mGy.
Lunar trajectory.
Jim Lovell's main job as Command Module Pilot was as navigator. Although Mission Control performed all the actual navigation calculations, it was necessary to have a crew member serving as navigator so that the crew could successfully return to Earth in case of communication loss with Mission Control. Lovell navigated by star sightings using a sextant built into the spacecraft, measuring the angle between a star and the Earth's (or the Moon's) horizon. This task proved to be difficult, as a large cloud of debris around the spacecraft formed by the venting S-IVB made it hard to distinguish the stars.
By seven hours into the mission, the crew was about one hour and 40 minutes behind flight plan due to the issues of moving away from the S-IVB and Lovell's obscured star sightings. The crew now placed the spacecraft into Passive Thermal Control (PTC), also known as "barbecue" roll. PTC involved the spacecraft rotating about once per hour along its long axis to ensure even heat distribution across the surface of the spacecraft. In direct sunlight, the spacecraft could be heated to over while the parts in shadow would be . These temperatures could cause the heat shield to crack or propellant lines to burst. As it was impossible to get a perfect roll, the spacecraft actually swept out a cone as it rotated. The crew had to make minor adjustments every half-hour as the cone pattern got larger and larger.
The first mid-course correction came 11 hours into the flight. Testing on the ground had shown that the Service Propulsion System (SPS) engine had a small chance of exploding when burned for long periods unless its combustion chamber was "coated" first. Burning the engine for a short period would accomplish coating. This first correction burn was only 2.4 seconds and added about velocity prograde (in the direction of travel). This change was less than the planned due to a bubble of helium in the oxidizer lines causing lower than expected propellant pressure. The crew had to use the small RCS thrusters to make up the shortfall. Two later planned mid-course corrections were canceled as the Apollo 8 trajectory was found to be perfect.
Eleven hours into the flight, the crew had been awake for over 16 hours. Before launch, NASA had decided that at least one crew member should be awake at all times to deal with any issues that might arise. Borman started the first sleep shift, but between the constant radio chatter and mechanical noises, he found sleep difficult.
About an hour after starting his sleep shift, Borman requested clearance to take a Seconal sleeping pill. However, the pill had little effect. Borman eventually fell asleep but then awoke feeling ill. He vomited twice and had a bout of diarrhea that left the spacecraft full of small globules of vomit and feces that the crew cleaned up to the best of their ability. Borman initially decided that he did not want everyone to know about his medical problems, but Lovell and Anders wanted to inform Mission Control. The crew decided to use the Data Storage Equipment (DSE), which could tape voice recordings and telemetry and dump them to Mission Control at high speed. After recording a description of Borman's illness they requested that Mission Control check the recording, stating that they "would like an evaluation of the voice comments".
The Apollo 8 crew and Mission Control medical personnel held a conference using an unoccupied second floor control room (there were two identical control rooms in Houston on the second and third floor, only one of which was used during a mission). The conference participants decided that there was little to worry about and that Borman's illness was either a 24-hour flu, as Borman thought, or a reaction to the sleeping pill. Researchers now believe that he was suffering from space adaptation syndrome, which affects about a third of astronauts during their first day in space as their vestibular system adapts to weightlessness. Space adaptation syndrome had not been an issue on previous spacecraft (Mercury and Gemini), as those astronauts were unable to move freely in the comparatively smaller cabins of those spacecraft. The increased cabin space in the Apollo Command Module afforded astronauts greater freedom of movement, contributing to symptoms of space sickness for Borman and, later, astronaut Russell Schweickart during Apollo 9.
The cruise phase was a relatively uneventful part of the flight, except for the crew checking that the spacecraft was in working order and that they were on course. During this time, NASA scheduled a television broadcast at 31 hours after launch. The Apollo 8 crew used a 2 kg camera that broadcast in black-and-white only, using a Vidicon tube. The camera had two lenses, a very wide-angle (160°) lens, and a telephoto (9°) lens.
During this first broadcast, the crew gave a tour of the spacecraft and attempted to show how the Earth appeared from space. However, difficulties aiming the narrow-angle lens without the aid of a monitor to show what it was looking at made showing the Earth impossible. Additionally, the Earth image became saturated by any bright source without proper filters. In the end, all the crew could show the people watching back on Earth was a bright blob. After broadcasting for 17 minutes, the rotation of the spacecraft took the high-gain antenna out of view of the receiving stations on Earth and they ended the transmission with Lovell wishing his mother a happy birthday.
By this time, the crew had completely abandoned the planned sleep shifts. Lovell went to sleep 32½ hours into the flight—3½ hours before he had planned to. A short while later, Anders also went to sleep after taking a sleeping pill.
The crew was unable to see the Moon for much of the outward cruise. Two factors made the Moon almost impossible to see from inside the spacecraft: three of the five windows fogging up due to out-gassed oils from the silicone sealant, and the attitude required for the PTC. It was not until the crew had gone behind the Moon that they would be able to see it for the first time.
The Apollo 8 made a second television broadcast at 55 hours into the flight. This time, the crew rigged up filters meant for the still cameras so they could acquire images of the Earth through the telephoto lens. Although difficult to aim, as they had to maneuver the entire spacecraft, the crew was able to broadcast back to Earth the first television pictures of the Earth. The crew spent the transmission describing the Earth and what was visible and the colors they could see. The transmission lasted 23 minutes.
Lunar sphere of influence.
At about 55 hours and 40 minutes into the flight, the crew of Apollo 8 became the first humans to enter the gravitational sphere of influence of another celestial body. In other words, the effect of the Moon's gravitational force on Apollo 8 became stronger than that of the Earth. At the time it happened, Apollo 8 was from the Moon and had a speed of relative to the Moon. This historic moment was of little interest to the crew since they were still calculating their trajectory with respect to the launch pad at Kennedy Space Center. They would continue to do so until they performed their last mid-course correction, switching to a reference frame based on ideal orientation for the second engine burn they would make in lunar orbit. It was only 13 hours until they would be in lunar orbit.
The last major event before Lunar Orbit Insertion (LOI) was a second mid-course correction. It was in retrograde (against direction of travel) and slowed the spacecraft down by , effectively lowering the closest distance that the spacecraft would pass the moon. At exactly 61 hours after launch, about from the Moon, the crew burned the RCS for 11 seconds. They would now pass from the lunar surface.
At 64 hours into the flight, the crew began to prepare for Lunar Orbit Insertion-1 (LOI-1). This maneuver had to be performed perfectly, and due to orbital mechanics had to be on the far side of the Moon, out of contact with the Earth. After Mission Control was polled for a "go/no go" decision, the crew was told at 68 hours, they were Go and "riding the best bird we can find". At 68 hours and 58 minutes, the spacecraft went behind the Moon and out of radio contact with the Earth.
With 10 minutes before the LOI-1, the crew began one last check of the spacecraft systems and made sure that every switch was in the correct place. At that time, they finally got their first glimpses of the Moon. They had been flying over the unlit side, and it was Lovell who saw the first shafts of sunlight obliquely illuminating the lunar surface. The LOI burn was only two minutes away, so the crew had little time to appreciate the view.
Lunar orbit.
The SPS ignited at 69 hours, 8 minutes, and 16 seconds after launch and burned for 4 minutes and 13 seconds, placing the Apollo 8 spacecraft in orbit around the Moon. The crew described the burn as being the longest four minutes of their lives. If the burn had not lasted exactly the correct amount of time, the spacecraft could have ended up in a highly elliptical lunar orbit or even flung off into space. If it lasted too long they could have struck the Moon. After making sure the spacecraft was working, they finally had a chance to look at the Moon, which they would orbit for the next 20 hours.
On Earth, Mission Control continued to wait. If the crew had not burned the engine or the burn had not lasted the planned length of time, the crew would appear early from behind the Moon. However, this time came and went without Apollo 8 reappearing. Exactly at the calculated moment, the signal was received from the spacecraft, indicating it was in a orbit about the Moon.
After reporting on the status of the spacecraft, Lovell gave the first description of what the lunar surface looked like:
Lovell continued to describe the terrain they were passing over. One of the crew's major tasks was reconnaissance of planned future landing sites on the Moon, especially one in Mare Tranquillitatis that would be the Apollo 11 landing site. The launch time of Apollo 8 had been chosen to give the best lighting conditions for examining the site. A film camera had been set up in one of the spacecraft windows to record a frame every second of the Moon below. Bill Anders spent much of the next 20 hours taking as many photographs as possible of targets of interest. By the end of the mission the crew had taken 700 photographs of the Moon and 150 of the Earth.
Throughout the hour that the spacecraft was in contact with Earth, Borman kept asking how the data for the SPS looked. He wanted to make sure that the engine was working and could be used to return early to the Earth if necessary. He also asked that they receive a "go/no go" decision before they passed behind the Moon on each orbit.
As they reappeared for their second pass in front of the Moon, the crew set up the equipment to broadcast a view of the lunar surface. Anders described the craters that they were passing over. At the end of this second orbit they performed the 11-second LOI-2 burn of the SPS to circularize the orbit to .
Through the next two orbits, the crew continued to keep check of the spacecraft and to observe and photograph the Moon. During the third pass, Borman read a small prayer for his church. He had been scheduled to participate in a service at St. Christopher's Episcopal Church near Seabrook, Texas, but due to the Apollo 8 flight he was unable to. A fellow parishioner and engineer at Mission Control, Rod Rose, suggested that Borman read the prayer which could be recorded and then replayed during the service.
In the foreword to the Millennial Edition of his novel "" Arthur C. Clarke says the crew had told him that "they had been tempted to radio back the discovery of a large black monolith", but discretion prevailed.
Earthrise.
When the spacecraft came out from behind the Moon for its fourth pass across the front, the crew witnessed "Earthrise" for the first time in human history (NASA's Lunar Orbiter 1 took the very first picture of an Earthrise from the vicinity of the Moon, on August 23, 1966). Borman saw the Earth emerging from behind the lunar horizon and called in excitement to the others, taking a black-and-white photo as he did so. In the ensuing scramble Anders took the more famous color photo, later picked by "Life" magazine as one of its hundred photos of the century. Due to the synchronous rotation of the Moon about the Earth, Earthrise is not generally visible from the lunar surface. Earthrise is generally only visible when orbiting the Moon, other than at selected places near the Moon's limb, where libration carries the Earth slightly above and below the lunar horizon.
Anders continued to take photographs while Lovell assumed control of the spacecraft so Borman could rest. Despite the difficulty resting in the cramped and noisy spacecraft, Borman was able to sleep for two orbits, awakening periodically to ask questions about their status. Borman awoke fully, however, when he started to hear his fellow crew members make mistakes. They were beginning to not understand questions and would have to ask for the answers to be repeated. Borman realized that everyone was extremely tired having not had a good night's sleep in over three days. Taking command, he ordered Anders and Lovell to get some sleep and that the rest of the flight plan regarding observing the Moon be scrubbed. At first Anders protested saying that he was fine, but Borman would not be swayed. At last Anders agreed as long as Borman would set up the camera to continue to take automatic shots of the Moon. Borman also remembered that there was a second television broadcast planned, and with so many people expected to be watching he wanted the crew to be alert. For the next two orbits Anders and Lovell slept while Borman sat at the helm. On subsequent Apollo missions, crews would avoid this situation by sleeping on the same schedule.
As they rounded the Moon for the ninth time, the second television transmission began. Borman introduced the crew, followed by each man giving his impression of the lunar surface and what it was like to be orbiting the Moon. Borman described it as being "a vast, lonely, forbidding expanse of nothing". Then, after talking about what they were flying over, Anders said that the crew had a message for all those on Earth. Each man on board read a section from the Biblical creation story from the Book of Genesis. Borman finished the broadcast by wishing a Merry Christmas to everyone on Earth. His message appeared to sum up the feelings that all three crewmen had from their vantage point in lunar orbit. Borman said, "And from the crew of Apollo 8, we close with good night, good luck, a Merry Christmas and God bless all of you—all of you on the good Earth."
The only task left for the crew at this point was to perform the Trans-Earth Injection (TEI), which was scheduled for 2½ hours after the end of the television transmission. The TEI was the most critical burn of the flight, as any failure of the SPS to ignite would strand the crew in lunar orbit, with little hope of escape. As with the previous burn, the crew had to perform the maneuver above the far side of the Moon, out of contact with Earth.
The burn occurred exactly on time. The spacecraft telemetry was reacquired as it re-emerged from behind the Moon at 89 hours, 28 minutes, and 39 seconds, the exact time calculated. When voice contact was regained, Lovell announced, "Please be informed, there is a Santa Claus", to which Ken Mattingly, the current CAPCOM, replied, "That's affirmative, you are the best ones to know." The spacecraft began its journey back to Earth on December 25, Christmas Day.
Unplanned manual re-alignment.
Later, Lovell used some otherwise idle time to do some navigational sightings, maneuvering the module to view various stars by using the computer keyboard. However, he accidentally erased some of the computer's memory, which caused the Inertial Measurement Unit (IMU) to think the module was in the same relative position it had been in before lift-off and fire the thrusters to "correct" the module's attitude.
Once the crew realized why the computer had changed the module's attitude, they realized they would have to re-enter data that would tell the computer its real position. It took Lovell ten minutes to figure out the right numbers, using the thrusters to get the stars Rigel and Sirius aligned, and another 15 minutes to enter the corrected data into the computer.
Sixteen months later, Lovell would once again have to perform a similar manual re-alignment, under more critical conditions, during the Apollo 13 mission, after that module's IMU had to be turned off to conserve energy. In his 1994 book, "Lost Moon: The Perilous Voyage of Apollo 13", Lovell wrote, "My training [on Apollo 8] came in handy!" In that book he dismissed the incident as a "planned experiment", requested by the ground crew. In subsequent interviews Lovell has acknowledged that the incident was an accident, caused by his mistake.
Cruise back to Earth and re-entry.
The cruise back to Earth was mostly a time for the crew to relax and monitor the spacecraft. As long as the trajectory specialists had calculated everything correctly, the spacecraft would re-enter two-and-half days after TEI and splashdown in the Pacific.
On Christmas afternoon, the crew made their fifth television broadcast. This time they gave a tour of the spacecraft, showing how an astronaut lived in space. When they finished broadcasting they found a small present from Deke Slayton in the food locker: a real turkey dinner with stuffing, in the same kind of pack that the troops in Vietnam received. Another Slayton surprise was a gift of three miniature bottles of brandy, that Borman ordered the crew to leave alone until after they landed. They remained unopened, even years after the flight. There were also small presents to the crew from their wives. The next day, at about 124 hours into the mission, the sixth and final TV transmission showed the mission's best video images of the earth, in a four-minute broadcast.
After two uneventful days the crew prepared for re-entry. The computer would control the re-entry and all the crew had to do was put the spacecraft in the correct attitude, blunt end forward. If the computer broke down, Borman would take over.
Once the Command Module was separated from the Service Module, the astronauts were committed to re-entry. Six minutes before they hit the top of the atmosphere, the crew saw the Moon rising above the Earth's horizon, just as had been predicted by the trajectory specialists. As they hit the thin outer atmosphere they noticed it was becoming hazy outside as glowing plasma formed around the spacecraft. The spacecraft started slowing down and the deceleration peaked at 6 g (59 m/s). With the computer controlling the descent by changing the attitude of the spacecraft, Apollo 8 rose briefly like a skipping stone before descending to the ocean. At the drogue parachute stabilized the spacecraft and was followed at by the three main parachutes. The spacecraft splashdown position was officially reported as in the North Pacific Ocean south of Hawaii.
When it hit the water, the parachutes dragged the spacecraft over and left it upside down, in what was termed Stable 2 position. About six minutes later the Command Module was righted into its normal apex-up splashdown orientation by the inflatable bag uprighting system. As they were buffeted by a swell, Borman was sick, waiting for the three flotation balloons to right the spacecraft. It was 43 minutes after splashdown before the first frogman from the USS "Yorktown" arrived, as the spacecraft had landed before sunrise. Forty-five minutes later, the crew was safe on the deck of the aircraft carrier.
Historical importance.
Apollo 8 came at the end of 1968, a year that had seen much upheaval in the United States and most of the world. Even though the year saw political assassinations, political unrest in the streets of Europe and America, and the Prague Spring, "Time" magazine chose the crew of Apollo 8 as their Men of the Year for 1968, recognizing them as the people who most influenced events in the preceding year. They had been the first people ever to leave the gravitational influence of the Earth and orbit another celestial body. They had survived a mission that even the crew themselves had rated as only having a fifty-fifty chance of fully succeeding. The effect of Apollo 8 can be summed up by a telegram from a stranger, received by Borman after the mission, that simply stated, "Thank you Apollo 8. You saved 1968."
One of the most famous aspects of the flight was the Earthrise picture that was taken as they came around for their fourth orbit of the Moon. This was the first time that humans had taken such a picture whilst actually behind the camera, and it has been credited with a role in inspiring the first Earth Day in 1970. It was selected as the first of "Life" magazine's "100 Photographs That Changed the World". Apollo 11 astronaut Michael Collins said, "Eight's momentous historic significance was foremost"; while many space historians, such as Robert K. Poole, see Apollo 8 as the most historically significant of all the Apollo missions.
The mission was the most widely covered by the media since the first American orbital flight, Mercury-Atlas 6 by John Glenn in 1962. There were 1200 journalists covering the mission, with the BBC coverage being broadcast in 54 countries in 15 different languages. The Soviet newspaper "Pravda" featured a quote from Boris Nikolaevich Petrov, Chairman of the Soviet Interkosmos program, who described the flight as an "outstanding achievement of American space sciences and technology". It is estimated that a quarter of the people alive at the time saw—either live or delayed—the Christmas Eve transmission during the ninth orbit of the Moon. The Apollo 8 broadcasts won an Emmy Award, the highest honor given by the Academy of Television Arts & Sciences.
Madalyn Murray O'Hair, an atheist, later caused controversy by bringing a lawsuit against NASA over the reading from Genesis. O'Hair wished the courts to ban American astronauts—who were all government employees—from public prayer in space. Though the case was rejected by the Supreme Court of the United States for lack of jurisdiction, it caused NASA to be skittish about the issue of religion throughout the rest of the Apollo program. Buzz Aldrin, on Apollo 11, self-communicated Presbyterian Communion on the surface of the Moon after landing; he refrained from mentioning this publicly for several years, and only obliquely referred to it at the time.
In 1969, the United States Postal Service issued a postage stamp (Scott catalogue #1371) commemorating the Apollo 8 flight around the Moon. The stamp featured a detail of the famous photograph of the Earthrise over the Moon taken by Anders on Christmas Eve, and the words, "In the beginning God ..." Just 18 days after the crew's return to Earth, they were featured during the 1969 Super Bowl pre-game show reciting the Pledge of Allegiance prior to the national anthem being performed by Anita Bryant.
Spacecraft location.
In January 1970, the spacecraft was delivered to Osaka, Japan, for display in the U.S. pavilion at Expo '70. It is now displayed at the Chicago Museum of Science and Industry, along with a collection of personal items from the flight donated by Lovell and the space suit worn by Frank Borman. Jim Lovell's Apollo 8 space suit is on public display in the Visitor Center at NASA's Glenn Research Center. Bill Anders's space suit is on display at the Science Museum in London, United Kingdom.
In film.
Apollo 8's historic mission has been shown and referred to in several forms, both documentary and fiction. The various television transmissions and 16 mm footage shot by the crew of Apollo 8 was compiled and released by NASA in the 1969 documentary, "Debrief: Apollo 8", which was hosted by Burgess Meredith. In addition, Spacecraft Films released, in 2003, a three-disc DVD set containing all of NASA's TV and 16 mm film footage related to the mission including all TV transmissions from space, training and launch footage, and motion pictures taken in flight. Portions of the Apollo 8 Mission can be seen in the 1989 documentary "For All Mankind", which won the Grand Jury Prize Documentary at the Sundance Film Festival. The Apollo 8 mission was well-covered in the 2007 British documentary "In the Shadow of the Moon".
Portions of the Apollo 8 mission are dramatized in the 1998 miniseries "From the Earth to the Moon" episode "1968". The S-IVB stage of Apollo 8 was also portrayed as the location of an alien device in the 1970 "UFO" episode "Conflict".
At the Kennedy Space Center Visitor Complex's Apollo/Saturn V Center, the history of the U.S. space program leading up to the launch of Apollo 8 is the subject of a multi-screen multimedia presentation which also features the actual control panels used in the Firing Room for the launch.

</doc>
<doc id="664" url="https://en.wikipedia.org/wiki?curid=664" title="Astronaut">
Astronaut

An astronaut or cosmonaut is a person trained by a human spaceflight program to command, pilot, or serve as a crew member of a spacecraft. Although generally reserved for professional space travelers, the terms are sometimes applied to anyone who travels into space, including scientists, politicians, journalists, and tourists.
Starting in the 1950s up to 2002, astronauts were sponsored and trained exclusively by governments, either by the military or by civilian space agencies. With the suborbital flight of the privately funded SpaceShipOne in 2004, a new category of astronaut was created: the commercial astronaut.
Definition.
The criteria for what constitutes human spaceflight vary. The Fédération Aéronautique Internationale (FAI) Sporting Code for astronautics recognizes only flights that exceed an altitude of. In the United States, professional, military, and commercial astronauts who travel above an altitude of are awarded astronaut wings.
, a total of 532 people from 36 countries have reached or more in altitude, of which 529 reached low Earth orbit or beyond.
Of these, 24 people have traveled beyond Low Earth orbit, to either lunar or trans-lunar orbit or to the surface of the moon; three of the 24 did so twice: Jim Lovell, John Young and Eugene Cernan. The three astronauts who have not reached low Earth orbit are spaceplane pilots Joe Walker, Mike Melvill, and Brian Binnie.
, under the U.S. definition 538 people qualify as having reached space, above altitude. Of eight X-15 pilots who exceeded in altitude, only one exceeded 100 kilometers (about 62 miles).
Space travelers have spent over 41,790 man-days (114.5 man-years) in space, including over 100 astronaut-days of spacewalks.
As of 2008, the man with the longest cumulative time in space is Sergei K. Krikalev, who has spent 803 days, 9 hours and 39 minutes, or 2.2 years, in space.
Peggy A. Whitson holds the record for the most time in space by a woman, 377 days.
Terminology.
In 1959, when both the United States and Soviet Union were planning, but had yet to launch humans into space, NASA Administrator T. Keith Glennan and his Deputy Administrator, Dr. Hugh Dryden, discussed whether spacecraft crew members should be called "astronauts" or "cosmonauts". Dryden preferred "cosmonaut", on the grounds that flights would occur in the "cosmos" (near space), while the "astro" prefix suggested flight to the stars. Most NASA Space Task Group members preferred "astronaut", which survived by common usage as the preferred American term. When the Soviet Union launched the first man into space, Yuri Gagarin in 1961, they chose a term which anglicizes to "cosmonaut".
English.
In English-speaking nations, a professional space traveler is called an "astronaut". The term derives from the Greek words "ástron" (ἄστρον), meaning "star", and "nautes" (ναύτης), meaning "sailor". The first known use of the term "astronaut" in the modern sense was by Neil R. Jones in his short story "The Death's Head Meteor" in 1930. The word itself had been known earlier. For example, in Percy Greg's 1880 book "Across the Zodiac", "astronaut" referred to a spacecraft. In "Les Navigateurs de l'Infini" (1925) of J.-H. Rosny aîné, the word "astronautique" (astronautic) was used. The word may have been inspired by "aeronaut", an older term for an air traveler first applied (in 1784) to balloonists. An early use in a non-fiction publication is Eric Frank Russell's poem "The Astronaut" in the November 1934 "Bulletin of the British Interplanetary Society".
The first known formal use of the term astronautics in the scientific community was the establishment of the annual International Astronautical Congress in 1950 and the subsequent founding of the International Astronautical Federation the following year.
NASA applies the term astronaut to any crew member aboard NASA spacecraft bound for Earth orbit or beyond. NASA also uses the term as a title for those selected to join its Astronaut Corps. The European Space Agency similarly uses the term astronaut for members of its Astronaut Corps.
Russian.
By convention, an astronaut employed by the Russian Federal Space Agency (or its Soviet predecessor) is called a "cosmonaut" in English texts. The word is an anglicisation of the Russian word "kosmonavt" ( ), one who works in space outside the Earth's atmosphere, a space traveler, which derives from the Greek words "kosmos" (κόσμος), meaning "universe", and "nautes" (ναύτης), meaning "sailor". Other countries of the former Eastern Bloc use variations of the Russian word "kosmonavt", such as the Polish "kosmonauta".
Coinage of the term "kosmonavt" has been credited to Soviet aeronautics pioneer Mikhail Tikhonravov (1900–1974). The first cosmonaut was Soviet Air Force pilot Yuri Gagarin, also the first person in space. Valentina Tereshkova, a Russian factory worker, was the first woman in space, as well as the first civilian among the Soviet cosmonaut or NASA astronaut corps to make a spaceflight. On March 14, 1995, Norman Thagard became the first American to ride to space on board a Russian launch vehicle, and thus became the first "American cosmonaut".
Chinese.
Official English-language texts issued by the government of China use "astronaut" while texts in Russian use космонавт ("cosmonaut"). In official Chinese-language texts, "yǔ háng yuán" (, "space navigating personnel") is used for astronauts and cosmonauts, and "háng tiān yuán" (, "space navigating personnel") is used for Chinese astronauts. The phrase "tài kōng rén" (, "spaceman") is often used in Hong Kong and Taiwan.
The term "taikonaut" is used by some English-language news media organizations for professional space travelers from China. The word has featured in the Longman and Oxford English dictionaries, the latter of which describes it as "a hybrid of the Chinese term "taikong" (space) and the Greek "naut" (sailor)"; the term became more common in 2003 when China sent its first astronaut Yang Liwei into space aboard the "Shenzhou 5" spacecraft. This is the term used by Xinhua News Agency in the English version of the Chinese "People's Daily" since the advent of the Chinese space program. The origin of the term is unclear; as early as May 1998, Chiew Lee Yih () from Malaysia, used it in newsgroups.
Other terms.
With the rise of space tourism, NASA and the Russian Federal Space Agency agreed to use the term "spaceflight participant" to distinguish those space travelers from professional astronauts on missions coordinated by those two agencies.
While no nation other than the Russian Federation (and previously the former Soviet Union), the United States, and China have launched a manned spacecraft, several other nations have sent people into space in cooperation with one of these countries. Inspired partly by these missions, other synonyms for astronaut have entered occasional English usage. For example, the term "spationaut" (French spelling: "spationaute") is sometimes used to describe French space travelers, from the Latin word "spatium" for "space", the Malay term "angkasawan" was used to describe participants in the Angkasawan program, and the Indian Space Research Organisation hope to launch a spacecraft in 2018 that would carry "vyomanauts", coined from the Sanskrit word for space.
Space travel milestones.
The first human in space was Soviet Yuri Gagarin, who was launched on April 12, 1961 aboard Vostok 1 and orbited around the Earth for 108 minutes. The first woman in space was Soviet Valentina Tereshkova, who launched on June 16, 1963 aboard Vostok 6 and orbited Earth for almost three days.
Alan Shepard became the first American and second person in space on May 5, 1961 on a 15-minute sub-orbital flight. The first American woman in space was Sally Ride, during Space Shuttle Challenger's mission STS-7, on June 18, 1983. In 1992 Mae Jemison became the first African American woman to travel in space aboard STS-47.
Cosmonaut Alexei Leonov was the first person to conduct an extravehicular activity (EVA), (commonly called a "spacewalk"), on March 18, 1965, on the Soviet Union's Voskhod 2 mission. This was followed two and a half months later by astronaut Ed White who made the first American EVA on NASA's Gemini 4 mission.
The first manned mission to orbit the Moon, "Apollo 8", included American William Anders who was born in Hong Kong, making him the first Asian-born astronaut in 1968.
The Soviet Union, through its Intercosmos program, allowed people from other "socialist" (i.e. Warsaw Pact and other Soviet-allied) countries to fly on its missions, with the notable exception of France participating in Soyuz TM-7. An example is Czechoslovak Vladimír Remek, the first cosmonaut from a country other than the Soviet Union or the United States, who flew to space in 1978 on a Soyuz-U rocket.
On July 23, 1980, Pham Tuan of Vietnam became the first Asian in space when he flew aboard Soyuz 37. Also in 1980, Cuban Arnaldo Tamayo Méndez became the first person of Hispanic and black African descent to fly in space, and in 1983, Guion Bluford became the first African American to fly into space. In April 1985, Taylor Wang became the first ethnic Chinese person in space. The first person born in Africa to fly in space was Patrick Baudry (France), in 1985. In 1985, Saudi Arabian Prince Sultan Bin Salman Bin AbdulAziz Al-Saud became the first Arab Muslim astronaut in space. In 1988, Abdul Ahad Mohmand became the first Afghan to reach space, spending nine days aboard the Mir space station.
With the larger number of seats available on the Space Shuttle, the U.S. began taking international astronauts. In 1983, Ulf Merbold of West Germany became the first non-US citizen to fly in a US spacecraft. In 1984, Marc Garneau became the first of 8 Canadian astronauts to fly in space (through 2010).
In 1985, Rodolfo Neri Vela became the first Mexican-born person in space. In 1991, Helen Sharman became the first Briton to fly in space.
In 2002, Mark Shuttleworth became the first citizen of an African country to fly in space, as a paying spaceflight participant. In 2003, Ilan Ramon became the first Israeli to fly in space, although he died during a re-entry accident.
On October 15, 2003, Yang Liwei became China's first astronaut on the Shenzhou 5 spacecraft.
Age milestones.
The youngest person to fly in space is Gherman Titov, who was 25 years old when he flew Vostok 2. (Titov was also the first person to suffer space sickness).
The oldest person who has flown in space is John Glenn, who was 77 when he flew on STS-95.
Duration and distance milestones.
The longest stay in space thus far has been 438 days, by Russian Valeri Polyakov.
As of 2006, the most spaceflights by an individual astronaut is seven, a record held by both Jerry L. Ross and Franklin Chang-Diaz. The farthest distance from Earth an astronaut has traveled was , when Jim Lovell, Jack Swigert, and Fred Haise went around the Moon during the Apollo 13 emergency.
Civilian and non-government milestones.
The first civilian in space was Valentina Tereshkova aboard Vostok 6 (she also became the first woman in space on that mission).
Tereshkova was only honorarily inducted into the USSR's Air Force, which had no female pilots whatsoever at that time. A month later, Joseph Albert Walker became the first American civilian in space when his X-15 Flight 90 crossed the line, qualifying him by the international definition of spaceflight. Walker had joined the US Army Air Force but was not a member during his flight. 
The first people in space who had never been a member of any country's armed forces were both Konstantin Feoktistov and Boris Yegorov aboard Voskhod 1.
The first non-governmental space traveler was Byron K. Lichtenberg, a researcher from the Massachusetts Institute of Technology who flew on STS-9 in 1983. In December 1990, Toyohiro Akiyama became the first paying space traveler as a reporter for Tokyo Broadcasting System, a visit to Mir as part of an estimated $12 million (USD) deal with a Japanese TV station, although at the time, the term used to refer to Akiyama was "Research Cosmonaut". Akiyama suffered severe space sickness during his mission, which affected his productivity.
The first self-funded space tourist was Dennis Tito on board the Russian spacecraft Soyuz TM-3 on April 28, 2001.
Self-funded travelers.
The first person to fly on an entirely privately funded mission was Mike Melvill, piloting SpaceShipOne flight 15P on a suborbital journey, although he was a test pilot employed by Scaled Composites and not an actual paying space tourist. Seven others have paid the Russian Space Agency to fly into space:
Training.
The first NASA astronauts were selected for training in 1959. Early in the space program, military jet test piloting and engineering training were often cited as prerequisites for selection as an astronaut at NASA, although neither John Glenn nor Scott Carpenter (of the Mercury Seven) had any university degree, in engineering or any other discipline at the time of their selection. Selection was initially limited to military pilots. The earliest astronauts for both America and the USSR tended to be jet fighter pilots, and were often test pilots.
Once selected, NASA astronauts go through twenty months of training in a variety of areas, including training for extravehicular activity in a facility such as NASA's Neutral Buoyancy Laboratory. Astronauts-in-training may also experience short periods of weightlessness in aircraft called the "vomit comet", the nickname given to a pair of modified KC-135s (retired in 2000 and 2004 respectively, and replaced in 2005 with a C-9) which perform parabolic flights. Astronauts are also required to accumulate a number of flight hours in high-performance jet aircraft. This is mostly done in T-38 jet aircraft out of Ellington Field, due to its proximity to the Johnson Space Center. Ellington Field is also where the Shuttle Training Aircraft is maintained and developed, although most flights of the aircraft are done out of Edwards Air Force Base.
Mission Specialist Educators, or "Educator Astronauts", were first selected in 2004, and as of 2007, there are three NASA Educator astronauts: Joseph M. Acaba, Richard R. Arnold, and Dorothy Metcalf-Lindenburger.
NASA candidacy requirements.
Mission Specialist Educator.
Barbara Morgan, selected as back-up teacher to Christa McAuliffe in 1985, is considered to be the first Educator astronaut by the media, but she trained as a mission specialist.
The Educator Astronaut program is a successor to the Teacher in Space program from the 1980s.
Health risks of space travel.
Astronauts are susceptible to a variety of health risks including decompression sickness, barotrauma, immunodeficiencies, loss of bone and muscle, loss of eyesight, orthostatic intolerance, sleep disturbances, and radiation injury. A variety of large scale medical studies are being conducted in space via the National Space and Biomedical Research Institute (NSBRI) to address these issues. Prominent among these is the Advanced Diagnostic Ultrasound in Microgravity Study in which astronauts (including former ISS commanders Leroy Chiao and Gennady Padalka) perform ultrasound scans under the guidance of remote experts to diagnose and potentially treat hundreds of medical conditions in space. This study's techniques are now being applied to cover professional and Olympic sports injuries as well as ultrasound performed by non-expert operators in medical and high school students. It is anticipated that remote guided ultrasound will have application on Earth in emergency and rural care situations, where access to a trained physician is often rare.
On December 31, 2012, a NASA-supported study reported that manned spaceflight may harm the brain and accelerate the onset of Alzheimer's disease.
In October 2015, the NASA Office of Inspector General issued a health hazards report related to space exploration, including a human mission to Mars.
Food and drink.
An astronaut on the International Space Station requires about 0.83 kilograms (1.83 pounds) weight of food inclusive of food packaging per meal each day. (The packaging for each meal weighs around 0.12 kilograms - 0.27 pounds) Longer-duration missions require more food.
Shuttle astronauts worked with nutritionists to select menus that appeal to their individual tastes. Five months before flight, menus are selected and analyzed for nutritional content by the shuttle dietician. Foods are tested to see how they will react in a reduced gravity environment. Caloric requirements are determined using a basal energy expenditure (BEE) formula.
On Earth, the average American uses about 35 gallons (132 liters) of water every day. On board the ISS astronauts limit water use to only about three gallons (11 liters) per day.
Insignia.
In Russia, cosmonauts are awarded Pilot-Cosmonaut of the Russian Federation upon completion of their missions, often accompanied with the award of Hero of the Russian Federation. This follows the practice established in the USSR where cosmonauts were usually awarded the title Hero of the Soviet Union.
At NASA, those who complete astronaut candidate training receive a silver lapel pin. Once they have flown in space, they receive a gold pin. U.S. astronauts who also have active-duty military status receive a special qualification badge, known as the Astronaut Badge, after participation on a spaceflight. The United States Air Force also presents an Astronaut Badge to its pilots who exceed in altitude.
Deaths.
Eighteen astronauts (fourteen men and four women) have lost their lives during four space flights. By nationality, thirteen were American (including one of Indian origin), four were Russian (Soviet Union), and one was Israeli.
Eleven people (all men) have lost their lives training for spaceflight: eight Americans and three Russians. Six of these were in crashes of training jet aircraft, one drowned during water recovery training, and four were due to fires in pure oxygen environments.
The Space Mirror Memorial, which stands on the grounds of the John F. Kennedy Space Center Visitor Complex, commemorates the lives of the men and women who have died during spaceflight and during training in the space programs of the United States. In addition to twenty NASA career astronauts, the memorial includes the names of a U.S. Air Force X-15 test pilot, a U.S. Air Force officer who died while training for a then-classified military space program, and a civilian spaceflight participant.

</doc>
<doc id="665" url="https://en.wikipedia.org/wiki?curid=665" title="A Modest Proposal">
A Modest Proposal

A Modest Proposal for Preventing the Children of Poor People From Being a Burthen to Their Parents or Country, and for Making Them Beneficial to the Publick, commonly referred to as A Modest Proposal, is a Juvenalian satirical essay written and published anonymously by Jonathan Swift in 1729. Swift suggests that the impoverished Irish might ease their economic troubles by selling their children as food for rich gentlemen and ladies. This satirical hyperbole mocks heartless attitudes towards the poor, as well as British policy toward Ireland in general.
In English writing, the phrase "a modest proposal" is now conventionally an allusion to this style of straight-faced satire.
Details.
Swift goes to great lengths to support his argument, including a list of possible preparation styles for the children, and calculations showing the financial benefits of his suggestion. He uses methods of argument throughout his essay which lampoon the then-influential William Petty and the social engineering popular among followers of Francis Bacon. These lampoons include appealing to the authority of "a very knowing American of my acquaintance in London" and "the famous Psalmanazar, a native of the island Formosa" (who had already confessed to "not" being from Formosa in 1706). This essay is widely held to be one of the greatest examples of sustained irony in the history of the English language. Much of its shock value derives from the fact that the first portion of the essay describes the plight of starving beggars in Ireland, so that the reader is unprepared for the surprise of Swift's solution when he states, "A young healthy child well nursed, is, at a year old, a most delicious nourishing and wholesome food, whether stewed, roasted, baked, or boiled; and I make no doubt that it will equally serve in a fricassee, or a ragout."
In the tradition of Roman satire, Swift introduces the reforms he is actually suggesting by paralipsis:
Population solutions.
George Wittkowsky argued that Swift’s main target in "A Modest Proposal" was not the conditions in Ireland, but rather the can-do spirit of the times that led people to devise a number of illogical schemes that would purportedly solve social and economic ills. Swift was especially insulted by projects that tried to fix population and labour issues with a simple cure-all solution. A memorable example of these sorts of schemes "involved the idea of running the poor through a joint-stock company". In response, Swift's "Modest Proposal" was "a burlesque of projects concerning the poor" that were in vogue during the early 18th century.
"A Modest Proposal" also targets the calculating way people perceived the poor in designing their projects. The pamphlet targets reformers who "regard people as commodities". In the piece, Swift adopts the "technique of a political arithmetician" to show the utter ridiculousness of trying to prove any proposal with dispassionate statistics.
Critics differ about Swift's intentions in using this faux-mathematical philosophy. Edmund Wilson argues that statistically "the logic of the 'Modest proposal' can be compared with defense of crime (arrogated to Marx) in which he argues that crime takes care of the superfluous population". Wittkowsky counters that Swift's satiric use of statistical analysis is an effort to enhance his satire that "springs from a spirit of bitter mockery, not from the delight in calculations for their own sake".
Rhetoric.
Charles K. Smith argues that Swift's rhetorical style persuades the reader to detest the speaker and pity the Irish. Swift's specific strategy is twofold, using a "trap" to create sympathy for the Irish and a dislike of the narrator who, in the span of one sentence, "details vividly and with rhetorical emphasis the grinding poverty" but feels emotion solely for members of his own class. Swift's use of gripping details of poverty and his narrator's cool approach towards them create "two opposing points of view" that "alienate the reader, perhaps unconsciously, from a narrator who can view with 'melancholy' detachment a subject that Swift has directed us, rhetorically, to see in a much less detached way."
Swift has his proposer further degrade the Irish by using language ordinarily reserved for animals. Lewis argues that the speaker uses "the vocabulary of animal husbandry" to describe the Irish. Once the children have been commodified, Swift's rhetoric can easily turn "people into animals, then meat, and from meat, logically, into tonnage worth a price per pound".
Swift uses the proposer's serious tone to highlight the absurdity of his proposal. In making his argument, the speaker uses the conventional, text book approved order of argument from Swift's time (which was derived from the Latin rhetorician Quintilian). The contrast between the "careful control against the almost inconceivable perversion of his scheme" and "the ridiculousness of the proposal" create a situation in which the reader has "to consider just what perverted values and assumptions would allow such a diligent, thoughtful, and conventional man to propose so perverse a plan".
Influences.
Scholars have speculated about which earlier works Swift may have had in mind when he wrote "A Modest Proposal".
Tertullian's "Apology".
James Johnson argued that "A Modest Proposal" was largely influenced and inspired by Tertullian's "Apology": a satirical attack against early Roman persecution of Christianity. James William Johnson believes that Swift saw major similarities between the two situations. Johnson notes Swift's obvious affinity for Tertullian and the bold stylistic and structural similarities between the works "A Modest Proposal" and "Apology". In structure, Johnson points out the same central theme, that of cannibalism and the eating of babies as well as the same final argument, that "human depravity is such that men will attempt to justify their own cruelty by accusing their victims of being lower than human." Stylistically, Swift and Tertullian share the same command of sarcasm and language. In agreement with Johnson, Donald C. Baker points out the similarity between both authors' tones and use of irony. Baker notes the uncanny way that both authors imply an ironic "justification by ownership" over the subject of sacrificing children—Tertullian while attacking pagan parents, and Swift while attacking the English mistreatment of the Irish poor.
Defoe's "The Generous Projector".
It has also been argued that "A Modest Proposal" was, at least in part, a response to the 1728 essay "The Generous Projector or, A Friendly Proposal to Prevent Murder and Other Enormous Abuses, By Erecting an Hospital for Foundlings and Bastard Children" by Swift's rival Daniel Defoe.
Economic themes.
Robert Phiddian's article "Have you eaten yet? The Reader in A Modest Proposal" focuses on two aspects of "A Modest Proposal": the voice of Swift and the voice of the Proposer. Phiddian stresses that a reader of the pamphlet must learn to distinguish between the satiric voice of Jonathan Swift and the apparent economic projections of the Proposer. He reminds readers that "there is a gap between the narrator's meaning and the text's, and that a moral-political argument is being carried out by means of parody".
While Swift's proposal is obviously not a serious economic proposal, George Wittkowsky, author of "Swift's Modest Proposal: The Biography of an Early Georgian Pamphlet", argues that to understand the piece fully, it is important to understand the economics of Swift’s time. Wittowsky argues that not enough critics have taken the time to focus directly on the mercantilism and theories of labour in 18th century England. "f one regards the "Modest Proposal" simply as a criticism of condition, about all one can say is that conditions were bad and that Swift's irony brilliantly underscored this fact".
"People are the riches of a nation".
At the start of a new industrial age in the 18th century, it was believed that "people are the riches of the nation", and there was a general faith in an economy that paid its workers low wages because high wages meant workers would work less. Furthermore, "in the mercantilist view no child was too young to go into industry". In those times, the "somewhat more humane attitudes of an earlier day had all but disappeared and the laborer had come to be regarded as a commodity".
Louis A. Landa presents Swift's "A Modest Proposal" as a critique of the popular and unjustified maxim of mercantilism in the 18th century that "people are the riches of a nation". Swift presents the dire state of Ireland and shows that mere population itself, in Ireland's case, did not always mean greater wealth and economy. The uncontrolled maxim fails to take into account that a person who does not produce in an economic or political way makes a country poorer, not richer. Swift also recognises the implications of such a fact in making mercantilist philosophy a paradox: the wealth of a country is based on the poverty of the majority of its citizens. Swift however, Landa argues, is not merely criticising economic maxims but also addressing the fact that England was denying Irish citizens their natural rights and dehumanising them by viewing them as a mere commodity.
Modern usage.
"A Modest Proposal" is included in many literature programs as an example of early modern western satire. It also serves as an exceptional introduction to the concept and use of argumentative language, lending itself well to secondary and post-secondary essay courses. Outside of the realm of English studies, "A Modest Proposal" is a relevant piece included in many comparative and global literature and history courses, as well as those of numerous other disciplines in the arts, humanities, and even the social sciences.
The essay has been emulated many times. In his book "A Modest Proposal" (1984), evangelical author Frank Schaeffer emulated Swift's work in social conservative polemic against abortion and euthanasia in a future dystopia that advocated recycling of aborted embryos and fetuses, as well as some disabled infants with compound intellectual, physical and physiological difficulties. (Such Baby Doe Rules cases were then a major concern of the pro-life movement of the early 1980s, which viewed selective treatment of those infants as disability discrimination.) In his book "A Modest Proposal for America" (2013), statistician Howard Friedman opens with a satirical reflection of the extreme drive to fiscal stability by ultra-conservatives.
Hunter S. Thompson's "Fear and Loathing in America: The Brutal Odyssey of an Outlaw Journalist", which contains hundreds of private letters written by Thompson over the years, contains a letter in which he uses "A Modest Proposal"'s satire technique against the Vietnam War. Thompson writes a letter to a local Aspen newspaper informing them that, on Christmas Eve, he was going to use napalm to burn a number of dogs and hopefully any humans they find. This letter protests the burning of Vietnamese people occurring overseas.

</doc>
<doc id="666" url="https://en.wikipedia.org/wiki?curid=666" title="Alkali metal">
Alkali metal

The alkali metals are a group (column) in the periodic table consisting of the chemical elements lithium (Li), sodium (Na), potassium (K), rubidium (Rb), caesium (Cs), and francium (Fr). This group lies in the s-block of the periodic table of elements as all alkali metals have their outermost electron in an s-orbital: this element/electron configuration results in their characteristic properties. The alkali metals provide the best example of group trends in properties in the periodic table, with elements exhibiting well-characterized homologous behaviour.
The alkali metals have very similar properties: they are all shiny, soft, highly reactive metals at standard temperature and pressure and readily lose their outermost electron to form cations with charge +1. They can all be cut easily with a knife due to their softness, exposing a shiny surface that tarnishes rapidly in air due to oxidation by atmospheric moisture and oxygen. Because of their high reactivity, they must be stored under oil to prevent reaction with air, and are found naturally only in salts and never as the free element. In the modern IUPAC nomenclature, the alkali metals comprise the group 1 elements, excluding hydrogen (H), which is nominally a group 1 element but not normally considered to be an alkali metal as it rarely exhibits behaviour comparable to that of the alkali metals. All the alkali metals react with water, with the heavier alkali metals reacting more vigorously than the lighter ones.
All of the discovered alkali metals occur in nature: in order of abundance, sodium is the most abundant, followed by potassium, lithium, rubidium, caesium, and finally francium, which is very rare due to its extremely high radioactivity; francium occurs only in traces, the product of natural decay chains. Experiments have been conducted to attempt the synthesis of ununennium (Uue), which is likely to be the next member of the group, but they have all met with failure. However, ununennium may not be an alkali metal due to relativistic effects, which are predicted to have a large influence on the chemical properties of superheavy elements; even if it does turn out to be an alkali metal, it is predicted to have some differences in physical and chemical properties from its lighter homologues.
Most alkali metals have many different applications. One of the best-known applications of the pure elements the use of rubidium and caesium in atomic clocks, of which caesium atomic clocks are the most accurate and precise representation of time. A common application of the compounds of sodium is the sodium-vapour lamp, which emits very efficient light. Table salt, or sodium chloride, has been used since antiquity. Sodium and potassium are also essential elements, having major biological roles as electrolytes, and although the other alkali metals are not essential, they also have various effects on the body, both beneficial and harmful.
Properties.
Physical and chemical.
The physical and chemical properties of the alkali metals can be readily explained by their having an ns valence electron configuration, which results in weak metallic bonding. Hence, all the alkali metals are soft and have low densities, melting and boiling points, as well as heats of sublimation, vaporisation, and dissociation They all crystallize in the body-centered cubic crystal structure, and have distinctive flame colours because their outer s electron is very easily excited. The ns configuration also results in the alkali metals having very large atomic and ionic radii, as well as high thermal and electrical conductivity. Their chemistry is dominated by the loss of their lone valence electron to form the +1 oxidation state, due to the ease of ionizing this electron and the very high second ionization energy. Most of the chemistry has been observed only for the first five members of the group. The chemistry of francium is not well established due to its extreme radioactivity; thus, the presentation of its properties here is limited.
The alkali metals are more similar to each other than the elements in any other group are to each other. For instance, when moving down the table, all known alkali metals show increasing atomic radius, decreasing electronegativity, increasing reactivity, and decreasing melting and boiling points as well as heats of fusion and vaporisation. In general, their densities increase when moving down the table, with the exception that potassium is less dense than sodium. One of the very few properties of the alkali metals that does not display a very smooth trend is their reduction potentials: lithium's value is anomalous, being more negative than the others. This is because the Li ion has a very high hydration energy in the gas phase: though the lithium ion disrupts the structure of water significantly, causing a higher change in entropy, this high hydration energy is enough to make the reduction potentials indicate it as being the most electropositive alkali metal, despite the difficulty of ionizing it in the gas phase.
The stable alkali metals are all silver-coloured metals except for caesium, which has a golden tint: it is one of only three metals that are clearly coloured (the other two being copper and gold). Additionally, the heavy alkaline earth metals calcium, strontium, and barium, as well as the divalent lanthanides europium and ytterbium, are pale yellow, though the colour is much less prominent than it is for caesium. Their lustre tarnishes rapidly in air due to oxidation. They all crystallize in the body-centered cubic crystal structure, and have distinctive flame colours because their outer s electron is very easily excited.
All the alkali metals are highly reactive and are never found in elemental forms in nature. Because of this, they are usually stored in mineral oil or kerosene (paraffin oil). They react aggressively with the halogens to form the alkali metal halides, which are white ionic crystalline compounds that are all soluble in water except lithium fluoride (LiF). The alkali metals also react with water to form strongly alkaline hydroxides and thus should be handled with great care. The heavier alkali metals react more vigorously than the lighter ones; for example, when dropped into water, caesium produces a larger explosion than potassium. The alkali metals have the lowest first ionisation energies in their respective periods of the periodic table because of their low effective nuclear charge and the ability to attain a noble gas configuration by losing just one electron. The second ionisation energy of all of the alkali metals is very high as it is in a full shell that is also closer to the nucleus; thus, they almost always lose a single electron, forming cations. The alkalides are an exception: they are unstable compounds which contain alkali metals in a −1 oxidation state, which is very unusual as before the discovery of the alkalides, the alkali metals were not expected to be able to form anions and were thought to be able to appear in salts only as cations. The alkalide anions have filled s-subshells, which gives them more stability and allows them to exist. All the stable alkali metals except lithium are known to be able to form alkalides, and the alkalides have much theoretical interest due to their unusual stoichiometry and low ionisation potentials. Alkalides are chemically similar to the electrides, which are salts with trapped electrons acting as anions. A particularly striking example of an alkalide is "inverse sodium hydride", HNa (both ions being complexed), as opposed to the usual sodium hydride, NaH: it is unstable in isolation, due to its high energy resulting from the displacement of two electrons from hydrogen to sodium, although several derivatives are predicted to be metastable or stable.
In aqueous solution, the alkali metal ions form aqua ions of the formula (HO) values of potassium chloride and sodium chloride. Caesium chloride has been promoted as an alternative cancer therapy, but has been linked to the deaths of over 50 patients, on whom it was used as part of a scientifically unvalidated cancer treatment. Radioisotopes of caesium require special precautions: the improper handling of caesium-137 gamma ray sources can lead to release of this radioisotope and radiation injuries. Perhaps the best-known case is the Goiânia accident of 1987, in which an improperly-disposed-of radiation therapy system from an abandoned clinic in the city of Goiânia, Brazil, was scavenged from a junkyard, and the glowing caesium salt sold to curious, uneducated buyers. This led to four deaths and serious injuries from radiation exposure. Together with caesium-134, iodine-131, and strontium-90, caesium-137 was among the isotopes distributed by the Chernobyl disaster which constitute the greatest risk to health.
Francium has no biological role and is most likely to be toxic due to its extreme radioactivity, causing radiation poisoning, but since the greatest quantity of francium ever assembled to date is about 300,000 neutral atoms, it is unlikely that most people will ever encounter francium.

</doc>
<doc id="670" url="https://en.wikipedia.org/wiki?curid=670" title="Alphabet">
Alphabet

An alphabet is a standard set of letters (basic written symbols or graphemes) which is used to write one or more languages based on the general principle that the letters represent phonemes (basic significant sounds) of the spoken language. This is in contrast to other types of writing systems, such as syllabaries (in which each character represents a syllable) and logographies (in which each character represents a word, morpheme, or semantic unit
"אלף בית גימל דלת"
The Proto-Canaanite script, later known as the Phoenician alphabet, is the first fully phonemic script. Thus the Phoenician alphabet is considered to be the first alphabet. The Phoenician alphabet is the ancestor of most modern alphabets, including Arabic, Greek, Latin, Cyrillic, Hebrew, and possibly Brahmic. According to terminology introduced by Peter T. Daniels, an "alphabet" is a script that represents both vowels and consonants as letters equally. In this narrow sense of the word the first "true" alphabet was the Greek alphabet, which was developed on the basis of the earlier Phoenician alphabet. In other alphabetic scripts such as the original Phoenician, Hebrew or Arabic, letters predominantly or exclusively represent consonants; such a script is also called an abjad. A third type, called abugida or alphasyllabary, is one where vowels are shown by diacritics or modifications of consonantal base letters, as in Devanagari and other South Asian scripts.
There are dozens of alphabets in use today, the most popular being the Latin alphabet (which was derived from the Greek). Many languages use modified forms of the Latin alphabet, with additional letters formed using diacritical marks. While most alphabets have letters composed of lines (linear writing), there are also exceptions such as the alphabets used in Braille, fingerspelling, and Morse code.
Alphabets are usually associated with a standard ordering of letters. This makes them useful for purposes of collation, specifically by allowing words to be sorted in alphabetical order. It also means that their letters can be used as an alternative method of "numbering" ordered items, in such contexts as numbered lists and number placements.
Etymology.
The English word "alphabet" came into Middle English from the Late Latin word "alphabetum", which in turn originated in the Greek ἀλφάβητος ("alphabētos"), from "alpha" and "beta," the first two letters of the Greek alphabet. "Alpha" and "beta" in turn came from the first two letters of the Phoenician alphabet, and originally meant "ox" and "house" respectively.
Informally the term "ABCs" is sometimes used for the alphabet as in the alphabet song ("Now I know my ABCs" ...), and knowing one's ABCs for literacy, or as a metaphor for knowing the basics about anything.
History.
Ancient Northeast African and Middle Eastern scripts.
The history of the alphabet started in ancient Egypt. By the 27th century BC Egyptian writing had a set of some 24 hieroglyphs which are called uniliterals, to represent syllables that begin with a single consonant of their language, plus a vowel (or no vowel) to be supplied by the native speaker. These glyphs were used as pronunciation guides for logograms, to write grammatical inflections, and, later, to transcribe loan words and foreign names.
In the Middle Bronze Age an apparently "alphabetic" system known as the Proto-Sinaitic script appears in Egyptian turquoise mines in the Sinai peninsula dated to circa the 15th century BC, apparently left by Canaanite workers. In 1999, John and Deborah Darnell discovered an even earlier version of this first alphabet at Wadi el-Hol dated to circa 1800 BC and showing evidence of having been adapted from specific forms of Egyptian hieroglyphs that could be dated to circa 2000 BC, strongly suggesting that the first alphabet had been developed circa that time. Based on letter appearances and names, it is believed to be based on Egyptian hieroglyphs. This script had no characters representing vowels, although originally it probably was a syllabary, but unneeded symbols were discarded. An alphabetic cuneiform script with 30 signs including three which indicate the following vowel was invented in Ugarit before the 15th century BC. This script was not used after the destruction of Ugarit.
The Proto-Sinaitic script eventually developed into the Phoenician alphabet, which is conventionally called "Proto-Canaanite" before ca. 1050 BC. The oldest text in Phoenician script is an inscription on the sarcophagus of King Ahiram. This script is the parent script of all western alphabets. By the tenth century two other forms can be distinguished namely Canaanite and Aramaic. The Aramaic gave rise to Hebrew. The South Arabian alphabet, a sister script to the Phoenician alphabet, is the script from which the Ge'ez alphabet (an abugida) is descended. Vowelless alphabets, which are not true alphabets, are called abjads, currently exemplified in scripts including Arabic, Hebrew, and Syriac. The omission of vowels was not a satisfactory solution and some "weak" consonants were used to indicate the vowel quality of a syllable (matres lectionis). These had dual function since they were also used as pure consonants.
The Proto-Sinatic or Proto Canaanite script and the Ugaritic script were the first scripts with limited number of signs, in contrast to the other widely used writing systems at the time, Cuneiform, Egyptian hieroglyphs, and Linear B. The Phoenician script was probably the first phonemic script and it contained only about two dozen distinct letters, making it a script simple enough for common traders to learn. Another advantage of Phoenician was that it could be used to write down many different languages, since it recorded words phonemically.
The script was spread by the Phoenicians, across the Mediterranean. In Greece, the script was modified to add the vowels, giving rise to the ancestor of all alphabets in the West. The indication of the vowels is the same way as the indication of the consonants, therefore it was the first true alphabet. The Greeks chose letters representing sounds that did not exist in Greek to represent the vowels. The vowels are significant in the Greek language, and the syllabical Linear B script which was used by the Mycenaean Greeks from the 16th century BC had 87 symbols including 5 vowels. In its early years, there were many variants of the Greek alphabet, a situation which caused many different alphabets to evolve from it.
European alphabets.
The Greek alphabet, in its Euboean form, was carried over by Greek colonists to the Italian peninsula, where it gave rise to a variety of alphabets used to write the Italic languages. One of these became the Latin alphabet, which was spread across Europe as the Romans expanded their empire. Even after the fall of the Roman state, the alphabet survived in intellectual and religious works. It eventually became used for the descendant languages of Latin (the Romance languages) and then for most of the other languages of Europe.
Some adaptations of the Latin alphabet are augmented with ligatures, such as æ in Danish and Icelandic and Ȣ in Algonquian; by borrowings from other alphabets, such as the thorn þ in Old English and Icelandic, which came from the Futhark runes; and by modifying existing letters, such as the eth ð of Old English and Icelandic, which is a modified "d". Other alphabets only use a subset of the Latin alphabet, such as Hawaiian, and Italian, which uses the letters "j, k, x, y" and "w" only in foreign words.
Another notable script is Elder Futhark, which is believed to have evolved out of one of the Old Italic alphabets. Elder Futhark gave rise to a variety of alphabets known collectively as the Runic alphabets. The Runic alphabets were used for Germanic languages from AD 100 to the late Middle Ages. Its usage is mostly restricted to engravings on stone and jewelry, although inscriptions have also been found on bone and wood. These alphabets have since been replaced with the Latin alphabet, except for decorative usage for which the runes remained in use until the 20th century.
The Old Hungarian script is a contemporary writing system of the Hungarians. It was in use during the entire history of Hungary, albeit not as an official writing system. From the 19th century it once again became more and more popular.
The Glagolitic alphabet was the initial script of the liturgical language Old Church Slavonic and became, together with the Greek uncial script, the basis of the Cyrillic script. Cyrillic is one of the most widely used modern alphabetic scripts, and is notable for its use in Slavic languages and also for other languages within the former Soviet Union. Cyrillic alphabets include the Serbian, Macedonian, Bulgarian, and Russian alphabets. The Glagolitic alphabet is believed to have been created by Saints Cyril and Methodius, while the Cyrillic alphabet was invented by the Bulgarian scholar Clement of Ohrid, who was their disciple. They feature many letters that appear to have been borrowed from or influenced by the Greek alphabet and the Hebrew alphabet.
Asian alphabets.
Beyond the logographic Chinese writing, many phonetic scripts are in existence in Asia. The Arabic alphabet, Hebrew alphabet, Syriac alphabet, and other abjads of the Middle East are developments of the Aramaic alphabet, but because these writing systems are largely consonant-based they are often not considered true alphabets.
Most alphabetic scripts of India and Eastern Asia are descended from the Brahmi script, which is often believed to be a descendant of Aramaic.
In Korea, the Hangul alphabet was created by Sejong the Great. Hangul is a unique alphabet: it is a featural alphabet, where many of the letters are designed from a sound's place of articulation (P to look like the widened mouth, L to look like the tongue pulled in, etc.); its design was planned by the government of the day; and it places individual letters in syllable clusters with equal dimensions, in the same way as Chinese characters, to allow for mixed-script writing (one syllable always takes up one type-space no matter how many letters get stacked into building that one sound-block).
Zhuyin (sometimes called "Bopomofo") is a semi-syllabary used to phonetically transcribe Mandarin Chinese in the Republic of China. After the later establishment of the People's Republic of China and its adoption of Hanyu Pinyin, the use of Zhuyin today is limited, but it is still widely used in Taiwan where the Republic of China still governs. Zhuyin developed out of a form of Chinese shorthand based on Chinese characters in the early 1900s and has elements of both an alphabet and a syllabary. Like an alphabet the phonemes of syllable initials are represented by individual symbols, but like a syllabary the phonemes of the syllable finals are not; rather, each possible final (excluding the medial glide) is represented by its own symbol. For example, "luan" is represented as ㄌㄨㄢ ("l-u-an"), where the last symbol ㄢ represents the entire final "-an". While Zhuyin is not used as a mainstream writing system, it is still often used in ways similar to a romanization system—that is, for aiding in pronunciation and as an input method for Chinese characters on computers and cellphones.
European alphabets, especially Latin and Cyrillic, have been adapted for many languages of Asia. Arabic is also widely used, sometimes as an abjad (as with Urdu and Persian) and sometimes as a complete alphabet (as with Kurdish and Uyghur).
Types.
The term "alphabet" is used by linguists and paleographers in both a wide and a narrow sense. In the wider sense, an alphabet is a script that is "segmental" at the phoneme level—that is, it has separate glyphs for individual sounds and not for larger units such as syllables or words. In the narrower sense, some scholars distinguish "true" alphabets from two other types of segmental script, abjads and abugidas. These three differ from each other in the way they treat vowels: abjads have letters for consonants and leave most vowels unexpressed; abugidas are also consonant-based, but indicate vowels with diacritics to or a systematic graphic modification of the consonants. In alphabets in the narrow sense, on the other hand, consonants and vowels are written as independent letters. The earliest known alphabet in the wider sense is the Wadi el-Hol script, believed to be an abjad, which through its successor Phoenician is the ancestor of modern alphabets, including Arabic, Greek, Latin (via the Old Italic alphabet), Cyrillic (via the Greek alphabet) and Hebrew (via Aramaic).
Examples of present-day abjads are the Arabic and Hebrew scripts; true alphabets include Latin, Cyrillic, and Korean hangul; and abugidas are used to write Tigrinya, Amharic, Hindi, and Thai. The Canadian Aboriginal syllabics are also an abugida rather than a syllabary as their name would imply, since each glyph stands for a consonant which is modified by rotation to represent the following vowel. (In a true syllabary, each consonant-vowel combination would be represented by a separate glyph.)
All three types may be augmented with syllabic glyphs. Ugaritic, for example, is basically an abjad, but has syllabic letters for . (These are the only time vowels are indicated.) Cyrillic is basically a true alphabet, but has syllabic letters for (я, е, ю); Coptic has a letter for . Devanagari is typically an abugida augmented with dedicated letters for initial vowels, though some traditions use अ as a zero consonant as the graphic base for such vowels.
The boundaries between the three types of segmental scripts are not always clear-cut. For example, Sorani Kurdish is written in the Arabic script, which is normally an abjad. However, in Kurdish, writing the vowels is mandatory, and full letters are used, so the script is a true alphabet. Other languages may use a Semitic abjad with mandatory vowel diacritics, effectively making them abugidas. On the other hand, the Phagspa script of the Mongol Empire was based closely on the Tibetan abugida, but all vowel marks were written after the preceding consonant rather than as diacritic marks. Although short "a" was not written, as in the Indic abugidas, one could argue that the linear arrangement made this a true alphabet. Conversely, the vowel marks of the Tigrinya abugida and the Amharic abugida (ironically, the original source of the term "abugida") have been so completely assimilated into their consonants that the modifications are no longer systematic and have to be learned as a syllabary rather than as a segmental script. Even more extreme, the Pahlavi abjad eventually became logographic. (See below.)
Thus the primary classification of alphabets reflects how they treat vowels. For tonal languages, further classification can be based on their treatment of tone, though names do not yet exist to distinguish the various types. Some alphabets disregard tone entirely, especially when it does not carry a heavy functional load, as in Somali and many other languages of Africa and the Americas. Such scripts are to tone what abjads are to vowels. Most commonly, tones are indicated with diacritics, the way vowels are treated in abugidas. This is the case for Vietnamese (a true alphabet) and Thai (an abugida). In Thai, tone is determined primarily by the choice of consonant, with diacritics for disambiguation. In the Pollard script, an abugida, vowels are indicated by diacritics, but the placement of the diacritic relative to the consonant is modified to indicate the tone. More rarely, a script may have separate letters for tones, as is the case for Hmong and Zhuang. For most of these scripts, regardless of whether letters or diacritics are used, the most common tone is not marked, just as the most common vowel is not marked in Indic abugidas; in Zhuyin not only is one of the tones unmarked, but there is a diacritic to indicate lack of tone, like the virama of Indic.
The number of letters in an alphabet can be quite small. The Book Pahlavi script, an abjad, had only twelve letters at one point, and may have had even fewer later on. Today the Rotokas alphabet has only twelve letters. (The Hawaiian alphabet is sometimes claimed to be as small, but it actually consists of 18 letters, including the ʻokina and five long vowels. However, Hawaiian Braille has only 13 letters.) While Rotokas has a small alphabet because it has few phonemes to represent (just eleven), Book Pahlavi was small because many letters had been "conflated"—that is, the graphic distinctions had been lost over time, and diacritics were not developed to compensate for this as they were in Arabic, another script that lost many of its distinct letter shapes. For example, a comma-shaped letter represented "g, d, y, k," or "j". However, such apparent simplifications can perversely make a script more complicated. In later Pahlavi papyri, up to half of the remaining graphic distinctions of these twelve letters were lost, and the script could no longer be read as a sequence of letters at all, but instead each word had to be learned as a whole—that is, they had become logograms as in Egyptian Demotic.
The largest segmental script is probably an abugida, Devanagari. When written in Devanagari, Vedic Sanskrit has an alphabet of 53 letters, including the "visarga" mark for final aspiration and special letters for "kš" and "jñ," though one of the letters is theoretical and not actually used. The Hindi alphabet must represent both Sanskrit and modern vocabulary, and so has been expanded to 58 with the "khutma" letters (letters with a dot added) to represent sounds from Persian and English. Thai has a total of 59 symbols, consisting of 44 consonants, 13 vowels and 2 syllabics, not including 4 diacritics for tone marks and one for vowel length.
The largest known abjad is Sindhi, with 51 letters. The largest alphabets in the narrow sense include Kabardian and Abkhaz (for Cyrillic), with 58 and 56 letters, respectively, and Slovak (for the Latin script), with 46. However, these scripts either count di- and tri-graphs as separate letters, as Spanish did with "ch" and "ll" until recently, or uses diacritics like Slovak "č".
The Georgian alphabet (Georgian: "ანბანი" "Anbani") is alphabetical writing system. It is the largest true alphabet where each letter is graphically independent with 33 letters. Original Georgian alphabet had 38 letters but 5 letters was removed in 19th century by Ilia Chavchavadze.Georgian Alphabet is much nearer to Greek than the other Caucasian alphabets. The numeric value runs parallel to the Greek one, the consonants without a Greek equivalent are organized at the end of the alphabet. Origins of Alphabet is still unknown, some Armenian and Western scholars believe it was created by Mastots, other Georgian and Western, scholars are against this theory.
Syllabaries typically contain 50 to 400 glyphs, and the glyphs of logographic systems typically number from the many hundreds into the thousands. Thus a simple count of the number of distinct symbols is an important clue to the nature of an unknown script.
The Armenian alphabet (Armenian: Հայոց գրեր Hayots grer or Հայոց այբուբեն Hayots aybuben) is a graphically unique alphabetical writing system that has been used to write the Armenian language. It was introduced by Mesrob Mashdots around 405 AD, an Armenian linguist and ecclesiastical leader, and originally contained 36 letters. Two more letters, օ (o) and ֆ (f), were added in the Middle Ages. During the 1920s orthography reform, a new letter և (capital ԵՎ) was added, which was a ligature before ե+ւ, while the letter Ւ ւ was discarded and reintroduced as part of a new letter ՈՒ ու (which was a digraph before).
The Armenian word for "alphabet" is այբուբեն aybuben (Armenian pronunciation: jbubɛ), named after the first two letters of the Armenian alphabet Ա այբ ayb and Բ բեն ben. The Armenian script's directionality is horizontal left-to-right, like the Latin and Greek alphabets.
Alphabetical order.
Alphabets often come to be associated with a standard ordering of their letters, which can then be used for purposes of collation – namely for the listing of words and other items in what is called "alphabetical order".
The basic ordering of the Latin alphabet (A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
P
Q
R
S
T
U
V
W
X
Y
Z), which is derived from the Northwest Semitic "Abgad" order, is well established, although languages using this alphabet have different conventions for their treatment of modified letters (such as the French "é", "à", and "ô") and of certain combinations of letters (multigraphs). In French, these are not considered to be additional letters for the purposes of collation. However, in Icelandic, the accented letters such as "á", "í", and "ö" are considered to be distinct letters of the alphabet. In Spanish, "ñ" is considered a separate letter, but accented vowels such as "á" and "é" are not. The "ll" and "ch" were also considered single letters, but in 1994 the Real Academia Española changed the collating order so that "ll" is between "lk" and "lm" in the dictionary and "ch" is between "cg" and "ci", and in 2010 the tenth congress of the Association of Spanish Language Academies changed it so they were no longer letters at all.
In German, words starting with "sch-" (which spells the German phoneme ) are inserted between words with initial "sca-" and "sci-" (all incidentally loanwords) instead of appearing after initial "sz", as though it were a single letter—in contrast to several languages such as Albanian, in which "dh-", "ë-", "gj-", "ll-", "rr-", "th-", "xh-" and "zh-" (all representing phonemes and considered separate single letters) would follow the letters "d", "e", "g", "l", "n", "r", "t", "x" and "z" respectively, as well as Hungarian and Welsh. Further, German words with umlaut are collated ignoring the umlaut—contrary to Turkish which adopted the graphemes ö and ü, and where a word like "tüfek", would come after "tuz", in the dictionary. An exception is the German telephone directory where umlauts are sorted like "ä" = "ae" since names as "Jäger" appear also with the spelling "Jaeger", and are not distinguished in the spoken language.
The Danish and Norwegian alphabets end with "æ"—"ø"—"å", whereas the Icelandic, Swedish and Finnish ones conventionally put "å"—"ä"—"ö" at the end.
It is unknown whether the earliest alphabets had a defined sequence. Some alphabets today, such as the Hanuno'o script, are learned one letter at a time, in no particular order, and are not used for collation where a definite order is required. However, a dozen Ugaritic tablets from the fourteenth century BC preserve the alphabet in two sequences. One, the "ABCDE" order later used in Phoenician, has continued with minor changes in Hebrew, Greek, Armenian, Gothic, Cyrillic, and Latin; the other, "HMĦLQ," was used in southern Arabia and is preserved today in Ethiopic. Both orders have therefore been stable for at least 3000 years.
Runic used an unrelated Futhark sequence, which was later simplified. Arabic uses its own sequence, although Arabic retains the traditional abjadi order for numbering.
The Brahmic family of alphabets used in India use a unique order based on phonology: The letters are arranged according to how and where they are produced in the mouth. This organization is used in Southeast Asia, Tibet, Korean hangul, and even Japanese kana, which is not an alphabet.
Names of letters.
The Phoenician letter names, in which each letter was associated with a word that begins with that sound (acrophony), continue to be used to varying degrees in Samaritan, Aramaic, Syriac, Hebrew, Greek and Arabic.
The names were abandoned in Latin, which instead referred to the letters by adding a vowel (usually e) before or after the consonant; the two exceptions were Y and Z, which were borrowed from the Greek alphabet rather than Etruscan, and were known as "Y Graeca" "Greek Y" (pronounced "I Graeca" "Greek I") and "zeta" (from Greek) – this discrepancy was inherited by many European languages, as in the term "zed" for Z in British English. Over time names sometimes shifted or were added, as in "double U" for W ("double V" in French), the English name for Y, and American "zee" for Z. Comparing names in English and French gives a clear reflection of the Great Vowel Shift: A, B, C and D are pronounced /eɪ, biː, siː, diː/ in today's English, but in contemporary French they are /a, be, se, de/. The French names (from which the English names are derived) preserve the qualities of the English vowels from before the Great Vowel Shift. By contrast, the names of F, L, M, N and S (/ɛf, ɛl, ɛm, ɛn, ɛs/) remain the same in both languages, because "short" vowels were largely unaffected by the Shift.
In Cyrillic originally the letters were given names based on Slavic words; this was later abandoned as well in favor of a system similar to that used in Latin.
Orthography and pronunciation.
When an alphabet is adopted or developed to represent a given language, an orthography generally comes into being, providing rules for the spelling of words in that language. In accordance with the principle on which alphabets are based, these rules will generally map letters of the alphabet to the phonemes (significant sounds) of the spoken language. In a perfectly phonemic orthography there would be a consistent one-to-one correspondence between the letters and the phonemes, so that a writer could predict the spelling of a word given its pronunciation, and a speaker would always know the pronunciation of a word given its spelling, and vice versa. However this ideal is not usually achieved in practice; some languages (such as Spanish and Finnish) come close to it, while others (such as English) deviate from it to a much larger degree.
The pronunciation of a language often evolves independently of its writing system, and writing systems have been borrowed for languages they were not designed for, so the degree to which letters of an alphabet correspond to phonemes of a language varies greatly from one language to another and even within a single language.
Languages may fail to achieve a one-to-one correspondence between letters and sounds in any of several ways:
National languages sometimes elect to address the problem of dialects by simply associating the alphabet with the national standard. However, with an international language with wide variations in its dialects, such as English, it would be impossible to represent the language in all its variations with a single phonetic alphabet.
Some national languages like Finnish, Turkish, Russian, Serbo-Croatian (Serbian, Croatian and Bosnian) and Bulgarian have a very regular spelling system with a nearly one-to-one correspondence between letters and phonemes. Strictly speaking, these national languages lack a word corresponding to the verb "to spell" (meaning to split a word into its letters), the closest match being a verb meaning to split a word into its syllables. Similarly, the Italian verb corresponding to 'spell (out)', "compitare", is unknown to many Italians because spelling is usually trivial, as Italian spelling is highly phonemic. In standard Spanish, one can tell the pronunciation of a word from its spelling, but not vice versa, as certain phonemes can be represented in more than one way, but a given letter is consistently pronounced. French, with its silent letters and its heavy use of nasal vowels and elision, may seem to lack much correspondence between spelling and pronunciation, but its rules on pronunciation, though complex, are actually consistent and predictable with a fair degree of accuracy.
At the other extreme are languages such as English, where the pronunciations of many words simply have to be memorized as they do not correspond to the spelling in a consistent way. For English, this is partly because the Great Vowel Shift occurred after the orthography was established, and because English has acquired a large number of loanwords at different times, retaining their original spelling at varying levels. Even English has general, albeit complex, rules that predict pronunciation from spelling, and these rules are successful most of the time; rules to predict spelling from the pronunciation have a higher failure rate.
Sometimes, countries have the written language undergo a spelling reform to realign the writing with the contemporary spoken language. These can range from simple spelling changes and word forms to switching the entire writing system itself, as when Turkey switched from the Arabic alphabet to a Latin-based Turkish alphabet.
The standard system of symbols used by linguists to represent sounds in any language, independently of orthography, is called the International Phonetic Alphabet.

</doc>
<doc id="673" url="https://en.wikipedia.org/wiki?curid=673" title="Atomic number">
Atomic number

In chemistry and physics, the atomic number of a chemical element (also known as its proton number) is the number of protons found in the nucleus of an atom of that element, and therefore identical to the charge number of the nucleus. It is conventionally represented by the symbol Z. The atomic number uniquely identifies a chemical element. In an uncharged atom, the atomic number is also equal to the number of electrons.
The atomic number, "Z", should not be confused with the mass number, "A", which is the number of nucleons, the total number of protons and neutrons in the nucleus of an atom. The number of neutrons, "N", is known as the neutron number of the atom; thus, "A" = "Z" + "N" (these quantities are always whole numbers). Since protons and neutrons have approximately the same mass (and the mass of the electrons is negligible for many purposes) and the mass defect of nucleon binding is always small compared to the nucleon mass, the atomic mass of any atom, when expressed in unified atomic mass units (making a quantity called the "relative isotopic mass"), is roughly (to within 1%) equal to the whole number "A".
Atoms with the same atomic number "Z" but different neutron numbers "N", and hence different atomic masses, are known as isotopes. A little more than three-quarters of naturally occurring elements exist as a mixture of isotopes (see monoisotopic elements), and the average isotopic mass of an isotopic mixture for an element (called the relative atomic mass) in a defined environment on Earth, determines the element's standard atomic weight. Historically, it was these atomic weights of elements (in comparison to hydrogen) that were the quantities measurable by chemists in the 19th century.
The conventional symbol "Z" comes from the German word meaning number/numeral/figure, which, prior to the modern synthesis of ideas from chemistry and physics, merely denoted an element's numerical place in the periodic table, whose order is approximately, but not completely, consistent with the order of the elements by atomic weights. Only after 1915, with the suggestion and evidence that this "Z" number was also the nuclear charge and a physical characteristic of atoms, did the word (and its English equivalent "atomic number") come into common use in this context.
History.
The periodic table and a natural number for each element.
Loosely speaking, the existence or construction of a periodic table of elements creates an ordering of the elements, and so they can be numbered in order.
Dmitri Mendeleev claimed that he arranged his first periodic tables in order of atomic weight ("Atomgewicht"). However, in consideration of the elements' observed chemical properties, he changed the order slightly and placed tellurium (atomic weight 127.6) ahead of iodine (atomic weight 126.9). This placement is consistent with the modern practice of ordering the elements by proton number, "Z", but that number was not known or suspected at the time.
A simple numbering based on periodic table position was never entirely satisfactory, however. Besides the case of iodine and tellurium, later several other pairs of elements (such as argon and potassium, cobalt and nickel) were known to have nearly identical or reversed atomic weights, thus requiring their placement in the periodic table to be determined by their chemical properties. However the gradual identification of more and more chemically similar lanthanide elements, whose atomic number was not obvious, led to inconsistency and uncertainty in the periodic numbering of elements at least from lutetium (element 71) onwards (hafnium was not known at this time). 
The Rutherford-Bohr model and van den Broek.
In 1911, Ernest Rutherford gave a model of the atom in which a central core held most of the atom's mass and a positive charge which, in units of the electron's charge, was to be approximately equal to half of the atom's atomic weight, expressed in numbers of hydrogen atoms. This central charge would thus be approximately half the atomic weight (though it was almost 25% different from the atomic number of gold ("Z" = 79, "A" = 197), the single element from which Rutherford made his guess). Nevertheless, in spite of Rutherford's estimation that gold had a central charge of about 100 (but was element Z = 79 on the periodic table), a month after Rutherford's paper appeared, Antonius van den Broek first formally suggested that the central charge and number of electrons in an atom was "exactly" equal to its place in the periodic table (also known as element number, atomic number, and symbolized "Z"). This proved eventually to be the case.
Moseley's 1913 experiment.
The experimental position improved dramatically after research by Henry Moseley in 1913. Moseley, after discussions with Bohr who was at the same lab (and who had used Van den Broek's hypothesis in his Bohr model of the atom), decided to test Van den Broek's and Bohr's hypothesis directly, by seeing if spectral lines emitted from excited atoms fitted the Bohr theory's postulation that the frequency of the spectral lines be proportional to the square of "Z".
To do this, Moseley measured the wavelengths of the innermost photon transitions (K and L lines) produced by the elements from aluminum ("Z" = 13) to gold ("Z" = 79) used as a series of movable anodic targets inside an x-ray tube. The square root of the frequency of these photons (x-rays) increased from one target to the next in an arithmetic progression. This led to the conclusion (Moseley's law) that the atomic number does closely correspond (with an offset of one unit for K-lines, in Moseley's work) to the calculated electric charge of the nucleus, i.e. the element number "Z". Among other things, Moseley demonstrated that the lanthanide series (from lanthanum to lutetium inclusive) must have 15 members—no fewer and no more—which was far from obvious from the chemistry at that time.
The proton and the idea of nuclear electrons.
In 1915 the reason for nuclear charge being quantized in units of Z, which were now recognized to be the same as the element number, was not understood. An old idea called Prout's hypothesis had postulated that the elements were all made of residues (or "protyles") of the lightest element hydrogen, which in the Bohr-Rutherford model had a single electron and a nuclear charge of one. However, as early as 1907 Rutherford and Thomas Royds had shown that alpha particles, which had a charge of +2, were the nuclei of helium atoms, which had a mass four times that of hydrogen, not two times. If Prout's hypothesis were true, something had to be neutralizing some of the charge of the hydrogen nuclei present in the nuclei of heavier atoms.
In 1917 Rutherford succeeded in generating hydrogen nuclei from a nuclear reaction between alpha particles and nitrogen gas, and believed he had proven Prout's law. He called the new heavy nuclear particles protons in 1920 (alternate names being proutons and protyles). It had been immediately apparent from the work of Moseley that the nuclei of heavy atoms have more than twice as much mass as would be expected from their being made of hydrogen nuclei, and thus there was required a hypothesis for the neutralization of the extra protons presumed present in all heavy nuclei. A helium nucleus was presumed to be composed of four protons plus two "nuclear electrons" (electrons bound inside the nucleus) to cancel two of the charges. At the other end of the periodic table, a nucleus of gold with a mass 197 times that of hydrogen, was thought to contain 118 nuclear electrons in the nucleus to give it a residual charge of + 79, consistent with its atomic number.
The discovery of the neutron makes Z the proton number.
All consideration of nuclear electrons ended with James Chadwick's discovery of the neutron in 1932. An atom of gold now was seen as containing 118 neutrons rather than 118 nuclear electrons, and its positive charge now was realized to come entirely from a content of 79 protons. After 1932, therefore, an element's atomic number Z was also realized to be identical to the proton number of its nuclei.
The symbol of Z.
The conventional symbol "Z" possibly comes from the German word (atomic number). However, prior to 1915, the word "Zahl" (simply "number") was used for an element's assigned number in the periodic table.
Chemical properties.
Each element has a specific set of chemical properties as a consequence of the number of electrons present in the neutral atom, which is "Z" (the atomic number). The configuration of these electrons follows from the principles of quantum mechanics. The number of electrons in each element's electron shells, particularly the outermost valence shell, is the primary factor in determining its chemical bonding behavior. Hence, it is the atomic number alone that determines the chemical properties of an element; and it is for this reason that an element can be defined as consisting of "any" mixture of atoms with a given atomic number.
New elements.
The quest for new elements is usually described using atomic numbers. As of 2010, elements with atomic numbers 1 to 118 have been observed. Synthesis of new elements is accomplished by bombarding target atoms of heavy elements with ions, such that the sum of the atomic numbers of the target and ion elements equals the atomic number of the element being created. In general, the half-life becomes shorter as atomic number increases, though an "island of stability" may exist for undiscovered isotopes with certain numbers of protons and neutrons.

</doc>
<doc id="674" url="https://en.wikipedia.org/wiki?curid=674" title="Anatomy">
Anatomy

Anatomy is the branch of biology concerned with the study of the structure of organisms and their parts. In some of its facets, anatomy is related to embryology and comparative anatomy, which itself is closely related to evolutionary biology and phylogeny. Human anatomy is one of the basic essential sciences of medicine.
The discipline of anatomy is divided into macroscopic and microscopic anatomy. Macroscopic anatomy, or gross anatomy, is the examination of an animal's body parts using unaided eyesight. Gross anatomy also includes the branch of superficial anatomy. Microscopic anatomy involves the use of optical instruments in the study of the tissues of various structures, known as histology, and also in the study of cells.
The history of anatomy is characterized by a progressive understanding of the functions of the organs and structures of the human body. Methods have also improved dramatically, advancing from the examination of animals by dissection of carcasses and cadavers (corpses) to 20th century medical imaging techniques including X-ray, ultrasound, and magnetic resonance imaging.
Anatomy and physiology, which study (respectively) the structure and function of organisms and their parts, make a natural pair of related disciplines, and they are often studied together.
Definition.
Derived from the Greek "anatemnō" "I cut up, cut open" from ἀνά "ana" "up", and τέμνω "temnō" "I cut", anatomy is the scientific study of the structure of organisms including their systems, organs and tissues. It includes the appearance and position of the various parts, the materials from which they are composed, their locations and their relationships with other parts. Anatomy is quite distinct from physiology and biochemistry, which deal respectively with the functions of those parts and the chemical processes involved. For example, an anatomist is concerned with the shape, size, position, structure, blood supply and innervation of an organ such as the liver; while a physiologist is interested in the production of bile, the role of the liver in nutrition and the regulation of bodily functions.
The discipline of anatomy can be subdivided into a number of branches including gross or macroscopic anatomy and microscopic anatomy. Gross anatomy is the study of structures large enough to be seen with the naked eye, and also includes superficial anatomy or surface anatomy, the study by sight of the external body features. Microscopic anatomy is the study of structures on a microscopic scale, including histology (the study of tissues), and embryology (the study of an organism in its immature condition).
Anatomy can be studied using both invasive and non-invasive methods with the goal of obtaining information about the structure and organization of organs and systems. Methods used include dissection, in which a body is opened and its organs studied, and endoscopy, in which a video camera-equipped instrument is inserted through a small incision in the body wall and used to explore the internal organs and other structures. Angiography using X-rays or magnetic resonance angiography are methods to visualize blood vessels.
The term "anatomy" is commonly taken to refer to human anatomy. However, substantially the same structures and tissues are found throughout the rest of the animal kingdom and the term also includes the anatomy of other animals. The term "zootomy" is also sometimes used to specifically refer to animals. The structure and tissues of plants are of a dissimilar nature and they are studied in plant anatomy.
Animal tissues.
The kingdom Animalia or metazoa, contains multicellular organisms that are heterotrophic and motile (although some have secondarily adopted a sessile lifestyle). Most animals have bodies differentiated into separate tissues and these animals are also known as eumetazoans. They have an internal digestive chamber, with one or two openings; the gametes are produced in multicellular sex organs, and the zygotes include a blastula stage in their embryonic development. Metazoans do not include the sponges, which have undifferentiated cells.
Unlike plant cells, animal cells have neither a cell wall nor chloroplasts. Vacuoles, when present, are more in number and much smaller than those in the plant cell. The body tissues are composed of numerous types of cell, including those found in muscles, nerves and skin. Each typically has a cell membrane formed of phospholipids, cytoplasm and a nucleus. All of the different cells of an animal are derived from the embryonic germ layers. Those simpler invertebrates which are formed from two germ layers of ectoderm and endoderm are called diploblastic and the more developed animals whose structures and organs are formed from three germ layers are called triploblastic. All of a triploblastic animal's tissues and organs are derived from the three germ layers of the embryo, the ectoderm, mesoderm and endoderm.
Animal tissues can be grouped into four basic types: connective, epithelial, muscle and nervous tissue.
Connective tissue.
Connective tissues are fibrous and made up of cells scattered among inorganic material called the extracellular matrix. Connective tissue gives shape to organs and holds them in place. The main types are loose connective tissue, adipose tissue, fibrous connective tissue, cartilage and bone. The extracellular matrix contains proteins, the chief and most abundant of which is collagen. Collagen plays a major part in organizing and maintaining tissues. The matrix can be modified to form a skeleton to support or protect the body. An exoskeleton is a thickened, rigid cuticle which is stiffened by mineralisation, as in crustaceans or by the cross-linking of its proteins as in insects. An endoskeleton is internal and present in all developed animals, as well as in many of those less developed.
Epithelium.
Epithelial tissue is composed of closely packed cells, bound to each other by cell adhesion molecules, with little intercellular space. Epithelial cells can be squamous (flat), cuboidal or columnar and rest on a basal lamina, the upper layer of the basement membrane, the lower layer is the reticular lamina lying next to the connective tissue in the extracellular matrix secreted by the epithelial cells. There are many different types of epithelium, modified to suit a particular function. In the respiratory tract there is a type of ciliated epithelial lining; in the small intestine there are microvilli on the epithelial lining and in the large intestine there are intestinal villi. Skin consists of an outer layer of keratinised stratified squamous epithelium that covers the exterior of the vertebrate body. Keratinocytes make up to 95% of the cells in the skin. The epithelial cells on the external surface of the body typically secrete an extracellular matrix in the form of a cuticle. In simple animals this may just be a coat of glycoproteins. In more advanced animals, many glands are formed of epithelial cells.
Muscle tissue.
Muscle cells (myocytes) form the active contractile tissue of the body. Muscle tissue functions to produce force and cause motion, either locomotion or movement within internal organs. Muscle is formed of contractile filaments and is separated into three main types; smooth muscle, skeletal muscle and cardiac muscle. Smooth muscle has no striations when examined microscopically. It contracts slowly but maintains contractibility over a wide range of stretch lengths. It is found in such organs as sea anemone tentacles and the body wall of sea cucumbers. Skeletal muscle contracts rapidly but has a limited range of extension. It is found in the movement of appendages and jaws. Obliquely striated muscle is intermediate between the other two. The filaments are staggered and this is the type of muscle found in earthworms that can extend slowly or make rapid contractions. In higher animals striated muscles occur in bundles attached to bone to provide movement and are often arranged in antagonistic sets. Smooth muscle is found in the walls of the uterus, bladder, intestines, stomach, esophagus, respiratory airways, and blood vessels. Cardiac muscle is found only in the heart, allowing it to contract and pump blood round the body.
Nervous tissue.
Nervous tissue is composed of many nerve cells known as neurons which transmit information. In some slow-moving radially symmetrical marine animals such as ctenophores and cnidarians (including sea anemones and jellyfish), the nerves form a nerve net, but in most animals they are organized longitudinally into bundles. In simple animals, receptor neurons in the body wall cause a local reaction to a stimulus. In more complex animals, specialised receptor cells such as chemoreceptors and photoreceptors are found in groups and send messages along neural networks to other parts of the organism. Neurons can be connected together in ganglia. In higher animals, specialized receptors are the basis of sense organs and there is a central nervous system (brain and spinal cord) and a peripheral nervous system. The latter consists of sensory nerves that transmit information from sense organs and motor nerves that influence target organs. The peripheral nervous system is divided into the somatic nervous system which conveys sensation and controls voluntary muscle, and the autonomic nervous system which involuntarily controls smooth muscle, certain glands and internal organs, including the stomach.
Vertebrate anatomy.
All vertebrates have a similar basic body plan and at some point in their lives, (mostly in the embryonic stage), share the major chordate characteristics; a stiffening rod, the notochord; a dorsal hollow tube of nervous material, the neural tube; pharyngeal arches; and a tail posterior to the anus. The spinal cord is protected by the vertebral column and is above the notochord and the gastrointestinal tract is below it. Nervous tissue is derived from the ectoderm, connective tissues are derived from mesoderm, and gut is derived from the endoderm. At the posterior end is a tail which continues the spinal cord and vertebrae but not the gut. The mouth is found at the anterior end of the animal, and the anus at the base of the tail. The defining characteristic of a vertebrate is the vertebral column, formed in the development of the segmented series of vertebrae. In most vertebrates the notochord becomes the nucleus pulposus of the intervertebral discs. However, a few vertebrates, such as the sturgeon and the coelacanth retain the notochord into adulthood. Jawed vertebrates are typified by paired appendages, fins or legs, which may be secondarily lost. The limbs of vertebrates are considered to be homologous because the same underlying skeletal structure was inherited from their last common ancestor. This is one of the arguments put forward by Charles Darwin to support his theory of evolution.
Fish anatomy.
The body of a fish is divided into a head, trunk and tail, although the divisions between the three are not always externally visible. The skeleton, which forms the support structure inside the fish, is either made of cartilage, in cartilaginous fish, or bone in bony fish. The main skeletal element is the vertebral column, composed of articulating vertebrae which are lightweight yet strong. The ribs attach to the spine and there are no limbs or limb girdles. The main external features of the fish, the fins, are composed of either bony or soft spines called rays, which with the exception of the caudal fins, have no direct connection with the spine. They are supported by the muscles which compose the main part of the trunk. The heart has two chambers and pumps the blood through the respiratory surfaces of the gills and on round the body in a single circulatory loop. The eyes are adapted for seeing underwater and have only local vision. There is an inner ear but no external or middle ear. Low frequency vibrations are detected by the lateral line system of sense organs that run along the length of the sides of fish, and these respond to nearby movements and to changes in water pressure.
Sharks and rays are basal fish with numerous primitive anatomical features similar to those of ancient fish, including skeletons composed of cartilage. Their bodies tend to be dorso-ventrally flattened, they usually have five pairs of gill slits and a large mouth set on the underside of the head. The dermis is covered with separate dermal placoid scales. They have a cloaca into which the urinary and genital passages open, but not a swim bladder. Cartilaginous fish produce a small number of large, yolky eggs. Some species are ovoviviparous and the young develop internally but others are oviparous and the larvae develop externally in egg cases.
The bony fish lineage shows more derived anatomical traits, often with major evolutionary changes from the features of ancient fish. They have a bony skeleton, are generally laterally flattened, have five pairs of gills protected by an operculum, and a mouth at or near the tip of the snout. The dermis is covered with overlapping scales. Bony fish have a swim bladder which helps them maintain a constant depth in the water column, but not a cloaca. They mostly spawn a large number of small eggs with little yolk which they broadcast into the water column.
Amphibian anatomy.
Amphibians are a class of animals comprising frogs, salamanders and caecilians. They are tetrapods, but the caecilians and a few species of salamander have either no limbs or their limbs are much reduced in size. Their main bones are hollow and lightweight and are fully ossified and the vertebrae interlock with each other and have articular processes. Their ribs are usually short and may be fused to the vertebrae. Their skulls are mostly broad and short, and are often incompletely ossified. Their skin contains little keratin and lacks scales, but contains many mucous glands and in some species, poison glands. The hearts of amphibians have three chambers, two atria and one ventricle. They have a urinary bladder and nitrogenous waste products are excreted primarily as urea. Amphibians breathe by means of buccal pumping, a pump action in which air is first drawn into the buccopharyngeal region through the nostrils. These are then closed and the air is forced into the lungs by contraction of the throat. They supplement this with gas exchange through the skin which needs to be kept moist.
In frogs the pelvic girdle is robust and the hind legs are much longer and stronger than the forelimbs. The feet have four or five digits and the toes are often webbed for swimming or have suction pads for climbing. Frogs have large eyes and no tail. Salamanders resemble lizards in appearance; their short legs project sideways, the belly is close to or in contact with the ground and they have a long tail. Caecilians superficially resemble earthworms and are limbless. They burrow by means of zones of muscle contractions which move along the body and they swim by undulating their body from side to side.
Reptile anatomy.
Reptiles are a class of animals comprising turtles, tuataras, lizards, snakes and crocodiles. They are tetrapods, but the snakes and a few species of lizard either have no limbs or their limbs are much reduced in size. Their bones are better ossified and their skeletons stronger than those of amphibians. The teeth are conical and mostly uniform in size. The surface cells of the epidermis are modified into horny scales which create a waterproof layer. Reptiles are unable to use their skin for respiration as do amphibians and have a more efficient respiratory system drawing air into their lungs by expanding their chest walls. The heart resembles that of the amphibian but there is a septum which more completely separates the oxygenated and deoxygenated bloodstreams. The reproductive system is designed for internal fertilisation, with a copulatory organ present in most species. The eggs are surrounded by amniotic membranes which prevents them from drying out and are laid on land, or develop internally in some species. The bladder is small as nitrogenous waste is excreted as uric acid.
Turtles are notable for their protective shells. They have an inflexible trunk encased in a horny carapace above and a plastron below. These are formed from bony plates embedded in the dermis which are overlain by horny ones and are partially fused with the ribs and spine. The neck is long and flexible and the head and the legs can be drawn back inside the shell. Turtles are vegetarians and the typical reptile teeth have been replaced by sharp, horny plates. In aquatic species, the front legs are modified into flippers.
Tuataras superficially resemble lizards but the lineages diverged in the Triassic period. There is one living species, "Sphenodon punctatus". The skull has two openings (fenestrae) on either side and the jaw is rigidly attached to the skull. There is one row of teeth in the lower jaw and this fits between the two rows in the upper jaw when the animal chews. The teeth are merely projections of bony material from the jaw and eventually wear down. The brain and heart are more primitive than is the case in other reptiles and the lungs have a single chamber and lack bronchi. The tuatara has a well-developed parietal eye on its forehead.
Lizards have skulls with only one fenestra on each side, the lower bar of bone below the second fenestra having been lost. This results in the jaws being less rigidly attached which allows the mouth to open wider. Lizards are mostly quadrupeds, with the trunk held off the ground by short, sideways-facing legs, but a few species have no limbs and resemble snakes. Lizards have moveable eyelids, eardrums are present and some species have a central parietal eye.
Snakes are closely related to lizards, having branched off from a common ancestral lineage during the Cretaceous period, and they share many of the same features. The skeleton consists of a skull, a hyoid bone, spine and ribs though a few species retain a vestige of the pelvis and rear limbs in the form of pelvic spurs. The bar under the second fenestra has also been lost and the jaws have extreme flexibility allowing the snake to swallow its prey whole. Snakes lack moveable eyelids, the eyes being covered by transparent "spectacle" scales. They do not have eardrums but can detect ground vibrations through the bones of their skull. Their forked tongues are used as organs of taste and smell and some species have sensory pits on their heads enabling them to locate warm-blooded prey.
Crocodilians are large, low-slung aquatic reptiles with long snouts and large numbers of teeth. The head and trunk are dorso-ventrally flattened and the tail is laterally compressed. It undulates from side to side to force the animal through the water when swimming. The tough keratinised scales provide body armour and some are fused to the skull. The nostrils, eyes and ears are elevated above the top of the flat head enabling them to remain above the surface of the water when the animal is floating. Valves seal the nostrils and ears when it is submerged. Unlike other reptiles, crocodilians have hearts with four chambers allowing complete separation of oxygenated and deoxygenated blood.
Bird anatomy.
Birds are tetrapods but though their hind limbs are used for walking or hopping, their front limbs are wings covered with feathers and adapted for flight. Birds are endothermic, have a high metabolic rate, a light skeletal system and powerful muscles. The long bones are thin, hollow and very light. Air sac extensions from the lungs occupy the centre of some bones. The sternum is wide and usually has a keel and the caudal vertebrae are fused. There are no teeth and the narrow jaws are adapted into a horn-covered beak. The eyes are relatively large, particularly in nocturnal species such as owls. They face forwards in predators and sideways in ducks.
The feathers are outgrowths of the epidermis and are found in localized bands from where they fan out over the skin. Large flight feathers are found on the wings and tail, contour feathers cover the bird's surface and fine down occurs on young birds and under the contour feathers of water birds. The only cutaneous gland is the single uropygial gland near the base of the tail. This produces an oily secretion that waterproofs the feathers when the bird preens. There are scales on the legs, feet and claws on the tips of the toes.
Mammal anatomy.
Mammals are a diverse class of animals, mostly terrestrial but some are aquatic and others have evolved flapping or gliding flight. They mostly have four limbs but some aquatic mammals have no limbs or limbs modified into fins and the forelimbs of bats are modified into wings. The legs of most mammals are situated below the trunk, which is held well clear of the ground. The bones of mammals are well ossified and their teeth, which are usually differentiated, are coated in a layer of prismatic enamel. The teeth are shed once (milk teeth) during the animal's lifetime or not at all, as is the case in cetaceans. Mammals have three bones in the middle ear and a cochlea in the inner ear. They are clothed in hair and their skin contains glands which secrete sweat. Some of these glands are specialised as mammary glands, producing milk to feed the young. Mammals breathe with lungs and have a muscular diaphragm separating the thorax from the abdomen which helps them draw air into the lungs. The mammalian heart has four chambers and oxygenated and deoxygenated blood are kept entirely separate. Nitrogenous waste is excreted primarily as urea.
Mammals are amniotes, and most are viviparous, giving birth to live young. The exception to this are the egg-laying monotremes, the platypus and the echidnas of Australia. Most other mammals have a placenta through which the developing foetus obtains nourishment, but in marsupials, the foetal stage is very short and the immature young is born and finds its way to its mother's pouch where it latches on to a nipple and completes its development.
Human anatomy.
Humans have the overall body plan of a mammal. Humans have a head, neck, trunk (which includes the thorax and abdomen), two arms and hands and two legs and feet.
Generally, students of certain biological sciences, paramedics, prosthetists and orthotists, physiotherapists, occupational therapists, nurses, and medical students learn gross anatomy and microscopic anatomy from anatomical models, skeletons, textbooks, diagrams, photographs, lectures and tutorials, and in addition, medical students generally also learn gross anatomy through practical experience of dissection and inspection of cadavers. The study of microscopic anatomy (or histology) can be aided by practical experience examining histological preparations (or slides) under a microscope.
Human anatomy, physiology and biochemistry are complementary basic medical sciences, which are generally taught to medical students in their first year at medical school. Human anatomy can be taught regionally or systemically; that is, respectively, studying anatomy by bodily regions such as the head and chest, or studying by specific systems, such as the nervous or respiratory systems. The major anatomy textbook, Gray's Anatomy, has been reorganized from a systems format to a regional format, in line with modern teaching methods. A thorough working knowledge of anatomy is required by physicians, especially surgeons and doctors working in some diagnostic specialties, such as histopathology and radiology.
Academic anatomists are usually employed by universities, medical schools or teaching hospitals. They are often involved in teaching anatomy, and research into certain systems, organs, tissues or cells.
Invertebrate anatomy.
Invertebrates constitute a vast array of living organisms ranging from the simplest unicellular eukaryotes such as "Paramecium" to such complex multicellular animals as the octopus, lobster and dragonfly. They constitute about 95% of the animal species. By definition, none of these creatures has a backbone. The cells of single-cell protozoans have the same basic structure as those of multicellular animals but some parts are specialised into the equivalent of tissues and organs. Locomotion is often provided by cilia or flagella or may proceed via the advance of pseudopodia, food may be gathered by phagocytosis, energy needs may be supplied by photosynthesis and the cell may be supported by an endoskeleton or an exoskeleton. Some protozoans can form multicellular colonies.
Metazoans are multicellular organism, different groups of cells of which have separate functions. The most basic types of metazoan tissues are epithelium and connective tissue, both of which are present in nearly all invertebrates. The outer surface of the epidermis is normally formed of epithelial cells and secretes an extracellular matrix which provides support to the organism. An endoskeleton derived from the mesoderm is present in echinoderms, sponges and some cephalopods. Exoskeletons are derived from the epidermis and is composed of chitin in arthropods (insects, spiders, ticks, shrimps, crabs, lobsters). Calcium carbonate constitutes the shells of molluscs, brachiopods and some tube-building polychaete worms and silica forms the exoskeleton of the microscopic diatoms and radiolaria. Other invertebrates may have no rigid structures but the epidermis may secrete a variety of surface coatings such as the pinacoderm of sponges, the gelatinous cuticle of cnidarians (polyps, sea anemones, jellyfish) and the collagenous cuticle of annelids. The outer epithelial layer may include cells of several types including sensory cells, gland cells and stinging cells. There may also be protrusions such as microvilli, cilia, bristles, spines and tubercles.
Marcello Malpighi, the father of microscopical anatomy, discovered that plants had tubules similar to those he saw in insects like the silk worm. He observed that when a ring-like portion of bark was removed on a trunk a swelling occurred in the tissues above the ring, and he unmistakably interpreted this as growth stimulated by food coming down from the leaves, and being captured above the ring.
Arthropod anatomy.
Arthropods comprise the largest phylum in the animal kingdom with over a million known invertebrate species.
Insects possess segmented bodies supported by a hard-jointed outer covering, the exoskeleton, made mostly of chitin. The segments of the body are organized into three distinct parts, a head, a thorax and an abdomen. The head typically bears a pair of sensory antennae, a pair of compound eyes, one to three simple eyes (ocelli) and three sets of modified appendages that form the mouthparts. The thorax has three pairs of segmented legs, one pair each for the three segments that compose the thorax and one or two pairs of wings. The abdomen is composed of eleven segments, some of which may be fused and houses the digestive, respiratory, excretory and reproductive systems. There is considerable variation between species and many adaptations to the body parts, especially wings, legs, antennae and mouthparts.
Spiders a class of arachnids have four pairs of legs; a body of two segments—a cephalothorax and an abdomen. Spiders have no wings and no antennae. They have mouthparts called chelicerae which are often connected to venom glands as most spiders are venomous. They have a second pair of appendages called pedipalps attached to the cephalothorax. These have similar segmentation to the legs and function as taste and smell organs. At the end of each male pedipalp is a spoon-shaped cymbium that acts to support the copulatory organ.
History.
Ancient.
Ancient Greek anatomy and physiology underwent great changes and advances throughout the early medieval world. Over time, this medical practice expanded by a continually developing understanding of the functions of organs and structures in the body. Phenomenal anatomical observations of the human body were made, which have contributed towards the understanding of the brain, eye, liver, reproductive organs and the nervous system.
The city of Alexandria was the stepping-stone for Greek anatomy and physiology. Alexandria not only housed the biggest library for medical records and books of the liberal arts in the world during the time of the Greeks, but was also home to many medical practitioners and philosophers. Great patronage of the arts and sciences from the Ptolemy rulers helped raise Alexandria up, further rivaling the cultural and scientific achievements of other Greek states.
Some of the most striking advances in early anatomy and physiology took place in Hellenistic Alexandria. Two of the most famous Greek anatomists and physiologists of the third century were Herophilus and Erasistratus. These two physicians helped pioneer human dissection for medical research. They also conducted vivisections on the cadavers of condemned criminals, which was considered taboo until the Renaissance – Herophilus was recognized as the first person to perform systematic dissections. Herophilus became known for his anatomical works making impressing contributions to many branches of anatomy and many other aspects of medicine. Some of the works included classifying the system of the pulse, the discovery that human arteries had thicker walls then veins, and that the atria were parts of the heart. Herophilus’s knowledge of the human body has provided vital input towards understanding the brain, eye, liver, reproductive organs and nervous system, and characterizing the course of disease. Erasistratus accurately described the structure of the brain, including the cavities and membranes, and made a distinction between its cerebrum and cerebellum During his study in Alexandria, Erasistratus was particularly concerned with studies of the circulatory and nervous systems. He was able to distinguish the sensory and the motor nerves in the human body and believed that air entered the lungs and heart, which was then carried throughout the body. His distinction between the arteries and veins – the arteries carrying the air through the body, while the veins carried the blood from the heart was a great anatomical discovery. Erasistratus was also responsible for naming and describing the function of the epiglottis and the valves of the heart, including the tricuspid. During the third century, Greek physicians were able to differentiate nerves from blood vessels and tendons and to realize that the nerves convey neural impulses. It was Herophilus who made the point that damage to motor nerves induced paralysis. Herophilus named the meninges and ventricles in the brain, appreciated the division between cerebellum and cerebrum and recognized that the brain was the "seat of intellect" and not a "cooling chamber" as propounded by Aristotle Herophilus is also credited with describing the optic, oculomotor, motor division of the trigeminal, facial, vestibulocochlear and hypoglossal nerves 
Great feats were made during the third century in both the digestive and reproductive systems. Herophilus was able to discover and describe not only the salivary glands, but the small intestine and liver. He showed that the uterus is a hollow organ and described the ovaries and uterine tubes. He recognized that spermatozoa were produced by the testes and was the first to identify the prostate gland.
In 1600 BCE, the Edwin Smith Papyrus, an Ancient Egyptian medical text, described the heart, its vessels, liver, spleen, kidneys, hypothalamus, uterus and bladder, and showed the blood vessels diverging from the heart. The Ebers Papyrus (c. 1550 BCE) features a "treatise on the heart", with vessels carrying all the body's fluids to or from every member of the body.
The anatomy of the muscles and skeleton is described in the "Hippocratic Corpus", an Ancient Greek medical work written by unknown authors. Aristotle described vertebrate anatomy based on animal dissection. Praxagoras identified the difference between arteries and veins. Also in the 4th century BCE, Herophilos and Erasistratus produced more accurate anatomical descriptions based on vivisection of criminals in Alexandria during the Ptolemaic dynasty.
In the 2nd century, Galen of Pergamum, an anatomist, clinician, writer and philosopher, wrote the final and highly influential anatomy treatise of ancient times. He compiled existing knowledge and studied anatomy through dissection of animals. He was one of the first experimental physiologists through his vivisection experiments on animals. Galen's drawings, based mostly on dog anatomy, became effectively the only anatomical textbook for the next thousand years. His work was known to Renaissance doctors only through Islamic Golden Age medicine until it was translated from the Greek some time in the 15th century.
Medieval to early modern.
Anatomy developed little from classical times until the sixteenth century; as the historian Marie Boas writes, "Progress in anatomy before the sixteenth century is as mysteriously slow as its development after 1500 is startlingly rapid". Between 1275 and 1326, the anatomists Mondino de Luzzi, Alessandro Achillini and Antonio Benivieni at Bologna carried out the first systematic human dissections since ancient times. Mondino's "Anatomy" of 1316 was the first textbook in the medieval rediscovery of human anatomy. It describes the body in the order followed in Mondino's dissections, starting with the abdomen, then the thorax, then the head and limbs. It was the standard anatomy textbook for the next century.
Leonardo da Vinci (1452–1519) was trained in anatomy by Andrea del Verrocchio. He made use of his anatomical knowledge in his artwork, making many sketches of skeletal structures, muscles and organs of humans and other vertebrates that he dissected.
Andreas Vesalius (1514–1564) (Latinized from Andries van Wezel), professor of anatomy at the University of Padua, is considered the founder of modern human anatomy. Originally from Brabant, Vesalius published the influential book "De humani corporis fabrica" ("the structure of the human body"), a large format book in seven volumes, in 1543. The accurate and intricately detailed illustrations, often in allegorical poses against Italianate landscapes, are thought to have been made by the artist Jan van Calcar, a pupil of Titian.
In England, anatomy was the subject of the first public lectures given in any science; these were given by the Company of Barbers and Surgeons in the 16th century, joined in 1583 by the Lumleian lectures in surgery at the Royal College of Physicians.
Late modern.
In the United States, medical schools began to be set up towards the end of the 18th century. Classes in anatomy needed a continual stream of cadavers for dissection and these were difficult to obtain. Philadelphia, Baltimore and New York were all renowned for body snatching activity as criminals raided graveyards at night, removing newly buried corpses from their coffins. A similar problem existed in Britain where demand for bodies became so great that grave-raiding and even anatomy murder were practised to obtain cadavers. Some graveyards were in consequence protected with watchtowers. The practice was halted in Britain by the Anatomy Act of 1832, while in the United States, similar legislation was enacted after the physician William S. Forbes of Jefferson Medical College was found guilty in 1882 of "complicity with resurrectionists in the despoliation of graves in Lebanon Cemetery".
The teaching of anatomy in Britain was transformed by Sir John Struthers, Regius Professor of Anatomy at the University of Aberdeen from 1863 to 1889. He was responsible for setting up the system of three years of "pre-clinical" academic teaching in the sciences underlying medicine, including especially anatomy. This system lasted until the reform of medical training in 1993 and 2003. As well as teaching, he collected many vertebrate skeletons for his museum of comparative anatomy, published over 70 research papers, and became famous for his public dissection of the Tay Whale. From 1822 the Royal College of Surgeons regulated the teaching of anatomy in medical schools. Medical museums provided examples in comparative anatomy, and were often used in teaching. Ignaz Semmelweis investigated puerperal fever and he discovered how it was caused. He noticed that the frequently fatal fever occurred more often in mothers examined by medical students than by midwives. The students went from the dissecting room to the hospital ward and examined women in childbirth. Semmelweis showed that when the trainees washed their hands in chlorinated lime before each clinical examination, the incidence of puerperal fever among the mothers could be reduced dramatically.
Before the era of modern medical procedures, the main means for studying the internal structure of the body were palpation and dissection. It was the advent of microscopy that opened up an understanding of the building blocks that constituted living tissues. Technical advances in the development of achromatic lenses increased the resolving power of the microscope and around 1839, Matthias Jakob Schleiden and Theodor Schwann identified that cells were the fundamental unit of organization of all living things. Study of small structures involved passing light through them and the microtome was invented to provide sufficiently thin slices of tissue to examine. Staining techniques using artificial dyes were established to help distinguish between different types of tissue. The fields of cytology and histology developed from here in the late 19th century. The invention of the electron microscope brought a great advance in resolution power and allowed research into the ultrastructure of cells and the organelles and other structures within them. About the same time, in the 1950s, the use of X-ray diffraction for studying the crystal structures of proteins, nucleic acids and other biological molecules gave rise to a new field of molecular anatomy.
Short wavelength electromagnetic radiation such as X-rays can be passed through the body and used in medical radiography to view interior structures that have different degrees of opaqueness. Nowadays, modern techniques such as magnetic resonance imaging, computed tomography, fluoroscopy and ultrasound imaging have enabled researchers and practitioners to examine organs, living or dead, in unprecedented detail. They are used for diagnostic and therapeutic purposes and provide information on the internal structures and organs of the body to a degree far beyond the imagination of earlier generations.
Bibliography.
"Main article:" Bibliography of anatomy

</doc>
<doc id="675" url="https://en.wikipedia.org/wiki?curid=675" title="Affirming the consequent">
Affirming the consequent

Affirming the consequent, sometimes called converse error, fallacy of the converse or confusion of necessity and sufficiency, is a formal fallacy of inferring the converse from the original statement. The corresponding argument has the general form:
An argument of this form is invalid, i.e., the conclusion can be false even when statements 1 and 2 are true. Since "P" was never asserted as the "only" sufficient condition for "Q", other factors could account for "Q" (while "P" was false).
To put it differently, if "P" implies "Q", the only inference that can be made is "non-Q" implies "non-P". ("Non-P" and "non-Q" designate the opposite propositions to "P" and "Q".) This is known as logical contraposition. Symbolically:
formula_1
The name "affirming the consequent" derives from the premise "Q", which affirms the "then" clause of the conditional premise.
Examples.
One way to demonstrate the invalidity of this argument form is with a counterexample with true premises but an obviously false conclusion. For example:
Owning Fort Knox is not the "only" way to be rich. Any number of other ways exist to be rich.
However, one can affirm with certainty that "if Bill Gates is not rich" ("non-Q") then "Bill Gates does not own Fort Knox" ("non-P"). This is the contrapositive of the first statement, and it must be true if the original statement is true.
Arguments of the same form can sometimes seem superficially convincing, as in the following example:
But having the flu is not the "only" cause of a sore throat since many illnesses cause sore throat, such as the common cold or strep throat.

</doc>
<doc id="676" url="https://en.wikipedia.org/wiki?curid=676" title="Andrei Tarkovsky">
Andrei Tarkovsky

Andrei Arsenyevich Tarkovsky (; 4 April 1932 – 29 December 1986) was a Soviet and Russian film-maker, writer, film editor, film theorist, theatre and opera director.
Tarkovsky's films include "Ivan's Childhood" (1962), "Andrei Rublev" (1966), "Solaris" (1972), "Mirror" (1975), and "Stalker" (1979). He directed the first five of his seven feature films in the Soviet Union; his last two films, "Nostalghia" (1983) and "The Sacrifice" (1986), were produced in Italy and Sweden, respectively. His work is characterized by long takes, unconventional dramatic structure, distinctly authored use of cinematography, and spiritual and metaphysical themes. His contribution to cinema was so influential that works done in a similar way are described as Tarkovskian.
Ingmar Bergman said of Tarkovsky:"Tarkovsky for me is the greatest (director), the one who invented a new language, true to the nature of film, as it captures life as a reflection, life as a dream."
Life.
Childhood and early life.
Tarkovsky was born in the village of Zavrazhye in the Yuryevetsky District of the Ivanovo Industrial Oblast to poet and translator Arseny Alexandrovich Tarkovsky, native of Kirovohrad, Ukraine; and Maria Ivanova Vishnyakova, a graduate of the Maxim Gorky Literature Institute. Andrei's grandfather Aleksander Tarkowski was a Polish nobleman who worked as a bank clerk.
Tarkovsky spent his childhood in Yuryevets. He was described by childhood friends as active and popular, having many friends and being typically in the center of action. In 1937, his father left the family, subsequently volunteering for the army in 1941. Tarkovsky stayed with his mother, moving with her and his sister Marina to Moscow, where she worked as a proofreader at a printing press. In 1939, Tarkovsky enrolled at the Moscow School № 554. During the war, the three evacuated to Yuryevets, living with his maternal grandmother. In 1943, the family returned to Moscow. Tarkovsky continued his studies at his old school, where the poet Andrey Voznesensky was one of his class-mates. He studied piano at a music school and attended classes at an art school. The family lived on Shchipok Street in the Zamoskvorechye District in Moscow. From November 1947 to Spring 1948 he was in the hospital with tuberculosis. Many themes of his childhood – the evacuation, his mother and her two children, the withdrawn father, the time in the hospital – feature prominently in his film "Mirror".
Following high school graduation, from 1951 to 1952, Tarkovsky studied Arabic at the Oriental Institute in Moscow, a branch of the Academy of Sciences of the USSR. Although he already spoke some Arabic and was a successful student in his first semesters, he did not finish his studies and dropped out to work as a prospector for the Academy of Science Institute for Non-Ferrous Metals and Gold. He participated in a year-long research expedition to the river Kureikye near Turukhansk in the Krasnoyarsk Province. During this time in the Taiga, Tarkovsky decided to study film.
Film school student.
Upon returning from the research expedition in 1954, Tarkovsky applied at the State Institute of Cinematography (VGIK) and was admitted to the film directing program. He was in the same class as Irma Raush whom he married in April 1957.
The early Khrushchev era offered unique opportunities for young film directors. Before 1953, annual film production was low and most films were directed by veteran directors. After 1953, more films were produced, many of them by young directors. The Khrushchev Thaw relaxed Soviet social restrictions a bit and permitted a limited influx of European and North American literature, films and music. This allowed Tarkovsky to see films of the Italian neorealists, French New Wave, and of directors such as Kurosawa, Buñuel, Bergman, Bresson, Andrzej Wajda (whose film "Ashes and Diamonds" influenced Tarkovsky) and Mizoguchi. Tarkovsky absorbed the idea of the auteur as a necessary condition for creativity.
Tarkovsky's teacher and mentor was Mikhail Romm, who taught many film students who would later become influential film directors. In 1956, Tarkovsky directed his first student short film, "The Killers", from a short story of Ernest Hemingway. The short film "There Will Be No Leave Today" and the screenplay "Concentrate" followed in 1958 and 1959.
An important influence on Tarkovsky was the film director Grigori Chukhrai, who was teaching at the VGIK. Impressed by the talent of his student, Chukhrai offered Tarkovsky a position as assistant director for his film "Clear Skies". Tarkovsky initially showed interest but then decided to concentrate on his studies and his own projects.
During his third year at the VGIK, Tarkovsky met Andrei Konchalovsky. They found much in common as they liked the same film directors and shared ideas on cinema and films. In 1959, they wrote the script "Antarctica – Distant Country", which was later published in the "Moskovskij Komsomolets". Tarkovsky submitted the script to Lenfilm, but it was rejected. They were more successful with the script "The Steamroller and the Violin", which they sold to Mosfilm. This became Tarkovsky's graduation project, earning him his diploma in 1960 and winning First Prize at the New York Student Film Festival in 1961.
Career.
Film career in the Soviet Union.
Tarkovsky's first feature film was "Ivan's Childhood" in 1962. He had inherited the film from director Eduard Abalov, who had to abort the project. The film earned Tarkovsky international acclaim and won the Golden Lion award at the Venice Film Festival in 1962. In the same year, on 30 September, his first son Arseny (called Senka in Tarkovsky's diaries) Tarkovsky was born.
In 1965, he directed the film "Andrei Rublev" about the life of Andrei Rublev, the fifteenth-century Russian icon painter. "Andrei Rublev" was not, except for a single screening in Moscow in 1966, immediately released after completion due to problems with Soviet authorities. Tarkovsky had to cut the film several times, resulting in several different versions of varying lengths. A version of the film was presented at the Cannes Film Festival in 1969 and won the FIPRESCI prize. The film was widely released in the Soviet Union in a cut version in 1971.
He divorced his wife, Irma Raush, in June 1970. In the same year, he married Larissa Kizilova (née Egorkina), who had been a production assistant for the film "Andrei Rublev" (they had been living together since 1965). Their son, Andrei Andreyevich Tarkovsky, was born in the same year on 7 August.
In 1972, he completed "Solaris", an adaptation of the novel "Solaris" by Stanisław Lem. He had worked on this together with screenwriter Fridrikh Gorenshtein as early as 1968. The film was presented at the Cannes Film Festival, won the Grand Prix Spécial du Jury and the FIPRESCI prize, and was nominated for the Palme d'Or. From 1973 to 1974, he shot the film "Mirror", a highly autobiographical and unconventionally structured film drawing on his childhood and incorporating some of his father's poems. Tarkovsky had worked on the screenplay for this film since 1967, under the consecutive titles "Confession", "White day" and "A white, white day". From the beginning the film was not well received by Soviet authorities due to its content and its perceived elitist nature. Russian authorities placed the film in the "third category," a severely limited distribution, and only allowed it to be shown in third-class cinemas and workers' clubs. Few prints were made and the film-makers received no returns. Third category films also placed the film-makers in danger of being accused of wasting public funds, which could have serious effects on their future productivity. These difficulties are presumed to have made Tarkovsky play with the idea of going abroad and producing a film outside the Soviet film industry.
During 1975, Tarkovsky also worked on the screenplay "Hoffmanniana", about the German writer and poet E. T. A. Hoffmann. In December 1976, he directed "Hamlet", his only stage play, at the Lenkom Theatre in Moscow. The main role was played by Anatoly Solonitsyn, who also acted in several of Tarkovsky's films. At the end of 1978, he also wrote the screenplay "Sardor" together with the writer Aleksandr Misharin.
The last film Tarkovsky completed in the Soviet Union was "Stalker", inspired by the novel "Roadside Picnic" by the brothers Arkady and Boris Strugatsky. Tarkovsky had met the brothers first in 1971 and was in contact with them until his death in 1986. Initially he wanted to shoot a film based on their novel "Dead Mountaineer's Hotel" and he developed a raw script. Influenced by a discussion with Arkady Strugatsky he changed his plan and began to work on the script based on "Roadside Picnic". Work on this film began in 1976. The production was mired in troubles; improper development of the negatives had ruined all the exterior shots. Tarkovsky's relationship with cinematographer Georgy Rerberg deteriorated to the point where he hired Alexander Knyazhinsky as a new first cinematographer. Furthermore, Tarkovsky suffered a heart attack in April 1978, resulting in further delay. The film was completed in 1979 and won the Prize of the Ecumenical Jury at the Cannes Film Festival.
In the same year Tarkovsky also began the production of the film "The First Day" (Russian: Первый День "Pervyj Dyen′"), based on a script by his friend and long-term collaborator Andrei Konchalovsky. The film was set in 18th-century Russia during the reign of Peter the Great and starred Natalya Bondarchuk and Anatoli Papanov. To get the project approved by Goskino, Tarkovsky submitted a script that was different from the original script, omitting several scenes that were critical of the official atheism in the Soviet Union. After shooting roughly half of the film the project was stopped by Goskino after it became apparent that the film differed from the script submitted to the censors. Tarkovsky was reportedly infuriated by this interruption and destroyed most of the film.
Film career outside the Soviet Union.
During the summer of 1979, Tarkovsky traveled to Italy, where he shot the documentary "Voyage in Time" together with his long-time friend Tonino Guerra. Tarkovsky returned to Italy in 1980 for an extended trip during which he and Guerra completed the script for the film "Nostalghia".
Tarkovsky returned to Italy in 1982 to start shooting "Nostalghia". He did not return to his home country. As Mosfilm withdrew from the project, he had to complete the film with financial support provided by the Italian RAI. Tarkovsky completed the film in 1983. "Nostalghia" was presented at the Cannes Film Festival and won the FIPRESCI prize and the Prize of the Ecumenical Jury. Tarkovsky also shared a special prize called "Grand Prix du cinéma de creation" with Robert Bresson. Soviet authorities prevented the film from winning the Palme d'Or, a fact that hardened Tarkovsky's resolve to never work in the Soviet Union again. In the same year, he also staged the opera "Boris Godunov" at the Royal Opera House in London under the musical direction of Claudio Abbado.
He spent most of 1984 preparing the film "The Sacrifice". At a press conference in Milan on 10 July 1984, he announced that he would never return to the Soviet Union and would remain in Europe. At that time, his son Andrei Jr. was still in the Soviet Union and not allowed to leave the country.
During 1985, he shot the film "The Sacrifice" in Sweden. At the end of the year he was diagnosed with terminal lung cancer. In January 1986, he began treatment in Paris and was joined there by his son, who was finally allowed to leave the Soviet Union. "The Sacrifice" was presented at the Cannes Film Festival and received the Grand Prix Spécial du Jury, the FIPRESCI prize and the Prize of the Ecumenical Jury. As Tarkovsky was unable to attend due to his illness, the prizes were collected by his son, Andrei Jr.
In Tarkovsky's last entry (15 December 1986), he wrote: "But now I have no strength left – that is the problem". The diaries are sometimes also known as "" and were published posthumously in 1989 and in English in 1991.
Tarkovsky died in Paris on 29 December 1986. His funeral ceremony was held at the Alexander Nevsky Cathedral. He was buried on 3 January 1987 in the Russian Cemetery in Sainte-Geneviève-des-Bois in France. The inscription on his gravestone, which was created by the Russian sculptor Ernst Neizvestny, reads: "To the man who saw the Angel".
A controversy emerged in Russia in the early 1990s when it was alleged that Tarkovsky did not die of natural causes but was assassinated by the KGB. Evidence for this hypothesis includes testimonies by former KGB agents who claim that Viktor Chebrikov gave the order to eradicate Tarkovsky to curtail what the Soviet government and the KGB saw as anti-Soviet propaganda by Tarkovsky. Other evidence includes several memoranda that surfaced after the 1991 coup and the claim by one of Tarkovsky's doctors that his cancer could not have developed from a natural cause.
As Tarkovsky, his wife Larisa Tarkovskaya and actor Anatoli Solonitsyn all died from the very same type of lung cancer, Vladimir Sharun, sound designer in "Stalker", is convinced that they were all poisoned when shooting the film near a chemical plant.
Filmography.
Tarkovsky is mainly known as a film director. During his career he directed only seven feature films, as well as three shorts from his time at VGIK. He also wrote several screenplays. He furthermore directed the play "Hamlet" for the stage in Moscow, directed the opera "Boris Godunov" in London, and he directed a radio production of the short story "Turnabout" by William Faulkner. He also wrote "Sculpting in Time", a book on film theory.
Tarkovsky's first feature film was "Ivan's Childhood" in 1962. He then directed "Andrei Rublev" in 1966, "Solaris" in 1972, "Mirror" in 1975 and "Stalker" in 1979. The documentary "Voyage in Time" was produced in Italy in 1982, as was "Nostalghia" in 1983. His last film "The Sacrifice" was produced in Sweden in 1986. Tarkovsky was personally involved in writing the screenplays for all his films, sometimes with a cowriter. Tarkovsky once said that a director who realizes somebody else's screenplay without being involved in it becomes a mere illustrator, resulting in dead and monotonous films.
A book of 60 photos, "Instant Light, Tarkovsky Polaroids", taken by Tarkovsky in Russia and Italy between 1979 and 1984 was published in 2006. The collection was selected by Italian photographer Giovanni Chiaramonte and Tarkovsky's son Andrey A. Tarkovsky.
Awards.
Numerous awards were bestowed on Tarkovsky throughout his lifetime. At the Venice Film Festival he was awarded the Golden Lion for "Ivan's Childhood". At the Cannes Film Festival, he won the FIPRESCI prize four times, the Prize of the Ecumenical Jury three times (more than any other director), and the Grand Prix Spécial du Jury twice. He was also nominated for the Palme d'Or two times. In 1987, the British Academy of Film and Television Arts awarded the BAFTA Award for Best Foreign Language Film to "The Sacrifice".
Under the influence of Glasnost and Perestroika, Tarkovsky was finally recognized in the Soviet Union in the Autumn of 1986, shortly before his death, by a retrospective of his films in Moscow. After his death, an entire issue of the film magazine "Iskusstvo Kino" was devoted to Tarkovsky. In their obituaries, the film committee of the Council of Ministers of the USSR and the Union of Soviet Film Makers expressed their sorrow that Tarkovsky had to spend the last years of his life in exile.
Posthumously, he was awarded the Lenin Prize in 1990, one of the highest state honors in the Soviet Union. In 1989 the "Andrei Tarkovsky Memorial Prize" was established, with its first recipient being the Russian animator Yuriy Norshteyn. Since 1993, the Moscow International Film Festival awards the annual "Andrei Tarkovsky Award". In 1996 the Andrei Tarkovsky Museum opened in Yuryevets, his childhood town. A minor planet, 3345 Tarkovskij, discovered by Soviet astronomer Lyudmila Georgievna Karachkina in 1982, has also been named after him.
Tarkovsky has been the subject of several documentaries. Most notable is the 1988 documentary "Moscow Elegy", by Russian film director Alexander Sokurov. Sokurov's own work has been heavily influenced by Tarkovsky. The film consists mostly of narration over stock footage from Tarkovsky's films. "Directed by Andrei Tarkovsky" is 1988 documentary film by Michal Leszczylowski, an editor of the film "The Sacrifice". Film director Chris Marker produced the television documentary "One Day in the Life of Andrei Arsenevich" as an homage to Andrei Tarkovsky in 2000.
Ingmar Bergman was quoted as saying: "Tarkovsky for me is the greatest f us al, the one who invented a new language, true to the nature of film, as it captures life as a reflection, life as a dream". Film historian Steven Dillon claims that much of subsequent film was deeply influenced by the films of Tarkovsky.
At the entrance to the Gerasimov Institute of Cinematography in Moscow, Russia there is a monument that includes statues of Tarkovsky, Gennady Shpalikov and Vasily Shukshin.
Concentrate.
Concentrate (, "Konsentrat") is a never-filmed 1958 screenplay by Russian film director Andrei Tarkovsky. The screenplay is based on Tarkovsky's year in the taiga as a member of a research expedition, prior to his enrollment in film school.
Plot.
"Concentrate" is about the leader of a geological expedition, who waits for the boat that brings back the concentrates collected by the expedition. The expedition is surrounded by mystery, and its purpose is a state secret. This screenplay refers to Tarkovsky's year in the taiga, where he was a member of a research expedition prior to enrolling at the film school.
Background.
Although some authors claim that the screenplay was filmed, according to Marina Tarkovskaya, Tarkovsky's sister (and wife of Aleksandr Gordon, a fellow student of Tarvosky during his film school years) the screenplay was never filmed. Tarkovsky wrote the screenplay during his entrance examination at the State Institute of Cinematography (VGIK) in a single sitting. He earned the highest possible grade, excellent () for this work. In 1994 fragments of the "Concentrate" were filmed and used in the documentary "Andrei Tarkovsky's Taiga Summer" by Marina Tarkovskaya and Aleksandr Gordon.
Influences.
Tarkovsky became a film director during the mid and late 1950s, a period referred to as the Khrushchev Thaw, during which Soviet society opened to foreign films, literature and music, among other things. This allowed Tarkovsky to see films of European, American and Japanese directors, an experience which influenced his own film making. His teacher and mentor at the film school, Mikhail Romm, allowed his students considerable freedom and emphasized the independence of the film director.
Tarkovsky was, according to Shavkat Abdusalmov, a fellow student at the film school, fascinated by Japanese films. He was amazed by how every character on the screen is exceptional and how everyday events such as a Samurai cutting bread with his sword are elevated to something special and put into the limelight. Tarkovsky has also expressed interest in the art of Haiku and its ability to create "images in such a way that they mean nothing beyond themselves."
In 1972, Tarkovsky told film historian Leonid Kozlov his ten favorite films. The list includes: "Diary of a Country Priest" and "Mouchette", by Robert Bresson; "Winter Light", "Wild Strawberries" and "Persona", by Ingmar Bergman; "Nazarín", by Luis Buñuel; "City Lights", by Charlie Chaplin; "Ugetsu", by Kenji Mizoguchi; "Seven Samurai", by Akira Kurosawa, and "Woman in the Dunes", by Hiroshi Teshigahara. Among his favorite directors were Buñuel, Mizoguchi, Bergman, Bresson, Kurosawa, Michelangelo Antonioni, Jean Vigo, and Carl Theodor Dreyer.
With the exception of "City Lights", the list does not contain any films of the early silent era. The reason is that Tarkovsky saw film as an art as only a relatively recent phenomenon, with the early film-making forming only a prelude. The list has also no films or directors from Tarkovsky's native Russia, although he rated Soviet directors such as Boris Barnet, Sergei Parajanov and Alexander Dovzhenko highly.
Although strongly opposed to commercial cinema, in a famous exception Tarkovsky praised the blockbuster film "The Terminator", saying its "vision of the future and the relation between man and its destiny is pushing the frontier of cinema as an art". He was critical of the "brutality and low acting skills", but nevertheless impressed by this film.
Cinematic style.
Tarkovsky's films are characterized by metaphysical themes, extremely long takes, and memorable images of exceptional beauty. Recurring motifs are dreams, memory, childhood, running water accompanied by fire, rain indoors, reflections, levitation, and characters re-appearing in the foreground of long panning movements of the camera. He once said, "Juxtaposing a person with an environment that is boundless, collating him with a countless number of people passing by close to him and far away, relating a person to the whole world, that is the meaning of cinema.”
Tarkovsky included levitation scenes into several of his films, most notably "Solaris". To him these scenes possess great power and are used for their photogenic value and magical inexplicability.
Water, clouds, and reflections were used by him for their surreal beauty and photogenic value, as well as their symbolism, such as waves or the forms of brooks or running water.
Bells and candles are also frequent symbols. These are symbols of film, sight and sound, and Tarkovsky's film frequently has themes of self-reflection.
Tarkovsky developed a theory of cinema that he called "sculpting in time". By this he meant that the unique characteristic of cinema as a medium was to take our experience of time and alter it. Unedited movie footage transcribes time in real time. By using long takes and few cuts in his films, he aimed to give the viewers a sense of time passing, time lost, and the relationship of one moment in time to another.
Up to, and including, his film "Mirror", Tarkovsky focused his cinematic works on exploring this theory. After "Mirror", he announced that he would focus his work on exploring the dramatic unities proposed by Aristotle: a concentrated action, happening in one place, within the span of a single day.
Several of Tarkovsky's films have color or black and white sequences. This first occurs in the otherwise monochrome "Andrei Rublev", which features a color epilogue of Rublev's authentic religious icon paintings. All of his films afterwards contain monochrome, and in "Stalker's" case sepia sequences, while otherwise being in color. In 1966, in an interview conducted shortly after finishing "Andrei Rublev", Tarkovsky dismissed color film as a "commercial gimmick" and cast doubt on the idea that contemporary films meaningfully use color. He claimed that in everyday life one does not consciously notice colors most of the time, and that color should therefore be used in film mainly to emphasize certain moments, but not all the time, as this distracts the viewer. To him, films in color were like moving paintings or photographs, which are too beautiful to be a realistic depiction of life.
Vadim Yusov.
Tarkovsky worked in close collaboration with cinematographer Vadim Yusov from 1958 to 1972, and much of the visual style of Tarkovsky's films can be attributed to this collaboration. Tarkovsky would spend two days preparing for Yusov to film a single long take, and due to the preparation, usually only a single take was needed.
Sven Nykvist.
In his last film, "The Sacrifice", Tarkovsky worked with cinematographer Sven Nykvist, who had worked closely with director Ingmar Bergman on many of Ingmar Bergman's films – multiple people who worked with Bergman worked on the production, notably lead actor Erland Josephson, who had acted for Tarkovsky in "Nostalghia". Nykvist complained that Tarkovsky would frequently look through the camera and even direct actors through it.
References.
Notes
Bibliography

</doc>
<doc id="677" url="https://en.wikipedia.org/wiki?curid=677" title="Ambiguity">
Ambiguity

Ambiguity is a type of uncertainty of meaning in which several interpretations are plausible. It is thus an attribute of any idea or statement whose intended meaning cannot be definitively resolved according to a rule or process with a finite number of steps. (The "ambi-" part of the name reflects an idea of "two" as in two meanings.)
The concept of ambiguity is generally contrasted with vagueness. In ambiguity, specific and distinct interpretations are permitted (although some may not be immediately apparent), whereas with information that is vague, it is difficult to form any interpretation at the desired level of specificity.
Context may play a role in resolving ambiguity. For example, the same piece of information may be ambiguous in one context and unambiguous in another.
Linguistic forms.
The lexical ambiguity of a word or phrase pertains to its having more than one meaning in the language to which the word belongs. "Meaning" here refers to whatever should be captured by a good dictionary. For instance, the word "bank" has several distinct lexical definitions, including "financial institution" and "edge of a river". Another example is as in "apothecary". One could say "I bought herbs from the apothecary". This could mean one actually spoke to the apothecary (pharmacist) or went to the apothecary (pharmacy).
The context in which an ambiguous word is used often makes it evident which of the meanings is intended. If, for instance, someone says "I buried $100 in the bank", most people would not think someone used a shovel to dig in the mud. However, some linguistic contexts do not provide sufficient information to disambiguate a used word. 
Lexical ambiguity can be addressed by algorithmic methods that automatically associate the appropriate meaning with a word in context, a task referred to as word sense disambiguation.
The use of multi-defined words requires the author or speaker to clarify their context, and sometimes elaborate on their specific intended meaning (in which case, a less ambiguous term should have been used). The goal of clear concise communication is that the receiver(s) have no misunderstanding about what was meant to be conveyed. An exception to this could include a politician whose "weasel words" and obfuscation are necessary to gain support from multiple constituents with mutually exclusive conflicting desires from their candidate of choice. Ambiguity is a powerful tool of political science.
More problematic are words whose senses express closely related concepts. "Good", for example, can mean "useful" or "functional" ("That's a good hammer"), "exemplary" ("She's a good student"), "pleasing" ("This is good soup"), "moral" ("a good person" versus "the lesson to be learned from a story"), "righteous", etc. " I have a good daughter" is not clear about which sense is intended. The various ways to apply prefixes and suffixes can also create ambiguity ("unlockable" can mean "capable of being unlocked" or "impossible to lock").
Syntactic ambiguity arises when a sentence can have two (or more) different meanings because of the structure of the sentence—its syntax. This is often due to a modifying expression, such as a prepositional phrase, the application of which is unclear. "He ate the cookies on the couch", for example, could mean that he ate those cookies that were on the couch (as opposed to those that were on the table), or it could mean that he was sitting on the couch when he ate the cookies. "To get in, you will need an entrance fee of $10 or your voucher and your drivers' license." This could mean that you need EITHER ten dollars OR BOTH your voucher and your license. Or it could mean that you need your license AND you need EITHER ten dollars OR a voucher. Only rewriting the sentence, or placing appropriate punctuation can resolve a syntactic ambiguity.
For the notion of, and theoretic results about, syntactic ambiguity in artificial, formal languages (such as computer programming languages), see Ambiguous grammar.
Spoken language can contain many more types of ambiguities which are called phonological ambiguities, where there is more than one way to compose a set of sounds into words. For example, "ice cream" and "I scream". Such ambiguity is generally resolved according to the context. A mishearing of such, based on incorrectly resolved ambiguity, is called a mondegreen.
Semantic ambiguity happens when a sentence contains an ambiguous word or phrase—a word or phrase that has more than one meaning. In "We saw her duck" (example due to Richard Nordquist), the word "duck" can refer either
For example, "You could do with a new automobile. How about a test drive?" The clause "You could do with" presents a statement with such wide possible interpretation as to be essentially meaningless. Lexical ambiguity is contrasted with semantic ambiguity. The former represents a choice between a finite number of known and meaningful context-dependent interpretations. The latter represents a choice between any number of possible interpretations, none of which may have a standard agreed-upon meaning. This form of ambiguity is closely related to vagueness.
Linguistic ambiguity can be a problem in law, because the interpretation of written documents and oral agreements is often of paramount importance.
Intentional application.
Philosophers (and other users of logic) spend a lot of time and effort searching for and removing (or intentionally adding) ambiguity in arguments, because it can lead to incorrect conclusions and can be used to deliberately conceal bad arguments. For example, a politician might say "I oppose taxes which hinder economic growth", an example of a glittering generality. Some will think he opposes taxes in general, because they hinder economic growth. Others may think he opposes only those taxes that he believes will hinder economic growth. In writing, the sentence can be rewritten to reduce possible misinterpretation, either by adding a comma after "taxes" (to convey the first sense) or by changing "which" to "that" (to convey the second sense), or by rewriting it in other ways. The devious politician hopes that each constituent will interpret the statement in the most desirable way, and think the politician supports everyone's opinion. However, the opposite can also be true – An opponent can turn a positive statement into a bad one, if the speaker uses ambiguity (intentionally or not). The logical fallacies of amphiboly and equivocation rely heavily on the use of ambiguous words and phrases.
In Continental philosophy (particularly phenomenology and existentialism), there is much greater tolerance of ambiguity, as it is generally seen as an integral part of the human condition. Martin Heidegger argued that the relation between the subject and object is ambiguous, as is the relation of mind and body, and part and whole. In Heidegger's phenomenology, Dasein is always in a meaningful world, but there is always an underlying background for every instance of signification. Thus, although some things may be certain, they have little to do with Dasein's sense of "care" and existential anxiety, e.g., in the face of death. In calling his work Being and Nothingness an "essay in phenomenological ontology" Jean-Paul Sartre follows Heidegger in defining the human essence as ambiguous, or relating fundamentally to such ambiguity. Simone de Beauvoir tries to base an ethics on Heidegger's and Sartre's writings (The Ethics of Ambiguity), where she highlights the need to grapple with ambiguity: "as long as philosophers and they e have thought, most of them have tried to mask it...And the ethics which they have proposed to their disciples has always pursued the same goal. It has been a matter of eliminating the ambiguity by making oneself pure inwardness or pure externality, by escaping from the sensible world or being engulfed by it, by yielding to eternity or enclosing oneself in the pure moment.". Ethics cannot be based on the authoritative certainty given by mathematics and logic, or prescribed directly from the empirical findings of science. She states: "Since we do not succeed in fleeing it, let us therefore try to look the truth in the face. Let us try to assume our fundamental ambiguity. It is in the knowledge of the genuine conditions of our life that we must draw our strength to live and our reason for acting". Other continental philosophers suggest that concepts such as life, nature, and sex are ambiguous. Recently, Corey Anton has argued that we cannot be certain what is separate from or unified with something else: language, he asserts, divides what is not in fact separate. Following Ernest Becker, he argues that the desire to 'authoritatively disambiguate' the world and existence has led to numerous ideologies and historical events such as genocide. On this basis, he argues that ethics must focus on 'dialectically integrating opposites' and balancing tension, rather than seeking a priori validation or certainty. Like the existentialists and phenomenologists, he sees the ambiguity of life as the basis of creativity.
In literature and rhetoric, ambiguity can be a useful tool. Groucho Marx's classic joke depends on a grammatical ambiguity for its humor, for example: "Last night I shot an elephant in my pajamas. How he got in my pajamas, I'll never know". Songs and poetry often rely on ambiguous words for artistic effect, as in the song title "Don't It Make My Brown Eyes Blue" (where "blue" can refer to the color, or to sadness).
In narrative, ambiguity can be introduced in several ways: motive, plot, character. F. Scott Fitzgerald uses the latter type of ambiguity with notable effect in his novel "The Great Gatsby".
Christianity and Judaism employ the concept of paradox synonymously with 'ambiguity'. Many Christians and Jews endorse Rudolf Otto's description of the sacred as 'mysterium tremendum et fascinans', the awe-inspiring mystery which fascinates humans. The orthodox Catholic writer G. K. Chesterton regularly employed paradox to tease out the meanings in common concepts which he found ambiguous, or to reveal meaning often overlooked or forgotten in common phrases. (The title of one of his most famous books, Orthodoxy, itself employing such a paradox.)
Metonymy involves the use of the name of a subcomponent part as an abbreviation, or jargon, for the name of the whole object (for example "wheels" to refer to a car, or "flowers" to refer to beautiful offspring, an entire plant, or a collection of blooming plants). In modern vocabulary critical semiotics, metonymy encompasses any potentially ambiguous word substitution that is based on contextual contiguity (located close together), or a function or process that an object performs, such as "sweet ride" to refer to a nice car. Metonym miscommunication is considered a primary mechanism of linguistic humour.
Psychology and management.
In sociology and social psychology, the term "ambiguity" is used to indicate situations that involve uncertainty. An increasing amount of research is concentrating on how people react and respond to ambiguous situations. Much of this focuses on ambiguity tolerance. A number of correlations have been found between an individual's reaction and tolerance to ambiguity and a range of factors.
Apter and Desselles (2001) for example, found a strong correlation with such attributes and factors like a greater preference for safe as opposed to risk-based sports, a preference for endurance-type activities as opposed to explosive activities, a more organized and less casual lifestyle, greater care and precision in descriptions, a lower sensitivity to emotional and unpleasant words, a less acute sense of humor, engaging a smaller variety of sexual practices than their more risk-comfortable colleagues, a lower likelihood of the use of drugs, pornography and drink, a greater likelihood of displaying obsessional behavior.
In the field of leadership, David Wilkinson (2006) found strong correlations between an individual leader's reaction to ambiguous situations and the Modes of Leadership they use, the type of creativity, Kirton (2003) and how they relate to others.
Music.
In music, pieces or sections which confound expectations and may be or are interpreted simultaneously in different ways are ambiguous, such as some polytonality, polymeter, other ambiguous meters or rhythms, and ambiguous phrasing, or (Stein 2005, p. 79) any aspect of music. The music of Africa is often purposely ambiguous. To quote Sir Donald Francis Tovey (1935, p. 195), "Theorists are apt to vex themselves with vain efforts to remove uncertainty just where it has a high aesthetic value."
Visual art.
In visual art, certain images are visually ambiguous, such as the Necker cube, which can be interpreted in two ways. Perceptions of such objects remain stable for a time, then may flip, a phenomenon called multistable perception.
The opposite of such ambiguous images are impossible objects.
Pictures or photographs may also be ambiguous at the semantic level: the visual image is unambiguous, but the meaning and narrative may be ambiguous: is a certain facial expression one of excitement or fear, for instance?
Constructed language.
Some languages have been created with the intention of avoiding ambiguity, especially lexical ambiguity. Lojban and Loglan are two related languages which have been created for this, focusing chiefly on syntactic ambiguity as well. The languages can be both spoken and written. These languages are intended to provide a greater technical precision over big natural languages, although historically, such attempts at language improvement have been criticized. Languages composed from many diverse sources contain much ambiguity and inconsistency. The many exceptions to syntax and semantic rules are time-consuming and difficult to learn.
Computer science.
In computer science, the SI prefixes kilo-, mega- and giga- are used ambiguously to mean either the first three powers of 1000 (1000, 1000 and 1000) or the first three powers of 1024 (1024, 1024 and 1024), respectively.
Mathematical notation.
Mathematical notation, widely used in physics and other sciences, avoids many ambiguities compared to expression in natural language. However, for various reasons, several lexical, syntactic and semantic ambiguities remain.
Names of functions.
The ambiguity in the style of writing a function should not be confused with a multivalued function, which can (and should) be defined in a deterministic and unambiguous way. Several special functions still do not have established notations. Usually, the conversion to another notation requires to scale the argument or the resulting value; sometimes, the same name of the function is used, causing confusions. Examples of such underestablished functions:
Expressions.
Ambiguous expressions often appear in physical and mathematical texts.
It is common practice to omit multiplication signs in mathematical expressions. Also, it is common to give the same name to a variable and a function, for example, formula_1. Then, if one sees formula_2, there is no way to distinguish whether it means formula_1 multiplied by formula_4, or function formula_5 evaluated at argument equal to formula_4. In each case of use of such notations, the reader is supposed to be able to perform the deduction and reveal the true meaning.
Creators of algorithmic languages try to avoid ambiguities. Many algorithmic languages (C++ and Fortran) require the character * as symbol of multiplication. The Wolfram Language used in Mathematica allows the user to omit the multiplication symbol, but requires square brackets to indicate the argument of a function; square brackets are not allowed for grouping of expressions. Fortran, in addition, does not allow use of the same name (identifier) for different objects, for example, function and variable; in particular, the expression f=f(x) is qualified as an error.
The order of operations may depend on the context. In most programming languages, the operations of division and multiplication have equal priority and are executed from left to right. Until the last century, many editorials assumed that multiplication is performed first, for example, formula_7 is interpreted as formula_8; in this case, the insertion of parentheses is required when translating the formulas to an algorithmic language. In addition, it is common to write an argument of a function without parenthesis, which also may lead to ambiguity.
Sometimes, one uses "italics" letters to denote elementary functions.
In the scientific journal style, the expression
formula_9
means
product of variables
formula_10,
formula_11,
formula_12 and
formula_13, although in a slideshow, it may mean formula_14.
A comma in subscripts and superscripts sometimes is omitted; it is also ambiguous notation.
If it is written formula_15, the reader should guess from the context, does it mean a single-index object, evaluated while the subscript is equal to product of variables
formula_16, formula_12 and formula_18, or it is indication to a trivalent tensor.
The writing of formula_15 instead of formula_20 may mean that the writer either is stretched in space (for example, to reduce the publication fees) or aims to increase number of publications without considering readers. The same may apply to any other use of ambiguous notations.
Subscripts are also used to denote the argument to a function, as in formula_21.
Examples of potentially confusing ambiguous mathematical expressions.
formula_22, which could be understood to mean either formula_23 or formula_24. In addition, formula_25 may mean formula_26, as formula_27 means formula_28 (see tetration).
formula_29, which by convention means formula_30, though it might be thought to mean formula_31, since formula_32 means formula_33.
formula_34, which arguably should mean formula_35 but would commonly be understood to mean formula_36 .
Notations in quantum optics and quantum mechanics.
It is common to define the coherent states in quantum optics with formula_37 and states with fixed number of photons with formula_38. Then, there is an "unwritten rule": the state is coherent if there are more Greek characters than Latin characters in the argument, and formula_39photon state if the Latin characters dominate. The ambiguity becomes even worse, if formula_40 is used for the states with certain value of the coordinate, and formula_41 means the state with certain value of the momentum, which may be used in books on quantum mechanics. Such ambiguities easy lead to confusions, especially if some normalized adimensional, dimensionless variables are used. Expression formula_42 may mean a state with single photon, or the coherent state with mean amplitude equal to 1, or state with momentum equal to unity, and so on. The reader is supposed to guess from the context.
Ambiguous terms in physics and mathematics.
Some physical quantities do not yet have established notations; their value (and sometimes even dimension, as in the case of the Einstein coefficients), depends on the system of notations. Many terms are ambiguous. Each use of an ambiguous term should be preceded by the definition, suitable for a specific case. Just like Ludwig Wittgenstein states in Tractatus Logico-Philosophicus: "... Only in the context of a proposition has a name meaning."
A highly confusing term is "gain". For example, the sentence "the gain of a system should be doubled", without context, means close to nothing.
It may mean that the ratio of the output voltage of an electric circuit to the input voltage should be doubled.
It may mean that the ratio of the output power of an electric or optical circuit to the input power should be doubled.
It may mean that the gain of the laser medium should be doubled, for example, doubling the population of the upper laser level in a quasi-two level system (assuming negligible absorption of the ground-state).
The term "intensity" is ambiguous when applied to light. The term can refer to any of irradiance, luminous intensity, radiant intensity, or radiance, depending on the background of the person using the term.
Also, confusions may be related with the use of atomic percent as measure of concentration of a dopant, or resolution of an imaging system, as measure of the size of the smallest detail which still can be resolved at the background of statistical noise. See also Accuracy and precision and its talk.
The Berry paradox arises as a result of systematic ambiguity in the meaning of terms such as "definable" or "nameable". Terms of this kind give rise to vicious circle fallacies. Other terms with this type of ambiguity are: satisfiable, true, false, function, property, class, relation, cardinal, and ordinal.
Mathematical interpretation of ambiguity.
In mathematics and logic, ambiguity can be considered to be an instance of the logical concept of underdetermination—for example, formula_43 leaves open what the value of "X" is—while its opposite is a self-contradiction, also called inconsistency, paradoxicalness, or oxymoron, or in mathematics an inconsistent system—such as formula_44, which has no solution.
Logical ambiguity and self-contradiction is analogous to visual ambiguity and impossible objects, such as the Necker cube and impossible cube, or many of the drawings of M. C. Escher.
Pedagogic use of ambiguous expressions.
Ambiguity can be used as a pedagogical trick, to force students to reproduce the deduction by themselves. Some textbooks
give the same name to the function and to its Fourier transform:
Rigorously speaking, such an expression requires that formula_46;
even if function formula_47 is a self-Fourier function, the expression should be written as
formula_48; however, it is assumed that
the shape of the function (and even its norm
formula_49) depend on the character used to denote its argument.
If the Greek letter is used, it is assumed to be a Fourier transform of another function,
The first function is assumed, if the expression in the argument contains more characters formula_50 or formula_51, than characters formula_52, and the second function is assumed in the opposite case. Expressions like formula_53 or formula_54 contain symbols formula_50 and formula_52 in equal amounts; they are ambiguous and should be avoided in serious deduction.

</doc>
<doc id="679" url="https://en.wikipedia.org/wiki?curid=679" title="Animal (disambiguation)">
Animal (disambiguation)

An animal is a multicellular, eukaryotic organism of the kingdom Animalia or Metazoa.
Animal or Animals may also refer to:

</doc>
<doc id="680" url="https://en.wikipedia.org/wiki?curid=680" title="Aardvark">
Aardvark

The aardvark ( ; "Orycteropus afer") is a medium-sized, burrowing, nocturnal mammal native to Africa. It is the only living species of the order Tubulidentata, although other prehistoric species and genera of Tubulidentata are known. Unlike other insectivores, it has a long pig-like snout, which is used to sniff out food. It roams over most of the southern two-thirds of the African continent, avoiding mainly rocky areas. A nocturnal feeder, it subsists on ants and termites, which it will dig out of their hills using its sharp claws and powerful legs. It also digs to create burrows in which to live and rear its young. It receives a "least concern" rating from the IUCN, although its numbers seem to be decreasing.
Naming and taxonomy.
Naming.
The aardvark is sometimes colloquially called "African ant bear", "anteater", or the "Cape anteater" after the Cape of Good Hope. The name "aardvark" () comes from earlier Afrikaans (erdvark) and means "earth pig" or "ground pig" ("aarde" earth/ground, "vark" pig), because of its burrowing habits (similar origin to the name groundhog). The name "Orycteropus" means burrowing foot, and the name "afer" refers to Africa. The name of the aardvarks's order, "Tubulidentata" comes from the tubule style teeth.
Taxonomy.
The aardvark is not closely related to the pig; rather, it is the sole extant representative of the obscure mammalian order Tubulidentata, in which it is usually considered to form one variable species of the genus "Orycteropus", the sole surviving genus in the family Orycteropodidae. The aardvark is not closely related to the South American anteater, despite sharing some characteristics and a superficial resemblance. The similarities are based on convergent evolution. The closest living relatives of the aardvark are the elephant shrews, tenrecs and golden moles. Along with the sirenians, hyraxes, elephants, and their extinct relatives, these animals form the superorder Afrotheria. Studies of the brain have shown the similarities with Condylarthra, and given the clade's status as a wastebasket taxon it may mean some species traditionally classified as "condylarths" are actually stem-aardvarks.
Evolutionary history.
Based on fossils, Bryan Patterson has concluded that early relatives of the aardvark appeared in Africa around the end of the Paleocene. The ptolemaiidans, a mysterious clade of mammals with uncertain affinities, may actually be stem-aardvarks, either as a sister clade to Tubulidentata or as a grade leading to true tubulidentates.
The first unambiguous tubulidentate was probably "Myorycteropus africanus" from Kenyan Miocene deposits. The earliest example from the "Orycteropus" genus was the "Orycteropus mauritanicus" found in Algeria in deposits from the middle Miocene, with an equally aged version found in Kenya. Fossils from the aardvark have been dated to 5 million years, and have been located throughout Europe and the Near East. A close relative lived in Madagascar during the last ice age.
The mysterious Pleistocene "Plesiorycteropus" from Madagascar was originally thought to be a tubulidentate that was descended from ancestors that entered the island during the Eocene. However, a number of subtle anatomical differences coupled with recent molecular evidence now lead researchers to believe that "Plesiorycteropus" is a relative of golden moles and tenrecs that achieved an aardvark-like appearance and ecological niche through convergent evolution.
Subspecies.
The aardvark has seventeen poorly defined subspecies listed:
Description.
The aardvark is vaguely pig-like in appearance. Its body is stout with a prominently arched back and is sparsely covered with coarse hairs. The limbs are of moderate length, with the rear legs being longer than the forelegs. The front feet have lost the pollex (or 'thumb'), resulting in four toes, while the rear feet have all five toes. Each toe bears a large, robust nail which is somewhat flattened and shovel-like, and appears to be intermediate between a claw and a hoof. Whereas the aardvark is considered digitigrade, it appears at time to be plantigrade. This confusion happens because when it squats it stands on its soles.
An aardvark's weight is typically between . An aardvark's length is usually between , and can reach lengths of when its tail (which can be up to ) is taken into account. It is tall at the shoulder, and has a girth of about . It is the largest member of the proposed clade Afroinsectiphilia. The aardvark is pale yellowish-gray in color and often stained reddish-brown by soil. The aardvark's coat is thin, and the animal's primary protection is its tough skin. Its hair is short on its head and tail; however its legs tend to have longer hair. The hair on the majority of its body is grouped in clusters of 3-4 hairs. The hair surrounding its nostrils is dense to help filter particulate matter out as it digs. Its tail is very thick at the base and gradually tapers.
Head.
The greatly elongated head is set on a short, thick neck, and the end of the snout bears a disc, which houses the nostrils. It contains a thin but complete zygomatic arch. The head of the aardvark contains many unique and different features. One of the most distinctive characteristics of the Tubulidentata is their teeth. Instead of having a pulp cavity, each tooth has a cluster of thin, hexagonal, upright, parallel tubes of vasodentin (a modified form of dentine), with individual pulp canals, held together by cementum. The number of columns is dependent on the size of the tooth, with the largest having about 1,500. The teeth have no enamel coating and are worn away and regrow continuously. The aardvark is born with conventional incisors and canines at the front of the jaw, which fall out and are not replaced. Adult aardvarks have only cheek teeth at the back of the jaw, and have a dental formula of: These remaining teeth are peg-like and rootless and are of unique composition. The teeth consist of 14 upper and 12 lower jaw molars. The nasal area of the aardvark is another unique area, as it contains ten nasal conchae, more than any other placental mammal.
The sides of the nostrils are thick with hair. The tip of the snout is highly mobile and is moved by modified mimetic muscles. The fleshy dividing tissue between its nostrils probably has sensory functions, but it is uncertain if it is olfactory or vibration in nature. Its nose is made up of more turbinate bones than any other mammal, with between 9 and 11, compared to dogs with 4 to 5. With a large quantity of turbinate bones, the aardvark has more space for the moist epithelium, which is the location of the olfactory bulb. The nose contains nine olfactory bulbs, more than any other mammal. Its keen sense of smell is not just from the quantity of bulbs in the nose but also in the development of the brain, as its olfactory lobe is very developed. The snout resembles an elongated pig snout. The mouth is small and tubular, typical of species that feed on ants and termites. The aardvark has a long, thin, snakelike, protruding tongue (as much as long) and elaborate structures supporting a keen sense of smell. The ears, which are very effective, are disproportionately long, about long. The eyes are small for its head, and consist only of rods.
Digestive system.
The aardvark's stomach has a muscular pyloric area that acts as a gizzard to grind swallowed food up, thereby rendering chewing unnecessary. Its cecum is large. Both sexes emit a strong smelling secretion from an anal gland. Its salivary glands are highly developed and almost completely ring the neck; their output is what causes the tongue to maintain its tackiness. The female has two pairs of teats in the inguinal region.
Genetically speaking, the aardvark is a living fossil, as its chromosomes are highly conserved, reflecting much of the early eutherian arrangement before the divergence of the major modern taxa.
Habitat and range.
Aardvarks are found in sub-Saharan Africa, where suitable habitat (savannas, grasslands, woodlands and bushland) and food (i.e., ants and termites) is available. They spend the daylight hours in dark underground burrows to avoid the heat of the day. The only major habitat that they are not present in is swamp forest, as the high water table precludes digging to a sufficient depth. They also avoid terrain rocky enough to cause problems with digging. They have been documented as high as in Ethiopia. They are present throughout sub-Saharan Africa all the way to South Africa with few exceptions. These exceptions include the coastal areas of Namibia, Ivory Coast, and Ghana. They are not found in Madagascar.
Ecology and behavior.
Aardvarks live for up to 23 years in captivity. Its keen hearing warns it of predators: lions, leopards, hunting dogs, hyenas, and pythons. Some humans also hunt aardvarks for meat. Aardvarks can dig fast or run in zigzag fashion to elude enemies, but if all else fails, they will strike with their claws, tail and shoulders, sometimes flipping onto their backs lying motionless except to lash out with all four feet. They are capable of causing substantial damage to unprotected areas of an attacker. They will also dig to escape as they can, when pressed, dig extremely quickly. Their thick skin also protects them to some extent.
Feeding.
The aardvark is nocturnal and is a solitary creature that feeds almost exclusively on ants and termites (formivore); the only fruit eaten by aardvarks is the aardvark cucumber. In fact, the cucumber and the aardvark have a symbiotic relationship as they eat the subterranean fruit, then defecate the seeds near their burrows, which then grow rapidly due to the loose soil and fertile nature of the area. The time spent in the intestine of the aardvark helps the fertility of the seed, and the fruit provides needed moisture for the aardvark. They avoid eating the African driver ant and red ants. Due to their stringent diet requirements, they require a large range to survive. An aardvark emerges from its burrow in the late afternoon or shortly after sunset, and forages over a considerable home range encompassing . While foraging for food, the aardvark will keep its nose to the ground and its ears pointed forward, which indicates that both smell and hearing are involved in the search for food. They zig-zag as they forage and will usually not repeat a route for 5–8 days as they appear to allow time for the termite nests to recover before feeding on it again.
During a foraging period, they will stop and dig a "V" shaped trench with their forefeet and then sniff it profusely as a means to explore their location. When a concentration of ants or termites is detected, the aardvark digs into it with its powerful front legs, keeping its long ears upright to listen for predators, and takes up an astonishing number of insects with its long, sticky tongue—as many as 50,000 in one night have been recorded. Its claws enable it to dig through the extremely hard crust of a termite or ant mound quickly. It avoids inhaling the dust by sealing the nostrils. When successful, the aardvark's long (up to ) tongue licks up the insects; the termites' biting, or the ants' stinging attacks are rendered futile by the tough skin. After an aardvark visit at a termite mound, other animals will visit to pick up all the leftovers. Termite mounds alone don't provide enough food for the aardvark, so they look for termites that are on the move. When these insects move, they can form columns long and these tend to provide easy pickings with little effort exerted by the aardvark. These columns are more common in areas of livestock or other hoofed animals. The trampled grass and dung attract termites from Odontotermes, Microtermes, and Pseudacanthotermes genera.
On a nightly basis they tend to be more active during the first portion of the night time (2000-2400); however, they don't seem to prefer bright or dark nights over the other. During adverse weather or if disturbed they will retreat to their burrow systems. They cover between per night; however, some studies have shown that they may traverse as far as in a night.
Vocalization.
The aardvark is a rather quiet animal. However, it does make soft grunting sounds as it forages and loud grunts as it makes for its tunnel entrance. It makes a bleating sound if frightened. When it is threatened it will make for one of its burrows. If one is not close it will dig a new one rapidly. This new one will be short and require the aardvark to back out when the coast is clear.
Movement.
The aardvark is known to be a good swimmer and has been witnessed successfully swimming in strong currents. It can dig a yard of tunnel in about five minutes, but otherwise moves fairly slowly.
When leaving the burrow at night, they pause at the entrance for about ten minutes, sniffing and listening. After this period of watchfulness, it will bound out and within seconds it will be away. It will then pause, prick its ears, twisting its head to listen, then jump and move off to start foraging.
Aside from digging out ants and termites, the aardvark also excavates burrows in which to live; of which they generally fall into three categories: burrows made while foraging, refuge and resting location, and permanent homes. Temporary sites are scattered around the home range and are used as refuges, while the main burrow is also used for breeding. Main burrows can be deep and extensive, have several entrances and can be as long as . These burrows can be large enough for a man to enter. The aardvark changes the layout of its home burrow regularly, and periodically moves on and makes a new one. The old burrows are an important part of the African wildlife scene. As they are vacated, then they are inhabited by smaller animals like the African wild dog, ant-eating chat, "Nycteris thebaica" and warthogs. Other animals that use them are hares, mongooses, hyenas, owls, pythons, and lizards. Without these refuges many animals would die during wildfire season. Only mothers and young share burrows; however, the aardvark is known to live in small family groups or as a solitary creature. If attacked in the tunnel, it will escape by digging out of the tunnel thereby placing the fresh fill between it and its predator, or if it decides to fight it will roll onto its back, and attack with its claws. The aardvark has been known to sleep in a recently excavated ant nest, which also serves as protection from its predators.
Reproduction.
Aardvarks pair only during the breeding season; after a gestation period of seven months, one cub weighing around is born during May–July. When born, the young has flaccid ears and many wrinkles. When nursing, it will nurse off each teat in succession. After two weeks, the folds of skin disappear and after three, the ears can be held upright. After 5–6 weeks, body hair starts growing. It is able to leave the burrow to accompany its mother after only two weeks, and is eating termites at 9 weeks and is weaned by 16 weeks. By three months of age the young has been weaned. At six months of age, it is able to dig its own burrows, but it will often remain with the mother until the next mating season, and is sexually mature from approximately two years of age.
Conservation.
Aardvarks were thought to have declining numbers, however, this is possibly due to the fact that they are not readily seen. There are no definitive counts because of their nocturnal and secretive habits; however, their numbers seem to be stable overall. They are not considered common anywhere in Africa, but due to their large range, they maintain sufficient numbers. There may be a slight decrease in numbers in eastern, northern, and western Africa. Southern African numbers are not decreasing. It receives an official designation from the IUCN as least concern. However, they are a species in a precarious situation, as they are so dependent on such specific food; therefore if a problem arises with the abundance of termites, the species as a whole would be affected drastically.
Aardvarks handle captivity well. The first zoo to have one was London Zoo in 1869, which had an animal from South Africa.
Mythology and popular culture.
In African folklore, the aardvark is much admired because of its diligent quest for food and its fearless response to soldier ants. Hausa magicians make a charm from the heart, skin, forehead, and nails of the aardvark, which they then proceed to pound together with the root of a certain tree. Wrapped in a piece of skin and worn on the chest, the charm is said to give the owner the ability to pass through walls or roofs at night. The charm is said to be used by burglars and those seeking to visit young girls without their parents' permission. Also, some tribes, such as the Margbetu, Ayanda, and Logo, will use aardvark teeth to make bracelets, which are regarded as good luck charms. The meat, which has a resemblance to pork, is eaten in certain cultures.
The Egyptian god Set is said (by some) to have the head of an aardvark or to be part aardvark.
The titular character of "Arthur", an animated television series for children based on a book series and produced by WGBH, shown in more than 180 countries, is an aardvark.
An aardvark features as the antagonist in the cartoon "The Ant and the Aardvark".
In the military, the Air Force supersonic fighter-bomber F-111/FB-111 was nicknamed the Aardvark because of its long nose resembling the animal. It also had similarities with its nocturnal missions flown at a very low level employing ordnance that could penetrate deep into the ground. In the US Navy, the squadron VF-114 was nicknamed the Aardvarks, flying F-4's and then F-14's. The squadron mascot was adapted from the animal in the comic strip B.C., which the F-4 was said to resemble.

</doc>
<doc id="681" url="https://en.wikipedia.org/wiki?curid=681" title="Aardwolf">
Aardwolf

The aardwolf ("Proteles cristata") is a small, insectivorous mammal, native to East and Southern Africa. Its name means "earth wolf" in Afrikaans and Dutch. It is also called "maanhaar jackal" (Afrikaans for "mane jackal") or "civet hyena", based on the secretions from their anal glands, reminiscent of civets. The aardwolf is in the same family as the hyenas. Unlike many of its relatives in the order Carnivora, the aardwolf does not hunt large animals. It eats insects, mainly termites – one aardwolf can eat about 250,000 termites during a single night, using its long, sticky tongue to capture them. The aardwolf lives in the scrublands of eastern and southern Africa – open lands covered with stunted trees and shrubs. It is nocturnal, resting in burrows during the day and emerging at night to seek food. Its diet consists mainly of termites and insect larvae.
Taxonomy.
The aardwolf is the only surviving species in the mammalian subfamily "Protelinae". There is disagreement as to whether the species is monotypic. or can be divided into subspecies "P. c. cristatus" of Southern Africa and "P. c. septentrionalis" of East Africa. Recent studies have shown that the aardwolf probably broke away from the rest of the hyena family early on; however, how early is still unclear as the fossil record and genetic studies disagree by 10 million years.
The aardwolf is generally classified with the Hyaenidae, though it was formerly placed into the family "Protelidae". Early on, scientists felt that it was merely mimicking the striped hyena, which subsequently led to the creation of Protelidae.
Etymology.
The genus name "proteles" comes from two words both of Greek origin, "protos" and "teleos" which combined means "complete in front" based on the fact that they have five toes on their front feet and four on the rear. The species name, "cristatus" comes from Latin and means "provided with a comb", relating to their mane.
Physical characteristics.
The aardwolf resembles a very thin striped hyena, but with a more slender muzzle, black vertical stripes on a coat of yellowish fur, and a long, distinct mane down the midline of the neck and back. It also has one or two diagonal stripes down the fore and hindquarters, along with several stripes on its legs. The mane is raised during confrontations in order to make the aardwolf appear larger. It is missing the throat spot that others in the family have. Its lower leg (from the knee down) is all black, and its tail is bushy with a black tip. The aardwolf is about long, excluding its bushy tail, which is about long, and stands about tall at the shoulders. An adult aardwolf weighs approximately , sometimes reaching . The aardwolves in the south of the continent tend to be smaller (about ), whereas the eastern version weighs more (around ). The front feet have five toes each, unlike the four-toed hyena. The teeth and skull are similar to those of other hyenas, though smaller, and its cheek teeth are specialised for eating insects. It does still have canines; however, unlike other hyenas, these teeth are used primarily for fighting and defense. Its ears, which are large, are very similar to those of the striped hyena.
As an aardwolf ages, it will normally lose some of its teeth, though this has little impact on its feeding habits due to the softness of the insects that it eats. The aardwolf has two anal glands that secrete a musky fluid for marking territory and for communicating with other aardwolves.
Distribution and habitat.
Aardwolves live in open, dry plains and bushland, avoiding mountainous areas. Due to their specific food requirements, they are only found in regions where termites of the family Hodotermitidae occur. Termites of this family depend on dead and withered grass and are most populous in heavily grazed grasslands and savannahs, including farmland. For most of the year, aardwolves spend time in shared territories consisting of up to a dozen dens, which are occupied for six weeks at a time.
There are two distinct populations: one in Southern Africa, and another in East and Northeast Africa. The species does not occur in the intermediary miombo forests.
An adult pair, along with their most recent offspring, will occupy a territory of .
Behavior.
Aardwolves are shy and nocturnal, sleeping in underground burrows by day. They will, on occasion during the winter, become diurnal feeders. This happens during the coldest periods as they then stay in at night to conserve heat.
They have often been mistaken for solitary animals. In fact, they live as monogamous pairs with their young. If their territory is infringed upon, they will chase the intruder up to or to the border. If the intruder is caught, which rarely happens, a fight will occur, which is accompanied by soft clucking, hoarse barking, and a type of roar. The majority of incursions occur during mating season, when they can occur 1–2 times per week. When food is scarce the stringent territorial system may be abandoned and as many as three pairs may occupy a "single territory."
The territory is marked by both sexes, as they both have developed anal glands from which they extrude a black substance that is smeared on rocks or grass stalks in long streaks. They often mark near termite mounds within their territory every 20 minutes or so. If they are patrolling their territorial boundaries, the marking frequency increases drastically, to once every . At this rate, an individual may mark 60 marks per hour, and upwards of 200 per night.
An aardwolf pair may have up to ten dens, and numerous middens, within their territory. When they deposit feces at their middens, they dig a small hole and then cover it with sand. Their dens are usually abandoned aardvark, springhare, or porcupine dens, or on occasion they are crevices in rocks. They will also dig their own dens, or enlarge dens started by springhares. They typically will only use one or two dens at a time, rotating through all of their dens every six months. During the summer, they may rest outside their den during the night, and sleep underground during the heat of the day.
Aardwolfs are not fast runners nor are they particularly adept at fighting off predators. Therefore, when threatened, the aardwolf will attempt to mislead its foe by doubling back on its tracks. If confronted, it will raise its mane in an attempt to appear more menacing. It will also emit a foul-smelling liquid from its anal glands.
Feeding.
The aardwolf feeds primarily on termites and more specifically on "Trinervitermes". This genus of termites has different species throughout the aardwolfs range. In East Africa, they eat "Trinervitermes bettonianus", and in central Africa they eat "Trinervitermes rhodesiensis", and finally in southern Africa, they eat "Trinervitermes trinervoides". Their technique consists of licking them off the ground as opposed to the aardvark which will dig into the mound. They locate their food by sound and also from the scent secreted by the soldier termites. An aardwolf may consume up to 250,000 termites per night using its sticky, long tongue. They do not destroy the termite mound or consume the entire colony, thus ensuring that the termites can rebuild and provide a continuous supply of food. They will often memorize the location of such nests and return to them every few months. During certain seasonal events, such as the onset of the rainy season and the cold of mid-winter, the primary termites become scarce and so the need for other forms of sustenance becomes pronounced. During these times, the southern aardwolf will seek out "Hodotermes mossambicus", a type of harvester termite, a termite active in the afternoon, which explains some of their diurnal behavior in the winter. The eastern aardwolf will, during the rainy season, get variety by subsisting on termites from the genera "Odontotermes" and "Macrotermes". They are also known to feed on other insects, larvae, eggs and, some sources say, occasionally small mammals and birds, but these constitute a very small percentage of their total diet. Unlike other hyenas, aardwolves do not scavenge or kill larger animals. Contrary to popular myths, aardwolfs do not eat carrion, and if they are seen eating while hunched over a dead carcass, it is actually eating larvae and beetles. Also, contrary to some sources, they do not like meat, unless it is finely ground or cooked for them. The adult aardwolf was formerly assumed to forage in small groups, however, more recent research has shown that they are primarily solitary foragers, necessary because of the scarcity of their insect prey. Their primary source, "Trinervitermes", forages in small but dense patches of . While foraging, the aardwolf will cover about per hour, which translates to per summer night and per winter night.
Breeding.
The breeding season varies depending on their location, but normally takes place during autumn or spring. In South Africa, breeding occurs in early July. During the breeding season, unpaired male aardwolves will search their own territory, as well as others, for a female to mate with. Dominant males will also mate opportunistically with the females of less dominant neighboring aardwolves, which can result in conflict between rival males. Dominant males will even go a step further and as the breeding season approaches, they will make increasingly greater and greater incursions onto weaker males' territories. As the female comes into oestrus, they will add pasting to their tricks inside of the other territories, sometimes doing so more in rivals' territories than their own. Females will also, when given the opportunity, mate with the dominant male, which will increase the chances of the dominat male guarding "his" cubs with her. Gestation lasts between 89 and 92 days, producing two to five cubs (most often two or three) during the rainy season (Nov–Dec), when termites are more active. They are born with their eyes open but initially are helpless, and weigh around . The first six to eight weeks are spent in the den with their parents. The male may spend up to six hours a night watching over the cubs while the mother is out looking for food. After three months, they begin supervised foraging and by four months are normally independent, though they will often share a den with their mother until the next breeding season. By the time the next set of cubs is born, the older cubs have moved on. Aardwolves generally achieve sexual maturity at one and a half to two years of age.
Conservation.
The aardwolf has not seen decreasing numbers and they are relatively widespread throughout eastern Africa. They are not common throughout their range, as they maintain a density of no more than 1 per square kilometer, if the food is good. Because of these factors, the IUCN has rated the aardwolf as least concern. In some areas, they are persecuted by man because of the mistaken belief that they prey on livestock; however, they are actually beneficial to the farmers because they eat termites that are detrimental. In other areas, the farmers have recognized this, but they are still killed, on occasion, for their fur. Dogs and insecticides are also common killers of the aardwolf.
Interaction with humans.
Aardwolfs are common sights at zoos. Frankfurt Zoo in Germany was home to the oldest recorded aardwolf in captivity at 18 years and 11 months.

</doc>
<doc id="682" url="https://en.wikipedia.org/wiki?curid=682" title="Adobe">
Adobe

Adobe (, , , from Spanish: mud brick, from Arabic) is a building material made from earth and often organic material. Most adobe buildings are similar to cob and rammed earth buildings. Adobe is among the earliest building materials, and is used throughout the world.
Description.
Adobe bricks are most often made into units weighing less than 100 pounds and small enough that they can quickly air dry individually without cracking and subsequently assembled, with the application of adobe mud, to bond the individual bricks into a structure. Modern methods of construction allow the pouring of whole adobe walls that are reinforced with steel.
Strength.
In dry climates, adobe structures are extremely durable, and account for some of the oldest existing buildings in the world. Adobe buildings offer significant advantages due to their greater thermal mass, but they are known to be particularly susceptible to earthquake damage if they are not somehow reinforced. Cases where adobe structures were widely damaged during earthquakes include the 1976 Guatemala earthquake, the 2003 Bam earthquake and the 2010 Chile earthquake.
Distribution.
Buildings made of sun-dried earth are common throughout the world (Middle East, West Asia, North Africa, West Africa, South America, southwestern North America, Spain, and Eastern Europe.) Adobe had been in use by indigenous peoples of the Americas in the Southwestern United States, Mesoamerica, and the Andean region of South America for several thousand years. The Pueblo people built their adobe structures with handfuls or basketfuls of adobe, until the Spanish introduced them to making bricks. Adobe bricks were used in Spain from the Late Bronze Age and Iron Age, (8th century B.C. onwards). Its wide use can be attributed to its simplicity of design and manufacture, and economics.
A distinction is sometimes made between the smaller "adobes", which are about the size of ordinary baked bricks, and the larger "adobines", some of which may be one to two yards (1–2 m) long.
Etymology.
The word "adobe" has existed for around 4,000 years, with relatively little change in either pronunciation or meaning. The word can be traced from the Middle Egyptian (c. 2000 BC) word "dbt" "mud brick." As Middle Egyptian evolved into Late Egyptian, Demotic (Egyptian), and finally Coptic (c. 600 BC), τωωβε "dj-b-t" became "tobe" "u brick." This was borrowed into Arabic as "al tob", "tuba", or "Al-ţŭb." (الطّوب "al" "the" + "ţŭb." "brick") "u brick," which was assimilated into Old Spanish as "adobe" , still with the meaning "mud brick." English borrowed the word from Spanish in the early 18th century.
In more modern English usage, the term "adobe" has come to include a style of architecture popular in the desert climates of North America, especially in New Mexico.
Composition.
An adobe brick is a composite material made of earth mixed with water and an organic material such as straw or dung. The soil composition typically contains sand, silt and clay. Straw is useful in binding the brick together and allowing the brick to dry evenly, thereby preventing cracking due to uneven shrinkage rates through the brick. Dung offers the same advantage. The most desirable soil texture for producing the mud of adobe is 15% clay, 10-30% silt and 55-75% fine sand. Another source quotes 15-25% clay and the remainder sand and coarser particles up to cobbles with no deleterious effect. Modern adobe is stabilized with either emulsified asphalt or Portland cement up to 10% by weight.
No more than half the clay content should be expansive clays with the remainder non-expansive illite or kaolinite. Too much expansive clay results in uneven drying through the brick resulting in cracking, while too much kaolinite will make a weak brick. Typically the soils of the Southwest United States, where such construction is in use, are an adequate composition.
Material properties.
Adobe walls are load bearing, i.e. they carry their own weight into the foundation rather than by another structure, hence the adobe must have sufficient compressive strength. In the United States, most building codes call for a minimum compressive strength of 300 lbf/in (2.07 newton/mm) for the adobe block. Adobe construction should be designed so as to avoid lateral structural loads that would cause bending loads. The building codes require the building sustain a 1 g lateral acceleration earthquake load. Such an acceleration will cause lateral loads on the walls, resulting in shear and bending and inducing tensile stresses. To withstand such loads, the codes typically call for a tensile modulus of rupture strength of at least 50 lbf/in (0.345 newton/mm) for the finished block.
In addition to being an inexpensive material with a small resource cost, adobe can serve as a significant heat reservoir due to the thermal properties inherent in the massive walls typical in adobe construction. In climates typified by hot days and cool nights, the high thermal mass of adobe mediates the high and low temperatures of the day, moderating the living space temperature. The massive walls require a large and relatively long input of heat from the sun (radiation) and from the surrounding air (convection) before they warm through to the interior. After the sun sets and the temperature drops, the warm wall will then continue to transfer heat to the interior for several hours due to the time-lag effect. Thus, a well-planned adobe wall of the appropriate thickness is very effective at controlling inside temperature through the wide daily fluctuations typical of desert climates, a factor which has contributed to its longevity as a building material.
Thermodynamic material properties are sparsely quoted. The thermal resistance of adobe is quoted as having an R-value of R = 0.41 hr-ft-°F/(Btu-in) and a conductivity of 0.57 W/(m-K) quoted from another source. A third source provides the following properties: conductivity=0.30 Btu/(hr-ft-°F); heat capacity=0.24 Btu/(lbm-°F); density=106 lbm/ft (1700 kg/m). To determine the total R-value of a wall for example, multiply R by the thickness of the wall. From knowledge of the adobe density, heat capacity and a diffusivity value, the conductivity is found to be k = 0.20 Btu/(hr-ft-°F) or 0.35 W/(m-K). The heat capacity is commonly quoted as c = 0.20 Btu/(lbm*F) or 840 joules/(kg-K). The density is 95 lbm/ft or 1520 kg/m. The thermal diffusivity is calculated to be 0.0105 ft/hr or 2.72x10 m/s.
Uses.
Poured and puddled adobe walls.
Poured and puddled adobe (puddled clay, piled earth) today called "cob", is made by placing soft adobe in layers, rather than making by individual dried bricks or using a form. Puddle is a general term for a clay or clay and sand based material worked into a dense, plastic state. These are the oldest methods of building with adobe in the Americas until holes in the ground were used as forms and then later wooden forms used to make individual bricks were introduced by the Spanish.
Adobe bricks.
Bricks made from adobe are usually made by pressing the mud mixture into an open timber frame. In North America, the brick is typically about in size. The mixture is molded into the frame, which is then is removed after initial setting. After drying for a few hours, the bricks are turned on edge to finish drying. Slow drying in shade reduces cracking.
The same mixture, without straw, is used to make mortar and often plaster on interior and exterior walls. Some ancient cultures used lime-based cement for the plaster to protect against rain damage.
Depending on the form into which the mixture is pressed, adobe can encompass nearly any shape or size, provided drying is even and the mixture includes reinforcement for larger bricks. Reinforcement can include manure, straw, cement, rebar or wooden posts. Experience has shown straw, cement, or manure added to a standard adobe mixture can all produce a stronger, more crack-resistant brick. A test is done on the soil content first. To do so, a sample of the soil is mixed into a clear container with some water, creating an almost completely saturated liquid. The container is shaken vigorously for one minute. It is then allowed to settle for a day until the soil has settled into layers. Heavier particles settle out first, sand above, silt above that and very fine clay and organic matter will stay in suspension for days. After the water has cleared, percentages of the various particles can be determined. Fifty to 60 percent sand and 35 to 40 percent clay will yield strong bricks. The New Mexico State University Extension Service recommends a mix of not more than 1/3 clay, not less than 1/2 sand, and never more than 1/3 silt.
Adobe wall construction.
The ground supporting an adobe structure should be compressed, as the weight of adobe wall is significant and foundation settling may cause cracking of the wall. Footing depth is to below the ground frost level. The footing and stem wall are commonly 24 and 14 inches thick, respectively. Modern construction codes call for the use of reinforcing steel in the footing and stem wall. Adobe bricks are laid by course. Adobe walls usually never rise above two stories as they are load bearing and adobe has low structural strength. When creating window and door openings, a lintel is placed on top of the opening to support the bricks above. Atop the last courses of brick, bond beams made of heavy wood beams or modern reinforced concrete are laid to provide a horizontal bearing plate for the roof beams and to redistribute lateral earthquake loads to shear walls more able to carry the forces. To protect the interior and exterior adobe walls, finishes such as mud plaster, whitewash or stucco can be applied. These protect the adobe wall from water damage, but need to be reapplied periodically. Alternatively, the walls can be finished with other nontraditional plasters that provide longer protection. Bricks made with stabilized adobe generally do not need protection of plasters.
Adobe roof.
The traditional adobe roof has been constructed using a mixture of soil/clay, water, sand and organic materials. The mixture was then formed and pressed into wood forms, producing rows of dried earth bricks that would then be laid across a support structure of wood and plastered into place with more adobe.
Depending on the materials available, a roof may be assembled using wood or metal beams to create a framework to begin layering adobe bricks. Depending on the thickness of the adobe bricks, the framework has been preformed using a steel framing and a layering of a metal fencing or wiring over the framework to allow an even load as masses of adobe are spread across the metal fencing like cob and allowed to air dry accordingly. This method was demonstrated with an adobe blend heavily impregnated with cement to allow even drying and prevent cracking.
The more traditional flat adobe roofs are functional only in dry climates that are not exposed to snow loads. The heaviest wooden beams, called vigas, lie atop the wall. Across the vigas lie smaller members called latillas and upon those brush is then laid. Finally, the adobe layer is applied.
To construct a flat adobe roof, beams of wood were laid to span the building, the ends of which were attached to the tops of the walls. Once the vigas, latillas and brush are laid, adobe bricks are placed. An adobe roof is often laid with bricks slightly larger in width to ensure a greater expanse is covered when placing the bricks onto the roof. Following each individual brick should be a layer of adobe mortar, recommended to be at least thick to make certain there is ample strength between the brick’s edges and also to provide a relative moisture barrier during rain. 
Depending on the materials, adobe roofs can be inherently fire-proof. The construction of a chimney can greatly influence the construction of the roof supports, creating an extra need for care in choosing the materials. The builders can make an adobe chimney by stacking simple adobe bricks in a similar fashion as the surrounding walls.
Adobe around the world.
The largest structure ever made from adobe is the Bam Citadel. Other large adobe structures are the Huaca del Sol in Peru, with 100 million signed bricks, the "ciudellas" of Chan Chan and Tambo Colorado, both in Peru.

</doc>
<doc id="683" url="https://en.wikipedia.org/wiki?curid=683" title="Adventure">
Adventure

An adventure is an exciting or unusual experience. It may also be a bold, usually risky undertaking, with an uncertain outcome. Adventures may be activities with some potential for physical danger such as exploring, skydiving, mountain climbing, river rafting or participating in extreme sports. The term also broadly refers to any enterprise that is potentially fraught with physical, financial or psychological risk, such as a business venture, a love affair, or other major life undertakings .
Motivation.
Adventurous experiences create psychological arousal, which can be interpreted as negative (e.g. fear) or positive (e.g. flow), and which can be detrimental as stated by the Yerkes-Dodson law. For some people, adventure becomes a major pursuit in and of itself. According to adventurer André Malraux, in his "La Condition Humaine" (1933), "If a man is not ready to risk his life, where is his dignity?". Similarly, Helen Keller stated that "Life is either a daring adventure or nothing."
Outdoor adventurous activities are typically undertaken for the purposes of recreation or excitement: examples are adventure racing and adventure tourism. Adventurous activities can also lead to gains in knowledge, such as those undertaken by explorers and pioneers – the British adventurer Jason Lewis, for example, uses adventures to draw global sustainability lessons from living within finite environmental constraints on expeditions to share with schoolchildren. Adventure education intentionally uses challenging experiences for learning.
Adventure in mythology.
Some of the oldest and most widespread stories in the world are stories of adventure such as Homer's "The Odyssey". Mythologist Joseph Campbell discussed his notion of the monomyth in his book, "The Hero with a Thousand Faces". Campbell proposed that the heroic mythological stories from culture to culture followed a similar underlying pattern, starting with the "call to adventure", followed by a hazardous journey, and eventual triumph.
The knight errant was the form the "adventure seeker" character took in the late Middle Ages.
The adventure novel exhibits these "protagonist on adventurous journey" characteristics as do many popular feature films, such as "Star Wars" and "Raiders of the Lost Ark".

</doc>
<doc id="689" url="https://en.wikipedia.org/wiki?curid=689" title="Asia">
Asia

Asia ( or ) is the Earth's largest and most populous continent, located primarily in the eastern and northern hemispheres. Asia covers an area of 44,579,000 square kilometers, about 30% of Earth's total land area and 8.7% of the Earth's total surface area. It has historically been home to the world's first modern civilizations and has always hosted the bulk of the planet's human population. Asia is notable for not only its overall large size and population, but unusually dense and large settlements as well as vast barely populated regions within the continent of 4.4 billion people. The boundaries of Asia are traditionally determined as that of Eurasia, as there is no significant geographical separation between Asia and Europe. The most commonly accepted boundaries place Asia to the east of the Suez Canal, the Ural River, and the Ural Mountains, and south of the Caucasus Mountains and the Caspian and Black Seas. It is bounded on the east by the Pacific Ocean, on the south by the Indian Ocean and on the north by the Arctic Ocean.
China and India alternated in being the largest economies in the world from 1 to 1800 A.D. China was a major economic power and attracted many to the east, and for many the legendary wealth and prosperity of the ancient culture of India personified Asia, attracting European commerce, exploration and colonialism. The accidental discovery of America by Columbus in search for India demonstrates this deep fascination. The Silk Road became the main East-West trading route in the Asian hitherland while the Straits of Malacca stood as a major sea route. Asia has exhibited economic dynamism (particularly East Asia) as well as robust population growth during the 20th century, but overall population growth has since fallen to world average levels. Asia was the birthplace of most of the world's mainstream religions including Christianity, Islam, Judaism, Hinduism, Buddhism, Confucianism, Taoism (or Daoism), Jainism, Sikhism, Zoroastranism, as well as many other religions.
Given its size and diversity, the concept of Asia—a name dating back to classical antiquity—may actually have more to do with human geography than physical geography. Asia varies greatly across and within its regions with regard to ethnic groups, cultures, environments, economics, historical ties and government systems. It also has a mix of many different climates ranging from the equatorial south via the hot desert in the Middle East, temperate areas in the east and the extremely continental centre to vast subarctic and polar areas in Siberia.
Definition and boundaries.
Asia-Africa boundary.
The current boundary between Asia and Africa is the Red Sea, the Gulf of Suez, and the Suez Canal. This makes Egypt a transcontinental country, with the Sinai peninsula in Asia and the remainder of the country in Africa.
Asia–Europe boundary.
The Don River became unsatisfactory to northern Europeans when Peter the Great, king of the Tsardom of Russia, defeating rival claims of Sweden and the Ottoman Empire to the eastern lands, and armed resistance by the tribes of Siberia, synthesized a new Russian Empire extending to the Ural Mountains and beyond, founded in 1721. The major geographical theorist of the empire was actually a former Swedish prisoner-of-war, taken at the Battle of Poltava in 1709 and assigned to Tobolsk, where he associated with Peter's Siberian official, Vasily Tatishchev, and was allowed freedom to conduct geographical and anthropological studies in preparation for a future book.
In Sweden, five years after Peter's death, in 1730 Philip Johan von Strahlenberg published a new atlas proposing the Urals as the border of Asia. The Russians were enthusiastic about the concept, which allowed them to keep their European identity in geography. Tatishchev announced that he had proposed the idea to von Strahlenberg. The latter had suggested the Emba River as the lower boundary. Over the next century various proposals were made until the Ural River prevailed in the mid-19th century. The border had been moved perforce from the Black Sea to the Caspian Sea into which the Ural River projects. In the maps of the period, Transcaucasia was counted as Asian. The incorporation of most of that region into the Soviet Union tended to push views of the border to the south. Asian cultures had no say in this system of determining the imaginary boundaries separating them from Europe.
Asia–Oceania boundary.
The border between Asia and the loosely defined region of Oceania is usually placed somewhere in the Malay Archipelago. The terms Southeast Asia and Oceania, devised in the 19th century, have had several vastly different geographic meanings since their inception. The chief factor in determining which islands of the Malay Archipelago are Asian has been the location of the colonial possessions of the various empires there (not all European). Lewis and Wigen assert, "The narrowing of 'Southeast Asia' to its present boundaries was thus a gradual process."
Ongoing definition.
Geographical Asia is a cultural artifact of European conceptions of the world, beginning with the Ancient Greeks, being imposed onto other cultures, an imprecise concept causing endemic contention about what it means. Asia is larger and more culturally diverse than Europe. It does not exactly correspond to the cultural borders of its various types of constituents.
From the time of Herodotus a minority of geographers have rejected the three-continent system (Europe, Africa, Asia) on the grounds that there is no or is no substantial physical separation between them. For example, Sir Barry Cunliffe, the emeritus professor of European archeology at Oxford, argues that Europe has been geographically and culturally merely "the western excrescence of the continent of Asia". Geographically, Asia is the major eastern constituent of the continent of Eurasia with Europe being a northwestern peninsula of the landmass – or of Afro-Eurasia; geologically, Asia, Europe and Africa make up a single continuous landmass (except for the Suez Canal) and share a common continental shelf. Almost all of Europe and the better part of Asia sit atop the Eurasian Plate, adjoined on the south by the Arabian and Indian Plate and with the easternmost part of Siberia (east of the Chersky Range) on the North American Plate.
Etymology.
The English word, "Asia," was originally a concept of Greek civilization. The place name, "Asia", in various forms in a large number of modern languages is of unknown ultimate provenience. Its etymology and language of origin are uncertain. It appears to be one of the most ancient of recorded names. A number of theories have been published. English Asia can be traced through the formation of English literature to Latin literature, where it has the same form, Asia. Whether all uses and all forms of the name derive also from the Latin of the Roman Empire is much less certain.
Bronze Age.
Before Greek poetry, the Aegean Sea area was in a Greek Dark Age, at the beginning of which syllabic writing was lost and alphabetic writing had not begun. Prior to then in the Bronze Age the records of the Assyrian Empire, the Hittite Empire and the various Mycenaean states of Greece mention a region undoubtedly Asia, certainly in Anatolia, including if not identical to Lydia. These records are administrative and do not include poetry.
The Mycenaean states were destroyed about 1200 BC by unknown agents although one school of thought assigns the Dorian invasion to this time. The burning of the palaces baked clay diurnal administrative records written in a Greek syllabic script called Linear B, deciphered by a number of interested parties, most notably by a young World War II cryptographer, Michael Ventris, subsequently assisted by the scholar, John Chadwick. A major cache discovered by Carl Blegen at the site of ancient Pylos included hundreds of male and female names formed by different methods.
Some of these are of women held in servitude (as study of the society implied by the content reveals). They were used in trades, such as cloth-making, and usually came with children. The epithet, lawiaiai, "captives," associated with some of them identifies their origin. Some are ethnic names. One in particular, aswiai, identifies "women of Asia." Perhaps they were captured in Asia, but some others, Milatiai, appear to have been of Miletus, a Greek colony, which would not have been raided for slaves by Greeks. Chadwick suggests that the names record the locations where these foreign women were purchased. The name is also in the singular, Aswia, which refers both to the name of a country and to a female of it. There is a masculine form, aswios. This Aswia appears to have been a remnant of a region known to the Hittites as Assuwa, centered on Lydia, or "Roman Asia." This name, "Assuwa", has been suggested as the origin for the name of the continent "Asia". The Assuwa league was a confederation of states in western Anatolia, defeated by the Hittites under Tudhaliya I around 1400 BC.
Alternatively, the etymology of the term may be from the Akkadian word "", which means 'to go outside' or 'to ascend', referring to the direction of the sun at sunrise in the Middle East and also likely connected with the Phoenician word "asa" meaning east. This may be contrasted to a similar etymology proposed for "Europe", as being from Akkadian "erēbu(m)" 'to enter' or 'set' (of the sun).
T.R. Reid supports this alternative etymology, noting that the ancient Greek name must have derived from "asu", meaning 'east' in Assyrian ("ereb" for "Europe" meaning 'west'). The ideas of "Occidental" (form Latin "Occidens" 'setting') and "Oriental" (from Latin "Oriens" for 'rising') are also European invention, synonymous with "Western" and "Eastern". Reid further emphasizes that it explains the Western point of view of placing all the peoples and cultures of Asia into a single classification, almost as if there were a need for setting the distinction between Western and Eastern civilizations on the Eurasian continent. Ogura Kazuo and Tenshin Okakura are two outspoken Japanese figures on the subject.
Classical antiquity.
Latin Asia and Greek Ἀσία appear to be the same word. Roman authors translated Ἀσία as Asia. The Romans named a province Asia (Roman province), which roughly corresponds with modern-day central-western Turkey. There was an Asia Minor and an Asia Major located in modern-day Iraq. As the earliest evidence of the name is Greek, it is likely circumstantially that Asia came from Ἀσία, but ancient transitions, due to the lack of literary contexts, are difficult to catch in the act. The most likely vehicles were the ancient geographers and historians, such as Herodotus, who were all Greek. Ancient Greek certainly evidences early and rich uses of the name.
The first continental use of Asia is attributed to Herodotus (about 440 BC), not because he innovated it, but because his "Histories" are the earliest surviving prose to describe it in any detail. He defines it carefully, mentioning the previous geographers whom he had read, but whose works are now missing. By it he means Anatolia and the Persian Empire, in contrast to Greece and Egypt. Herodotus comments that he is puzzled as to why three women's names were "given to a tract which is in reality one" (Europa, Asia, and Libya, referring to Africa), stating that most Greeks assumed that Asia was named after the wife of Prometheus (i.e. Hesione), but that the Lydians say it was named after Asies, son of Cotys, who passed the name on to a tribe at Sardis. In Greek mythology, "Asia" ("Ἀσία") or "Asie" ("Ἀσίη") was the name of a "Nymph or Titan goddess of Lydia."
In ancient Greek religion, places were under the care of female divinities, parallel to guardian angels. The poets detailed their doings and generations in allegoric language salted with entertaining stories, which subsequently playwrights transformed into classical Greek drama and became "Greek mythology." For example, Hesiod mentions the daughters of Tethys and Ocean, among whom are a "holy company", "who with the Lord Apollo and the Rivers have youths in their keeping." Many of these are geographic: Doris, Rhodea, Europa, Asia. Hesiod explains:"For there are three-thousand neat-ankled daughters of Ocean who are dispersed far and wide, and in every place alike serve the earth and the deep waters." The Iliad (attributed by the ancient Greeks to Homer) mentions two Phrygians (the tribe that replaced the Luvians in Lydia) in the Trojan War named Asios (an adjective meaning "Asian"); and also a marsh or lowland containing a marsh in Lydia as ασιος.
History.
The history of Asia can be seen as the distinct histories of several peripheral coastal regions: East Asia, South Asia, Southeast Asia and the Middle East, linked by the interior mass of the Central Asian steppes.
The coastal periphery was home to some of the world's earliest known civilizations, each of them developing around fertile river valleys. The civilizations in Mesopotamia, the Indus Valley and the Huanghe shared many similarities. These civilizations may well have exchanged technologies and ideas such as mathematics and the wheel. Other innovations, such as writing, seem to have been developed individually in each area. Cities, states and empires developed in these lowlands.
The central steppe region had long been inhabited by horse-mounted nomads who could reach all areas of Asia from the steppes. The earliest postulated expansion out of the steppe is that of the Indo-Europeans, who spread their languages into the Middle East, South Asia, and the borders of China, where the Tocharians resided. The northernmost part of Asia, including much of Siberia, was largely inaccessible to the steppe nomads, owing to the dense forests, climate and tundra. These areas remained very sparsely populated.
The center and the peripheries were mostly kept separated by mountains and deserts. The Caucasus and Himalaya mountains and the Karakum and Gobi deserts formed barriers that the steppe horsemen could cross only with difficulty. While the urban city dwellers were more advanced technologically and socially, in many cases they could do little in a military aspect to defend against the mounted hordes of the steppe. However, the lowlands did not have enough open grasslands to support a large horsebound force; for this and other reasons, the nomads who conquered states in China, India, and the Middle East often found themselves adapting to the local, more affluent societies.
The Islamic Caliphate took over the Middle East and Central Asia during the Muslim conquests of the 7th century. The Mongol Empire conquered a large part of Asia in the 13th century, an area extending from China to Europe. Before the Mongol invasion, Song dynasty reportedly had approximately 120 million citizens; the 1300 census which followed the invasion reported roughly 60 million people.
The Black Death, one of the most devastating pandemics in human history, is thought to have originated in the arid plains of central Asia, where it then travelled along the Silk Road.
The Russian Empire began to expand into Asia from the 17th century, and would eventually take control of all of Siberia and most of Central Asia by the end of the 19th century. The Ottoman Empire controlled Anatolia, the Middle East, North Africa and the Balkans from the 16th century onwards. In the 17th century, the Manchu conquered China and established the Qing Dynasty. The Islamic Mughal Empire and the Hindu Maratha Empire controlled much of India in the 16th and 18th centuries respectively.
Geography and climate.
Asia is the largest continent on Earth. It covers 8.8% of the Earth's total surface area (or 30% of its land area), and has the largest coastline, at . Asia is generally defined as comprising the eastern four-fifths of Eurasia. It is located to the east of the Suez Canal and the Ural Mountains, and south of the Caucasus Mountains (or the Kuma–Manych Depression) and the Caspian and Black Seas. It is bounded on the east by the Pacific Ocean, on the south by the Indian Ocean and on the north by the Arctic Ocean. Asia is subdivided into 48 countries, two of them (Russia and Turkey) having part of their land in Europe.
Asia has extremely diverse climates and geographic features. Climates range from arctic and subarctic in Siberia to tropical in southern India and Southeast Asia. It is moist across southeast sections, and dry across much of the interior. Some of the largest daily temperature ranges on Earth occur in western sections of Asia. The monsoon circulation dominates across southern and eastern sections, due to the presence of the Himalayas forcing the formation of a thermal low which draws in moisture during the summer. Southwestern sections of the continent are hot. Siberia is one of the coldest places in the Northern Hemisphere, and can act as a source of arctic air masses for North America. The most active place on Earth for tropical cyclone activity lies northeast of the Philippines and south of Japan. The Gobi Desert is in Mongolia and the Arabian Desert stretches across much of the Middle East. The Yangtze River in China is the longest river in the continent. The Himalayas between Nepal and China is the tallest mountain range in the world. Tropical rainforests stretch across much of southern Asia and coniferous and deciduous forests lie farther north.
Climate change.
A survey carried out in 2010 by global risk analysis farm Maplecroft identified 16 countries that are extremely vulnerable to climate change. Each nation's vulnerability was calculated using 42 socio, economic and environmental indicators, which identified the likely climate change impacts during the next 30 years. The Asian countries of Bangladesh, India, Vietnam, Thailand, Pakistan and Sri Lanka were among the 16 countries facing extreme risk from climate change. Some shifts are already occurring. For example, in tropical parts of India with a semi-arid climate, the temperature increased by 0.4 °C between 1901 and 2003.
A 2013 study by the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) aimed to find science-based, pro-poor approaches and techniques that would enable Asia's agricultural systems to cope with climate change, while benefitting poor and vulnerable farmers. The study's recommendations ranged from improving the use of climate information in local planning and strengthening weather-based agro-advisory services, to stimulating diversification of rural household incomes and providing incentives to farmers to adopt natural resource conservation measures to enhance forest cover, replenish groundwater and use renewable energy.
Economy.
Asia has the second largest nominal GDP of all continents, after Europe, but the largest when measured in purchasing power parity. As of 2011, the largest economies in Asia are China, Japan, India, South Korea and Indonesia. Based on Global Office Locations 2011, Asia dominated the office locations with 4 of top 5 being in Asia, Hong Kong, Singapore, Tokyo, Seoul and Shanghai. Around 68 percent of international firms have office in Hong Kong.
In the late 1990s and early 2000s, the economies of the PRC and India have been growing rapidly, both with an average annual growth rate of more than 8%. Other recent very-high-growth nations in Asia include Israel, Malaysia, Indonesia, Bangladesh, Pakistan, Thailand, Vietnam, Mongolia, Uzbekistan, Cyprus and the Philippines, and mineral-rich nations such as Kazakhstan, Turkmenistan, Iran, Brunei, United Arab Emirates, Qatar, Kuwait, Saudi Arabia, Bahrain and Oman.
According to economic historian Angus Maddison in his book "The World Economy: A Millennial Perspective", India had the world's largest economy during 0 BCE and 1000 BCE. China was the largest and most advanced economy on earth for much of recorded history, until the British Empire (excluding India) overtook it in the mid-19th century. For several decades in the late twentieth century Japan was the largest economy in Asia and second-largest of any single nation in the world, after surpassing the Soviet Union (measured in net material product) in 1986 and Germany in 1968. (NB: A number of supernational economies are larger, such as the European Union (EU), the North American Free Trade Agreement (NAFTA) or APEC). This ended in 2010 when China overtook Japan to become the world's second largest economy.
In the late 1980s and early 1990s, Japan's GDP was almost as large (current exchange rate method) as that of the rest of Asia combined. In 1995, Japan's economy nearly equaled that of the USA as the largest economy in the world for a day, after the Japanese currency reached a record high of 79 yen/US$. Economic growth in Asia since World War II to the 1990s had been concentrated in Japan as well as the four regions of South Korea, Taiwan, Hong Kong and Singapore located in the Pacific Rim, known as the Asian tigers, which have now all received developed country status, having the highest GDP per capita in Asia.
It is forecasted that India will overtake Japan in terms of nominal GDP by 2020. By 2027, according to Goldman Sachs, China will have the largest economy in the world. Several trade blocs exist, with the most developed being the Association of Southeast Asian Nations.
Asia is the largest continent in the world by a considerable margin, and it is rich in natural resources, such as petroleum, forests, fish, water, rice, copper and silver. Manufacturing in Asia has traditionally been strongest in East and Southeast Asia, particularly in China, Taiwan, South Korea, Japan, India, the Philippines, and Singapore. Japan and South Korea continue to dominate in the area of multinational corporations, but increasingly the PRC and India are making significant inroads. Many companies from Europe, North America, South Korea and Japan have operations in Asia's developing countries to take advantage of its abundant supply of cheap labour and relatively developed infrastructure.
According to Citigroup 9 of 11 Global Growth Generators countries came from Asia driven by population and income growth. They are Bangladesh, China, India, Indonesia, Iraq, Mongolia, Philippines, Sri Lanka and Vietnam. Asia has four main financial centers: Tokyo, Hong Kong, Singapore and Shanghai. Call centers and business process outsourcing (BPOs) are becoming major employers in India and the Philippines due to the availability of a large pool of highly skilled, English-speaking workers. The increased use of outsourcing has assisted the rise of India and the China as financial centers. Due to its large and extremely competitive information technology industry, India has become a major hub for outsourcing.
In 2010, Asia had 3.3 million millionaires (people with net worth over US$1 million excluding their homes), slightly below North America with 3.4 million millionaires. Last year Asia had toppled Europe.
Citigroup in The Wealth Report 2012 stated that Asian centa-millionaire overtook North America's wealth for the first time as the world's "economic center of gravity" continued moving east. At the end of 2011, there were 18,000 Asian people mainly in Southeast Asia, China and Japan who have at least $100 million in disposable assets, while North America with 17,000 people and Western Europe with 14,000 people.
Tourism.
With growing Regional Tourism with domination of Chinese visitors, MasterCard has released Global Destination Cities Index 2013 with 10 of 20 are dominated by Asia and Pacific Region Cities and also for the first time a city of a country from Asia (Bangkok) set in the top-ranked with 15.98 international visitors.
Demographics.
East Asia had by far the strongest overall Human Development Index (HDI) improvement of any region in the world, nearly doubling average HDI attainment over the past 40 years, according to the report's analysis of health, education and income data. China, the second highest achiever in the world in terms of HDI improvement since
1970, is the only country on the "Top 10 Movers" list due to income rather than health or education achievements. Its per capita income increased a stunning 21-fold over the last four decades, also lifting hundreds of millions out of income poverty. Yet it was not among the region's top performers in improving school enrollment and life expectancy.
<br>Nepal, a South Asian country, emerges as one of the world's fastest movers since 1970 mainly due to health and education achievements. Its present life expectancy is 25 years longer than in the 1970s. More than four of every five children of school age in Nepal now attend primary school, compared to just one in five 40 years ago.
<br> Japan and South Korea ranked highest among the countries grouped on the HDI (number 11 and 12 in the world, which are in the "very high human development" category), followed by Hong Kong (21) and Singapore (27). Afghanistan (155) ranked lowest amongst Asian countries out of the 169 countries assessed.
Languages.
Asia is home to several language families and many language isolates. Most Asian countries have more than one language that is natively spoken. For instance, according to Ethnologue, more than 600 languages are spoken in Indonesia, more than 800 languages spoken in India, and more than 100 are spoken in the Philippines. China has many languages and dialects in different provinces.
Religions.
Many of the world's major religions have their origins in Asia, including the five most practiced in the world (excluding irreligion), which are Christianity, Islam, Hinduism, Chinese folk religion (classified as Confucianism and Taoism), and Buddhism respectively. Asian mythology is complex and diverse. The story of the Great Flood for example, as presented to Christians in the Old Testament in the narrative of Noah, is first found in Mesopotamian mythology, in the "Epic of Gilgamesh". Hindu mythology tells about an Avatar of the God Vishnu in the form of a fish who warned Manu of a terrible flood. In ancient Chinese mythology, Shan Hai Jing, the Chinese ruler Da Yu, had to spend 10 years to control a deluge which swept out most of ancient China and was aided by the goddess Nüwa who literally fixed the broken sky through which huge rains were pouring.
Abrahamic.
The Abrahamic religions of Judaism, Christianity, Islam and Bahá'í Faith originated in West Asia.
Judaism, the oldest of the Abrahamic faiths, is practiced primarily in Israel, the birthplace and historical homeland of the Hebrew nation which today consists equally of those Israelites who remained in Asia/North Africa and those who returned from diaspora in Europe, North America, and other regions, though sizable communities continue to live abroad. Judaism is the predominant religion in Israel (75.6%), which has a nominal Jewish population of about 6.1 million, though the matter of distinguishing Jewish religious, cultural and ethnic identity is a complex one. Outside of Israel there are small ancient communities of Jewish still live in Turkey (17,400), Azerbaijan (9,100), Iran (8,756), India (5,000) and Uzbekistan (4,000).
Christianity is a widespread religion in Asia with more than 286 million adherents according to Pew Research Center in 2010, and nearly 364 million according to Britannica Book of the Year 2014. constituting around 12.6% of the total population of Asia. In the Philippines and East Timor, Roman Catholicism is the predominant religion; it was introduced by the Spaniards and the Portuguese, respectively. In Armenia, Cyprus, Georgia and Asian Russia, Eastern Orthodoxy is the predominant religion. Various Christian denominations have adherents in portions of the Middle East, as well as China and India. Saint Thomas Christians in India trace their origins to the evangelistic activity of Thomas the Apostle in the 1st century.
Islam, which originated in Saudi Arabia, is the largest and most widely spread religion in Asia. With 12.7% of the world Muslim population, the country currently with the largest Muslim population in the world is Indonesia, followed by Pakistan, India, Bangladesh, Iran and Turkey. Mecca, Medina and to a lesser extent Jerusalem are the holiest cities for Islam in all the world. These religious sites attract large numbers of devotees from all over the world, particularly during the Hajj and Umrah seasons. Iran is the largest Shi'a country and Pakistan has the largest Ahmadiyya population.
The Bahá'í Faith originated in Asia, in Iran (Persia), and spread from there to the Ottoman Empire, Central Asia, India, and Burma during the lifetime of Bahá'u'lláh. Since the middle of the 20th century, growth has particularly occurred in other Asian countries, because Bahá'í activities in many Muslim countries has been severely suppressed by authorities. Lotus Temple is a big Baha'i Temple in India.
Indian and East Asian religions.
Almost all Asian religions have philosophical character and Asian philosophical traditions cover a large spectrum of philosophical thoughts and writings. Indian philosophy includes Hindu philosophy and Buddhist philosophy. They include elements of nonmaterial pursuits, whereas another school of thought from India, Cārvāka, preached the enjoyment of the material world. The religions of Hinduism, Buddhism, Jainism and Sikhism originated in India, South Asia. In East Asia, particularly in China and Japan, Confucianism, Taoism and Zen Buddhism took shape.
As of 2012, Hinduism has around 1.1 billion adherents. The faith represents around 25% of Asia's population and is the second largest religion in Asia. However, it is mostly concentrated in South Asia. Over 80% of the populations of both India and Nepal adhere to Hinduism, alongside significant communities in Bangladesh, Pakistan, Bhutan, Sri Lanka and Bali, Indonesia. Many overseas Indians in countries such as Burma, Singapore and Malaysia also adhere to Hinduism.
Buddhism has a great following in mainland Southeast Asia and East Asia. Buddhism is the religion of the majority of the populations of Cambodia (96%), Thailand (95%), Burma (80%–89%), Japan (36%–96%), Bhutan (75%–84%), Sri Lanka (70%), Laos (60%–67%) and Mongolia (53%–93%). Large Buddhist populations also exist in Singapore (33%–51%), Taiwan (35%–93%), South Korea (23%–50%), Malaysia (19%–21%), Nepal (9%–11%), Vietnam (10%–75%), China (20%–50%), North Korea (1.5%–14%), and small communities in India and Bangladesh. In many Chinese communities, Mahayana Buddhism is easily syncretized with Taoism, thus exact religious statistics is difficult to obtain and may be understated or overstated. The Communist-governed countries of China, Vietnam and North Korea are officially atheist, thus the number of Buddhists and other religious adherents may be under-reported.
Jainism is found mainly in India and in oversea Indian communities such as the United States and Malaysia.
Sikhism is found in Northern India and amongst overseas Indian communities in other parts of Asia, especially Southeast Asia.
Confucianism is found predominantly in Mainland China, South Korea, Taiwan and in overseas Chinese populations.
Taoism is found mainly in Mainland China, Taiwan, Malaysia and Singapore. Taoism is easily syncretized with Mahayana Buddhism for many Chinese, thus exact religious statistics is difficult to obtain and may be understated or overstated.
Modern conflicts.
Some of the events pivotal in the Asia territory related to the relationship with the outside world in the post-Second World War were:
Culture.
Nobel prizes.
The polymath Rabindranath Tagore, a Bengali poet, dramatist, and writer from Santiniketan, now in West Bengal, India, became in 1913 the first Asian Nobel laureate. He won his Nobel Prize in Literature for notable impact his prose works and poetic thought had on English, French, and other national literatures of Europe and the Americas. He is also the writer of the national anthems of Bangladesh and India.
Other Asian writers who won Nobel Prize for literature include Yasunari Kawabata (Japan, 1968), Kenzaburō Ōe (Japan, 1994), Gao Xingjian (China, 2000), Orhan Pamuk (Turkey, 2006), and Mo Yan (China, 2012). Some may consider the American writer, Pearl S. Buck, an honorary Asian Nobel laureate, having spent considerable time in China as the daughter of missionaries, and based many of her novels, namely "The Good Earth" (1931) and "The Mother" (1933), as well as the biographies of her parents of their time in China, "The Exile" and "Fighting Angel", all of which earned her the Literature prize in 1938.
Also, Mother Teresa of India and Shirin Ebadi of Iran were awarded the Nobel Peace Prize for their significant and pioneering efforts for democracy and human rights, especially for the rights of women and children. Ebadi is the first Iranian and the first Muslim woman to receive the prize. Another Nobel Peace Prize winner is Aung San Suu Kyi from Burma for her peaceful and non-violent struggle under a military dictatorship in Burma. She is a nonviolent pro-democracy activist and leader of the National League for Democracy in Burma (Myanmar) and a noted prisoner of conscience. She is a Buddhist and was awarded the Nobel Peace Prize in 1991. Chinese dissident Liu Xiaobo was awarded the Nobel Peace Prize for "his long and non-violent struggle for fundamental human rights in China" on 8 October 2010. He is the first Chinese citizen to be awarded a Nobel Prize of any kind while residing in China. In 2014, Kailash Satyarthi from India and Malala Yousafzai from Pakistan were awarded the Nobel Peace Prize "for their struggle against the suppression of children and young people and for the right of all children to education".
Sir C. V. Raman is the first Asian to get a Nobel prize in Sciences. He won the Nobel Prize in Physics "for his work on the scattering of light and for the discovery of the effect named after him".
Japan has won the most Nobel Prizes of any Asian nation with 24 followed by India which has won 13.
Amartya Sen, (born 3 November 1933) is an Indian economist who was awarded the 1998 Nobel Memorial Prize in Economic Sciences for his contributions to welfare economics and social choice theory, and for his interest in the problems of society's poorest members.
Other Asian Nobel Prize winners include Subrahmanyan Chandrasekhar, Abdus Salam, Robert Aumann, Menachem Begin, Aaron Ciechanover, Avram Hershko, Daniel Kahneman, Shimon Peres, Yitzhak Rabin, Ada Yonath, Yasser Arafat, José Ramos-Horta and Bishop Carlos Filipe Ximenes Belo of Timor Leste, Kim Dae-jung, and 13 Japanese scientists. Most of the said awardees are from Japan and Israel except for Chandrasekhar and Raman (India), Salam (Pakistan), Arafat (Palestinian Territories) Kim (South Korea), Horta and Belo (Timor Leste).
In 2006, Dr. Muhammad Yunus of Bangladesh was awarded the Nobel Peace Prize for the establishment of Grameen Bank, a community development bank that lends money to poor people, especially women in Bangladesh. Dr. Yunus received his PhD in economics from Vanderbilt University, United States. He is internationally known for the concept of micro credit which allows poor and destitute people with little or no collateral to borrow money. The borrowers typically pay back money within the specified period and the incidence of default is very low.
The Dalai Lama has received approximately eighty-four awards over his spiritual and political career. On 22 June 2006, he became one of only four people ever to be recognized with Honorary Citizenship by the Governor General of Canada. On 28 May 2005, he received the Christmas Humphreys Award from the Buddhist Society in the United Kingdom. Most notable was the Nobel Peace Prize, presented in Oslo, Norway on 10 December 1989.
Political geography.
Within the above-mentioned states are several de facto independent countries with limited to no international recognition. None of them are members of the UN:
See also.
References to articles:
Special topics:
Lists:

</doc>
<doc id="690" url="https://en.wikipedia.org/wiki?curid=690" title="Aruba">
Aruba

Aruba ( ; ) is an island country in the southern Caribbean Sea, located about west of the main part of the Lesser Antilles and north of the coast of Venezuela. It measures long from its northwestern to its southeastern end and across at its widest point. Together with Bonaire and Curaçao, Aruba forms a group referred to as the ABC islands. Collectively, Aruba and the other Dutch islands in the Caribbean are often called the Dutch Caribbean.
Aruba is one of the four constituent countries that form the Kingdom of the Netherlands, along with the Netherlands, Curaçao and Sint Maarten. The citizens of these countries all share a single nationality: Dutch. Aruba has no administrative subdivisions, but, for census purposes, is divided into eight regions. Its capital is Oranjestad.
Unlike much of the Caribbean region, Aruba has a dry climate and an arid, cactus-strewn landscape. This climate has helped tourism as visitors to the island can reliably expect warm, sunny weather. It has a land area of and is densely populated, with a total of 102,484 inhabitants at the 2010 Census. It lies outside Hurricane Alley.
History.
Aruba's first inhabitants are thought to have been Caquetíos Amerinds from the Arawak tribe, who migrated there from Venezuela to escape attacks by the Caribs. Fragments of the earliest known Indian settlements date back to 1000 AD. As sea currents made canoe travel to other Caribbean islands difficult, Caquetio culture remained more closely associated with that of mainland South America.
Europeans first learned of Aruba following the explorations for Spain by Amerigo Vespucci and Alonso de Ojeda in the summer of 1499. Both described Aruba as an "island of giants", remarking on the comparatively large stature of the native Caquetíos compared to Europeans. Gold was not discovered on Aruba for another 300 years. Vespucci returned to Spain with stocks of cotton and brazilwood from the island and described houses built into the ocean. Vespucci and Ojeda's tales spurred interest in Aruba, and Spaniards soon colonized the island.
Because it had low rainfall, Aruba was not considered profitable for the plantation system and the economics of the slave trade.
Aruba was colonized by Spain for over a century. "Simas", the "Cacique", or chief, in Aruba, welcomed the first Catholic priests in Aruba, who gave him a wooden cross as a gift. In 1508, the Spanish Crown appointed Alonso de Ojeda as its first Governor of Aruba, as part of "Nueva Andalucía". Arawaks spoke the "broken Spanish" which their ancestors had learned on Hispaniola.
Another governor appointed by Spain was Juan Martínez de Ampiés. A "cédula real" decreed in November 1525 gave Ampiés, factor of Española, the right to repopulate Aruba. In 1528, Ampiés was replaced by a representative of the House of Welser.
The Dutch statutes have applied to Aruba since 1629. The Netherlands acquired Aruba in 1636. Since 1636, Aruba has been under Dutch administration, initially governed by Peter Stuyvesant, later appointed to New Amsterdam (New York City). Stuyvesant was on a special mission in Aruba in November and December 1642. The island was included under the Dutch West India Company (W.I.C.) administration, as "New Netherland and Curaçao", from 1648 to 1664. In 1667 the Dutch administration appointed an Irishman as "Commandeur" in Aruba.
The Dutch took control 135 years after the Spanish, leaving the Arawaks to farm and graze livestock, and used the island as a source of meat for other Dutch possessions in the Caribbean.
During the Napoleonic wars, the British Empire took control over the island, between 1799 and 1802, and between 1804 and 1816, before handing it back to the Dutch.
Move towards independence.
In August 1947, Aruba presented its first "Staatsreglement" (constitution), for Aruba's "status aparte" as an autonomous state within the Kingdom of the Netherlands. By 1954, the Charter of the Kingdom of the Netherlands was established, providing a framework for relations between Aruba and the rest of the Kingdom.
In 1972, at a conference in Suriname, Betico Croes (MEP), a politician from Aruba, proposed a "sui-generis" Dutch Commonwealth of four states: Aruba, the Netherlands, Suriname and the Netherlands Antilles, each to have its own nationality. C. Yarzagaray, a parliamentary member representing the AVP political party, proposed a referendum so that the people of Aruba could choose whether they wanted total independence or "Status Aparte" as a full autonomous state under the Crown.
Croes worked in Aruba to inform and prepare the people of Aruba for independence. In 1976, he appointed a committee that chose the national flag and anthem, introducing them as symbols of Aruba's sovereignty and independence. He set 1981 as a target date for independence. In March 1977, the first Referendum for Self Determination was held with the support of the United Nations; 82% of the participants voted for independence.
The Island Government of Aruba assigned the Institute of Social Studies in The Hague to prepare a study for independence; it was titled "Aruba en Onafhankelijkheid, achtergronden, modaliteiten en mogelijkheden; een rapport in eerste aanleg" (Aruba and independence, backgrounds, modalities and opportunities; a preliminary report) (1978). At the conference in The Hague in 1981, Aruba's independence was set for the year 1991.
In March 1983, Aruba reached an official agreement within the Kingdom for its independence, to be developed in a series of steps as the Crown granted increasing autonomy. In August 1985 Aruba drafted a constitution that was unanimously approved. On 1 January 1986, after elections were held for its first parliament, Aruba seceded from the Netherlands Antilles; it officially became a country of the Kingdom of the Netherlands. Full independence was projected in 1996.
After his death in 1986, Croes was proclaimed "Libertador di Aruba". At a convention in The Hague in 1990, at the request of Aruba's Prime Minister, the governments of Aruba, the Netherlands, and the Netherlands Antilles postponed indefinitely its transition to full independence. The article scheduling Aruba's complete independence was rescinded in 1995, although the process could be revived after another referendum.
Geography.
Aruba is a generally flat, riverless island in the Leeward Antilles island arc of the Lesser Antilles in the southern part of the Caribbean. It has white sandy beaches on the western and southern coasts of the island, relatively sheltered from fierce ocean currents. This is where most tourist development has occurred. The northern and eastern coasts, lacking this protection, are considerably more battered by the sea and have been left largely untouched by humans.
The hinterland of the island features some rolling hills, the best known of which are called Hooiberg at and Mount Jamanota, the highest on the island at above sea level. Oranjestad, the capital, is located at .
To the east of Aruba are Bonaire and Curaçao, two island territories which once formed the southwest part of the Netherlands Antilles. This group of islands is sometimes called the ABC islands. They are located on the South American continental shelf and therefore geographically listed as part of South America.
The Natural Bridge was a large, naturally formed limestone bridge on the island's north shore. It was a popular tourist destination until its collapse in 2005.
Cities and towns.
The island, with a population of just over 100,000 inhabitants, does not have major cities. However, most of the island's population resides in or surrounding the two major city-like districts of Oranjestad (Capital) and San Nicolaas. Furthermore, the island is divided into eight districts, which are:
Fauna.
The island of Aruba, being isolated from the main land of South America, has helped the evolution of multiple endemic animals. The island provides a habitat for the endemic Aruban Whiptail and Aruba Rattlesnake, as well as endemic subspecies of Burrowing Owl and Brown-throated Parakeet.
The rattlesnake and the owl are printed on the Aruban currency.
Flora.
The flora of Aruba differs from the typical tropical island vegetation. Xeric scrublands are common, with various forms of cacti, thorny shrubs and evergreens. With the most known plant being the Aloe vera, which has a place on the Coat of Arms of Aruba.
Climate.
In the Köppen climate classification, Aruba has a tropical semi-arid climate. Mean monthly temperature in Oranjestad varies little from to , moderated by constant trade winds from the Atlantic Ocean, which comes from north-east. Yearly precipitation barely exceeds in Oranjestad.
Demographics.
The population is estimated to be 80% mixed White/Caribbean Native Americans, 5% Africans and 15% other ethnicities.
The Arawak heritage is stronger on Aruba than on most Caribbean islands. Although no full-blooded Aboriginals remain, the features of the islanders clearly indicate their genetic Arawak heritage. Most of the population is descended from Caquetio Indians and Dutch and to a lesser extent of Africans, Spanish, Portuguese, English, French, and Sephardic Jewish ancestors.
Recently, there has been substantial immigration to the island from neighboring American and Caribbean nations, possibly attracted by the higher paid jobs. In 2007, new immigration laws were introduced to help control the growth of the population by restricting foreign workers to a maximum of three years residency on the island.
Demographically, Aruba has felt the impact of its proximity to Venezuela. Many of Aruba's families are descended from Venezuelan immigrants. There is a seasonal increase of Venezuelans living in second homes.
Language.
The official languages are Dutch and – since 2003 – Papiamento. Papiamento is the predominant language on Aruba. It is a creole language, spoken on Aruba, Bonaire, and Curaçao, that incorporates words from Portuguese, West African languages, Dutch, and Spanish. English is known by many; its usage has grown due to tourism. Other common languages spoken, based on the size of their community, are Portuguese, Chinese, German, Spanish, and French.
In recent years, the government of Aruba has shown an increased interest in acknowledging the cultural and historical importance of its native language. Although spoken Papiamento is fairly similar among the several Papiamento-speaking islands, there is a big difference in written Papiamento. The orthography differs per island and even per group of people. Some are more oriented towards Portuguese and use the equivalent spelling (e.g. "y" instead of "j"), where others are more oriented towards Dutch.
The book "The Buccaneers of America", first published in 1678, states through eyewitness account that the natives on Aruba spoke "Spanish". The oldest government official statement written in Papiamento dates from 1803. Around 12.6% of the population today speaks Spanish.
Aruba has four newspapers published in Papiamento: "Diario", "Bon Dia", "Solo di Pueblo" and "Awe Mainta"; and three in English: "Aruba Daily", "Aruba Today" and "The News". "Amigoe" is a newspaper published in Dutch. Aruba also has 18 radio stations (two AM and 16 FM) and three local television stations (Telearuba, Aruba Broadcast Company and Channel 22).
Regions.
For census purposes, Aruba is divided into eight regions, which have no administrative functions:
Government.
As a constituent country of the Kingdom of the Netherlands, Aruba's politics take place within a framework of a 21-member Parliament and an eight-member Cabinet. The governor of Aruba is appointed for a six-year term by the monarch, and the prime minister and deputy prime minister are elected by the Staten (or "Parlamento") for four-year terms. The Staten is made up of 21 members elected by direct, popular vote to serve a four-year term.
Together with the Netherlands, the countries of Aruba, Curaçao and Sint Maarten form the Kingdom of the Netherlands. As they share the same Dutch citizenship, these four countries still also share the Dutch passport as the Kingdom of the Netherlands passport. As Aruba, Curaçao and Sint Maarten have small populations, the three countries had to limit immigration. To protect their population, they have the right to control the admission and expulsion of people from the Netherlands.
Aruba is designated as a member of the Overseas Countries and Territories (OCT) and is thus officially not a part of the European Union, though Aruba can and does receive support from the European Development Fund.
Politics.
The Aruban legal system is based on the Dutch model. Instead of juries or grand juries, in Aruba, legal jurisdiction lies with the "Gerecht in Eerste Aanleg" (Court of First Instance) on Aruba, the "Gemeenschappelijk Hof van Justitie van Aruba, Curaçao, Sint Maarten en van Bonaire, Sint Eustatius en Saba" (Joint Court of Justice of Aruba, Curaçao, Sint Maarten, and of Bonaire, Sint Eustatius and Saba) and the "Hoge Raad der Nederlanden" (Supreme Court of Justice of the Netherlands). The "Korps Politie Aruba" (Aruba Police Force) is the island's law enforcement agency and operates district precincts in Oranjestad, Noord, San Nicolaas, and Santa Cruz, where it is headquartered.
Deficit spending has been a staple in Aruba's history, and modestly high inflation has been present as well. By 2006, the government's debt had grown to 1.883 billion Aruban florins. Aruba received some development aid from the Dutch government each year through 2009, as part of a deal (signed as "Aruba's Financial Independence") in which the Netherlands gradually reduced its financial help to the island each successive year.
In 2006, the Aruban government changed several tax laws to reduce the deficit. Direct taxes have been converted to indirect taxes as proposed by the IMF. A 3% tax has been introduced on sales and services, while income taxes have been lowered and revenue taxes for business reduced by 20%. The government compensated workers with 3.1% for the effect that the B.B.O. would have on the inflation for 2007.
Education.
Aruba's educational system is patterned after the Dutch system of education.
The Government of Aruba finances the public national education system.
There are private schools including the International School of Aruba and Schakel College.
There are two medical schools Aureus University School of Medicine and Xavier University School of Medicine, as well as its own national university, the University of Aruba.
Economy.
Aruba has one of the highest standards of living in the Caribbean region. There is a low unemployment rate.
The GDP per capita for Aruba was estimated to be $28,924 in 2014; among the highest in the Caribbean and the Americas. Its main trading partners are Colombia, the United States, Venezuela, and the Netherlands.
The island's economy has been dominated by three main industries: tourism, aloe export, and petroleum refining (The Lago Oil and Transport Company and the Arend Petroleum Maatschappij Shell Co.). Before the "Status Aparte" (a separate completely autonomous country/state within the Kingdom), oil processing was the dominant industry in Aruba despite expansion of the tourism sector. Today, the influence of the oil processing business is minimal. The size of the agriculture and manufacturing sectors also remains minimal.
The official exchange rate of the Aruban florin is pegged to the US dollar at 1.79 florins to 1 USD. Because of this fact, and due to a large number of American tourists, many businesses operate using US dollars instead of florins, especially in the hotel and resort districts.
Tourism.
About three quarters of the Aruban gross national product is earned through tourism or related activities. Most tourists are from the United States (predominantly from the north-east US), the Netherlands and South America, mainly Venezuela and Colombia.
As part of the Kingdom of the Netherlands, citizens of the Netherlands can travel with relative ease to Aruba and other islands of the Dutch Antilles. No visas are needed for Dutch citizens, only a passport, and although the currency used in Aruba is different (the Netherlands uses the Euro), money can be easily exchanged at a local bank for Aruban Florins.
For the facilitation of the passengers whose destination is the United States, the United States Department of Homeland Security (DHS), U.S. Customs and Border Protection (CBP) full pre-clearance facility in Aruba has been in effect since 1 February 2001 with the expansion in the Queen Beatrix Airport. United States and Aruba have had the agreement since 1986. It began as a USDA and Customs post. Since 2008, Aruba has been the only island to have this service for private flights.
Military.
In 1999, the U.S. Department of Defense established a Forward Operating Location (FOL) at the airport.
There is also a small Dutch marines base by Savaneta containing approximately 120 Dutch Marines and about 100 AruMil forces.
Culture.
On 18 March, Aruba celebrates its National Day. In 1976, Aruba presented its National Anthem (Aruba Dushi Tera) and Flag.
Aruba has a varied culture. According to the "Bureau Burgelijke Stand en Bevolkingsregister" (BBSB), in 2005 there were ninety-two different nationalities living on the island. Dutch influence can still be seen, as in the celebration of "Sinterklaas" on 5 and 6 December and other national holidays like 27 April, when in Aruba and the rest of the Kingdom of the Netherlands the King's birthday or "Dia di Rey" (Koningsdag) is celebrated.
Christmas and New Year's Eve are celebrated with the typical music and songs for gaitas for Christmas and the Dande for New Year, and "ayaca", "ponche crema", ham, and other typical foods and drinks. Millions of florins worth of fireworks are burnt at midnight on New Year's Eve. On 25 January, Betico Croes' birthday is celebrated. Dia di San Juan is celebrated on June 24.
Besides Christmas, the religious holy days of the Feast of the Ascension and Good Friday are holidays on the island.
The holiday of Carnaval is also an important one in Aruba, as it is in many Caribbean and Latin American countries, and, like Mardi Gras, that goes on for weeks. Its celebration in Aruba started, around the 1950s, influenced by the inhabitants from Venezuela and the nearby islands (Curaçao, St. Vincent, Trinidad, Barbados, St. Maarten and Anguilla) who came to work for the Oil refinery. Over the years the Carnival Celebration has changed and now starts from the beginning of January till the Tuesday before Ash Wednesday with a large parade on the last Sunday of the festivities (Sunday before Ash Wednesday).
Tourism from the United States has recently increased the visibility of American culture on the island, with such celebrations as Halloween and Thanksgiving Day in November.
Infrastructure.
Aruba's Queen Beatrix International Airport is located near Oranjestad. According to the Aruba Airport Authority, almost 1.7 million travelers used the airport in 2005, 61% of whom were Americans.
Aruba has two ports, Barcadera and Playa, which are located in Oranjestad and Barcadera. The Port of Playa services all the cruise-ship lines, including Royal Caribbean, Carnival Cruise Lines, NCL, Holland America Line, Disney Cruise Line and others. Nearly one million tourists enter this port per year. Aruba Ports Authority, owned and operated by the Aruban government, runs these seaports.
Arubus is a government-owned bus company. Its buses operate from 3:30 a.m. until 12:30 a.m. 365 days a year. Small private vans also provide transportation services in certain areas such Hotel Area, San Nicolaas, Santa Cruz and Noord.
A street car service runs on rails on the Mainstreet.
Utilities.
Water-en Energiebedrijf Aruba, N.V. (W.E.B.) produces potable industrial water at the world's third largest desalination plant. Average daily consumption in Aruba is about .
Communications.
There are three telecommunications providers: Setar, a government-based company, Mio Wireless and Digicel, both of which are privately owned. Setar is the provider of services such as internet, video conferencing, GSM wireless technology and land lines. Digicel is Setar's competitor in wireless technology using the GSM platform, and Mio Wireless provides wireless technology and services using CDMA.

</doc>
<doc id="691" url="https://en.wikipedia.org/wiki?curid=691" title="Articles of Confederation">
Articles of Confederation

The Articles of Confederation, formally the Articles of Confederation and Perpetual Union, was an agreement among all thirteen original states in the United States of America that served as its first constitution. Its drafting by a committee appointed by the Second Continental Congress began on July 12, 1776, and an approved version was sent to the states for ratification in late 1777. The formal ratification by all thirteen states was completed in early 1781. Government under the Articles was superseded by a new constitution and federal form of government in 1789.
Even unratified, the Articles provided a system for the Continental Congress to direct the American Revolutionary War, conduct diplomacy with Europe and deal with territorial issues and Native American relations. Nevertheless, the weakness of the government created by the Articles became a matter of concern for key nationalists. On March 4, 1789, the general government under the Articles was replaced with the federal government under the United States Constitution. The new Constitution provided for a much stronger federal government with a chief executive (the President), courts, and taxing powers.
Background and context.
The political push to increase cooperation among the then-loyal colonies began with the Albany Congress in 1754 and Benjamin Franklin's proposed Albany Plan of Union, an inter-colonial collaboration to help solve mutual local problems. The Articles of Confederation would bear some resemblance to it. Over the next two decades, some of the basic concepts it addressed would strengthen and others would weaken, particularly the degree of deserved loyalty to the crown. With civil disobedience resulting in coercive and intolerable acts, and armed conflict resulting in dissidents being proclaimed rebels and outside the King's protection, any loyalty remaining shifted toward independence and how to achieve it. In 1775, with events outpacing communications, the Second Continental Congress began acting as the provisional government to run the American Revolutionary War and gain the colonies their collective independence.
It was an era of constitution writing—most states were busy at the task—and leaders felt the new nation must have a written constitution, even though other nations did not. During the war, Congress exercised an unprecedented level of political, diplomatic, military and economic authority. It adopted trade restrictions, established and maintained an army, issued fiat money, created a military code and negotiated with foreign governments.
To transform themselves from outlaws into a legitimate nation, the colonists needed international recognition for their cause and foreign allies to support it. In early 1776, Thomas Paine argued in the closing pages of the first edition of "Common Sense" that the “custom of nations” demanded a formal declaration of American independence if any European power were to mediate a peace between the Americans and Great Britain. The monarchies of France and Spain in particular could not be expected to aid those they considered rebels against another legitimate monarch. Foreign courts needed to have American grievances laid before them persuasively in a “manifesto” which could also reassure them that the Americans would be reliable trading partners. Without such a declaration, Paine concluded, “he custom of all courts is against us, and will be so, until, by an independence, we take rank with other nations.”
Beyond improving their existing association, the records of the Second Continental Congress show that the need for a declaration of independence was intimately linked with the demands of international relations. On June 7, 1776, Richard Henry Lee introduced a resolution before the Continental Congress declaring the colonies independent; at the same time he also urged Congress to resolve “to take the most effectual measures for forming foreign Alliances” and to prepare a plan of confederation for the newly independent states. Congress then created three overlapping committees to draft the Declaration, a Model Treaty, and the Articles of Confederation. The Declaration announced the states' entry into the international system; the model treaty was designed to establish amity and commerce with other states; and the Articles of Confederation, which established “a firm league” among the thirteen free and independent states, constituted an international agreement to set up central institutions for the conduct of vital domestic and foreign affairs.
Drafting.
On June 12, 1776, a day after appointing a committee to prepare a draft of the Declaration of Independence, the Second Continental Congress resolved to appoint a committee of 13 to prepare a draft of a constitution for a union of the states. The committee met repeatedly, and chairman John Dickinson presented their results to the Congress on July 12, 1776. There were long debates on such issues as sovereignty, the exact powers to be given the confederate government, whether to have a judiciary, and voting procedures. The final draft of the Articles was prepared in the summer of 1777 and the Second Continental Congress approved them for ratification by the individual states on November 15, 1777, after a year of debate.
In practice, the Articles were in use beginning in 1777; the final draft of the Articles served as the de facto system of government used by the Congress ("the United States in Congress assembled") until it became de jure by final ratification on March 1, 1781; at which point Congress became the Congress of the Confederation. Under the Articles, the states retained sovereignty over all governmental functions not specifically relinquished to the national government. The individual articles set the rules for current and future operations of the United States government. It was made capable of making war and peace, negotiating diplomatic and commercial agreements with foreign countries, and deciding disputes between the states, including their additional and contested western territories. Article XIII stipulated that "their provisions shall be inviolably observed by every state" and "the Union shall be perpetual".
John Dickinson's and Benjamin Franklin's handwritten drafts of the Articles of Confederation are housed at the National Archives in Washington, DC.
Operation.
The Articles were created by delegates from the states in the Second Continental Congress out of a need to have "a plan of confederacy for securing the freedom, sovereignty, and independence of the United States." After the war, nationalists, especially those who had been active in the Continental Army, complained that the Articles were too weak for an effective government. There was no president, no executive agencies, no judiciary and no tax base. The absence of a tax base meant that there was no way to pay off state and national debts from the war years except by requesting money from the states, which seldom arrived.
In 1788, with the approval of Congress, the Articles were replaced by the United States Constitution and the new government began operations in 1789.
Ratification.
Congress began to move for ratification of the Articles of Confederation in 1777:
The document could not become officially effective until it was ratified by all 13 states. The first state to ratify was Virginia on December 16, 1777; the thirteenth state to ratify was Maryland on February 2, 1781. A ceremonial confirmation of this thirteenth, final ratification took place in the Congress on March 1, 1781 at high noon.
Dates of ratification are:
The ratification process dragged on for several years, stalled by the refusal of some states to rescind their claims to land in the West. Maryland was the last holdout; it refused to go along until Virginia and New York agreed to cede their claims in the Ohio River Valley. It took a little over three years for all states to ratify.
The Articles provided for a blanket acceptance of Province of Quebec (referred to as "Canada" in the Articles) into the United States if it chose to do so. It did not, and the subsequent Constitution carried no such special provision of admission.
Article summaries.
The Articles of Confederation contain a preamble, thirteen articles, a conclusion, and a signatory section. The preamble declares that the states "agree to certain articles of Confederation and perpetual Union."
What follows here summarizes the purpose and content of each of the thirteen articles.
While still at war with Britain, the revolution's leaders were divided between forming a national government with powers either strong and centralized (the "federalists"), or strictly limited (the "anti federalists"). The Continental Congress compromised by dividing sovereignty between the states and the central government, with a unicameral legislature that protected the liberty of the individual states. It empowered Congress to regulate military and monetary affairs, for example, but provided no mechanism to compel the States to comply with requests for either troops or funding. This left the military vulnerable to inadequate funding, supplies, or even food.
The end of the Revolutionary War.
The Treaty of Paris (1783), which ended hostilities with Great Britain, languished in Congress for months because several state representatives failed to attend sessions of the national legislature to ratify it. Yet Congress had no power to enforce attendance. In September 1783, George Washington complained that Congress was paralyzed. Many revolutionaries had gone to their respective home countries after the war, and local government and self-rule seemed quite satisfactory.
Function.
The Army.
The Articles supported the Congressional direction of the Continental Army, and allowed the states to present a unified front when dealing with the European powers. As a tool to build a centralized war-making government, they were largely a failure: Historian Bruce Chadwick wrote:
The Continental Congress, before the Articles were approved, had promised soldiers a pension of half pay for life. However Congress had no power to compel the states to fund this obligation, and as the war wound down after the victory at Yorktown the sense of urgency to support the military was no longer a factor. No progress was made in Congress during the winter of 1783–84. General Henry Knox, who would later become the first Secretary of War under the Constitution, blamed the weaknesses of the Articles for the inability of the government to fund the army. The army had long been supportive of a strong union. Knox wrote:
As Congress failed to act on the petitions, Knox wrote to Gouverneur Morris, four years before the Philadelphia Convention was convened, "As the present Constitution is so defective, why do not you great men call the people together and tell them so; that is, to have a convention of the States to form a better Constitution."
Once the war had been won, the Continental Army was largely disbanded. A very small national force was maintained to man the frontier forts and to protect against Native American attacks. Meanwhile, each of the states had an army (or militia), and 11 of them had Navies. The wartime promises of bounties and land grants to be paid for service were not being met. In 1783, George Washington defused the Newburgh conspiracy, but riots by unpaid Pennsylvania veterans forced Congress to leave Philadelphia temporarily.
The Congress from time to time during the Revolutionary War requisitioned troops from the states. Any contributions were voluntary, and in the debates of 1788 the Federalists (who supported the proposed new Constitution) claimed that state politicians acted unilaterally, and contributed when the Continental army protected their state's interests. The Anti-Federalists claimed that state politicians understood their duty to the Union and contributed to advance its needs. Dougherty (2009) concludes that generally the States' behavior validated the Federalist analysis. This helps explain why the Articles of Confederation needed reforms.
Foreign policy.
Even after peace had been achieved in 1783, the weakness of the Confederation government frustrated the ability of the government to conduct foreign policy. In 1789, Thomas Jefferson, concerned over the failure to fund an American naval force to confront the Barbary pirates, wrote to James Monroe, "It will be said there is no money in the treasury. There never will be money in the treasury till the Confederacy shows its teeth. The states must see the rod.”
Furthermore, the Jay–Gardoqui Treaty with Spain in 1789 also showed weakness in foreign policy. In this treaty — which was never ratified due to its immense unpopularity — the United States was to give up rights to use the Mississippi River for 25 years, which would have economically strangled the settlers west of the Appalachian Mountains. Finally, due to the Confederation's military weakness, it could not compel the British army to leave frontier forts which were on American soil — forts which, in 1783, the British promised to leave, but which they delayed leaving pending U.S. implementation of other provisions such as ending action against Loyalists and allowing them to seek compensation. This incomplete British implementation of the Treaty of Paris (1783) was superseded by the implementation of Jay's Treaty in 1795 under the new U.S. Constitution.
Taxation and commerce.
Under the Articles of Confederation, the central government's power was kept quite limited. The Confederation Congress could make decisions, but lacked enforcement powers. Implementation of most decisions, including modifications to the Articles, required unanimous approval of all thirteen state legislatures.
Congress was denied any powers of taxation: it could only request money from the states. The states often failed to meet these requests in full, leaving both Congress and the Continental Army chronically short of money. As more money was printed by Congress, the continental dollars depreciated. In 1779, George Washington wrote to John Jay, who was serving as the president of the Continental Congress, "that a wagon load of money will scarcely purchase a wagon load of provisions." Mr. Jay and the Congress responded in May by requesting $45 million from the States. In an appeal to the States to comply, Jay wrote that the taxes were "the price of liberty, the peace, and the safety of yourselves and posterity." He argued that Americans should avoid having it said "that America had no sooner become independent than she became insolvent" or that "her infant glories and growing fame were obscured and tarnished by broken contracts and violated faith." The States did not respond with any of the money requested from them.
Congress had also been denied the power to regulate either foreign trade or interstate commerce and, as a result, all of the States maintained control over their own trade policies. The states and the Confederation Congress both incurred large debts during the Revolutionary War, and how to repay those debts became a major issue of debate following the War. Some States paid off their war debts and others did not. Federal assumption of the states' war debts became a major issue in the deliberations of the Constitutional Convention.
Accomplishments of the Confederation.
Nevertheless, the Confederation Congress did take two actions with long-lasting impact. The Land Ordinance of 1785 and Northwest Ordinance created territorial government, set up protocols for the admission of new states and the division of land into useful units, and set aside land in each township for public use. This system represented a sharp break from imperial colonization, as in Europe, and provided the basis for the rest of American continental expansion through the 19th Century.
The Land Ordinance of 1785 established both the general practices of land surveying in the west and northwest and the land ownership provisions used throughout the later westward expansion beyond the Mississippi River. Frontier lands were surveyed into the now-familiar squares of land called the township (36 square miles), the section (one square mile), and the quarter section (160 acres). This system was carried forward to most of the States west of the Mississippi (excluding areas of Texas and California that had already been surveyed and divided up by the Spanish Empire). Then, when the Homestead Act was enacted in 1867, the quarter section became the basic unit of land that was granted to new settler-farmers.
The Northwest Ordinance of 1787 noted the agreement of the original states to give up northwestern land claims, organized the Northwest Territory and thus cleared the way for the entry of five new states and part of a sixth to the Union. To be specific, Massachusetts, Connecticut, New York, Pennsylvania, and Virginia gave up all of their claims to land north of the Ohio River and west of the (present) western border of Pennsylvania. Over several decades a number of new states were formed from this land: Ohio, Indiana, Illinois, Michigan, and Wisconsin, and the part of Minnesota east of the Mississippi River. The Northwest Ordinance of 1787 also made great advances in the abolition of slavery. New states admitted to the union in said territory would never be slave states.
The United States of America under the Articles.
The peace treaty left the United States independent and at peace but with an unsettled governmental structure. The Articles envisioned a permanent confederation, but granted to the Congress—the only federal institution—little power to finance itself or to ensure that its resolutions were enforced. There was no president and no national court. Although historians generally agree that the Articles were too weak to hold the fast-growing nation together, they do give credit to the settlement of the western issue, as the states voluntarily turned over their lands to national control.
By 1783, with the end of the British blockade, the new nation was regaining its prosperity. However, trade opportunities were restricted by the mercantilism of the British and French empires. The ports of the British West Indies were closed to all staple products which were not carried in British ships. France and Spain established similar policies. Simultaneously, new manufacturers faced sharp competition from British products which were suddenly available again. Political unrest in several states and efforts by debtors to use popular government to erase their debts increased the anxiety of the political and economic elites which had led the Revolution. The apparent inability of the Congress to redeem the public obligations (debts) incurred during the war, or to become a forum for productive cooperation among the states to encourage commerce and economic development, only aggravated a gloomy situation. In 1786–87, Shays' Rebellion, an uprising of dissidents in western Massachusetts against the state court system, threatened the stability of state government.
The Continental Congress printed paper money which was so depreciated that it ceased to pass as currency, spawning the expression "not worth a continental". Congress could not levy taxes and could only make requisitions upon the States. Less than a million and a half dollars came into the treasury between 1781 and 1784, although the governors had been asked for two million in 1783 alone.
When John Adams went to London in 1785 as the first representative of the United States, he found it impossible to secure a treaty for unrestricted commerce. Demands were made for favors and there was no assurance that individual states would agree to a treaty. Adams stated it was necessary for the States to confer the power of passing navigation laws to Congress, or that the States themselves pass retaliatory acts against Great Britain. Congress had already requested and failed to get power over navigation laws. Meanwhile, each State acted individually against Great Britain to little effect. When other New England states closed their ports to British shipping, Connecticut hastened to profit by opening its ports.
By 1787 Congress was unable to protect manufacturing and shipping. State legislatures were unable or unwilling to resist attacks upon private contracts and public credit. Land speculators expected no rise in values when the government could not defend its borders nor protect its frontier population.
The idea of a convention to revise the Articles of Confederation grew in favor. Alexander Hamilton realized while serving as Washington's top aide that a strong central government was necessary to avoid foreign intervention and allay the frustrations due to an ineffectual Congress. Hamilton led a group of like-minded nationalists, won Washington's endorsement, and convened the Annapolis Convention in 1786 to petition Congress to call a constitutional convention to meet in Philadelphia to remedy the long-term crisis.
Signatures.
The Second Continental Congress approved the Articles for distribution to the states on November 15, 1777. A copy was made for each state and one was kept by the Congress. On November 28, the copies sent to the states for ratification were unsigned, and the cover letter, dated November 17, had only the signatures of Henry Laurens and Charles Thomson, who were the President and Secretary to the Congress.
The "Articles", however, were unsigned, and the date was blank. Congress began the signing process by examining their copy of the "Articles" on June 27, 1778. They ordered a final copy prepared (the one in the National Archives), and that delegates should inform the secretary of their authority for ratification.
On July 9, 1778, the prepared copy was ready. They dated it, and began to sign. They also requested each of the remaining states to notify its delegation when ratification was completed. On that date, delegates present from New Hampshire, Massachusetts, Rhode Island, Connecticut, New York, Pennsylvania, Virginia and South Carolina signed the Articles to indicate that their states had ratified. New Jersey, Delaware and Maryland could not, since their states had not ratified. North Carolina and Georgia also didn't sign that day, since their delegations were absent.
After the first signing, some delegates signed at the next meeting they attended. For example, John Wentworth of New Hampshire added his name on August 8. John Penn was the first of North Carolina's delegates to arrive (on July 10), and the delegation signed the "Articles" on July 21, 1778.
The other states had to wait until they ratified the "Articles" and notified their Congressional delegation. Georgia signed on July 24, New Jersey on November 26, and Delaware on February 12, 1779. Maryland refused to ratify the "Articles" until every state had ceded its western land claims.
On February 2, 1781, the much-awaited decision was taken by the Maryland General Assembly in Annapolis. As the last piece of business during the afternoon Session, "among engrossed Bills" was "signed and sealed by Governor Thomas Sim Lee in the Senate Chamber, in the presence of the members of both Houses... an Act to empower the delegates of this state in Congress to subscribe and ratify the articles of confederation" and perpetual union among the states. The Senate then adjourned "to the first Monday in August next." The decision of Maryland to ratify the Articles was reported to the Continental Congress on February 12. The confirmation signing of the "Articles" by the two Maryland delegates took place in Philadelphia at noon time on March 1, 1781 and was celebrated in the afternoon. With these events, the Articles were entered into force and the United States of America came into being as a sovereign federal state.
Congress had debated the "Articles" for over a year and a half, and the ratification process had taken nearly three and a half years. Many participants in the original debates were no longer delegates, and some of the signers had only recently arrived. The "Articles of Confederation and Perpetual Union" were signed by a group of men who were never present in the Congress at the same time.
Signers.
The signers and the states they represented were:
Connecticut
Delaware
Georgia
Maryland
Massachusetts Bay
New Hampshire
New Jersey
New York
North Carolina
Pennsylvania
Rhode Island and Providence Plantations
South Carolina
Virginia
Roger Sherman (Connecticut) was the only person to sign all four great state papers of the United States: the Continental Association, the United States Declaration of Independence, the Articles of Confederation and the United States Constitution.
Robert Morris (Pennsylvania) signed three of the great state papers of the United States: the United States Declaration of Independence, the Articles of Confederation and the United States Constitution.
John Dickinson (Delaware), Daniel Carroll (Maryland) and Gouverneur Morris (New York), along with Sherman and Robert Morris, were the only five people to sign both the Articles of Confederation and the United States Constitution (Gouverneur Morris represented Pennsylvania when signing the Constitution).
Presidents of the Congress.
The following list is of those who led the Congress of the Confederation under the "Articles of Confederation" as the Presidents of the United States in Congress Assembled. Under the Articles, the president was the presiding officer of Congress, chaired the Committee of the States when Congress was in recess, and performed other administrative functions. He was not, however, an executive in the way the successor President of the United States is a chief executive, since all of the functions he executed were under the direct control of Congress.
"For a full list of Presidents of the Congress Assembled and Presidents under the two Continental Congresses before the Articles, see President of the Continental Congress."
Gallery.
Images of an original draft of the Articles of Confederation stored at the United States National Archive.
Revision and replacement.
On January 21, 1786, the Virginia Legislature, following James Madison's recommendation, invited all the states to send delegates to Annapolis, Maryland to discuss ways to reduce interstate conflict. At what came to be known as the Annapolis Convention, the few state delegates in attendance endorsed a motion that called for all states to meet in Philadelphia in May 1787 to discuss ways to improve the Articles of Confederation in a "Grand Convention." Although the states' representatives to the Constitutional Convention in Philadelphia were only authorized to amend the Articles, the representatives held secret, closed-door sessions and wrote a new constitution. The new Constitution gave much more power to the central government, but characterization of the result is disputed. The general goal of the authors was to get close to a republic as defined by the philosophers of the Age of Enlightenment, while trying to address the many difficulties of the interstate relationships. Historian Forrest McDonald, using the ideas of James Madison from "Federalist 39", describes the change this way:
In May 1786, Charles Pinckney of South Carolina proposed that Congress revise the Articles of Confederation. Recommended changes included granting Congress power over foreign and domestic commerce, and providing means for Congress to collect money from state treasuries. Unanimous approval was necessary to make the alterations, however, and Congress failed to reach a consensus. The weakness of the Articles in establishing an effective unifying government was underscored by the threat of internal conflict both within and between the states, especially after Shays' Rebellion threatened to topple the state government of Massachusetts.
Historian Ralph Ketcham comments on the opinions of Patrick Henry, George Mason, and other Anti-Federalists who were not so eager to give up the local autonomy won by the revolution:
Historians have given many reasons for the perceived need to replace the articles in 1787. Jillson and Wilson (1994) point to the financial weakness as well as the norms, rules and institutional structures of the Congress, and the propensity to divide along sectional lines.
Rakove (1988) identifies several factors that explain the collapse of the Confederation. The lack of compulsory direct taxation power was objectionable to those wanting a strong centralized state or expecting to benefit from such power. It could not collect customs after the war because tariffs were vetoed by Rhode Island. Rakove concludes that their failure to implement national measures "stemmed not from a heady sense of independence but rather from the enormous difficulties that all the states encountered in collecting taxes, mustering men, and gathering supplies from a war-weary populace." The second group of factors Rakove identified derived from the substantive nature of the problems the Continental Congress confronted after 1783, especially the inability to create a strong foreign policy. Finally, the Confederation's lack of coercive power reduced the likelihood for profit to be made by political means, thus potential rulers were uninspired to seek power.
When the war ended in 1783, certain special interests had incentives to create a new "merchant state," much like the British state people had rebelled against. In particular, holders of war scrip and land speculators wanted a central government to pay off scrip at face value and to legalize western land holdings with disputed claims. Also, manufacturers wanted a high tariff as a barrier to foreign goods, but competition among states made this impossible without a central government.
Legitimacy of closing down.
Political scientist David C. Hendrickson writes that two prominent political leaders in the Confederation, John Jay of New York and Thomas Burke of North Carolina believed that "the authority of the congress rested on the prior acts of the several states, to which the states gave their voluntary consent, and until those obligations were fulfilled, neither nullification of the authority of congress, exercising its due powers, nor secession from the compact itself was consistent with the terms of their original pledges."
According to Article XIII of the Confederation, any alteration had to be approved unanimously: 
he Articles of this Confederation shall be inviolably observed by every State, and the Union shall be perpetual; nor shall any alteration at any time hereafter be made in any of them; unless such alteration be agreed to in a Congress of the United States, and be afterwards confirmed by the legislatures of every State.
On the other hand, Article VII of the proposed Constitution stated that it would become effective after ratification by a mere nine states, without unanimity:
The Ratification of the Conventions of nine States, shall be sufficient for the Establishment of this Constitution between the States so ratifying the Same.
The apparent tension between these two provisions was addressed at the time, and remains a topic of scholarly discussion. In 1788, James Madison remarked (in "Federalist No. 40") that the issue had become moot: "As this objection...has been in a manner waived by those who have criticised the powers of the convention, I dismiss it without further observation." Nevertheless, it is an interesting historical and legal question whether opponents of the Constitution could have plausibly attacked the Constitution on that ground. At the time, there were state legislators who argued that the Constitution was not an alteration of the Articles of Confederation, but rather would be a complete replacement so the unanimity rule did not apply. Moreover, the Confederation had proven woefully inadequate and therefore was supposedly no longer binding.
Modern scholars such as Francisco Forrest Martin agree that the Articles of Confederation had lost its binding force because many states had violated it, and thus "other states-parties did not have to comply with the Articles' unanimous consent rule". In contrast, law professor Akhil Amar suggests that there may not have really been any conflict between the Articles of Confederation and the Constitution on this point; Article VI of the Confederation specifically allowed side deals among states, and the Constitution could be viewed as a side deal until all states ratified it.
Final months.
On July 3, 1788 the Congress received New Hampshire's all-important ninth ratification of the proposed Constitution, thus, according to its terms, establishing it as the new framework of governance for the ratifying states. The following day delegates considered a bill to admit Kentucky into the Union as a sovereign state. The discussion ended with Congress making the determination that, in light of this development, it would be "unadvisable" to admit Kentucky into the Union, as it could do so "under the Articles of Confederation" only, but not "under the Constitution".
By the end of July 1788, 11 of the 13 states had ratified the new Constitution. Congress continued to convene under the Articles with a quorum until October. On Saturday, September 13, 1788, the Confederation Congress voted the resolve to implement the new Constitution, and on Monday, September 15 published an announcement that the new Constitution had been ratified by the necessary nine states, set the first Wednesday in February 1789 for the presidential electors to meet and select a new president, and set the first Wednesday of March 1789 as the day the new government would take over and the government under the Articles of Confederation would come to an end.
On that same September 13, it determined that New York would remain the national capital.

</doc>
<doc id="694" url="https://en.wikipedia.org/wiki?curid=694" title="Asia Minor (disambiguation)">
Asia Minor (disambiguation)

Asia Minor is an alternative name for Anatolia, the westernmost protrusion of Asia, comprising the majority of the Republic of Turkey. It may also refer to:

</doc>
<doc id="696" url="https://en.wikipedia.org/wiki?curid=696" title="Aa River">
Aa River

Aa is the name of a large number of small European rivers. Aa originated from an Indo-European word meaning water, and it can be seen in the German "Ach" or "Aach" or the North Germanic "A" or "Aa".

</doc>
<doc id="698" url="https://en.wikipedia.org/wiki?curid=698" title="Atlantic Ocean">
Atlantic Ocean

The Atlantic Ocean is the second largest of the world's oceanic divisions, following the Pacific Ocean. With a total area of about , it covers approximately 20 percent of the Earth's surface and about 29 percent of its water surface area. The first part of its name refers to Atlas of Greek mythology, making the Atlantic the "Sea of Atlas".
The oldest known mention of "Atlantic" is in "The Histories" of Herodotus around 450 BC (Hdt. 1.202.4): "Atlantis thalassa" (Greek: Ἀτλαντὶς θάλασσα; English: Sea of Atlas). The term Ethiopic Ocean, derived from Ethiopia, was applied to the southern Atlantic as late as the mid-19th century. Before Europeans discovered other oceans, their term "ocean" was synonymous with the waters beyond the Strait of Gibraltar that are now known as the Atlantic. The early Greeks believed this ocean to be a gigantic river encircling the world.
The Atlantic Ocean occupies an elongated, S-shaped basin extending longitudinally between Eurasia and Africa to the east, and the Americas to the west. As one component of the interconnected global ocean, it is connected in the north to the Arctic Ocean, to the Pacific Ocean in the southwest, the Indian Ocean in the southeast, and the Southern Ocean in the south (other definitions describe the Atlantic as extending southward to Antarctica). The equator subdivides it into the North Atlantic Ocean and South Atlantic Ocean.
Geography.
The Atlantic Ocean is bounded on the west by North and South America. It connects to the Arctic Ocean through the Denmark Strait, Greenland Sea, Norwegian Sea and Barents Sea. To the east, the boundaries of the ocean proper are Europe: the Strait of Gibraltar (where it connects with the Mediterranean Sea–one of its marginal seas–and, in turn, the Black Sea, both of which also touch upon Asia) and Africa.
In the southeast, the Atlantic merges into the Indian Ocean. The 20° East meridian, running south from Cape Agulhas to Antarctica defines its border. Some authorities show it extending south to Antarctica, while others show it bounded at the 60° parallel by the Southern Ocean.
In the southwest, the Drake Passage connects it to the Pacific Ocean. The man-made Panama Canal links the Atlantic and Pacific. Besides those mentioned, other large bodies of water that form part of the Atlantic are the Caribbean Sea, the Gulf of Mexico, Hudson Bay, the Mediterranean Sea, the North Sea, the Baltic Sea, and the Celtic Sea.
Covering approximately 22% of Earth's surface, the Atlantic is second in size to the Pacific. With its adjacent seas, it occupies an area of about ; without them, it has an area of . The land that drains into the Atlantic covers four times that of either the Pacific or Indian oceans. The volume of the Atlantic with its adjacent seas is 354,700,000 cubic kilometers (85,100,000 cu mi) and without them 323,600,000 cubic kilometres (77,640,000 cu mi).
The average depth of the Atlantic with its adjacent seas, is ; without them it is . The greatest depth, Milwaukee Deep with , is in the Puerto Rico Trench.
Cultural significance.
The Atlantic Ocean was named by the ancient Greeks after either Atlas the Titan or the Atlas Mountains named for him; both involve the concept of holding up the sky. Transatlantic travel played a major role in the expansion of Western civilization into the Americas. It is the Atlantic that separates the "Old World" from the "New World".
In modern times, some idioms refer to the ocean in a humorously diminutive way as the Pond, describing both the geographical and cultural divide between North America and Europe, in particular between the English-speaking nations of both continents. Many Irish or British people refer to the United States and Canada as "across the pond", and vice versa.
The "Black Atlantic" refers to the role of this ocean in shaping black people's history, especially through the Atlantic slave trade. Irish migration to the US is meant when the term "The Green Atlantic" is used. The term "Red Atlantic" has been used in reference to the Marxian concept of an Atlantic working class, as well as to the Atlantic experience of indigenous Americans.
Ocean floor.
The principal feature of the bathymetry (bottom topography) is a submarine mountain range called the Mid-Atlantic Ridge. It extends from Iceland in the north to approximately 58° South latitude, reaching a maximum width of about . A great rift valley also extends along the ridge over most of its length. The depth of water at the apex of the ridge is less than in most places, while the bottom of the ridge is three times as deep. Several peaks rise above the water and form islands. The South Atlantic Ocean has an additional submarine ridge, the Walvis Ridge.
The Mid-Atlantic Ridge separates the Atlantic Ocean into two large troughs with depths from . Transverse ridges running between the continents and the Mid-Atlantic Ridge divide the ocean floor into numerous basins. Some of the larger basins are the Blake, Guiana, North American, Cape Verde, and Canaries basins in the North Atlantic. The largest South Atlantic basins are the Angola, Cape, Argentina, and Brazil basins.
The deep ocean floor is thought to be fairly flat with occasional deeps, abyssal plains, trenches, seamounts, basins, plateaus, canyons, and some guyots. Various shelves along the margins of the continents constitute about 11% of the bottom topography with few deep channels cut across the continental rise.
Ocean floor trenches and seamounts:
Ocean sediments are composed of:
Water characteristics.
On average, the Atlantic is the saltiest major ocean; surface water salinity in the open ocean ranges from 33 to 37 parts per thousand (3.3 – 3.7%) by mass and varies with latitude and season. Evaporation, precipitation, river inflow and sea ice melting influence surface salinity values. Although the lowest salinity values are just north of the equator (because of heavy tropical rainfall), in general the lowest values are in the high latitudes and along coasts where large rivers enter. Maximum salinity values occur at about 25° north and south, in subtropical regions with low rainfall and high evaporation.
Surface water temperatures, which vary with latitude, current systems, and season and reflect the latitudinal distribution of solar energy, range from below to over . Maximum temperatures occur north of the equator, and minimum values are found in the polar regions. In the middle latitudes, the area of maximum temperature variations, values may vary by .
The Atlantic Ocean consists of four major water masses. The North and South Atlantic central waters make up the surface. The sub-Antarctic intermediate water extends to depths of . The North Atlantic Deep Water reaches depths of as much as . The Antarctic Bottom Water occupies ocean basins at depths greater than 4,000 meters.
Within the North Atlantic, ocean currents isolate the Sargasso Sea, a large elongated body of water, with above average salinity. The Sargasso Sea contains large amounts of seaweed and is also the spawning ground for both the European eel and the American eel.
The Coriolis effect circulates North Atlantic water in a clockwise direction, whereas South Atlantic water circulates counter-clockwise. The south tides in the Atlantic Ocean are semi-diurnal; that is, two high tides occur during each 24 lunar hours. In latitudes above 40° North some east-west oscillation occurs.
Climate.
Climate is influenced by the temperatures of the surface waters and water currents as well as winds. Because of the ocean's great capacity to store and release heat, maritime climates are more moderate and have less extreme seasonal variations than inland climates. Precipitation can be approximated from coastal weather data and air temperature from water temperatures.
The oceans are the major source of the atmospheric moisture that is obtained through evaporation. Climatic zones vary with latitude; the warmest zones stretch across the Atlantic north of the equator. The coldest zones are in high latitudes, with the coldest regions corresponding to the areas covered by sea ice. Ocean currents influence climate by transporting warm and cold waters to other regions. The winds that are cooled or warmed when blowing over these currents influence adjacent land areas.
The Gulf Stream and its northern extension towards Europe, the North Atlantic Drift, for example, warms the atmosphere of the British Isles and north-western Europe and influences weather and climate as far south as the northern Mediterranean. The cold water currents contribute to heavy fog off the coast of eastern Canada (the Grand Banks of Newfoundland area) and Africa's north-western coast. In general, winds transport moisture and air over land areas. Hurricanes develop in the southern part of the North Atlantic Ocean. More local particular weather examples could be found in examples such as the Azores High, Benguela Current, and Nor'easter.
History.
The Atlantic Ocean appears to be the second youngest of the five oceans. It did not exist prior to 130 million years ago, when the continents that formed from the breakup of the ancestral super continent Pangaea were drifting apart. The Atlantic has been extensively explored since the earliest settlements along its shores.
The Norsemen, the Portuguese and the Spanish were the first to explore and to cross it systematically, from Europe to the Americas, as well as to its islands and archipelagos, and from the North Atlantic to the South Atlantic. It was after the voyages of Christopher Columbus in 1492, at the service of Castile (later Spain), that the Americas became well known in Europe and European exploration rapidly accelerated, leading to many new trade routes and the colonization of the Americas.
As a result, the Atlantic became and remains the major artery between Europe and the Americas (known as transatlantic trade). Scientific explorations include the Challenger expedition, the German Meteor expedition, Columbia University's Lamont-Doherty Earth Observatory and the United States Navy Hydrographic Office.
Economy.
The Atlantic has contributed significantly to the development and economy of surrounding countries. Besides major transatlantic transportation and communication routes, the Atlantic offers abundant petroleum deposits in the sedimentary rocks of the continental shelves. The Atlantic hosts the world's richest fishing resources, especially in the waters covering the shelves. The major fish are cod, haddock, hake, herring, and mackerel.
The most productive areas include the Grand Banks of Newfoundland, the Nova Scotia shelf, Georges Bank off Cape Cod, the Bahama Banks, the waters around Iceland, the Irish Sea, the Dogger Bank of the North Sea, and the Falkland Banks. Eel, lobster, and whales appear in great quantities. Various international treaties attempt to reduce pollution caused by environmental threats such as oil spills, marine debris, and the incineration of toxic wastes at sea. 
Terrain.
From October to June the surface is usually covered with sea ice in the Labrador Sea, Denmark Strait, and Baltic Sea. A clockwise warm-water gyre occupies the northern Atlantic, and a counter-clockwise warm-water gyre appears in the southern Atlantic. The Mid-Atlantic Ridge, a rugged north-south centerline for the entire Atlantic basin, first discovered by the Challenger Expedition dominates the ocean floor. This was formed by the vulcanism that also formed the ocean floor and the islands rising from it.
The Atlantic has irregular coasts indented by numerous bays, gulfs, and seas. These include the Norwegian Sea, Baltic Sea, North Sea, Labrador Sea, Black Sea, Gulf of Saint Lawrence, Bay of Fundy, Gulf of Maine, Mediterranean Sea, Gulf of Mexico, and Caribbean Sea.
Islands include Newfoundland (including hundreds of surrounding islands), Greenland, Iceland, Faroe Islands, British Isles (including Great Britain and Ireland), Rockall, Sable Island, Azores, St. Pierre and Miquelon, Madeira, Bermuda, Canary Islands, Caribbean Islands (including Greater Antilles, Leeward Islands, Windward Islands, Leeward Antilles), Cape Verde, São Tomé and Príncipe, Annobón Province, Fernando de Noronha, Rocas Atoll, Ascension Island, Saint Helena, Trindade and Martim Vaz, Tristan da Cunha, Gough Island (Also known as Diego Alvarez), Falkland Islands, Tierra del Fuego, South Georgia Island, South Sandwich Islands, and Bouvet Island.
Natural resources.
The Atlantic harbors petroleum and gas fields, fish, marine mammals (seals and whales), sand and gravel aggregates, placer deposits, polymetallic nodules, and precious stones.
Natural hazards.
Icebergs are common from February to August in the Davis Strait, Denmark Strait, and the northwestern Atlantic and have been spotted as far south as Bermuda and Madeira. Ships are subject to superstructure icing in the extreme north from October to May. Persistent fog can be a maritime hazard from May to September, as can hurricanes north of the equator (May to December).
The United States' southeast coast has a long history of shipwrecks due to its many shoals and reefs. The Virginia and North Carolina coasts were particularly dangerous.
The Bermuda Triangle is popularly believed to be the site of numerous aviation and shipping incidents because of unexplained and supposedly mysterious causes, but Coast Guard records do not support this belief.
Hurricanes are also a natural hazard in the Atlantic, but mainly in the northern part of the ocean, rarely tropical cyclones form in the southern parts. Hurricanes usually form between 1 June and 30 November of every year.
Current environmental issues.
Endangered marine species include the manatee, seals, sea lions, turtles, and whales. Drift net fishing can kill dolphins, albatrosses and other seabirds (petrels, auks), hastening the fish stock decline and contributing to international disputes. Municipal pollution comes from the eastern United States, southern Brazil, and eastern Argentina; oil pollution in the Caribbean Sea, Gulf of Mexico, Lake Maracaibo, Mediterranean Sea, and North Sea; and industrial waste and municipal sewage pollution in the Baltic Sea, North Sea, and Mediterranean Sea.
In 2005, there was some concern that warm northern European currents were slowing down.
On 7 June 2006, Florida's wildlife commission voted to take the manatee off the state's endangered species list. Some environmentalists worry that this could erode safeguards for the popular sea creature.
Marine pollution.
Marine pollution is a generic term for the entry into the ocean of potentially hazardous chemicals or particles. The biggest culprits are rivers and with them many agriculture fertilizer chemicals as well as livestock and human waste. The excess of oxygen-depleting chemicals leads to hypoxia and the creation of a dead zone.
Marine debris, which is also known as marine litter, describes human-created waste floating in a body of water. Oceanic debris tends to accumulate at the center of gyres and coastlines, frequently washing aground where it is known as beach litter.
Bordering countries and territories.
The states (territories in italics) with a coastline on the Atlantic Ocean (excluding the Black, Baltic and Mediterranean Seas) are:

</doc>
<doc id="700" url="https://en.wikipedia.org/wiki?curid=700" title="Arthur Schopenhauer">
Arthur Schopenhauer

Arthur Schopenhauer (; 22 February 1788 – 21 September 1860) was a German philosopher. He is best known for his 1818 work "The World as Will and Representation", in which he characterizes the phenomenal world, and consequently all human action, as the product of a blind, insatiable, and malignant metaphysical will. Proceeding from the transcendental idealism of Immanuel Kant, Schopenhauer developed an atheistic metaphysical and ethical system that has been described as an exemplary manifestation of philosophical pessimism, rejecting the contemporaneous post-Kantian philosophies of German idealism. Schopenhauer was among the first thinkers in Western philosophy to share and affirm significant tenets of Eastern philosophy (e.g., asceticism, the world-as-appearance), having initially arrived at similar conclusions as the result of his own philosophical work. His writing on aesthetics, morality, and psychology would exert important influence on thinkers and artists throughout the 19th and 20th centuries.
Though his work failed to garner substantial attention during his life, Schopenhauer has had a posthumous impact across various disciplines, including philosophy, literature, and science. Those who have cited his influence include Friedrich Nietzsche, Richard Wagner, Leo Tolstoy, Ludwig Wittgenstein, Erwin Schrödinger, Sigmund Freud, Gustav Mahler, Joseph Campbell, Albert Einstein, Carl Jung, Thomas Mann, Jorge Luis Borges, and Samuel Beckett, among others.
Life.
Schopenhauer was born on 22 February 1788, in the city of Danzig (present day Gdańsk, Poland) on Heiligegeistgasse (known in the present day as Św. Ducha 47), the son of Johanna Schopenhauer (née Trosiener) and Heinrich Floris Schopenhauer, both descendants of wealthy German patrician families. When Danzig became part of Prussia in 1793, Heinrich moved to Hamburg, although his firm continued trading in Danzig. As early as 1799, Arthur started playing the flute. In 1805, Schopenhauer's father died, possibly by suicide. Arthur endured two long years of drudgery as a merchant in honor of his dead father, but his mother soon moved with his sister Adele to Weimar—then the centre of German literature—to pursue her writing career. He dedicated himself wholly to studies at the Gotha gymnasium but left in disgust after seeing one of the masters lampooned. 
By that time, she had already opened her famous salon, and Arthur was not compatible with what he considered its vain and ceremonious ways. He was also disgusted by the ease with which his mother had forgotten his father's memory. He left to become a student at the University of Göttingen in 1809. There he studied metaphysics and psychology under Gottlob Ernst Schulze, the author of "Aenesidemus", who advised him to concentrate on Plato and Immanuel Kant. In Berlin, from 1811 to 1812, he had attended lectures by the prominent post-Kantian philosopher Johann Gottlieb Fichte and the theologian Friedrich Schleiermacher.
Schopenhauer had a notably strained relationship with his mother Johanna. He wrote his first book, "On the Fourfold Root of the Principle of Sufficient Reason", while at university. His mother informed him that the book was incomprehensible and it was unlikely that anyone would ever buy a copy. In a fit of temper Arthur Schopenhauer told her that his work would be read long after the "rubbish" she wrote would have been totally forgotten. In fact, although they considered her novels of dubious quality, the Brockhaus publishing firm held her in high esteem because they consistently sold well. Hans Brockhaus later recalled that, when she brought them some of her son's work, his predecessors "saw nothing in this manuscript, but wanted to please one of our best-selling authors by publishing her son's work. We published more and more of her son Arthur's work and today nobody remembers Johanna, but her son's works are in steady demand and contribute to Brockhaus' reputation." He kept large portraits of the pair in his office in Leipzig for the edification of his new editors.
In 1814, Schopenhauer began his seminal work "The World as Will and Representation" ("Die Welt als Wille und Vorstellung"). He finished it in 1818 and Brockhaus published it that December. In Dresden in 1819, Schopenhauer fathered, with a servant, an illegitimate daughter who was born and died the same year. In 1820, Schopenhauer became a lecturer at the University of Berlin. He scheduled his lectures to coincide with those of the famous philosopher G. W. F. Hegel, whom Schopenhauer described as a "clumsy charlatan." However, only five students turned up to Schopenhauer's lectures, and he dropped out of academia. A late essay, "On University Philosophy," expressed his resentment towards the work conducted in academies.
While in Berlin, Schopenhauer was named as a defendant in a lawsuit initiated by a woman named Caroline Marquet. She asked for damages, alleging that Schopenhauer had pushed her. According to Schopenhauer's court testimony, she deliberately annoyed him by raising her voice while standing right outside his door. Marquet alleged that the philosopher had assaulted and battered her after she refused to leave his doorway. Her companion testified that she saw Marquet prostrate outside his apartment. Because Marquet won the lawsuit, Schopenhauer made payments to her for the next twenty years. When she died, he wrote on a copy of her death certificate, "Obit anus, abit onus" ("The old woman dies, the burden is lifted"). In 1819 the fortunes of his mother and sister, and himself, were threatened by the failure of the firm in Danzig in which his father had been a director and shareholder. His sister accepted a compromise compensation package of 70 per cent, but Schopenhauer angrily refused this, and eventually recovered 9400 thalers.
In 1821, he fell in love with nineteen-year-old opera singer, Caroline Richter (called Medon), and had a relationship with her for several years. He discarded marriage plans, however, writing, "Marrying means to halve one's rights and double one's duties," and "Marrying means to grasp blindfolded into a sack hoping to find an eel amongst an assembly of snakes." When he was forty-three years old, he took interest in seventeen-year-old Flora Weiss but she rejected him as recorded in her diary.
In 1831, a cholera epidemic broke out in Berlin and Schopenhauer left the city. Schopenhauer settled permanently in Frankfurt in 1833, where he remained for the next twenty-seven years, living alone except for a succession of pet poodles named Atman and Butz. The numerous notes that he made during these years, amongst others on aging, were published posthumously under the title "Senilia". Schopenhauer had a robust constitution, but in 1860 his health began to deteriorate. He died of pulmonary-respiratory failure, on 21 September 1860 while sitting at home on his couch. He was 72.
Thought.
Philosophy of the "Will".
A key focus of Schopenhauer was his investigation of individual motivation. Before Schopenhauer, Hegel had popularized the concept of "Zeitgeist", the idea that society consisted of a collective consciousness that moved in a distinct direction, dictating the actions of its members. Schopenhauer, a reader of both Kant and Hegel, criticized their logical optimism and the belief that individual morality could be determined by society and reason. Schopenhauer believed that humans were motivated by only their own basic desires, or ("Will to Live"), which directed all of mankind.
For Schopenhauer, human desire was futile, illogical, directionless, and, by extension, so was all human action in the world. He wrote "Man can indeed do what he wants, but he cannot will what he wants." In this sense, he adhered to the Fichtean principle of idealism: "The world is "for" a subject." This idealism so presented, immediately commits it to an ethical attitude, unlike the purely epistemological concerns of Descartes and Berkeley. To Schopenhauer, the Will is a malignant, metaphysical existence that controls not only the actions of individual, intelligent agents, but ultimately all observable phenomena—an evil to be terminated via mankind's duties: asceticism and chastity. He is credited with one of the most famous opening lines of philosophy: "The world is my representation." Will, for Schopenhauer, is what Kant called the "thing-in-itself." Nietzsche was greatly influenced by this idea of Will, while developing it in a different direction.
Art and aesthetics.
For Schopenhauer, human desiring, "willing," and craving cause suffering or pain. A temporary way to escape this pain is through aesthetic contemplation (a method comparable to Zapffe's ""Sublimation"). Aesthetic contemplation allows one to escape this pain—albeit temporarily—because it stops one perceiving the world as mere presentation. Instead, one no longer perceives the world as an object of perception (therefore as subject to the Principle of Sufficient Grounds; time, space and causality) from which one is separated; rather one becomes one with that perception: "one can thus no longer separate the perceiver from the perception" ("The World as Will and Representation", section 34). From this immersion with the world one no longer views oneself as an individual who suffers in the world due to one's individual will but, rather, becomes a "subject of cognition" to a perception that is "Pure, will-less, timeless"" (section 34) where the essence, "ideas," of the world are shown. Art is the practical consequence of this brief aesthetic contemplation as it attempts to depict one's immersion with the world, thus tries to depict the essence/pure ideas of the world. Music, for Schopenhauer, was the purest form of art because it was the one that depicted the will itself without it appearing as subject to the Principle of Sufficient Grounds, therefore as an individual object. According to Daniel Albright, "Schopenhauer thought that music was the only art that did not merely copy ideas, but actually embodied the will itself."
He deemed music a timeless, universal language comprehended everywhere, that can imbue global enthusiasm, if in possession of a significant melody.
Mathematics.
Schopenhauer's realist views on mathematics are evident in his criticism of the contemporary attempts to prove the parallel postulate in Euclidean geometry. Writing shortly before the discovery of hyperbolic geometry demonstrated the logical independence of the axiom—and long before the general theory of relativity revealed that it does not necessarily express a property of physical space—Schopenhauer criticized mathematicians for trying to use indirect concepts to prove what he held to be directly evident from perception.
Throughout his writings, Schopenhauer criticized the logical derivation of philosophies and mathematics from mere concepts, instead of from intuitive perceptions.
Although Schopenhauer could see no justification for trying to prove Euclid's parallel postulate, he did see a reason for examining another of Euclid's axioms.
This follows Kant's reasoning.
Ethics.
Schopenhauer's moral theory proposed that only compassion can drive moral acts. According to Schopenhauer, compassion alone is the good of the object of the acts, that is, they cannot be inspired by either the prospect of personal utility or the feeling of duty. Mankind can also be guided by egoism and malice. Egotistic acts are those guided by self-interest, desire for pleasure or happiness. Schopenhauer believed most of our deeds belong to this class. Acts of malice are different from egotistic acts. As in the case of acts of compassion, these do not target personal utility. Their aim is to cause damage to others, independently of personal gains. He believed, like Swami Vivekananda in the unity of all with one-self and also believed that ego is the origin of pain and conflicts, that reduction of ego frames the moral principles.
Punishment.
According to Schopenhauer, whenever we make a choice, "We assume as necessary that decision was preceded by something from which it ensued, and which we call the ground or reason, or more accurately the motive, of the resultant action." Choices are not made freely. Our actions are necessary and determined because, "Every human being, even every animal, after the motive has appeared, must carry out the action which alone is in accordance with his inborn and immutable character." A definite action inevitably results when a particular motive influences a person's given, unchangeable character.
The State, Schopenhauer claimed, punishes criminals to prevent future crimes. It does so by placing "beside every possible motive for committing a wrong a more powerful motive for leaving it undone, in the inescapable punishment. Accordingly, the criminal code is as complete a register as possible of counter-motives to all criminal actions that can possibly be imagined..."
Should capital punishment be legal? "For safeguarding the lives of citizens," he asserted, "capital punishment is therefore absolutely necessary." "The murderer," wrote Schopenhauer, "who is condemned to death according to the law must, it is true, be now used as a mere "means", and with complete right. For public security, which is the principal object of the State, is disturbed by him; indeed it is abolished if the law remains unfulfilled. The murderer, his life, his person, must be the "means" of fulfilling the law, and thus of re-establishing public security." Schopenhauer disagreed with those who would abolish capital punishment. "Those who would like to abolish it should be given the answer: 'First remove murder from the world, and then capital punishment ought to follow.' "
People, according to Schopenhauer, cannot be improved. They can only be influenced by strong motives that overpower criminal motives. Schopenhauer declared that "real moral reform is not at all possible, but only determent from the deed..."
He claimed this doctrine was not original to him. Previously, it appeared in the writings of Plato, Seneca, Hobbes, Pufendorf, and Anselm Feuerbach. Schopenhauer declared that their teaching was corrupted by subsequent errors and therefore was in need of clarification.
God.
Even though Schopenhauer ended his treatise on the freedom of human will with the postulate of everyone's responsibility for their character and, consequently, acts—the responsibility following from one's being the Will as noumenon (from which also all the characters and creations come)—he considered his views incompatible with theism, on grounds of fatalism and, more generally, responsibility for evil. In Schopenhauer's philosophy the dogmas of Christianity lose their significance, and the "Last Judgment" is no longer preceded by anything—"The world is itself the Last Judgment on it." Whereas God, if he existed, would be evil.
Psychology.
Philosophers have not traditionally been impressed by the tribulations of sex, but Schopenhauer addressed it and related concepts forthrightly:
He named a force within man that he felt took invariable precedence over reason: the Will to Live or Will to Life ("Wille zum Leben"), defined as an inherent drive within human beings, and indeed all creatures, to stay alive; a force that inveigles us into reproducing.
Schopenhauer refused to conceive of love as either trifling or accidental, but rather understood it as an immensely powerful force that lay unseen within man's psyche and dramatically shaped the world:
These ideas foreshadowed the discovery of evolution, Freud's concepts of the libido and the unconscious mind, and evolutionary psychology in general.
Political and social thought.
Politics.
Schopenhauer's politics were, for the most part, an echo of his system of ethics (the latter being expressed in "Die beiden Grundprobleme der Ethik", available in English as two separate books, "On the Basis of Morality" and "On the Freedom of the Will"). Ethics also occupies about one quarter of his central work, "The World as Will and Representation".
In occasional political comments in his "Parerga and Paralipomena" and "Manuscript Remains", Schopenhauer described himself as a proponent of limited government. What was essential, he thought, was that the state should "leave each man free to work out his own salvation," and so long as government was thus limited, he would "prefer to be ruled by a lion than one of i fellow rats" — i.e., by a monarch, rather than a democrat. Schopenhauer shared the view of Thomas Hobbes on the necessity of the state, and of state action, to check the destructive tendencies innate to our species. He also defended the independence of the legislative, judicial and executive branches of power, and a monarch as an impartial element able to practise justice (in a practical and everyday sense, not a cosmological one). He declared monarchy as "that which is natural to man" for "intelligence has always under a monarchical government a much better chance against its irreconcilable and ever-present foe, stupidity" and disparaged republicanism as "unnatural as it is unfavourable to the higher intellectual life and the arts and sciences."
Schopenhauer, by his own admission, did not give much thought to politics, and several times he writes proudly of how little attention he had paid "to political affairs of i day." In a life that spanned several revolutions in French and German government, and a few continent-shaking wars, he did indeed maintain his aloof position of "minding not the times but the eternities." He wrote many disparaging remarks about Germany and the Germans. A typical example is, "For a German it is even good to have somewhat lengthy words in his mouth, for he thinks slowly, and they give him time to reflect."
Schopenhauer attributed civilizational primacy to the northern "white races" due to their sensitivity and creativity (except for the ancient Egyptians and Hindus whom he saw as equal):
The highest civilization and culture, apart from the ancient Hindus and Egyptians, are found exclusively among the white races; and even with many dark peoples, the ruling caste or race is fairer in colour than the rest and has, therefore, evidently immigrated, for example, the Brahmans, the Incas, and the rulers of the South Sea Islands. All this is due to the fact that necessity is the mother of invention because those tribes that emigrated early to the north, and there gradually became white, had to develop all their intellectual powers and invent and perfect all the arts in their struggle with need, want and misery, which in their many forms were brought about by the climate. This they had to do in order to make up for the parsimony of nature and out of it all came their high civilization.
Despite this, he was adamantly against differing treatment of races, was fervently anti-slavery, and supported the abolitionist movement in the United States. He describes the treatment of "u innocent black brothers whom force and injustice have delivered into he slave-master' devilish clutches" as "belonging to the blackest pages of mankind's criminal record."
Schopenhauer additionally maintained a marked metaphysical and political anti-Judaism. Schopenhauer argued that Christianity constituted a revolt against the materialistic basis of Judaism, exhibiting an Indian-influenced ethics reflecting the Aryan-Vedic theme of spiritual "self-conquest." This he saw as opposed to what he held to be the ignorant drive toward earthly utopianism and superficiality of a worldly Jewish spirit:
While all other religions endeavor to explain to the people by symbols the metaphysical significance of life, the religion of the Jews is entirely immanent and furnishes nothing but a mere war-cry in the struggle with other nations.
Views on women.
In Schopenhauer's 1851 essay "Of Women", he expressed his opposition to what he called "Teutonico-Christian stupidity" of reflexive unexamined reverence ("abgeschmackten Weiberveneration") for the female. Schopenhauer wrote that "Women are directly fitted for acting as the nurses and teachers of our early childhood by the fact that they are themselves childish, frivolous and short-sighted." He opined that women are deficient in artistic faculties and sense of justice, and expressed opposition to monogamy. Indeed, Rodgers and Thompson in "Philosophers Behaving Badly" call Schopenhauer "a misogynist without rival in...Western philosophy." He claimed that, "Woman is by nature meant to obey." The essay does give some compliments, however: that "women are decidedly more sober in their judgment than e are," and are more sympathetic to the suffering of others.
Schopenhauer's controversial writings have influenced many, from Friedrich Nietzsche to nineteenth-century feminists. Schopenhauer's biological analysis of the difference between the sexes, and their separate roles in the struggle for survival and reproduction, anticipates some of the claims that were later ventured by sociobiologists and evolutionary psychologists.
After the elderly Schopenhauer sat for a sculpture portrait by Elisabet Ney, he told Richard Wagner's friend Malwida von Meysenbug, "I have not yet spoken my last word about women. I believe that if a woman succeeds in withdrawing from the mass, or rather raising herself above the mass, she grows ceaselessly and more than a man."
Heredity and eugenics.
Schopenhauer believed that personality and intellect were inherited. He quotes Horace's saying, "From the brave and good are the brave descended" ("Odes", iv, 4, 29) and Shakespeare's line from "Cymbeline", "Cowards father cowards, and base things sire base" (IV, 2) to reinforce his hereditarian argument.
Mechanistically, Schopenhauer believed that a person inherits his level of intellect through his mother, and personal character through one's father.
This belief in heritability of traits informed Schopenhauer's view of love – placing it at the highest level of importance. For Schopenhauer the "final aim of all love intrigues, be they comic or tragic, is really of more importance than all other ends in human life. What it all turns upon is nothing less than the composition of the next generation... It is not the weal or woe of any one individual, but that of the human race to come, which is here at stake." This view of the importance for the species of whom we choose to love was reflected in his views on eugenics or good breeding. Here Schopenhauer wrote:
With our knowledge of the complete unalterability both of character and of mental faculties, we are led to the view that a real and thorough improvement of the human race might be reached not so much from outside as from within, not so much by theory and instruction as rather by the path of generation. Plato had something of the kind in mind when, in the fifth book of his "Republic", he explained his plan for increasing and improving his warrior caste. If we could castrate all scoundrels and stick all stupid geese in a convent, and give men of noble character a whole harem, and procure men, and indeed thorough men, for all girls of intellect and understanding, then a generation would soon arise which would produce a better age than that of Pericles.
In another context, Schopenhauer reiterated his antidemocratic-eugenic thesis: "If you want Utopian plans, I would say: the only solution to the problem is the despotism of the wise and noble members of a genuine aristocracy, a genuine nobility, achieved by mating the most magnanimous men with the cleverest and most gifted women. This proposal constitutes my Utopia and my Platonic Republic." Analysts (e.g., Keith Ansell-Pearson) have suggested that Schopenhauer's advocacy of anti-egalitarianism and eugenics influenced the neo-aristocratic philosophy of Friedrich Nietzsche, who initially considered Schopenhauer his mentor.
Animal welfare.
As a consequence of his monistic philosophy, Schopenhauer was very concerned about the welfare of animals. For him, all individual animals, including humans, are essentially the same, being phenomenal manifestations of the one underlying Will. The word "will" designated, for him, force, power, impulse, energy, and desire; it is the closest word we have that can signify both the real essence of all external things and also our own direct, inner experience. Since every living thing possesses will, then humans and animals are fundamentally the same and can recognize themselves in each other. For this reason, he claimed that a good person would have sympathy for animals, who are our fellow sufferers.
Compassion for animals is intimately associated with goodness of character, and it may be confidently asserted that he who is cruel to living creatures cannot be a good man.
Nothing leads more definitely to a recognition of the identity of the essential nature in animal and human phenomena than a study of zoology and anatomy.
The assumption that animals are without rights and the illusion that our treatment of them has no moral significance is a positively outrageous example of Western crudity and barbarity. Universal compassion is the only guarantee of morality.
In 1841, he praised the establishment, in London, of the Society for the Prevention of Cruelty to Animals, and also the Animals' Friends Society in Philadelphia. Schopenhauer even went so far as to protest against the use of the pronoun "it" in reference to animals because it led to the treatment of them as though they were inanimate things. To reinforce his points, Schopenhauer referred to anecdotal reports of the look in the eyes of a monkey who had been shot and also the grief of a baby elephant whose mother had been killed by a hunter.
He was very attached to his succession of pet poodles. Schopenhauer criticized Spinoza's belief that animals are to be used as a mere means for the satisfaction of humans.
Views on homosexuality and pederasty.
In the third, expanded edition of "The World as Will and Representation" (1859), Schopenhauer added an appendix to his chapter on the "Metaphysics of Sexual Love". He also wrote that homosexuality did have the benefit of preventing ill-begotten children. Concerning this, he stated, "... the vice we are considering appears to work directly against the aims and ends of nature, and that in a matter that is all important and of the greatest concern to her, it must in fact serve these very aims, although only indirectly, as a means for preventing greater evils." Shrewdly anticipating the interpretive distortion, on the part of the popular mind, of his attempted scientific "explanation" of pederasty as personal "advocacy" (when he had otherwise described the act, in terms of spiritual ethics, as an "objectionable aberration"), Schopenhauer sarcastically concludes the appendix with the statement that "by expounding these paradoxical ideas, I wanted to grant to the professors of philosophy a small favour, for they are very disconcerted by the ever-increasing publicization of my philosophy which they so carefully concealed. I have done so by giving them the opportunity of slandering me by saying that I defend and commend pederasty."
Intellectual interests and affinities.
Indology.
Schopenhauer read the Latin translation of the Upanishads, which French writer Anquetil du Perron had translated from the Persian translation of Prince Dara Shikoh entitled "Sirre-Akbar" ("The Great Secret"). He was so impressed by their philosophy that he called them "the production of the highest human wisdom," and believed they contained superhuman concepts. The Upanishads was a great source of inspiration to Schopenhauer. Writing about them, he said:
It is the most satisfying and elevating reading (with the exception of the original text) which is possible in the world; it has been the solace of my life and will be the solace of my death.
It is well known that the book "Oupnekhat" (Upanishad) always lay open on his table, and he invariably studied it before sleeping at night. He called the opening up of Sanskrit literature "the greatest gift of our century," and predicted that the philosophy and knowledge of the Upanishads would become the cherished faith of the West.
Schopenhauer was first introduced to the 1802 Latin Upanishad translation through Friedrich Majer. They met during the winter of 1813–1814 in Weimar at the home of Schopenhauer's mother according to the biographer Sanfranski. Majer was a follower of Herder, and an early Indologist. Schopenhauer did not begin a serious study of the Indic texts, however, until the summer of 1814. Sansfranski maintains that between 1815 and 1817, Schopenhauer had another important cross-pollination with Indian thought in Dresden. This was through his neighbor of two years, Karl Christian Friedrich Krause. Krause was then a minor and rather unorthodox philosopher who attempted to mix his own ideas with that of ancient Indian wisdom. Krause had also mastered Sanskrit, unlike Schopenhauer, and the two developed a professional relationship. It was from Krause that Schopenhauer learned meditation and received the closest thing to expert advice concerning Indian thought.
Most noticeable, in the case of Schopenhauer’s work, was the significance of the Chandogya Upanishad, whose Mahavakya, Tat Tvam Asi is mentioned throughout "The World as Will and Representation".
Buddhism.
Schopenhauer noted a correspondence between his doctrines and the Four Noble Truths of Buddhism. Similarities centered on the principles that life involves suffering, that suffering is caused by desire (taṇhā), and that the extinction of desire leads to liberation. Thus three of the four "truths of the Buddha" correspond to Schopenhauer's doctrine of the will. In Buddhism, however, while greed and lust are always unskillful, desire is ethically variable – it can be skillful, unskillful, or neutral.
For Schopenhauer, Will had ontological primacy over the intellect; in other words, desire is understood to be prior to thought. Schopenhauer felt this was similar to notions of puruṣārtha or goals of life in Vedānta Hinduism.
In Schopenhauer's philosophy, denial of the will is attained by either:
However, Buddhist nirvāṇa is not equivalent to the condition that Schopenhauer described as denial of the will. Nirvāṇa is not the extinguishing of the "person" as some Western scholars have thought, but only the "extinguishing" (the literal meaning of nirvana) of the flames of greed, hatred, and delusion that assail a person's character. Occult historian Joscelyn Godwin (1945– ) stated, "It was Buddhism that inspired the philosophy of Arthur Schopenhauer, and, through him, attracted Richard Wagner. This Orientalism reflected the struggle of the German Romantics, in the words of Leon Poliakov, to "...free themselves from Judeo-Christian fetters." In contradistinction to Godwin's claim that Buddhism inspired Schopenhauer, the philosopher himself made the following statement in his discussion of religions:
If I wished to take the results of my philosophy as the standard of truth, I should have to concede to Buddhism pre-eminence over the others. In any case, it must be a pleasure to me to see my doctrine in such close agreement with a religion that the majority of men on earth hold as their own, for this numbers far more followers than any other. And this agreement must be yet the more pleasing to me, inasmuch as "in my philosophizing I have certainly not been under its influence" mphasis adde. For up till 1818, when my work appeared, there was to be found in Europe only a very few accounts of Buddhism.
Buddhist philosopher Nishitani Keiji, however, sought to distance Buddhism from Schopenhauer. While Schopenhauer's philosophy may sound rather mystical in such a summary, his methodology was resolutely empirical, rather than speculative or transcendental:
Philosophy ... is a science, and as such has no articles of faith; accordingly, in it nothing can be assumed as existing except what is either positively given empirically, or demonstrated through indubitable conclusions.
Also note:
This actual world of what is knowable, in which we are and which is in us, remains both the material and the limit of our consideration.
The argument that Buddhism affected Schopenhauer’s philosophy more than any other Dharmic faith loses more credence when viewed in light of the fact that Schopenhauer did not begin a serious study of Buddhism until after the publication of "The World as Will and Representation" in 1818. Scholars have started to revise earlier views about Schopenhauer's discovery of Buddhism. Proof of early interest and influence, however, appears in Schopenhauer's 1815/16 notes (transcribed and translated by Urs App) about Buddhism. They are included in a recent case study that traces Schopenhauer's interest in Buddhism and documents its influence. Other scholarly work questions how similar Schopenhauer's philosophy actually is to Buddhism.
Influences.
Schopenhauer said he was influenced by the Upanishads, Immanuel Kant and Plato. References to Eastern philosophy and religion appear frequently in his writing. As noted above, he appreciated the teachings of the Buddha and even called himself a Buddhist. He said that his philosophy could not have been conceived before these teachings were available.
Concerning the Upanishads and Vedas, he writes in "The World as Will and Representation":
If the reader has also received the benefit of the Vedas, the access to which by means of the Upanishads is in my eyes the greatest privilege which this still young century (1818) may claim before all previous centuries, if then the reader, I say, has received his initiation in primeval Indian wisdom, and received it with an open heart, he will be prepared in the very best way for hearing what I have to tell him. It will not sound to him strange, as to many others, much less disagreeable; for I might, if it did not sound conceited, contend that every one of the detached statements which constitute the Upanishads, may be deduced as a necessary result from the fundamental thoughts which I have to enunciate, though those deductions themselves are by no means to be found there.
Among Schopenhauer's other influences were: Shakespeare, Jean-Jacques Rousseau, John Locke, Thomas Reid, Baruch Spinoza, Matthias Claudius, George Berkeley, David Hume, and René Descartes.
Critique of Kant and Hegel.
Critique of the Kantian philosophy.
Schopenhauer accepted Kant's double-aspect of the universe – the phenomenal (world of experience) and the noumenal (the true world, independent of experience). Some commentators suggest that Schopenhauer claimed that the noumenon, or thing-in-itself, was the basis for Schopenhauer's concept of the will. Other commentators suggest that Schopenhauer considered will to be only a subset of the "thing-in-itself" class, namely that which we can most directly experience.
Schopenhauer's identification of the Kantian "noumenon" (i.e., the actually existing entity) with what he termed "will" deserves some explanation. The noumenon was what Kant called the "Ding an sich" (the Thing in Itself), the reality that is the foundation of our sensory and mental representations of an external world. In Kantian terms, those sensory and mental representations are mere phenomena. Schopenhauer departed from Kant in his description of the relationship between the phenomenon and the noumenon. According to Kant, things-in-themselves ground the phenomenal representations in our minds; Schopenhauer, on the other hand, believed that phenomena and noumena are two different sides of the same coin. Noumena do not "cause" phenomena, but rather phenomena are simply the way by which our minds perceive the noumena, according to the principle of sufficient reason. This is explained more fully in Schopenhauer's doctoral thesis, "On the Fourfold Root of the Principle of Sufficient Reason" (1813).
Schopenhauer's second major departure from Kant's epistemology concerns the body. Kant's philosophy was formulated as a response to the radical philosophical skepticism of David Hume, who claimed that causality could not be observed empirically. Schopenhauer begins by arguing that Kant's demarcation between external objects, knowable only as phenomena, and the Thing in Itself of noumenon, contains a significant omission. There is, in fact, one physical object we know more intimately than we know any object of sense perception: our own body.
We know our human bodies have boundaries and occupy space, the same way other objects known only through our named senses do. Though we seldom think of our body as a physical object, we know even before reflection that it shares some of an object's properties. We understand that a watermelon cannot successfully occupy the same space as an oncoming truck; we know that if we tried to repeat the experiment with our own body, we would obtain similar results – we know this even if we do not understand the physics involved.
We know that our consciousness inhabits a physical body, similar to other physical objects only known as phenomena. Yet our consciousness is not commensurate with our body. Most of us possess the power of voluntary motion. We usually are not aware of the breathing of our lungs or the beating of our heart unless somehow our attention is called to them. Our ability to control either is limited. Our kidneys command our attention on their schedule rather than one we choose. Few of us have any idea what our liver is doing right now, though this organ is as needful as lungs, heart, or kidneys. The conscious mind is the servant, not the master, of these and other organs. These organs have an agenda the conscious mind did not choose, and over which it has limited power.
When Schopenhauer identifies the "noumenon" with the desires, needs, and impulses in us that we name "will," what he is saying is that we participate in the reality of an otherwise unachievable world outside the mind through will. We cannot "prove" that our mental picture of an outside world corresponds with a reality by reasoning; through will, we know – without thinking – that the world can stimulate us. We suffer fear, or desire: these states arise involuntarily; they arise prior to reflection; they arise even when the conscious mind would prefer to hold them at bay. The rational mind is, for Schopenhauer, a leaf borne along in a stream of pre-reflective and largely unconscious emotion. That stream is will, and through will, if not through logic, we can participate in the underlying reality beyond mere phenomena. It is for this reason that Schopenhauer identifies the "noumenon" with what we call our will.
In his criticism of Kant, Schopenhauer claimed that sensation and understanding are separate and distinct abilities. Yet, for Kant, an object is known through each of them. Kant wrote: "... here are two stems of human knowledge ... namely, sensibility and understanding, objects being given by the former ensibilit and thought by the latter nderstandin." Schopenhauer disagreed. He asserted that mere sense impressions, not objects, are given by sensibility. According to Schopenhauer, objects are intuitively perceived by understanding and are discursively thought by reason (Kant had claimed that (1) the understanding thinks objects through concepts and that (2) reason seeks the unconditioned or ultimate answer to "why?"). Schopenhauer said that Kant's mistake regarding perception resulted in all of the obscurity and difficult confusion that is exhibited in the Transcendental Analytic section of his critique.
Lastly, Schopenhauer departed from Kant in how he interpreted the Platonic ideas. In "The World as Will and Representation" Schopenhauer explicitly stated:
...Kant used the word de wrongly as well as illegitimately, although Plato had already taken possession of it, and used it most appropriately.
Instead Schopenhauer relied upon the Neoplatonist interpretation of the biographer Diogenes Laërtius from "Lives and Opinions of Eminent Philosophers". In reference to Plato’s Ideas, Schopenhauer quotes Laërtius verbatim in an explanatory footnote.
Diogenes Laërtius (III, 12) Plato ideas in natura velut exemplaria dixit subsistere; cetera his esse similia, ad istarum similitudinem consistencia.
Critique of Hegel.
Schopenhauer expressed his dislike for the philosophy of his contemporary Georg Wilhelm Friedrich Hegel many times in his published works. The following quotations are typical:
In his Foreword to the first edition of his work "Die beiden Grundprobleme der Ethik", Schopenhauer suggested that he had shown Hegel to have fallen prey to the "Post hoc ergo propter hoc" fallacy.
Schopenhauer suggested that Hegel's works were filled with "castles of abstraction," and that Hegel used deliberately impressive but ultimately vacuous verbiage. He also thought that his glorification of church and state were designed for personal advantage and had little to do with the search for philosophical truth. For instance, the Right Hegelians interpreted Hegel as viewing the Prussian state of his day as perfect and the goal of all history up until then.
Criticism of Schopenhauer's personal life.
The British philosopher and historian Bertrand Russell deemed Schopenhauer an insincere person, because judging by his life:
Bryan Magee points out that "the answer to such shallow, but not uncommon criticism" is found in a quotation from Schopenhauer:
Influence.
Schopenhauer has had a massive influence upon later thinkers, though more so in the arts (especially literature and music) and psychology than in philosophy. His popularity peaked in the early twentieth century, especially during the Modernist era, and waned somewhat thereafter. Nevertheless, a number of recent publications have reinterpreted and modernised the study of Schopenhauer. His theory is also being explored by some modern philosophers as a precursor to evolutionary theory and modern evolutionary psychology.
Russian writer and philosopher Leo Tolstoy was greatly influenced by Schopenhauer. After reading Schopenhauer's "The World as Will and Representation", Tolstoy gradually became converted to the ascetic morality upheld in that work as the proper spiritual path for the upper classes: "Do you know what this summer has meant for me? Constant raptures over Schopenhauer and a whole series of spiritual delights which I've never experienced before. ... no student has ever studied so much on his course, and learned so much, as I have this summer"
Richard Wagner, writing in his autobiography, remembered his first impression that Schopenhauer left on him (when he read "World as Will and Representation"):
Wagner also commented on that "serious mood, which was trying to find ecstatic expression" created by Schopenhauer inspired the conception of Tristan und Isolde. See also Influence of Schopenhauer on Tristan und Isolde.
Friedrich Nietzsche owed the awakening of his philosophical interest to reading "The World as Will and Representation" and admitted that he was one of the few philosophers that he respected, dedicating to him his essay "Schopenhauer als Erzieher" one of his "Untimely Meditations".
Jorge Luis Borges remarked that the reason he had never attempted to write a systematic account of his world view, despite his penchant for philosophy and metaphysics in particular, was because Schopenhauer had already written it for him.
As a teenager, Ludwig Wittgenstein adopted Schopenhauer's epistemological idealism. However, after his study of the philosophy of mathematics, he rejected epistemological idealism for Gottlob Frege's conceptual realism. In later years, Wittgenstein was highly dismissive of Schopenhauer, describing him as an ultimately shallow thinker: "Schopenhauer has quite a crude mind... where real depth starts, his comes to an end."
The philosopher Gilbert Ryle read Schopenhauer's works as a student, but later largely forgot them, only to unwittingly recycle ideas from Schopenhauer in his "The Concept of Mind".

</doc>
<doc id="701" url="https://en.wikipedia.org/wiki?curid=701" title="Angola">
Angola

Angola , officially the Republic of Angola ( ; Kikongo, Kimbundu and Umbundu: "Repubilika ya Ngola"), is a country in Southern Africa. It is the seventh-largest country in Africa, and is bordered by Namibia to the south, the Democratic Republic of the Congo to the north, Zambia to the east, and the Atlantic Ocean to west. The exclave province of Cabinda has borders with the Republic of the Congo and the Democratic Republic of the Congo. The capital and largest city is Luanda.
Although its territory has been inhabited since the Paleolithic Era, modern Angola originates in Portuguese colonization, which began with, and was for centuries limited to, coastal settlements and trading posts established from the 16th century onwards. In the 19th century, European settlers slowly and hesitantly began to establish themselves in the interior. As a Portuguese colony, Angola did not encompass its present borders until the early 20th century, following resistance by groups such as the Cuamato, the Kwanyama and the Mbunda. Independence was achieved in 1975 after the protracted liberation war. That same year, Angola descended into an intense civil war that lasted until 2002. It has since become a relatively stable unitary presidential republic.
Angola has vast mineral and petroleum reserves, and its economy is among the fastest growing in the world, especially since the end of the civil war. In spite of this, the standard of living remains low for the majority of the population, and life expectancy and infant mortality rates in Angola are among the worst in the world. Angola's economic growth is highly uneven, with the majority of the nation's wealth concentrated in a disproportionately small sector of the population.
Angola is a member state of the United Nations, OPEC, African Union, the Community of Portuguese Language Countries, the Latin Union and the Southern African Development Community. A highly multiethnic country, Angola's 24.3 million people span various tribal groups, customs, and traditions. Angolan culture reflects centuries of Portuguese rule, namely in the predominance of the Portuguese language and Roman Catholicism, combined with diverse indigenous influences.
Etymology.
The name "Angola" comes from the Portuguese colonial name "Reino de Angola (Kingdom of Angola)", appearing as early as Dias de Novais's 1571 charter. The toponym was derived by the Portuguese from the title "ngola" held by the kings of Ndongo. Ndongo was a kingdom in the highlands, between the Kwanza and Lukala Rivers, nominally tributary to the king of Kongo but which was seeking greater independence during the 16th century.
History.
Early migrations and political units.
Khoi and San hunter-gatherers are the earliest known modern human inhabitants of the area. They were largely absorbed or replaced by Bantu peoples during the Bantu migrations, though small numbers remain in parts of southern Angola to the present day. The Bantu came from the north, probably from somewhere near the present-day Republic of Cameroon.
During this time, the Bantu established a number of political units ("kingdoms", "empires") in most parts of what today is Angola. The best known of these is the Kingdom of the Kongo that had its centre in the northwest of contemporary Angola, but included important regions in the west of present-day Democratic Republic and Republic of Congo and in southern Gabon. It established trade routes with other trading cities and civilisations up and down the coast of southwestern and West Africa and even with the Great Zimbabwe Mutapa Empire, but engaged in little or no transoceanic trade. To its south lay the Kingdom of Ndongo, from which the area of the later Portuguese colony was sometimes known as Dongo.
Portuguese colonization.
The region now known as Angola was reached by the Portuguese explorer Diogo Cão in 1484. The year before, the Portuguese had established relations with the Kingdom of Kongo, which stretched at the time from modern Gabon in the north to the Kwanza River in the south. The Portuguese established their principal early trading post at Soyo, which now forms the northernmost city in Angola apart from the Cabinda enclave. Paulo Dias de Novais founded São Paulo de Loanda (Luanda) in 1575 with a hundred families of settlers and four hundred soldiers. Benguela was fortified in 1587 and elevated to a township in 1617.
The Portuguese established several other settlements, forts, and trading posts along the Angolan coast, principally trading in Angolan slaves for Brazilian plantations. Local slave dealers provided a large number of slaves for the Portuguese Empire, usually sold in exchange for manufactured goods from Europe. This part of the Atlantic slave trade continued until after Brazil's independence in the 1820s.
Despite Portugal's nominal claims, as late as the 19th century, their control over the interior country of Angola was minimal, but the 16th century saw them gain control of the coast through a series of treaties and wars. Life for European colonists was difficult and progress slow. Iliffe notes that "Portuguese records of Angola from the 16th century show that a great famine occurred on average every seventy years; accompanied by epidemic disease, it might kill one-third or one-half of the population, destroying the demographic growth of a generation and forcing colonists back into the river valleys".
Amid the Portuguese Restoration War, the Dutch occupied Luanda in 1641, using alliances with local peoples against Portuguese holdings elsewhere. A fleet under Salvador de Sá retook Luanda for Portugal in 1648; reconquest of the rest of the territory was completed by 1650. New treaties with Kongo were signed in 1649; others with Njinga's Kingdom of Matamba and Ndongo followed in 1656. The conquest of Pungo Andongo in 1671 was the last major Portuguese expansion from Luanda, as attempts to invade Kongo in 1670 and Matamba in 1681 failed. Portugal also expanded inward from Benguela, but until the late 19th century the inroads from Luanda and Benguela were very limited. Portugal had neither the intention nor the means to carry out a large scale territorial occupation and colonization.
Development of the hinterland began after the Berlin Conference in 1885 fixed the colony's borders, and British and Portuguese investment fostered mining, railways, and agriculture based on various forced-labour and voluntary labour systems. Full Portuguese administrative control of the hinterland did not establish itself until the beginning of the 20th century. Portugal had a minimalist presence in Angola for nearly five hundred years, and early calls for independence provoked little reaction amongst the population who had no social identity related to the territory as a whole. More overtly political and "nationalist" organisations first appeared in the 1950s and began to make demands for self-determination, especially in international forums such as the Non-Aligned Movement.
The Portuguese regime, meanwhile, refused to accede to the demands for independence, provoking an armed conflict that started in 1961 when freedom fighters attacked both white and black civilians in cross-border operations in northeastern Angola. The war came to be known as the Colonial War. In this struggle, the principal protagonists included, the People's Movement for the Liberation of Angola (MPLA), founded in 1956, the National Front for the Liberation of Angola (FNLA), which appeared in 1961 and the National Union for the Total Independence of Angola (UNITA), founded in 1966.
After many years of conflict that led to the weakening of all the insurgent parties, Angola gained its independence on 11 November 1975, after the 1974 coup d'état in Lisbon, Portugal, which overthrew the Portuguese regime headed by Marcelo Caetano.
Portugal's new revolutionary leaders began in 1974 a process of political change at home and accepted independence for its former colonies abroad. In Angola a fight for dominance broke out immediately between the three nationalist movements. The events prompted a mass exodus of Portuguese citizens, creating up to 300 000 destitute Portuguese refugees—the "retornados". The new Portuguese government tried to mediate an understanding between the three competing movements, and succeeded in getting them to agree, on paper, to form a common government. But in the end none of the African parties respected the commitments made, and military force resolved the issue.
Independence and civil war.
After it gained independence in November 1975, Angola experienced a devastating civil war which lasted several decades (with some interludes). It claimed millions of lives and produced many refugees; it came to an end only in 2002.
Following negotiations held in Portugal, itself experiencing severe social and political turmoil and uncertainty due to the April 1974 revolution, Angola's three main guerrilla groups agreed to establish a transitional government in January 1975. Within two months, however, the FNLA, MPLA and UNITA had started fighting each other and the country began splitting into zones controlled by rival armed political groups. The MPLA gained control of the capital Luanda and much of the rest of the country. With the support of the United States, Zaïre and South Africa intervened militarily in favour of the FNLA and UNITA with the intention of taking Luanda before the declaration of independence. In response, Cuba intervened in favor of the MPLA (see: Cuba in Angola), which became a flash point for the Cold War.
With Cuban support, the MPLA held Luanda and declared independence on 11 November 1975, with Agostinho Neto becoming the first president, though the civil war continued. At this time, most of the half-million Portuguese who lived in Angola – and who had accounted for the majority of the skilled work in the public administration, agriculture, industries and trade – fled the country, leaving its once prosperous and growing economy in a state of bankruptcy.
For most of 1975–1990, the MPLA organised and maintained a socialist régime. In 1990, when the Cold War ended, MPLA abandoned its ties to the Marxist–Leninist ideology and declared social democracy to be its official ideology, going on to win the 1992 general election. However, eight opposition parties rejected the elections as rigged, sparking the Halloween massacre.
Ceasefire with UNITA.
On 22 March 2002, Jonas Savimbi, the leader of UNITA, was killed in combat with government troops. A cease-fire was reached by the two factions shortly afterwards. UNITA gave up its armed wing and assumed the role of major opposition party, although in the knowledge that in the present regime a legitimate democratic election was impossible. Although the political situation of the country began to stabilize, regular democratic processes were not established before the Elections in Angola in 2008 and 2012 and the adoption of a new Constitution of Angola in 2010, all of which strengthened the prevailing Dominant-party system. MPLA head officials continue e.g. to be given senior positions in top level companies or other fields, although a few outstanding UNITA figures are given some shares in the economic as well as in the military share.
Among Angola's major problems are a serious humanitarian crisis (a result of the prolonged war), the abundance of minefields, the continuation of the political, and to a much lesser degree, military activities in favour of the independence of the northern exclave of Cabinda, carried out in the context of the protracted Cabinda Conflict by the Frente para a Libertação do Enclave de Cabinda, but most of all, the dilapidation of the country's rich mineral resources by the regime. While most of the internally displaced have now settled around the capital, in the so-called "Musseques", the general situation for Angolans remains desperate.
Geography.
At , Angola is the world's twenty-third largest country. It is comparable in size to Mali, or twice the size of France or Texas. It lies mostly between latitudes 4° and 18°S, and longitudes 12° and 24°E.
Angola is bordered by Namibia to the south, Zambia to the east, the Democratic Republic of the Congo to the north-east, and the South Atlantic Ocean to the west. The coastal exclave of Cabinda in the north, borders the Republic of the Congo to the north, and the Democratic Republic of the Congo to the south. Angola's capital, Luanda, lies on the Atlantic coast in the northwest of the country.
Climate.
Angola has three seasons, a dry season which lasts from May to October, a transitional season with some rain from November to January and a hot, rainy season from February to April. April is the wettest month.
Politics.
Angola's motto is "Virtus Unita Fortior", a Latin phrase meaning "Virtue is stronger when united". The Angolan government is composed of three branches of government: executive, legislative, and judicial. The executive branch of the government is composed of the President, the Vice-Presidents and the Council of Ministers. The legislative branch comprises a 220-seat unicameral legislature elected from both provincial and nationwide constituencies. For decades, political power has been concentrated in the presidency.
The Constitution of 2010 establishes the broad outlines of government structure and delineates the rights and duties of citizens. The legal system is based on Portuguese and customary law but is weak and fragmented, and courts operate in only 12 of more than 140 municipalities. A Supreme Court serves as the appellate tribunal; a Constitutional Court holds the powers of judicial review. Governors of the 18 provinces are appointed by the president.
After the end of the Civil War the regime came under pressure from within as well as from the international community to become more democratic and less authoritarian. Its reaction was to implement a number of changes without substantially changing its character.
Angola is classified as 'not free' by Freedom House in the Freedom in the World 2014 report. The report noted that the August 2012 parliamentary elections, in which the ruling Popular Movement for the Liberation of Angola won more than 70% of the vote, suffered from serious flaws, including outdated and inaccurate voter rolls. Voter turnout dropped from 80% in 2008 to 60%.
Angola scored poorly on the 2013 Ibrahim Index of African Governance. It was ranked 39 out of 52 sub-Saharan African countries, scoring particularly badly in the areas of participation and human rights, sustainable economic opportunity, and human development. The Ibrahim Index uses a number of variables to compile its list which reflects the state of governance in Africa.
The new constitution, adopted in 2010, further sharpened the authoritarian character of the regime. In the future, there will be no presidential elections; the president and the vice-president of the political party which wins the parliamentary elections automatically become president and vice-president of Angola. Through a variety of mechanisms, the state president controls all the other organs of the state, so that separation of powers is not maintained. As a consequence, Angola no longer has a presidential system in the sense of the systems existing, e.g., in the USA or in France. In terms of the classifications used in constitutional law, its regime is considered one of several authoritarian regimes in Africa.
On 16 October 2014, Angola was elected for the second time as a non-permanent member of the UN Security Council, with 190 favourable votes out of 193. The mandate begins on 1 January 2015 and lasts for two years.
Also in that month, the country took on the leadership of the Group of African Ministers and Governors at the International Monetary Fund and the World Bank, following the debates at the annual meetings of both entities.
Since January 2014 the Republic of Angola has held the presidency of the International Conference on the Great Lakes Region (ICGLR). In 2015, the executive secretary of ICGLR, Ntumba Luaba, said that Angola is the example to be followed by members of the organization, because of the significant progress made over the 12 years of peace, particularly in terms of socioeconomic and political-military stability.
Military.
The Angolan Armed Forces (AAF) is headed by a Chief of Staff who reports to the Minister of Defense. There are three divisions—the Army (Exército), Navy (Marinha de Guerra, MGA), and National Air Force (Força Aérea Nacional, FAN). Total manpower is about 110,000. Its equipment includes Russian-manufactured fighters, bombers, and transport planes. There are also Brazilian-made EMB-312 Tucano for training role, Czech-made L-39 for training and bombing role, Czech Zlin for training role and a variety of western made aircraft such as C-212\Aviocar, Sud Aviation Alouette III, etc. A small number of AAF personnel are stationed in the Democratic Republic of the Congo (Kinshasa) and the Republic of the Congo (Brazzaville).
Police.
The National Police departments are Public Order, Criminal Investigation, Traffic and Transport, Investigation and Inspection of Economic Activities, Taxation and Frontier Supervision, Riot Police and the Rapid Intervention Police. The National Police are in the process of standing up an air wing, which will provide helicopter support for operations. The National Police are developing their criminal investigation and forensic capabilities. The force has an estimated 6,000 patrol officers, 2,500 taxation and frontier supervision officers, 182 criminal investigators and 100 financial crimes detectives and around 90 economic activity inspectors.
The National Police have implemented a modernization and development plan to increase the capabilities and efficiency of the total force. In addition to administrative reorganization, modernization projects include procurement of new vehicles, aircraft and equipment, construction of new police stations and forensic laboratories, restructured training programs and the replacement of AKM rifles with 9 mm Uzis for officers in urban areas.
Justice.
In 2014, a new penal code is expected to take effect in Angola. The classification of money-laundering as a crime is one of the novelties in the new legislation.
Administrative divisions.
Angola is divided into eighteen provinces ("províncias") and 163 municipalities. The municipalities are further divided into 475 communes (townships). The provinces are:
Exclave of Cabinda.
With an area of approximately , the Northern Angolan province of Cabinda is unusual in being separated from the rest of the country by a strip, some wide, of the Democratic Republic of Congo along the lower Congo river. Cabinda borders the Congo Republic to the north and north-northeast and the DRC to the east and south. The town of Cabinda is the chief population center.
According to a 1995 census, Cabinda had an estimated population of 600,000, approximately 400,000 of whom live in neighboring countries. Population estimates are, however, highly unreliable. Consisting largely of tropical forest, Cabinda produces hardwoods, coffee, cocoa, crude rubber and palm oil. The product for which it is best known, however, is its oil, which has given it the nickname, "the Kuwait of Africa". Cabinda's petroleum production from its considerable offshore reserves now accounts for more than half of Angola's output. Most of the oil along its coast was discovered under Portuguese rule by the Cabinda Gulf Oil Company (CABGOC) from 1968 onwards.
Ever since Portugal handed over sovereignty of its former overseas province of Angola to the local independence groups (MPLA, UNITA, and FNLA), the territory of Cabinda has been a focus of separatist guerrilla actions opposing the Government of Angola (which has employed its military forces, the FAA—Forças Armadas Angolanas) and Cabindan separatists. The Front for the Liberation of the Enclave of Cabinda-Armed Forces of Cabinda (FLEC-FAC) announced a virtual Federal Republic of Cabinda under the Presidency of N'Zita Henriques Tiago. One of the characteristics of the Cabindan independence movement is its constant fragmentation, into smaller and smaller factions.
Economy.
Angola has a rich subsoil heritage, from diamonds, oil, gold, copper, and a rich wildlife (dramatically impoverished during the civil war), forest, and fossils. Since independence, oil and diamonds have been the most important economic resource. Smallholder and plantation agriculture have dramatically dropped because of the Angolan Civil War, but have begun to recover after 2002. The transformation industry that had come into existence in the late colonial period collapsed at independence, because of the exodus of most of the ethnic Portuguese population, but has begun to reemerge (with updated technologies), partly because of the influx of new Portuguese entrepreneurs. Similar developments can be verified in the service sector.
Overall, Angola's economy has undergone a period of transformation in recent years, moving from the disarray caused by a quarter century of civil war to being the fastest growing economy in Africa and one of the fastest in the world, with an average GDP growth of 20 percent between 2005 and 2007. In the period 2001–10, Angola had the world's highest annual average GDP growth, at 11.1 percent. In 2004, China's Eximbank approved a $2 billion line of credit to Angola. The loan is being used to rebuild Angola's infrastructure, and has also limited the influence of the International Monetary Fund in the country. China is Angola's biggest trade partner and export destination as well as the fourth-largest importer. Bilateral trade reached $27.67 billion in 2011, up 11.5 percent year-on-year. China's imports, mainly crude oil and diamonds, increased 9.1 percent to $24.89 billion while China's exports, including mechanical and electrical products, machinery parts and construction materials, surged 38.8 percent. The overabundance of oil led to a local unleaded gasoline "pricetag" of £0.37 per gallon.
"The Economist" reported in 2008 that diamonds and oil make up 60 percent of Angola's economy, almost all of the country's revenue and are its dominant exports. Growth is almost entirely driven by rising oil production which surpassed in late 2005 and was expected to grow to by 2007. Control of the oil industry is consolidated in Sonangol Group, a conglomerate which is owned by the Angolan government. In December 2006, Angola was admitted as a member of OPEC. However, operations in diamond mines include partnerships between state-run Endiama and mining companies such as ALROSA which continue operations in Angola. The economy grew 18% in 2005, 26% in 2006 and 17.6% in 2007. However, due to the global recession the economy contracted an estimated −0.3% in 2009. The security brought about by the 2002 peace settlement has led to the resettlement of 4 million displaced persons, thus resulting in large-scale increases in agriculture production.
Although the country's economy has developed significantly since achieving political stability in 2002, mainly thanks to the fast-rising earnings of the oil sector, Angola faces huge social and economic problems. These are in part a result of the almost continual state of conflict from 1961 onwards, although the highest level of destruction and socio-economic damage took place after the 1975 independence, during the long years of civil war. However, high poverty rates and blatant social inequality are chiefly the outcome of a combination of a persistent political authoritarianism, of "neo-patrimonial" practices at all levels of the political, administrative, military, and economic apparatuses, and of a pervasive corruption. The main beneficiary of this situation is a social segment constituted since 1975, but mainly during the last decades, around the political, administrative, economic, and military power holders, which has accumulated (and continues accumulating) enormous wealth. "Secondary beneficiaries" are the middle strata which are about to become social classes. However, overall almost half the population has to be considered as poor, but in this respect there are dramatic differences between the countryside and the cities (where by now slightly more than 50% of the people live).
An inquiry carried out in 2008 by the Angolan Instituto Nacional de Estatística has it that in the rural areas roughly 58% must be classified as "poor", according to UN norms, but in the urban areas only 19%, while the overall rate is 37%. In the cities, a majority of families, well beyond those officially classified as poor, have to adopt a variety of survival strategies. At the same time, in urban areas social inequality is most evident, and assumes extreme forms in the capital, Luanda. In the Human Development Index Angola constantly ranks in the bottom group.
According to The Heritage Foundation, a conservative American think tank, oil production from Angola has increased so significantly that Angola now is China's biggest supplier of oil. Growing oil revenues have also created opportunities for corruption: according to a recent Human Rights Watch report, 32 billion US dollars disappeared from government accounts from 2007 to 2010.
Before independence in 1975, Angola was a breadbasket of southern Africa and a major exporter of bananas, coffee and sisal, but three decades of civil war (1975–2002) destroyed the fertile countryside, leaving it littered with landmines and driving millions into the cities. The country now depends on expensive food imports, mainly from South Africa and Portugal, while more than 90 percent of farming is done at family and subsistence level. Thousands of Angolan small-scale farmers are trapped in poverty.
The enormous differences between the regions pose a serious structural problem in the Angolan economy. This is best illustrated by the fact that about one third of the economic activities is concentrated in Luanda and the neighbouring Bengo province, while several areas of the interior are characterized by stagnation and even regression.
One of the economic consequences of the social and regional disparities is a sharp increase in Angolan private investments abroad. The small fringe of Angolan society where most of the accumulation takes place seeks to spread its assets, for reasons of security and profit. For the time being, the biggest share of these investments is concentrated in Portugal where the Angolan presence (including that of the family of the state president) in banks as well as in the domains of energy, telecommunications, and mass media has become notable, as has the acquisition of vineyards and orchards as well as of touristic enterprises.
Sub-Saharan Africa nations are globally achieving impressive improvements in well-being, according to a report by Tony Blair Africa Governance Initiative and The Boston Consulting Group. Angola has upgraded critical infrastructure, an investment made possible by funds from the nation's development of oil resources. According to this report, just slightly more than ten years after the end of the civil war Angola's standard of living has overall greatly improved. Life expectancy, which was just 46 years in 2002, reached 51 in 2011. Mortality rates for children fell from 25 percent in 2001 to 19 percent in 2010 and the number of students enrolled in primary school has tripled since 2001. However, at the same time the social and economic inequality that has characterised the country since long has not diminished, but on the contrary deepened in all respects.
With a stock of assets corresponding to 70 billion USD (6.8 billion Kz), Angola is now the third largest financial market in sub-Saharan Africa, surpassed only by Nigeria and South Africa. According to the Angolan Minister of Economy, Abraão Gourgel, the financial market of the country grew modestly from 2002 and now lies in third place at the level of sub-Saharan Africa.
Angola's economy is expected to grow by 3.9 percent in 2014 said the International Monetary Fund (IMF). According to the Fund, robust growth in the nonoil economy, mainly driven by a very good performance in the agricultural sector, is expected to offset a temporary drop in oil production.
Angola's financial system is maintained by the National Bank of Angola and managed by governor . According to a study on the banking sector, carried out by Deloitte, the monetary policy led by Banco Nacional de Angola (BNA), the Angolan national bank, allowed a decrease in the inflation rate put at 7.96% in December 2013, which contributed to the sector's growth trend. According to estimates released by Angola's central bank, the country's economy should grow at an annual average rate of 5 percent over the next four years, boosted by the increasing participation of the private sector.
On 19 December 2014, the Capital Market in Angola started. BODIVA (Angola Securities and Debt Stock Exchange, in English) received the secondary public debt market, and it is expected to start the corporate debt market by 2015, but the stock market should only be a reality in 2016.
Transport.
Transport in Angola consists of:
Travel on highways outside of towns and cities in Angola (and in some cases within) is often not best advised for those without four-by-four vehicles. While a reasonable road infrastructure has existed within Angola, time and the war have taken their toll on the road surfaces, leaving many severely potholed, littered with broken asphalt. In many areas drivers have established alternate tracks to avoid the worst parts of the surface, although careful attention must be paid to the presence or absence of landmine warning markers by the side of the road. The Angolan government has contracted the restoration of many of the country's roads. The road between Lubango and Namibe, for example, was completed recently with funding from the European Union, and is comparable to many European main routes. Progress to complete the road infrastructure is likely to take some decades, but substantial efforts are already being made in the right directions.
Telecommunications.
The telecommunications industry is considered one of the main strategic sectors in Angola.
In October 2014, the building of a optic fiber underwater cable was announced. This project aims to turn Angola into a continental hub, thus improving Internet connections both nationally and internationally.
On 11 March 2015, the First Angolan Forum of Telecommunications and Information Technology was held, in Luanda under the motto "The challenges of telecommunications in the current context of Angola". The purpose of this forum was to promote the debate on topical issues on telecommunications in Angola and worldwide. A study about this sector was also presented at this forum, and some of its conclusions were: Angola had the first telecommunications operator in Africa to test the High Speed Internet technology (LTE-Advanced with speeds up to 400Mbit/s); It has a mobile penetration rate of about 75%; There are about 3.5 million smartphones in the Angolan market; There are about 25,000 kilometers of optical fiber installed in the country.
The first Angolan satellite, AngoSat-1, will be ready for launch into orbit in 2017 and it will ensure telecommunications throughout the country. According to Aristides Safeca, Secretary of State for Telecommunications, the satellite will provide telecommunications services, TV, internet and e-government and will remain into orbit "at best" for 18 years.
Technology.
The management of the domain '.ao' on web pages, will go from Portugal to Angola in 2015, following the approval of a new legislation by the Angolan Government. The joint decree of the minister of Telecommunications and Information Technologies, José Carvalho da Rocha, and the minister of Science and Technology, Maria Cândida Pereira Teixeira, states that "under the massification" of that Angolan domain, "conditions are created for the transfer of the domain root '.ao' of Portugal to Angola".
Demographics.
Angola has a population of 24,383,301 inhabitants according to the preliminary results of its 2014 census, the first one conducted or carried out since 15 December 1970. It is composed of Ovimbundu (language Umbundu) 37%, Ambundu (language Kimbundu) 25%, Bakongo 13%, and 32% other ethnic groups (including the Chokwe, the Ovambo, the Ganguela and the Xindonga) as well as about 2% "mestiços" (mixed European and African), 1.4% Chinese and 1% European. The Ambundu and Ovimbundu nations combined form a majority of the population, at 62%. The population is forecast to grow to over 60 million people to 2050, 2.7 times the 2014 population.
It is estimated that Angola was host to 12,100 refugees and 2,900 asylum seekers by the end of 2007. 11,400 of those refugees were originally from the Democratic Republic of Congo, who arrived in the 1970s. As of 2008 there were an estimated 400,000 Democratic Republic of the Congo migrant workers, at least 30,000 Portuguese, and about 259,000 Chinese living in Angola.
Since 2003, more than 400,000 Congolese migrants have been expelled from Angola. Prior to independence in 1975, Angola had a community of approximately 350,000 Portuguese, but the vast majority left after independence and the ensuing civil war. However, Angola has recovered its Portuguese minority in recent years; currently, there are about 200,000 registered with the consulates, and increasing due to the debt crisis in Portugal and the relative prosperity in Angola. The Chinese population stands at 258,920, mostly composed of temporary migrants. Also, there is a small Brazilian community of about 5,000 people.
The total fertility rate of Angola is 5.54 children born per woman (2012 estimates), the 11th highest in the world.
Languages.
The languages in Angola are those originally spoken by the different ethnic groups and Portuguese, introduced during the Portuguese colonial era. The indigenous languages with the largest usage are Umbundu, Kimbundu, and Kikongo, in that order. Portuguese is the official language of the country.
Mastery of the official language is probably more extended in Angola than it is elsewhere in Africa, and this certainly applies to its use in everyday life. Moreover, and above all, the proportion of native (or near native) speakers of the language of the former colonizer, turned official after independence, is no doubt considerably higher than in any other African country.
There are three intertwined historical reasons for this situation.
As a consequence of all this, the African "lower middle class" which at that stage formed in Luanda and other cities began to often prevent their children from learning the local African language, in order to guarantee that they learned Portuguese as their native language. At the same time, the white and "mestiço" population, where some knowledge of African languages could previously often been found, neglected this aspect more and more, to the point of frequently ignoring it totally.
After independence, these tendencies continued, and were even strengthened, under the rule of the MPLA which has its main social roots exactly in those social segments where the mastery of Portuguese as well as the proportion of native Portuguese speakers was highest. This became a political side issue, as FNLA and UNITA, given their regional constituencies, came out in favour of a greater attention to the African languages, and as the FNLA favoured French over Portuguese.
The dynamics of the language situation, as described above, were additionally fostered by the massive migrations triggered by the Civil War. Ovimbundu, the most populous ethnic group and the most affected by the war, appeared in great numbers in urban areas outside their areas, especially in Luanda and surroundings. At the same time, a majority of the Bakongo who had fled to the Democratic Republic of Congo in the early 1960s, or of their children and grandchildren, returned to Angola, but mostly did not settle in their original "habitat", but in the cities—and again above all in Luanda. As a consequence, more than half the population is now living in the cities which, from the linguistic point of view, have become highly heterogeneous. This means, of course, that Portuguese as the overall national language of communication is by now of paramount importance, and that the role of the African languages is steadily decreasing among the urban population—a trend which is beginning to spread into rural areas as well.
The exact numbers of those fluent in Portuguese or who speak Portuguese as a first language are unknown, although a census is expected to be carried out in July–August 2013. Quite a number of voices demand the recognition of "Angolan Portuguese" as a specific variant, comparable to those spoken in Portugal or in Brazil. However, while there exists a certain number of idiomatic particularities in everyday Portuguese, as spoken by Angolans, it remains to be seen whether or not the Angolan government comes to the conclusion that these particularities constitute a configuration that justifies the claim to be a new language variant.
Religion.
There are about 1000 mostly Christian religious communities in Angola. While reliable statistics are nonexistent, estimates have it that more than half of the population are Catholics, while about a quarter adhere to the Protestant churches introduced during the colonial period: the Congregationalists mainly among the Ovimbundu of the Central Highlands and the coastal region to its West, the Methodists concentrating on the Kimbundu speaking strip from Luanda to Malanje, the Baptists almost exclusively among the Bakongo of the Northwest (now present in Luanda as well) and dispersed Adventists, Reformed and Lutherans. In Luanda and region there subsists a nucleus of the "syncretic" Tocoists and in the northwest a sprinkling of Kimbanguism can be found, spreading from the Congo/Zaïre. Since independence, hundreds of Pentecostal and similar communities have sprung up in the cities, where by now about 50% of the population is living; several of these communities/churches are of Brazilian origin.
The U.S. Department of State estimates the Muslim population at 80,000–90,000, while the Islamic Community of Angola puts the figure closer to 500,000. Muslims consist largely of migrants from West Africa and the Middle East (especially Lebanon), although some are local converts. The Angolan government does not legally recognize any Muslim organizations and often shuts down mosques or prevents their construction.
In a study assessing nations' levels of religious regulation and persecution with scores ranging from 0 to 10 where 0 represented low levels of regulation or persecution, Angola was scored 0.8 on Government Regulation of Religion, 4.0 on Social Regulation of Religion, 0 on Government Favoritism of Religion and 0 on Religious Persecution.
Foreign missionaries were very active prior to independence in 1975, although since the beginning of the anti-colonial fight in 1961 the Portuguese colonial authorities expelled a series of Protestant missionaries and closed mission stations based on the belief that the missionaries were inciting pro-independence sentiments. Missionaries have been able to return to the country since the early 1990s, although security conditions due to the civil war have prevented them until 2002 from restoring many of their former inland mission stations.
The Catholic Church and some major Protestant denominations mostly keep to themselves in contrast to the "New Churches" which actively proselytize. Catholics, as well as some major Protestant denominations, provide help for the poor in the form of crop seeds, farm animals, medical care and education.
Culture.
In Angola, there is a Culture Ministry that is managed by Culture Minister Rosa Maria Martins da Cruz e Silva. Portugal has been present in Angola for 400 years, occupied the territory in the 19th and early 20th century, and ruled over it for about 50 years. As a consequence, both countries share cultural aspects: language (Portuguese) and main religion (Roman Catholic Christianity). The "substrate" of Angolan culture is African, mostly Bantu, while Portuguese culture has been imported. The diverse ethnic communities – the Ovimbundu, Ambundu, Bakongo, Chokwe, Mbunda and other peoples – maintain to varying degrees their own cultural traits, traditions and languages, but in the cities, where slightly more than half of the population now lives, a mixed culture has been emerging since colonial times – in Luanda since its foundation in the 16th century. In this urban culture, the Portuguese heritage has become more and more dominant. An African influence is evident in music and dance, and is moulding the way in which Portuguese is spoken, but is almost disappearing from the vocabulary. This process is well reflected in contemporary Angolan literature, especially in the works of Pepetela and Ana Paula Ribeiro Tavares.
Leila Lopes, Miss Angola 2011, was crowned Miss Universe 2011 in Brazil on 12 September 2011 making her the first Angolan to win the pageant.
In 2014, Angola resume the National Festival of Angolan Culture (FENACULT), after a 25-years break. The festival took place in all the provincial capitals of the country between 30 August and 20 September and had as theme "Culture as a Factor of Peace and Development".
Health.
Epidemics of cholera, malaria, rabies and African hemorrhagic fevers like Marburg hemorrhagic fever, are common diseases in several parts of the country. Many regions in this country have high incidence rates of tuberculosis and high HIV prevalence rates. Dengue, filariasis, leishmaniasis, and onchocerciasis (river blindness) are other diseases carried by insects that also occur in the region. Angola has one of the highest infant mortality rates in the world and one of the world's lowest life expectancies. A 2007 survey concluded that low and deficient niacin status was common in Angola. Demographic and Health Surveys is currently conducting several surveys in Angola on malaria, domestic violence and more.
In September 2014, the Angolan Institute for Cancer Control (IACC) was created by presidential decree, and it will integrate the National Health Service in Angola. The purpose of this new center is to ensure the health and medical care in oncology, policy implementation, programs and plans for prevention and specialized treatment. This cancer institute will be assumed as a reference institution in the central and southern regions of Africa.
In 2014, Angola launched a national campaign of vaccination against measles, extended to every child under ten years old and aiming to go to all 18 provinces in the country. The measure is part of the Strategic Plan for the Elimination of Measles 2014–2020 created by the Angolan Ministry of Health which includes strengthening routine immunization, a proper dealing with measles cases, national campaigns, introducing a second dose of vaccination in the national routine vaccination calendar and active epidemiological surveillance for measles. This campaign took place together with the vaccination against polio and vitamin A supplementation.
Education.
Although by law education in Angola is compulsory and free for eight years, the government reports that a percentage of students are not attending due to a lack of school buildings and teachers. Students are often responsible for paying additional school-related expenses, including fees for books and supplies.
In 1999, the gross primary enrollment rate was 74 percent and in 1998, the most recent year for which data are available, the net primary enrollment rate was 61 percent. Gross and net enrollment ratios are based on the number of students formally registered in primary school and therefore do not necessarily reflect actual school attendance. There continue to be significant disparities in enrollment between rural and urban areas. In 1995, 71.2 percent of children ages 7 to 14 years were attending school. It is reported that higher percentages of boys attend school than girls. During the Angolan Civil War (1975–2002), nearly half of all schools were reportedly looted and destroyed, leading to current problems with overcrowding.
The Ministry of Education hired 20,000 new teachers in 2005 and continued to implement teacher trainings. Teachers tend to be underpaid, inadequately trained, and overworked (sometimes teaching two or three shifts a day). Some teachers may reportedly demand payment or bribes directly from their students. Other factors, such as the presence of landmines, lack of resources and identity papers, and poor health prevent children from regularly attending school. Although budgetary allocations for education were increased in 2004, the education system in Angola continues to be extremely under-funded.
According to estimates by the UNESCO Institute for Statistics, the adult literacy rate in 2011 was 70.4%. 82.9% of males and 54.2% of women are literate as of 2001. Since independence from Portugal in 1975, a number of Angolan students continued to be admitted every year at high schools, polytechnical institutes, and universities in Portugal, Brazil and Cuba through bilateral agreements; in general, these students belong to the elites.
In September 2014, the Angolan Ministry of Education announced an investment of 16 million Euros in the computerization of over 300 classrooms across the country. The project also includes training teachers at a national level, "as a way to introduce and use new information technologies in primary schools, thus reflecting an improvement in the quality of teaching."
In 2010, the Angolan government started building the Angolan Media Libraries Network, distributed throughout several provinces in the country to facilitate the people's access to information and knowledge. Each site has a bibliographic archive, multimedia resources and computers with Internet access, as well as areas for reading, researching and socializing. The plan envisages the establishment of one media library in each Angolan province by 2017. The project also includes the implementation of several media libraries, in order to provide the several contents available in the fixed media libraries to the most isolated populations in the country. At this time, the mobile media libraries are already operating in the provinces of Luanda, Malanje, Uíge, Cabinda and Lunda South. As for REMA, the provinces of Luanda, Benguela, Lubango and Soyo have currently working media libraries.
Sports.
Angola is the top basketball team of FIBA Africa, and a regular competitor at the Summer Olympic Games and the FIBA World Cup. The Angola national football team qualified for the 2006 FIFA World Cup, as this was their first appearance on the World Cup finals stage. They were eliminated after one defeat and two draws in the group stage. They won 3 COSAFA Cups and finished runner up in 2011 African Nations Championship. Angola has participated in the World Women's Handball Championship for several years. The country has also appeared in the Summer Olympics for seven years and both compete and have hosted the FIRS Roller Hockey World Cup. Angola is also often believed to have historic roots in the martial art "Capoeira Angola" and "Batuque" which were practiced by enslaved African Angolans transported as part of the Atlantic slave trade.

</doc>
<doc id="704" url="https://en.wikipedia.org/wiki?curid=704" title="Demographics of Angola">
Demographics of Angola

This article is about the demographic features of the population of Angola, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.
According to 2014 census data, Angola had a population of 24.3 million inhabitants in 2014.
Ethnically, there are three main groups, each speaking a Bantu language: the Ovimbundu who represent 37% of the population, the (Ambundu) with 25%, and the Bakongo 13%. Other numerically important groups include the closely interrelated Chokwe and Lunda, the Ganguela and Nyaneka-Khumbi (in both cases classification terms that stand for a variety of small groups), the Ovambo, the Herero, the Xindonga and scattered residual groups of Khoisan. In addition, mixed race (European and African) people amount to about 2%, with a small (1%) population of whites, mainly ethnically Portuguese.
As a former overseas territory of Portugal until 1975, Angola possesses a Portuguese population of over 200,000, a number that has been growing from 2000 onwards, because of Angola's growing demand for qualified human resources. Besides the Portuguese, significant numbers of people from other European and from diverse Latin American countries (especially Brazil) can be found. From the 2000s, many Chinese have settled and started up small businesses, while at least as many have come as workers for large enterprises (construction or other). Observers claim that the Chinese community in Angola might include as many as 300,000 persons at the end of 2010, but reliable statistics are not at this stage available. In 1974/75, over 25,000 Cuban soldiers arrived in Angola to help the MPLA forces during the decolonization conflict. Once this was over, a massive development cooperation in the field of health and education brought in numerous civil personnel from Cuba. However, only a very small percentage of all these people has remained in Angola, either for personal reasons (intermarriage) or as professionals (e.g., medical doctors).
The largest religious denomination is Roman Catholicism, to which adheres about half the population. Roughly 26% are followers of traditional forms of Protestantism (Congregationals, Methodists, Baptista, Lutherans, Reformed), but over the last decades there has in addition been a growth of Pentecostal communities and African Initiated Churches. In 2006, one out of 221 people were Jehovah's Witnesses. Blacks from Mali, Nigeria and Senegal are mostly Sunnite Muslims, but do not make up more than 1 - 2% of the population. By now few Angolans retain African traditional religions following different ethnic faiths.
Population.
According to the 2010 revison of the World Population Prospects the total population was 19 082 000 in 2010, compared to only 4 148 000 in 1950. The proportion of children below the age of 15 in 2010 was 46.6%, 50.9% was between 15 and 65 years of age, while 2.5% was 65 years or older
Structure of the population (DHS 2011) (Males 19 707, Females 20 356 = 40 063) :
Vital statistics.
Registration of vital events is in Angola not complete. The Population Department of the United Nations prepared the following estimates.
Fertility and Births.
Total Fertility Rate (TFR) and Crude Birth Rate (CBR):
CIA World Factbook demographic statistics.
The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.
Population.
Population growth.
The population is growing by 2.184% annually. There are 44.51 births and 24.81 deaths per 1,000 citizens. The net migration rate is 2.14 migrants per 1,000 citizens. The fertility rate of Angola is 5.97 children born per woman as of 2011. The infant mortality rate is 184.44 deaths for every 1,000 live births with 196.55 deaths for males and 171.72 deaths for females for every 1,000 live births. Life expectancy at birth is 37.63 years; 36.73 years for males and 38.57 years for females.
Health.
According to the CIA World Factbook, 2% of adults (aged 15–49) are living with HIV/AIDS (as of 2009). The risk of contracting disease is very high. There are food and waterborne diseases, bacterial and protozoal diarrhea, hepatitis A, and typhoid fever; vectorborne diseases, malaria, African trypanosomiasis (sleeping sickness); respiratory disease: meningococcal meningitis, and schistosomiasis, a water contact disease, as of 2005.
Ethnic groups.
Roughly 37% of Angolans are Ovimbundu, 25% are Ambundu, 13% are Bakongo, 2% are mestiço, 1-2% are white Africans, and people from other Afrucan ethnicities make up 22% of Angola's population.
Religions.
Angola is a majority Christian country. Reliable statistics don't exist, but well over 80% belong in principle to a Christian church or community, although many of them don't practice their religion and are in fact non believers. More than half of the Christians (whether practising or not) are Roman Catholic, the remaining ones comprising members of traditional Protestant churches as well as of new, often Pentecostal communities. Only 1 - 2% are Muslims - generally immigrants from other African countries. Traditional indigenous religions are practized by a very small minority, generally in peripheral rural societies; however, some traditional beliefs are held by a substantial number of Christians.
Education.
Literacy is quite low, with 67.4% of the population over the age of 15 able to read and write in Portuguese. 82.9% of males and 54.2% of women are literate as of 2001.
Languages.
Portuguese is the official language of Angola, but Bantu and other African languages are also widely spoken. In fact, Kikongo, Kimbundu, Umbundu, Tuchokwe, Nganguela, and Ukanyama have the official status of "national languages". The mastery of Portuguese is widespread; in the cities the overwhelming majority are either fluent in Portuguese or have at least a reasonable working knowledge of this language; an increasing minority are native Portuguese speakers and have a poor, if any, knowledge of an African language.

</doc>
<doc id="705" url="https://en.wikipedia.org/wiki?curid=705" title="Politics of Angola">
Politics of Angola

Since the adoption of a new constitution in 2010, the politics of Angola takes place in a framework of a presidential republic, whereby the President of Angola is both head of state and head of government, and of a multi-party system. Executive power is exercised by the government. Legislative power is vested in the President, the government and parliament.
Angola changed from a one-party Marxist-Leninist system ruled by the Popular Movement for the Liberation of Angola (MPLA), in place since independence in 1975, to a multiparty democracy based on a new constitution adopted in 1992. That same year the first parliamentary and presidential elections were held. The MPLA won an absolute majority in the parliamentary elections. In the presidential elections, President José Eduardo dos Santos won the first round election with more than 49% of the vote to Jonas Savimbi's 40%. A runoff election would have been necessary, but never took place. The renewal of civil war immediately after the elections, which were considered as fraudulent by UNITA, and the collapse of the Lusaka Protocol, created a split situation. To a certain degree the new democratic institutions worked, notably the National Assembly, with the active participation of UNITA's and the FNLA's elected MPs - while José Eduardo dos Santos continued to exercise his functions without democratic legitimation. However the armed forces of the MPLA (now the official armed forces of the Angolan state) and of UNITA fought each other until the leader of UNITA, Jonas Savimbi, was killed in action in 2002.
From 2002 to 2010, the system as defined by the constitution of 1992 functioned in a relatively normal way. The executive branch of the government was composed of the President, the Prime Minister and Council of Ministers. The Council of Ministers, composed of all ministers and vice ministers, met regularly to discuss policy issues. Governors of the 18 provinces were appointed by and served at the pleasure of the president. The Constitutional Law of 1992 established the broad outlines of government structure and the rights and duties of citizens. The legal system was based on Portuguese and customary law but was weak and fragmented. Courts operated in only 12 of more than 140 municipalities. A Supreme Court served as the appellate tribunal; a Constitutional Court with powers of judicial review was never constituted despite statutory authorization. In practice, power was more and more concentrated in the hands of the President who, supported by an ever increasing staff, largely controlled parliament, government, and the judiciary.
The 26-year-long civil war has ravaged the country's political and social institutions. The UN estimates of 1.8 million internally displaced persons (IDPs), while generally the accepted figure for war-affected people is 4 million. Daily conditions of life throughout the country and specifically Luanda (population approximately 6 million) mirror the collapse of administrative infrastructure as well as many social institutions. The ongoing grave economic situation largely prevents any government support for social institutions. Hospitals are without medicines or basic equipment, schools are without books, and public employees often lack the basic supplies for their day-to-day work.
Executive branch.
The 2010 constitution grants the President almost absolute power. Elections for the National assembly are to take place every five years, and the President is automatically the leader of the winning party or coalition. It is for the President to appoint (and dismiss) all of the following:
The President is also provided a variety of powers, like defining the policy of the country. Even though it's not up to him/her to make laws (only to promulgate them and make edicts), the President is the leader of the winning party.
The only "relevant" post that is not directly appointed by the President is the Vice-President, which is the second in the winning party.
Legislative branch.
The National Assembly ("Assembleia Nacional") has 223 members, elected for a four-year term, 130 members by proportional representation, 90 members in provincial districts, and 3 members to represent Angolans abroad. The next general elections, due for 1997, have been rescheduled for 5 September 2008. The ruling party MPLA won 82% (191 seats in the National Assembly) and the main opposition party won only 10% (16 seats). The elections however have been described as only partly free but certainly not fair. A White Book on the elections in 2008 lists up all irregularities surrounding the Parliamentary elections of 2008.
Judicial branch.
Supreme Court (or "Tribunal da Relacao") judges of the Supreme Court are appointed by the president. The Constitutional Court, with the power of judicial review, contains 11 justices. Four are appointed by the President, four by the National Assembly, two by the Superior Council of the Judiciary, and one elected by the public.
Administrative divisions.
Angola has eighteen provinces (provincias, singular - provincia); Bengo, Benguela, Bie, Cabinda, Cuando Cubango, Cuanza Norte, Cuanza Sul, Cunene, Huambo, Huila, Luanda, Lunda Norte, Lunda Sul, Malanje, Moxico, Namibe, Uige, Zaire
Political pressure groups and leaders.
Front for the Liberation of the Enclave of Cabinda or FLEC (Henrique N'zita Tiago; António Bento Bembe)
International organization participation.
African, Caribbean and Pacific Group of States, AfDB, CEEAC, United Nations Economic Commission for Africa, FAO, Group of 77, IAEA, IBRD, ICAO, International Criminal Court (signatory), ICFTU, International Red Cross and Red Crescent Movement, International Development Association, IFAD, IFC, IFRCS, International Labour Organization, International Monetary Fund, International Maritime Organization, Interpol, IOC, International Organization for Migration, ISO (correspondent), ITU, Non-Aligned Council (temporary), UNCTAD, UNESCO, UNIDO, UPU, World Customs Organization, World Federation of Trade Unions, WHO, WIPO, WMO, WToO, WTrO

</doc>
<doc id="706" url="https://en.wikipedia.org/wiki?curid=706" title="Economy of Angola">
Economy of Angola

The Economy of Angola is one of the fastest-growing economies in the world, with reported annual average GDP growth of 11.1 percent for the period from 2001 to 2010. It is still recovering from the Angolan Civil War that plagued the country from independence in 1975 until 2002. Despite extensive oil and gas resources, diamonds, hydroelectric potential, and rich agricultural land, Angola remains poor, and a third of the population relies on subsistence agriculture. Since 2002, when the 27-year civil war ended, the nation has worked to repair and improve ravaged infrastructure and weakened political and social institutions. High international oil prices and rising oil production have contributed to the very strong economic growth since 1998, but corruption and public-sector mismanagement remain, particularly in the oil sector, which accounts for over 50 percent of GDP, over 90 percent of export revenue, and over 80 percent of government revenue.
History.
The Portuguese explorer Diogo Cão reached the Angolan coast in 1484, after which Portugal began to found trading posts and forts along the shore. Paulo Dias de Novais founded Sāo Paulo de Loanda (Luanda) in 1575. São Felipe de Benguella (Benguela) followed in 1587.
The principal early trade was in slaves. Portuguese merchants purchased the slaves from the local Imbangala and Mbundu peoples, notable slave hunters, and sold them to the sugarcane plantations in Brazil. Brazilian ships were frequent visitors to Luanda and Benguela and Angola functioned as a kind of colony of Brazil, with Brazilian Jesuits active in its religious and educational centers.
The Portuguese Empire was neglected during the period of the Iberian Union, which lasted from 1580 to 1640. The Dutch, bitter enemies of their former masters in Spain, invaded many Portuguese overseas possessions. During Portugal's separatist war against Spain, the Dutch occupied Luanda as "Fort Aardenburgh" from 1640 to 1648. They used the territory to supply their own slaves to the sugarcane plantations of Northeastern Brazil (Pernambuco, Olinda, Recife), which they had also seized from Portugal. John Maurice, Prince of Nassau-Siegen, conquered the Portuguese possessions of Saint George del Mina, Saint Thomas, and Luanda, Angola, on the west coast of Africa. Portugal recovered the territory from 1648 to 1650.
In the high plains, the Planalto, the most important native states were Bié and Bailundo, the latter being noted for its production of foodstuffs and rubber. Portugal expanded into their territory, but did not control much of the interior prior to the late 19th century.
The Portuguese started to develop townships, trading posts, logging camps and small processing factories. From 1764 onwards, there was a gradual change from a slave-based society to one based on production for domestic consumption and export. Following the independence of Brazil in 1822, the slave trade was formally abolished in 1836 (although it continued locally into the 20th century). In 1844, Angola's ports were opened to foreign shipping. By 1850, Luanda was one of the greatest and most developed Portuguese cities in the vast Portuguese Empire outside Mainland Portugal, full of trading companies, exporting peanut oil, copal, timber, and cocoa. The principal exports of the post-slave economy in the 19th century were rubber, beeswax, and ivory. Maize, tobacco, dried meat and cassava flour also began to be produced locally. Prior to the First World War, exportation of coffee, palm kernels and oil, cattle, leather and hides, and salt fish joined the principal exports, with small quantities of gold and cotton also being produced. Grains, sugar, and rum were also produced for local consumption. The principal imports were foodstuffs, cotton goods, hardware, and British coal. Legislation against foreign traders was implemented in the 1890s. The territory's prosperity, however, continued to depend on plantations worked by labor "indentured" from the interior.
From the 1920s to the 1960s, strong economic growth, abundant natural resources and development of infrastructure, led to the arrival of even more Portuguese settlers. Petroleum was known to exist from the mid-19th century, but modern exploitation began in 1955. Production began in the Cuanza basin in the 1950s, in the Congo basin in the 1960s, and in the exclave of Cabinda in 1968. The Portuguese government granted operating rights for Block Zero to the Cabinda Gulf Oil Company, a subsidiary of ChevronTexaco, in 1955. Oil production surpassed the exportation of coffee as Angola's largest export in 1973.
A leftist military-led coup d'état, started on April 25, 1974, in Lisbon, overthrew the Marcelo Caetano government in Portugal, and promised to hand over power to an independent Angolan government. Mobutu Sese Seko, the President of Zaire, met with António de Spínola, the transitional President of Portugal, on September 15, 1974 on Sal island in Cape Verde, crafting a plan to empower Holden Roberto of the National Liberation Front of Angola, Jonas Savimbi of UNITA, and Daniel Chipenda of the MPLA's eastern faction at the expense of MPLA leader Agostinho Neto while retaining the façade of national unity. Mobutu and Spínola wanted to present Chipenda as the MPLA head, Mobutu particularly preferring Chipenda over Neto because Chipenda supported autonomy for Cabinda. The Angolan exclave has immense petroleum reserves estimated at around 300 million tons (~300 kg) which Zaire, and thus the Mobutu government, depended on for economic survival. After independence thousands of white Portuguese left, most of them to Portugal and many travelling overland to South Africa. There was an immediate crisis because the indigenous African population lacked the skills and knowledge needed to run the country and maintain its well-developed infrastructure.
The Angolan government created Sonangol, a state-run oil company, in 1976. Two years later Sonangol received the rights to oil exploration and production in all of Angola. After independence from Portugal in 1975, Angola was ravaged by a horrific civil war between 1975 and 2002.
1990s.
United Nations Angola Verification Mission III and MONUA spent USD1.5 billion overseeing implementation of the Lusaka Protocol, a 1994 peace accord that ultimately failed to end the civil war. The protocol prohibited UNITA from buying foreign arms, a provision the United Nations largely did not enforce, so both sides continued to build up their stockpile. UNITA purchased weapons in 1996 and 1997 from private sources in Albania and Bulgaria, and from Zaire, South Africa, Republic of the Congo, Zambia, Togo, and Burkina Faso. In October 1997 the UN imposed travel sanctions on UNITA leaders, but the UN waited until July 1998 to limit UNITA's exportation of diamonds and freeze UNITA bank accounts. While the U.S. government gave USD250 million to UNITA between 1986 to 1991, UNITA made USD1.72 billion between 1994 and 1999 exporting diamonds, primarily through Zaire to Europe. At the same time the Angolan government received large amounts of weapons from the governments of Belarus, Brazil, Bulgaria, China, and South Africa. While no arms shipment to the government violated the protocol, no country informed the U.N. Register on Conventional Weapons as required.
Despite the increase in civil warfare in late 1998, the economy grew by an estimated 4% in 1999. The government introduced new currency denominations in 1999, including a 1 and 5 kwanza note.
2000s.
An economic reform effort was launched in 1998. Angola ranked 160 out of 174 nations in the United Nations Human Development Index of 2000. In April 2000 Angola started an International Monetary Fund (IMF) Staff-Monitored Program (SMP). The program formally lapsed in June 2001, but the IMF remains engaged. In this context the Government of Angola has succeeded in unifying exchange rates and has raised fuel, electricity, and water rates. The Commercial Code, telecommunications law, and Foreign Investment Code are being modernized. A privatization effort, prepared with World Bank assistance, has begun with the BCI bank. Nevertheless, a legacy of fiscal mismanagement and corruption persists. The civil war internally displaced 3.8 million people, 32% of the population, by 2001. The security brought about by the 2002 peace settlement has led to the resettlement of 4 million displaced persons, thus resulting in large-scale increases in agriculture production.
Angola produced over of diamonds per year in 2003, and production was expected to grow to per year by 2007. In 2004 China's Eximbank approved a $2 billion line of credit to Angola to rebuild infrastructure. The economy grew 18% in 2005 and growth was expected to reach 26% in 2006 and stay above 10% for the rest of the decade.
The construction industry is another sector taking advantage of the growing economy, with various housing projects stimulated by the government that created various initiatives for this. Examples are the program "Angola Investe" and the projects "Casa Feliz" or "Meña". However, not all public construction projects are functional; a case in point is Kilamba Kiaxi where a whole new satellite town of Luanda, consisting in the main of housing facilities for several hundreds of thousands of people, was completely inhibited for over four years because of the skyrocket prices but it completely sold out after the government decreased the original price and created mortgage plans at around the election time thus made it affordable for middle-class people. 
ChevronTexaco started pumping from Block 14 in January 2000, but production has decreased to in 2007 due to the poor quality of the oil. Angola joined the Organization of the Petroleum Exporting Countries on January 1, 2007.
Cabinda Gulf Oil Company found Malange-1, an oil reservoir in Block 14, on August 9, 2007.
Overview.
Despite its abundant natural resources, output per capita is among the world's lowest. Subsistence agriculture provides the main livelihood for 85% of the population. Oil production and the supporting activities are vital to the economy, contributing about 45% to GDP and 90% of exports. Growth is almost entirely driven by rising oil production which surpassed in late-2005 and which is expected to grow to by 2007. Control of the oil industry is consolidated in Sonangol Group, a conglomerate which is owned by the Angolan government. With revenues booming from oil exports, the government has started to implement ambitious development programs in building roads and other basic infrastructure for the nation.
In the last decade of the colonial period, Angola was a major African food exporter but now imports almost all its food. Because of severe wartime conditions, including extensive planting of landmines throughout the countryside, agricultural activities have been brought to a near standstill. Some efforts to recover have gone forward, however, notably in fisheries. Coffee production, though a fraction of its pre-1975 level, is sufficient for domestic needs and some exports. In sharp contrast to a bleak picture of devastation and bare subsistence is expanding oil production, now almost half of GDP and 90% of exports, at . Diamonds provided much of the revenue for Jonas Savimbi's UNITA rebellion through illicit trade. Other rich resources await development: gold, forest products, fisheries, iron ore, coffee, and fruits.
This is a chart of trend of nominal gross domestic product of Angola at market prices using International Monetary Fund data; figures are in millions of units.
Foreign trade.
Exports in 2004 reached US$10,530,764,911. The vast majority of Angola's exports, 92% in 2004, are petroleum products. US$785 million worth of diamonds, 7.5% of exports, were sold abroad that year. Nearly all of Angola's oil goes to the United States, in 2006, making it the eighth largest supplier of oil to the United States, and to China, in 2006. In the first quarter of 2008, Angola became the main exporter of oil to China. The rest of its petroleum exports go to Europe and Latin America. U.S. companies account for more than half the investment in Angola, with Chevron-Texaco leading the way. The U.S. exports industrial goods and services, primarily oilfield equipment, mining equipment, chemicals, aircraft, and food, to Angola, while principally importing petroleum. Trade between Angola and South Africa exceeded USD 300 million in 2007. From the 2000s many Chinese have settled and started up businesses.
Resources.
Petroleum.
Angola produces and exports more petroleum than any other nation in sub-Saharan Africa, surpassing Nigeria in the 2000s. In January 2007 Angola became a member of OPEC. By 2010 production is expected to double the 2006 output level with development of deep-water offshore oil fields. Oil sales generated USD 1.71 billion in tax revenue in 2004 and now makes up 80% of the government's budget, a 5% increase from 2003, and 45% of GDP.
Chevron Corporation produces and receives , 27% of Angolan oil. Total S.A., Chevron Corporation, ExxonMobil, Eni, Petrobras, and BP also operate in the country.
Block Zero provides the majority of Angola's crude oil production with produced annually. The largest fields in Block Zero are Takula (Area A), Numbi (Area A), and Kokongo (Area B). Chevron operates in Block Zero with a 39.2% share. SONANGOL, the state oil company, Total, and Eni own the rest of the block. Chevron also operates Angola's first producing deepwater section, Block 14, with .
The United Nations has criticized the Angolan government for using torture, rape, summary executions, arbitrary detention, and disappearances, actions which Angolan government has justified on the need to maintain oil output.
Angola is the third-largest trading partner of the United States in Sub-Saharan Africa, largely because of its petroleum exports. The U.S. imports 7% of its oil from Angola, about three times as much as it imported from Kuwait just prior to the Gulf War in 1991. The U.S. Government has invested USD $4 billion in Angola's petroleum sector.
Diamonds.
Angola is the third largest producer of diamonds in Africa and has only explored 40% of the diamond-rich territory within the country, but has had difficulty in attracting foreign investment because of corruption, human rights violations, and diamond smuggling. Production rose by 30% in 2006 and Endiama, the national diamond company of Angola, expects production to increase by 8% in 2007 to 10 million carats annually. The government is trying to attract foreign companies to the provinces of Bié, Malanje and Uíge.
The Angolan government loses $375 million annually from diamond smuggling. In 2003 the government began Operation Brilliant, an anti-smuggling investigation that arrested and deported 250,000 smugglers between 2003 and 2006. Rafael Marques, a journalist and human rights activist, described the diamond industry in his 2006 "Angola's Deadly Diamonds" report as plagued by "murders, beatings, arbitrary detentions and other human rights violations." Marques called on foreign countries to boycott Angola's "conflict diamonds".
In December 2014, the Bureau of International Labor Affairs issued a "List of Goods Produced by Child Labor or Forced Labor" that classified Angola as one of the major diamond producing African countries relying on both child labor and forced labor. The U.S. Department of Labor reported that "there is little publicly available information on ngola' efforts to enforce child labor law".
Iron.
Under Portuguese rule, Angola began mining iron in 1957, producing 1.2 million tons in 1967 and 6.2 million tons by 1971. In the early 1970s, 70% of Portuguese Angola's iron exports went to Western Europe and Japan. After independence in 1975, the Angolan Civil War (1975–2002) destroyed most of the territory's mining infrastructure. The redevelopment of the Angolan mining industry started in the late 2000s.

</doc>
<doc id="708" url="https://en.wikipedia.org/wiki?curid=708" title="Transport in Angola">
Transport in Angola

Transport in Angola comprises:
Railways.
There are three separate railway lines in Angola:
Reconstruction of these three lines began in 2005 and is expected to be completed by the end of the year 2012. The Benguela Railway already connects to the Democratic Republic of the Congo.
In April 2012, the Zambian Development Agency (ZDA) and an Angolan company signed a memorandum of understanding (MoU) to build a multi-product pipeline from Lobito to Lusaka, Zambia, to deliver various refined products to Zambia.
Pipelines.
Angola plans to build an oil refinery in Lobito in the coming years.
Ports and harbors.
The government plans to build a deep-water port at Barra do Dande, north of Luanda, in Bengo province near Caxito.
Airports.
History.
Angola had an estimated total of 43 airports as of 2004, of which 31 had paved runways as of 2005. There is an international airport at Luanda. International and domestic services are maintained by TAAG Angola Airlines, Aeroflot, British Airways, Brussels Airlines, Lufthansa, Air France, Air Namibia, Cubana, Ethiopian Airlines, Emirates, Delta Air Lines, Royal Air Maroc, Iberia, Hainan Airlines, Kenya Airways, South African Airways, TAP Air Portugal and several regional carriers. In 2003, domestic and international carriers carried 198,000 passengers. There are airstrips for domestic transport at Benguela, Cabinda, Huambo, Namibe, and Catumbela.
References.
"This article comes from the CIA World Factbook 2003."

</doc>
<doc id="709" url="https://en.wikipedia.org/wiki?curid=709" title="Angolan Armed Forces">
Angolan Armed Forces

The Angolan Armed Forces (Portuguese: "Forças Armadas Angolanas") are the military in Angola that succeeded the Armed Forces for the Liberation of Angola (FAPLA) following the abortive Bicesse Accord with the National Union for the Total Independence of Angola (UNITA) in 1991. As part of the peace agreement, troops from both armies were to be demilitarized and then integrated. Integration was never completed as UNITA went back to war in 1992. Later, consequences for UNITA members in Luanda were harsh with FAPLA veterans persecuting their erstwhile opponents in certain areas and reports of vigilantism.
The FAA is headed by Chief of Staff Geraldo Sachipengo Nunda since 2010, who reports to the Minister of Defense, currently João Lourenço.
There are three components, the Army ("Forças Armadas"), Navy ("Marinha de Guerra") and Air Force "Força Aérea Nacional Angolana". Reported total manpower in 2013 was about 107,000.
Angolan Army.
On August 1, 1974 a few months after a military coup d'état had overthrown the Lisbon regime and proclaimed its intention of granting independence to Angola, the MPLA announced the formation of FAPLA, which replaced the EPLA. By 1976 FAPLA had been transformed from lightly armed guerrilla units into a national army capable of sustained field operations.
In 1990-91, the Army had ten military regions and an estimated 73+ 'brigades', each with a mean strength of 1,000 and comprising inf, tank, APC, artillery, and AA units as required. The Library of Congress said in 1990 that 'he regular army's 91,500 troops were organized into more than seventy brigades ranging from 750 to 1,200 men each and deployed throughout the ten military regions. Most regions were commanded by lieutenant colonels, with majors as deputy commanders, but some regions were commanded by majors. Each region consisted of one to four provinces, with one or more infantry brigades assigned to it. The brigades were generally dispersed in battalion or smaller unit formations to protect strategic terrain, urban centers, settlements, and critical infrastructure such as bridges and factories. Counterintelligence agents were assigned to all field units to thwart UNITA infiltration. The army's diverse combat capabilities were indicated by its many regular and motorised infantry brigades with organic or attached armor, artillery, and air defense units; two militia infantry brigades; four antiaircraft artillery brigades; ten tank battalions; and six artillery battalions. These forces were concentrated most heavily in places of strategic importance and recurring conflict: the oil-producing Cabinda Province, the area around the capital, and the southern provinces where UNITA and South African forces operated.'
It was reported in 2011 that the army was by far the largest of the services with about 120,000 men and women. The Angolan Army has around 29,000 "ghost workers" who remain enrolled in the ranks of the FAA and therefore receive a salary.
In 2013, the International Institute for Strategic Studies reported that the FAA had six divisions, the 1st, 5th, and 6th with two or three infantry brigades, and the 2nd, 3rd, and 4th with five to six infantry brigades. The 4th Division included a tank regiment. A separate tank brigade and special forces brigade were also reported.
As of 2011, the IISS reported the ground forces had 42 armoured/infantry regiments ('detachments/groups - strength varies') and 16 infantry 'brigades'. These probably comprised infantry, tanks, APC, artillery, and AA units as required. Major equipment included over 140 main battle tanks, 600 reconnaissance vehicles, over 920 AFVs, infantry fighting vehicles, 298 howitzers.
It was reported on May 3, 2007, that the Special Forces Brigade of the Angolan Armed Forces (FAA) located at Cabo Ledo region, northern Bengo Province, would host a 29th anniversary celebration for the entire armed forces. The brigade was reportedly formed on 5 May 1978 and under the command at the time of Colonel Paulo Falcao.
Army Equipment.
The Army operates a large amount of Russian, Soviet and ex-Warsaw pact hardware. A large amount of its equipment was acquired in the 1980s and 1990s most likely because of hostilities with neighbouring countries and its civil war which lasted from November 1975 until 2002. There is an interest from the Angolan Army for the Brazilian ASTROS II multiple rocket launcher.
Infantry Weapons.
Many of Angola's weapons are of Portuguese colonial and Warsaw Pact origin. Jane's Information Group lists the following as in service:
Angolan Air Force.
The Angolan Air Force's personnel total about 8,000; its equipment includes transport aircraft and six Russian-manufactured Sukhoi Su-27 fighter aircraft. In 2002 one was lost during the civil war with UNITA forces.
In 1991, the Air Force/Air Defense Forces had 8,000 personnel and 90 combat-capable aircraft, including 22 fighters, 59 fighter ground attack aircraft and 16 attack helicopters.
Angolan Navy (Marinha de Guerra).
The Navy numbers about 1,000 personnel and operates only a handful of small patrol craft and barges.
The Angolan Navy (MGA) has been neglected and ignored as a military arm mainly due to the guerrilla struggle against the Portuguese and the nature of the civil war. From the early 1990s to the present the Angolan Navy has shrunk from around 4,200 personnel to around 1,000, resulting in the loss of skills and expertise needed to maintain equipment. In order to protect Angola’s 1 600 km long coastline, the Angolan Navy is undergoing modernisation but is still lacking in many ways. Portugal has been providing training through its Technical Military Cooperation (CTM) programme. The Navy is requesting procurement of a frigate, three corvettes, three offshore patrol vessel and additional fast patrol boats.
Most of the vessels in the navy's inventory dates back from the 1980s or earlier, and many of its ships are inoperable due to age and lack of maintenance. However the navy acquired new boats from Spain and France in the 1990s. Germany has delivered several Fast Attack Craft for border protection in 2011.
In September 2014 it was reported that the Angolan Navy would acquire seven Macaé-class patrol vessels from Brazil as part of a Technical Memorandum of Understanding (MoU) covering the production of the vessels as part of Angola’s Naval Power Development Programme (Pronaval). The military of Angola aims to modernize its naval capability, presumably due to a rise in maritime piracy within the Gulf of Guinea which may have an adverse effect on the country's economy.
The navy's current known inventory includes the following:
The navy also has several aircraft for maritime patrol:
Foreign deployments.
The FAPLA's main counterinsurgency effort was directed against UNITA in the southeast, and its conventional capabilities were demonstrated principally in the undeclared South African Border War. The FAPLA first performed its external assistance mission with the dispatch of 1,000 to 1,500 troops to São Tomé and Príncipe in 1977 to bolster the socialist regime of President Manuel Pinto da Costa. During the next several years, Angolan forces conducted joint exercises with their counterparts and exchanged technical operational visits. The Angolan expeditionary force was reduced to about 500 in early 1985.
The Angolan Armed Forces were controversially involved in training the armed forces of fellow Lusophone states Cape Verde and Guinea-Bissau. In the case of the latter, the 2012 Guinea-Bissau coup d'état was cited by the coup leaders as due to Angola's involvement in trying to "reform" the military in connivance with the civilian leadership.
A small number of FAA personnel are stationed in the Democratic Republic of the Congo (Kinshasa) and the Republic of the Congo (Brazzaville). A presence during the unrest in Ivory Coast, 2010–2011, were not officially confirmed. However, the "Frankfurter Allgemeine Zeitung", citing "Jeune Afrique", said that among President Gbagbo's guards were 92 personnel of President Dos Santos's Presidential Guard Unit. Angola is basically interested in the participation of the FAA operations of the African Union and has formed special units for this purpose.
David Birmingham, African Affairs, Vol. 77, No. 309 (Oct., 1978), pp. 554–564
Further reading.
Published by: Oxford University Press on behalf of The Royal African Society

</doc>
<doc id="710" url="https://en.wikipedia.org/wiki?curid=710" title="Foreign relations of Angola">
Foreign relations of Angola

The foreign relations of Angola are based on Angola's strong support of U.S. foreign policy as the Angolan economy is dependent on U.S. foreign aid.
From 1975 to 1989, Angola was aligned with the Eastern bloc, in particular the Soviet Union, Libya, and Cuba. Since then, it has focused on improving relationships with Western countries, cultivating links with other Portuguese-speaking countries, and asserting its own national interests in Central Africa through military and diplomatic intervention. In 1993, it established formal diplomatic relations with the United States. It has entered the Southern African Development Community as a vehicle for improving ties with its largely Anglophone neighbors to the south. Zimbabwe and Namibia joined Angola in its military intervention in the Democratic Republic of the Congo, where Angolan troops remain in support of the Joseph Kabila government. It also has intervened in the Republic of the Congo (Brazzaville) to support the existing government in that country.
Since 1998, Angola has successfully worked with the United Nations Security Council to impose and carry out sanctions on UNITA. More recently, it has extended those efforts to controls on conflict diamonds, the primary source of revenue for UNITA. At the same time, Angola has promoted the revival of the Community of Portuguese-Speaking Countries (CPLP) as a forum for cultural exchange and expanding ties with Portugal (its former ruler) and Brazil (which shares many cultural affinities with Angola) in particular.
Sub-Saharan Africa.
Cape Verde.
Cape Verde signed a friendship accord with Angola in December 1975, shortly after Angola gained its independence. Cape Verde and Guinea-Bissau served as stop-over points for Cuban troops on their way to Angola to fight UNITA rebels and South African troops. Prime Minister Pedro Pires sent FARP soldiers to Angola where they served as the personal bodyguards of Angolan President José Eduardo dos Santos.
Democratic Republic of the Congo.
Many thousands of Angolans fled the country after the civil war. More than 20,000 people were forced to leave the Democratic Republic of the Congo in 2009, an action the DR Congo said was in retaliation for regular expulsion of Congolese diamond miners who were in Angola illegally. Angola sent a delegation to DR Congo's capital Kinshasa and succeeded in stopping government-forced expulsions which had become a "tit-for-tat" immigration dispute. "Congo and Angola have agreed to suspend expulsions from both sides of the border," said Lambert Mende, DR Congo information minister, in October 2009. "We never challenged the expulsions themselves; we challenged the way they were being conducted — all the beating of people and looting their goods, even sometimes their clothes," Mende said.
Guinea-Bissau.
Following a request by the government of Guinea-Bissau, Angola sent there a contingent of about 300 troops meant to help putting an end to the political-military unrest in that country, and to reorganize the local military forces. In fact, these troops were perceived as a kind of Pretorian Guard for the ruling party, PAIGC. In the beginning of April 2012, when a new military Coup d'état was under preparation, the Angolan regime decided to withdraw its military mission from Guinea-Bissau.
Namibia.
Namibia borders Angola to the south. In 1999 Namibia signed a mutual defense pact with its northern neighbor Angola.
This affected the Angolan Civil War that had been ongoing since Angola's independence in 1975. Namibia's ruling party SWAPO sought to support the ruling party MPLA in Angola against the rebel movement UNITA, whose stronghold is in southern Angola, bordering to Namibia. The defence pact allowed Angolan troops to use Namibian territory when attacking Jonas Savimbi's UNITA.
Nigeria.
Angolan-Nigerian relations are primarily based on their roles as oil exporting nations. Both are members of the Organization of the Petroleum Exporting Countries, the African Union and other multilateral organizations.
South Africa.
Angola-South Africa relations are quite strong as the ruling parties in both nations, the African National Congress in South Africa and the MPLA in Angola, fought together during the Angolan Civil War and South African Border War. They fought against UNITA rebels, based in Angola, and the apartheid-era government in South Africa who supported them. Nelson Mandela mediated between the MPLA and UNITA factions during the last years of Angola's civil war.
Zimbabwe.
Angola-Zimbabwe relations have remained cordial since the birth of both states, Angola in 1975 and Zimbabwe in 1979, during the Cold War. While Angola's foreign policy shifted to a pro-U.S. stance based on substantial economic ties, under the rule of President Robert Mugabe Zimbabwe's ties with the West soured in the late 1990s.
Europe.
France.
Relations between the two countries have not always been cordial due to the former French government's policy of supporting militant separatists in Angola's Cabinda province and the international Angolagate scandal embarrassed both governments by exposing corruption and illicit arms deals. Following French President Nicolas Sarkozy's visit in 2008, relations have improved.
Portugal.
Angola-Portugal relations have significantly improved since the Angolan government abandoned communism and nominally embraced democracy in 1991, embracing a pro-U.S. and to a lesser degree pro-Europe foreign policy. Portugal ruled Angola for 400 years, colonizing the territory from 1483 until independence in 1975. Angola's war for independence did not end in a military victory for either side, but was suspended as a result of a coup in Portugal that replaced the Caetano regime.
Russia.
Russia has an embassy in Luanda. Angola has an embassy in Moscow and an honorary consulate in Saint Petersburg. Angola and the precursor to Russia, the Soviet Union, established relations upon Angola's independence.
The Defence Minister of Serbia, Dragan Šutanovac, stated in a 2011 meeting in Luanda that Serbia would negotiate with the Angolan military authorities for the construction of a new military hospital in Angola.
Americas.
Brazil.
Commercial and economic ties dominate the relations of each country. Parts of both countries were part of the Portuguese Empire from the early 16th century until Brazil's independence in 1822. As of November 2007, "trade between the two countries is booming as never before"
Cuba.
During Angola's civil war Cuban forces fought to install a Marxist–Leninist MPLA-PT government, against Western-backed UNITA and FLNA guerrillas and the South-African army.
Mexico.
Relations between Angola and Mexico have become of increasing priority due to the cultural similarities between the two nations. 
United States.
From the mid-1980s through at least 1992, the United States was the primary source of military and other support for the UNITA rebel movement, which was led from its creation through 2002 by Jonas Savimbi. The U.S. refused to recognize Angola diplomatically during this period.
Relations between the United States of America and the Republic of Angola (formerly the People's Republic of Angola) have warmed since Angola's ideological renunciation of Marxism before the 1992 elections.
Asia.
China.
Chinese Prime Minister Wen Jiabao visited Angola in June 2006, offering a US$9 billion loan for infrastructure improvements in return for petroleum. The PRC has invested heavily in Angola since the end of the civil war in 2002. João Manuel Bernardo, the current ambassador of Angola to China, visited the PRC in November 2007.
In February 2006, Angola surpassed Saudi Arabia to become the number one supplier of oil to China. 
Israel.
Angola-Israel relations, primarily based on trade and pro-United States foreign policies, are excellent. In March 2006, the trade volume between the two countries amounted to $400 million. The Israeli ambassador to Angola is Avraham Benjamin. In 2005, President José Eduardo dos Santos visited Israel.
Japan.
As of 2007, economic relations played "a fundamental role in the bilateral relations between the two governments". Japan has donated towards demining following the civil war.
Pakistan.
The Government of Angola called for the support of Pakistan for the candidature of Angola to the seat of non-permanent member of the UN Security Council, whose election is set for September this year, during the 69th session of the General Assembly of United Nations. On the fringes of the ceremony, the Angolan diplomat also met with officials in charge of the economic and commercial policy of Pakistan, to assess the business opportunities between the two states. It asked to discuss aspects related to the cooperation on several domains of common interest.
South Korea.
Establishment of diplomatic relations 6 January 1992. The number of South Koreans living in Angola in 2011 was 279.
Vietnam.
Angola-Vietnam relations were established in August 1971, four years before Angola gained its independence, when future President of Angola Agostinho Neto visited Vietnam. Angola and Vietnam have steadfast partners as both transitioned from Cold War-era foreign policies of international communism to pro-Western pragmatism following the fall of the Soviet Union.

</doc>
<doc id="711" url="https://en.wikipedia.org/wiki?curid=711" title="Albert Sidney Johnston">
Albert Sidney Johnston

Albert Sidney Johnston (February 2, 1803 – April 6, 1862) served as a general in three different armies: the Texian ("i.e.", Republic of Texas) Army, the United States Army, and the Confederate States Army. He saw extensive combat during his military career, fighting actions in the Texas War of Independence, the Mexican–American War, the Utah War, and the American Civil War.
Considered by Confederate President Jefferson Davis to be the finest general officer in the Confederacy before the emergence of Robert E. Lee, he was killed early in the Civil War at the Battle of Shiloh. Johnston was the highest-ranking officer, Union or Confederate, killed during the entire war. Davis believed the loss of Johnston "was the turning point of our fate".
Johnston was unrelated to Confederate general Joseph E. Johnston.
Early life and education.
Johnston was born in Washington, Kentucky, the youngest son of Dr. John and Abigail (Harris) Johnston. His father was a native of Salisbury, Connecticut. Although Albert Johnston was born in Kentucky, he lived much of his life in Texas, which he considered his home. He was first educated at Transylvania University in Lexington, where he met fellow student Jefferson Davis. Both were appointed to the United States Military Academy, Davis two years behind Johnston. In 1826 Johnston graduated eighth of 41 cadets in his class from West Point with a commission as a brevet second lieutenant in the 2nd U.S. Infantry.
Johnston was assigned to posts in New York and Missouri and served in the Black Hawk War in 1832 as chief of staff to Bvt. Brig. Gen. Henry Atkinson.
Marriage and family.
In 1829 he married Henrietta Preston, sister of Kentucky politician and future Civil War general William Preston. They had one son, William Preston Johnston, who became a colonel in the Confederate Army. The senior Johnston resigned his commission in 1834 in order to care for his dying wife in Kentucky, who succumbed two years later to tuberculosis.
After serving as Secretary of War for the Republic of Texas from 1838 to 1840, Johnston resigned and returned to Kentucky. In 1843, he married Eliza Griffin, his late wife's first cousin. The couple moved to Texas, where they settled on a large plantation in Brazoria County. Johnston named the property China Grove. Here they raised Johnston's two children from his first marriage and the first three children born to Eliza and him. (A sixth child was born later when they lived in Los Angeles, California).
Texas Army.
In April 1834, Johnston took up farming in Texas. In 1836, he enlisted as a private in the Texas Army during the Texas War of Independence against the Republic of Mexico. One month later, Johnston was promoted to major and the position of aide-de-camp to General Sam Houston. He was named Adjutant General as a colonel in the Republic of Texas Army on August 5, 1836. On January 31, 1837, he became senior brigadier general in command of the Texas Army.
On February 7, 1837, he fought in a duel with Texas Brig. Gen. Felix Huston, as they challenged each other for the command of the Texas Army; Johnston refused to fire on Huston and lost the position after he was wounded in the pelvis.
On December 22, 1838, Mirabeau B. Lamar, the second president of the Republic of Texas, appointed Johnston as Secretary of War. He provided for the defense of the Texas border against Mexican invasion, and in 1839 conducted a campaign against Indians in northern Texas. In February 1840, he resigned and returned to Kentucky.
U.S. Army.
Johnston returned to the Texas Army during the Mexican–American War under General Zachary Taylor as a colonel of the 1st Texas Rifle Volunteers. The enlistments of his volunteers ran out just before the Battle of Monterrey. Johnston convinced a few volunteers to stay and fight as he served as the inspector general of volunteers and fought at the battles of Monterrey and Buena Vista.
He remained on his plantation after the war until he was appointed by President Zachary Taylor to the U.S. Army as a major and was made a paymaster in December 1849. He served in that role for more than five years, making six tours, and traveling more than annually on the Indian frontier of Texas. He served on the Texas frontier at Fort Mason and elsewhere in the West.
In 1855 President Franklin Pierce appointed him colonel of the new 2nd U.S. Cavalry (the unit that preceded the modern 5th U.S.), a new regiment, which he organized. On August 19, 1856, Gen. Persifor Smith, at the request of Kansas Territorial Governor Wilson Shannon, sent Col. Johnston with 1300 men composed of the 2d Cavalry Dragoons from Fort Riley, a battalion of the 6th Infantry and Capt. Howe's artillery company from Jefferson Barracks, to protect the territorial capital at Lecompton from an imminent attack by Jim Lane and his abolitionist "Army of the North."
Utah War.
As a key figure in the Utah War, Johnston led U.S. troops who established a non-Mormon government in the formerly Mormon territory. He received a brevet promotion to brigadier general in 1857 for his service in Utah. He spent 1860 in Kentucky until December 21, when he sailed for California to take command of the Department of the Pacific.
Civil War.
At the outbreak of the Civil War, Johnston was the commander of the U.S. Army Department of the Pacific in California. Like many regular army officers from the South, he was opposed to secession. But he resigned his commission soon after he heard of the secession of his adopted state Texas. It was accepted by the War Department on May 6, 1861, effective May 3. On April 28 he moved to Los Angeles, the home of his wife's brother John Griffin. Considering staying in California with his wife and five children, Johnston remained there until May.
Soon, under suspicion by local Union officials, he evaded arrest and joined the Los Angeles Mounted Rifles as a private, leaving Warner's Ranch May 27. He participated in their trek across the southwestern deserts to Texas, crossing the Colorado River into the Confederate Territory of Arizona on July 4, 1861.
Early in the Civil War, Confederate President Jefferson Davis decided that the Confederacy would attempt to hold as much of its territory as possible, and therefore distributed military forces around its borders and coasts. In the summer of 1861, Davis appointed several generals to defend Confederate lines from the Mississippi River east to the Allegheny Mountains.
The most sensitive, and in many ways the most crucial areas, along the Mississippi River and in western Tennessee along the Tennessee and the Cumberland rivers were placed under the command of Maj. Gen. Leonidas Polk and Brig. Gen. Gideon J. Pillow. The latter had initially been in command in Tennessee as that State's top general. Their impolitic occupation of Columbus, Kentucky, on September 3, 1861, two days before Johnston arrived in the Confederacy's capital of Richmond, Virginia, after his cross–country journey, drove Kentucky from its stated neutrality. The majority of Kentuckians allied with the Union camp. Polk and Pillow's action gave Union Brig. Gen. Ulysses S. Grant an excuse to take control of the strategically located town of Paducah, Kentucky, without raising the ire of most Kentuckians and the pro-Union majority in the State legislature.
Confederate command in Western Theater.
On September 10, 1861, Johnston was assigned to command the huge area of the Confederacy west of the Allegheny Mountains, except for coastal areas. He became commander of the Confederacy's western armies in the area often called the Western Department or Western Military Department. After his appointment, Johnston immediately headed for his new territory. He was permitted to call on governors of Arkansas, Tennessee and Mississippi for new troops, although this authority was largely stifled by politics, especially with respect to Mississippi. On September 13, 1861, Johnston ordered Brig. Gen. Felix Zollicoffer with 4,000 men to occupy Cumberland Gap in Kentucky in order to block Union troops from coming into eastern Tennessee. The Kentucky legislature had voted to side with the Union after the occupation of Columbus by Polk. By September 18, Johnston had Brig. Gen. Simon Bolivar Buckner with another 4,000 men blocking the railroad route to Tennessee at Bowling Green, Kentucky.
Johnston had fewer than 40,000 men spread throughout Kentucky, Tennessee, Arkansas and Missouri. Of these, 10,000 were in Missouri under Missouri State Guard Maj. Gen. Sterling Price. Johnston did not quickly gain many recruits when he first requested them from the governors, but his more serious problem was lacking sufficient arms and ammunition for the troops he already had. As the Confederate government concentrated efforts on the units in the East, they gave Johnston small numbers of reinforcements and minimal amounts of arms and material. Johnston maintained his defense by conducting raids and other measures to make it appear he had larger forces than he did, a strategy that worked for several months. Johnston's tactics had so annoyed and confused Union Brig. Gen. William Tecumseh Sherman that he became somewhat unnerved, overestimated Johnston's forces, and had to be relieved by Brig. Gen. Don Carlos Buell on November 9, 1861.
Battle of Mill Springs.
Eastern Tennessee was held for the Confederacy by two unimpressive brigadier generals appointed by Jefferson Davis: Felix Zollicoffer, a brave but untrained and inexperienced officer' and soon-to-be Maj. Gen. George B. Crittenden, a former U.S. Army officer with apparent alcohol problems. While Crittenden was away in Richmond, Zollicoffer moved his forces to the north bank of the upper Cumberland River near Mill Springs (now Nancy, Kentucky), putting the river to his back and his forces into a trap. Zollicoffer decided it was impossible to obey orders to return to the other side of the river because of scarcity of transport and proximity of Union troops. When Union Brig. Gen. George H. Thomas moved against the Confederates, Crittenden decided to attack one of the two parts of Thomas's command at Logan's Cross Roads near Mill Springs before the Union forces could unite. On January 19, 1862, the ill-prepared Confederates, after a night march in the rain, attacked the Union force with some initial success. As the battle progressed, Zollicoffer was killed, Crittenden was unable to lead the Confederate force (he may have been intoxicated), and the Confederates were turned back and routed by a Union bayonet charge, suffering 533 casualties from their force of 4,000. The Confederate troops who escaped were assigned to other units as Crittenden faced an investigation of his conduct.
After this Confederate defeat at the Battle of Mill Springs, Davis sent Johnston a brigade and a few other scattered reinforcements. He also assigned him Gen. P. G. T. Beauregard, who was supposed to attract recruits because of his victories early in the war, and act as a competent subordinate for Johnston. The brigade was led by Brig. Gen. John B. Floyd, considered incompetent. He took command at Fort Donelson as the senior general present just before Union Brig. Gen. Ulysses S. Grant attacked the fort. Historians believe the assignment of Beauregard to the west stimulated Union commanders to attack the forts before Beauregard could make a difference in the theater. Union officers heard that he was bringing 15 regiments with him, but this was an exaggeration of his forces.
Fort Henry, Fort Donelson, Nashville.
Based on the assumption that Kentucky neutrality would act as a shield against a direct invasion from the north, Tennessee initially had sent men to Virginia and concentrated defenses in the Mississippi Valley, circumstances that no longer applied in September 1861. Even before Johnston arrived in Tennessee, construction of two forts had been started to defend the Tennessee and the Cumberland rivers, which provided avenues into the State from the north. Both forts were located in Tennessee in order to respect Kentucky neutrality, but these were not in ideal locations. Fort Henry on the Tennessee River was in an unfavorable low–lying location, commanded by hills on the Kentucky side of the river. Fort Donelson on the Cumberland River, although in a better location, had a vulnerable land side and did not have enough heavy artillery to defend against gunboats.
Maj. Gen. Polk ignored the problems of the forts when he took command. After Johnston took command, Polk at first refused to comply with Johnston's order to send an engineer, Lt. Joseph K. Dixon, to inspect the forts. After Johnston asserted his authority, Polk had to allow Dixon to proceed. Dixon recommended that the forts be maintained and strengthened, although they were not in ideal locations, because much work had been done on them and the Confederates might not have time to build new ones. Johnston accepted his recommendations. Johnston wanted Major, later Lt. Gen., Alexander P. Stewart to command the forts but President Davis appointed Brig. Gen. Lloyd Tilghman as commander.
To prevent Polk from dissipating his forces by allowing some men to join a partisan group, Johnston ordered him to send Brig. Gen. Gideon Pillow and 5,000 men to Fort Donelson. Pillow took up a position at nearby Clarksville, Tennessee and did not move into the fort until February 7, 1862. Alerted by a Union reconnaissance on January 14, 1862, Johnston ordered Tilghman to fortify the high ground opposite Fort Henry, which Polk had failed to do despite Johnston's orders. Tilghman failed to act decisively on these orders, which in any event were too late to be adequately carried out.
Gen. Beauregard arrived at Johnston's headquarters at Bowling Green on February 4, 1862 and was given overall command of Polk's force at the western end of Johnston's line at Columbus, Kentucky. On February 6, 1862, Union Navy gunboats quickly reduced the defenses of ill-sited Fort Henry, inflicting 21 casualties on the small remaining Confederate force. Brig. Gen. Lloyd Tilghman surrendered the 94 remaining officers and men of his approximately 3,000-man force which had not been sent to Fort Donelson before U.S. Grant's force could even take up their positions. Johnston knew he could be trapped at Bowling Green if Fort Donelson fell, so he moved his force to Nashville, the capital of Tennessee and an increasingly important Confederate industrial center, beginning on February 11, 1862.
Johnston also reinforced Fort Donelson with 12,000 more men, including those under Floyd and Pillow, a curious decision in view of his thought that the Union gunboats alone might be able to take the fort. He did order the commanders of the fort to evacuate the troops if the fort could not be held. The senior generals sent to the fort to command the enlarged garrison, Gideon J. Pillow and John B. Floyd, squandered their chance to avoid having to surrender most of the garrison and on February 16, 1862, Brig. Gen. Simon Buckner, having been abandoned by Floyd and Pillow, surrendered Fort Donelson. Colonel Nathan Bedford Forrest escaped with his cavalry force of about 700 men before the surrender. The Confederates suffered about 1,500 casualties with an estimated 12,000 to 14,000 taken prisoner. Union casualties were 500 killed, 2,108 wounded, 224 missing.
Johnston, who had little choice in allowing Floyd and Pillow to take charge at Fort Donelson on the basis of seniority after he ordered them to add their forces to the garrison, took the blame and suffered calls for his removal because a full explanation to the press and public would have exposed the weakness of the Confederate position. His passive defensive performance while positioning himself in a forward position at Bowling Green, spreading his forces too thinly, not concentrating his forces in the face of Union advances, and appointing or relying upon inadequate or incompetent subordinates subjected him to criticism at the time and by later historians. The fall of the forts exposed Nashville to imminent attack, and it fell without resistance to Union forces under Brig. Gen. Buell on February 25, 1862, two days after Johnston had to pull his forces out in order to avoid having them captured as well.
Concentration at Corinth.
Johnston had various remaining military units scattered throughout his territory and retreating to the south to avoid being cut off. Johnston himself retreated with the force under his personal command, the Army of Central Kentucky, from the vicinity of Nashville. With Beauregard's help, Johnston decided to concentrate forces with those formerly under Polk and now already under Beauregard's command at the strategically located railroad crossroads of Corinth, Mississippi, which he reached by a circuitous route. Johnston kept the Union forces, now under the overall command of the ponderous Maj. Gen. Henry Halleck, confused and hesitant to move, allowing Johnston to reach his objective undetected. This delay allowed Jefferson Davis finally to send reinforcements from the garrisons of coastal cities and another highly rated but prickly general, Braxton Bragg, to help organize the western forces. Bragg at least calmed the nerves of Beauregard and Polk who had become agitated by their apparent dire situation in the face of numerically superior forces before the arrival of Johnston on March 24, 1862.
Johnston's army of 17,000 men gave the Confederates a combined force of about 40,000 to 44,669 men at Corinth. On March 29, 1862, Johnston officially took command of this combined force, which continued to use the Army of the Mississippi name under which it had been organized by Beauregard on March 5.
Johnston now planned to defeat the Union forces piecemeal before the various Union units in Kentucky and Tennessee under Grant with 40,000 men at nearby Pittsburg Landing, Tennessee, and the now Maj. Gen. Don Carlos Buell on his way from Nashville with 35,000 men, could unite against him. Johnston started his army in motion on April 3, 1862, intent on surprising Grant's force as soon as the next day, but they moved slowly due to their inexperience, bad roads and lack of adequate staff planning. Johnston's army was finally in position within a mile or two of Grant's force, and undetected, by the evening of April 5, 1862.
Battle of Shiloh and death.
Johnston launched a massive surprise attack with his concentrated forces against Grant at the Battle of Shiloh on April 6, 1862. As the Confederate forces overran the Union camps, Johnston seemed to be everywhere, personally leading and rallying troops up and down the line on his horse. At about 2:30 p.m., while leading one of those charges against a Union camp near the "Peach Orchard", he was wounded, taking a bullet behind his right knee. He apparently did not think the wound was serious at the time, or even possibly did not feel it, and so he sent his personal physician away to attend to some wounded captured Union soldiers instead. It is possible that Johnston's duel in 1837 had caused nerve damage or numbness to his right leg and that he did not feel the wound to his leg as a result. The bullet had in fact clipped a part of his popliteal artery and his boot was filling up with blood. Within a few minutes, Johnston was observed by his staff to be nearly fainting. Among his staff was Isham G. Harris, the Governor of Tennessee, who had ceased to make any real effort to function as governor after learning that Abraham Lincoln had appointed Andrew Johnson as military governor of Tennessee. Seeing Johnston slumping in his saddle and his face turning deathly pale, Harris asked: "General, are you wounded?" Johnston glanced down at his leg wound, then faced Harris and replied in a weak voice his last words: "Yes... and I fear seriously." Harris and other staff officers removed Johnston from his horse and carried him to a small ravine near the "Hornets Nest" and desperately tried to aid the general by trying to make a tourniquet for his leg wound, but little could be done by this point since he had already lost so much blood. He soon lost consciousness and bled to death a few minutes later. It is believed that Johnston may have lived for as long as one hour after receiving his fatal wound. Harris and the other officers wrapped General Johnston's body in a blanket so as not to damage the troops' morale with the sight of the dead general. Johnston and his wounded horse, named Fire Eater, were taken to his field headquarters on the Corinth road, where his body remained in his tent until the Confederate Army withdrew to Corinth the next day, April 7, 1862. From there, his body was taken to the home of Colonel William Inge, which had been his headquarters in Corinth. It was covered in the Confederate flag and lay in state for several hours.
It is probable that a Confederate soldier fired the fatal round. No Union soldiers were observed to have ever gotten behind Johnston during the fatal charge, while it is known that many Confederates were firing at the Union lines while Johnston charged well in advance of his soldiers.
Johnston was the highest-ranking fatality of the war on either side, and his death was a strong blow to the morale of the Confederacy. Jefferson Davis considered him the best general in the country; this was two months before the emergence of Robert E. Lee as the pre-eminent general of the Confederacy.
Legacy and honors.
Johnston was survived by his wife Eliza and six children. His wife and five younger children, including one born after he went to war, chose to live out their days at home in Los Angeles with Eliza's brother, Dr. John Strother Griffin. Johnston's eldest son, Albert Sidney Jr. (born in Texas), had already followed him into the Confederate States Army. In 1863, after taking home leave in Los Angeles, Albert Jr. was on his way out of San Pedro harbor on a ferry. While a steamer was taking on passengers from the ferry, a wave swamped the smaller boat, causing its boilers to explode. Albert Jr. was killed in the accident.
Killed in action, General Johnston received the highest praise ever given by the Confederate government; accounts were published, on December 20, 1862 and thereafter, in the Los Angeles "Star" of his family's hometown. Johnston Street, Hancock Street, and Griffin Avenue, each in northeast Los Angeles, are named after the general and his family, who lived in the neighborhood.
Johnston was initially buried in New Orleans. In 1866, a joint resolution of the Texas Legislature was passed to have his body moved and reinterred at the Texas State Cemetery in Austin. The re-interment occurred in 1867. Forty years later, the state appointed Elisabet Ney to design a monument and sculpture of him to be erected at the gravesite.
The Texas Historical Commission has erected a historical marker near the entrance of what was once Johnston's plantation. An adjacent marker was erected by the San Jacinto Chapter of the Daughters of The Republic of Texas and the Lee, Roberts, and Davis Chapter of the United Daughters of the Confederate States of America.
The University of Texas at Austin has recognized Johnston with a statue on the South Mall.

</doc>
<doc id="713" url="https://en.wikipedia.org/wiki?curid=713" title="Android (robot)">
Android (robot)

An android is a robot or synthetic organism designed to look and act like a human, especially one with a body having a flesh-like resemblance. Historically, androids remained completely within the domain of science fiction, frequently seen in film and television. Only recently have advancements in robot technology allowed the design of functional and realistic humanoid robots.
Etymology.
The word was coined from the Greek root ἀνδρ- 'man' (male, as opposed to anthrop- = human being) and the suffix "" 'having the form or likeness of'.
The "Oxford English Dictionary" traces the earliest use (as "Androides") to Ephraim Chambers' "Cyclopaedia," in reference to an automaton that St. Albertus Magnus allegedly created. The term "android" appears in US patents as early as 1863 in reference to miniature human-like toy automatons. The term "android" was used in a more modern sense by the French author Auguste Villiers de l'Isle-Adam in his work "Tomorrow's Eve" (1886). This story features an artificial humanlike robot named Hadaly. As said by the officer in the story, "In this age of Realien advancement, who knows what goes on in the mind of those responsible for these mechanical dolls." The term made an impact into English pulp science fiction starting from Jack Williamson's "The Cometeers" (1936) and the distinction between mechanical robots and fleshy androids was popularized by Edmond Hamilton's Captain Future (1940–1944).
Although Karel Čapek's robots in "R.U.R. (Rossum's Universal Robots)" (1921)—the play that introduced the word "robot" to the world—were organic artificial humans, the word "robot" has come to primarily refer to mechanical humans, animals, and other beings. The term "android" can mean either one of these, while a cyborg ("cybernetic organism" or "bionic man") would be a creature that is a combination of organic and mechanical parts.
The term "droid", popularized by George Lucas in the original "Star Wars" film and now used widely within science fiction, originated as an abridgment of "android", but has been used by Lucas and others to mean any robot, including distinctly non-human form machines like R2-D2. The word "android" was used in "" episode "What Are Little Girls Made Of?" The abbreviation "andy", coined as a pejorative by writer Philip K. Dick in his novel "Do Androids Dream of Electric Sheep?", has seen some further usage, such as within the TV series "Total Recall 2070".
Authors have used the term "android" in more diverse ways than "robot" or "cyborg". In some fictional works, the difference between a robot and android is only their appearance, with androids being made to look like humans on the outside but with robot-like internal mechanics. In other stories, authors have used the word "android" to mean a wholly organic, yet artificial, creation. Other fictional depictions of androids fall somewhere in between.
Eric G. Wilson, who defines androids as a "synthetic human being", distinguishes between three types of androids, based on their body's composition:
Although human morphology is not necessarily the ideal form for working robots, the fascination in developing robots that can mimic it can be found historically in the assimilation of two concepts: "simulacra" (devices that exhibit likeness) and "automata" (devices that have independence).
Projects.
Several projects aiming to create androids that look, and, to a certain degree, speak or act like a human being have been launched or are underway.
Japan.
The Intelligent Robotics Lab, directed by Hiroshi Ishiguro at Osaka University, and Kokoro Co., Ltd. have demonstrated the Actroid at Expo 2005 in Aichi Prefecture, Japan and released the Telenoid R1 in 2010. In 2006, Kokoro Co. developed a new "DER 2" android. The height of the human body part of DER2 is 165 cm. There are 47 mobile points. DER2 can not only change its expression but also move its hands and feet and twist its body. The "air servosystem" which Kokoro Co. developed originally is used for the actuator. As a result of having an actuator controlled precisely with air pressure via a servosystem, the movement is very fluid and there is very little noise. DER2 realized a slimmer body than that of the former version by using a smaller cylinder. Outwardly DER2 has a more beautiful proportion. Compared to the previous model, DER2 has thinner arms and a wider repertoire of expressions. Once programmed, it is able to choreograph its motions and gestures with its voice.
The Intelligent Mechatronics Lab, directed by Hiroshi Kobayashi at the Tokyo University of Science, has developed an android head called "Saya", which was exhibited at Robodex 2002 in Yokohama, Japan. There are several other initiatives around the world involving humanoid research and development at this time, which will hopefully introduce a broader spectrum of realized technology in the near future. Now Saya is "working" at the Science University of Tokyo as a guide.
The Waseda University (Japan) and NTT Docomo's manufacturers have succeeded in creating a shape-shifting robot "WD-2". It is capable of changing its face. At first, the creators decided the positions of the necessary points to express the outline, eyes, nose, and so on of a certain person. The robot expresses its face by moving all points to the decided positions, they say. The first version of the robot was first developed back in 2003. After that, a year later, they made a couple of major improvements to the design. The robot features an elastic mask made from the average head dummy. It uses a driving system with a 3DOF unit. The WD-2 robot can change its facial features by activating specific facial points on a mask, with each point possessing three degrees of freedom. This one has 17 facial points, for a total of 56 degrees of freedom. As for the materials they used, the WD-2's mask is fabricated with a highly elastic material called Septom, with bits of steel wool mixed in for added strength. Other technical features reveal a shaft driven behind the mask at the desired facial point, driven by a DC motor with a simple pulley and a slide screw. Apparently, the researchers can also modify the shape of the mask based on actual human faces. To "copy" a face, they need only a 3D scanner to determine the locations of an individual's 17 facial points. After that, they are then driven into position using a laptop and 56 motor control boards. In addition, the researchers also mention that the shifting robot can even display an individual's hair style and skin color if a photo of their face is projected onto the 3D Mask.
Singapore.
Prof Nadia Thalmann, a Nanyang Technological University scientist, directed efforts of the Institute for Media Innovation along with the School of Computer Engineering in the development of a social robot, Nadine. Nadine is powered by software similar to Apple’s Siri or Microsoft’s Cortana. Nadine may become a personal assistant in offices and homes in future, or she may become a companion for the young and the elderly.
Assoc Prof Gerald Seet from the School of Mechanical & Aerospace Engineering and the BeingThere Centre led a three-year R&D development in tele-presence robotics, creating EDGAR. A remote user can control EDGAR with the user’s face and expressions displayed on the robot’s face in real time. The robot also mimics their upper body movements.
South Korea.
KITECH researched and developed EveR-1, an android interpersonal communications model capable of emulating human emotional expression via facial "musculature" and capable of rudimentary conversation, having a vocabulary of around 400 words. She is tall and weighs , matching the average figure of a Korean woman in her twenties. EveR-1's name derives from the Biblical Eve, plus the letter "r" for "robot". EveR-1's advanced computing processing power enables speech recognition and vocal synthesis, at the same time processing lip synchronization and visual recognition by 90-degree micro-CCD cameras with face recognition technology. An independent microchip inside her artificial brain handles gesture expression, body coordination, and emotion expression. Her whole body is made of highly advanced synthetic jelly silicon and with 60 artificial joints in her face, neck, and lower body; she is able to demonstrate realistic facial expressions and sing while simultaneously dancing. In South Korea, the Ministry of Information and Communication has an ambitious plan to put a robot in every household by 2020. Several robot cities have been planned for the country: the first will be built in 2016 at a cost of 500 billion won, of which 50 billion is direct government investment. The new robot city will feature research and development centers for manufacturers and part suppliers, as well as exhibition halls and a stadium for robot competitions. The country's new Robotics Ethics Charter will establish ground rules and laws for human interaction with robots in the future, setting standards for robotics users and manufacturers, as well as guidelines on ethical standards to be programmed into robots to prevent human abuse of robots and vice versa.
United Kingdom.
In 2001, Steve Grand OBE, creator of the computer game "Creatures", created an android, or anthropoid; he named it Lucy. The intention was that she would have to learn everything, including how to use her mechanical vocal chords to speak. Her systems were made to be similar to a human's.
United States.
Walt Disney and a staff of Imagineers created Great Moments with Mr. Lincoln that debuted at the 1964 New York World's Fair.
Hanson Robotics, Inc., of Texas and KAIST produced an android portrait of Albert Einstein, using Hanson's facial android technology mounted on KAIST's life-size walking bipedal robot body. This Einstein android, also called "Albert Hubo", thus represents the first full-body walking android in history (see video at). Hanson Robotics, the FedEx Institute of Technology, and the University of Texas at Arlington also developed the android portrait of sci-fi author Philip K. Dick (creator of "Do Androids Dream of Electric Sheep?", the basis for the film "Blade Runner"), with full conversational capabilities that incorporated thousands of pages of the author's works. In 2005, the PKD android won a first place artificial intelligence award from AAAI.
Use in fiction.
Androids are a staple of science fiction. Isaac Asimov pioneered the fictionalization of the science of robotics and artificial intelligence, notably in his 1950s series "I, Robot". One thing common to most fictional androids is that the real-life technological challenges associated with creating thoroughly human-like robots—such as the creation of strong artificial intelligence—are assumed to have been solved. Fictional androids are often depicted as mentally and physically equal or superior to humans—moving, thinking and speaking as fluidly as them.
The tension between the nonhuman substance and the human appearance—or even human ambitions—of androids is the dramatic impetus behind most of their fictional depictions. Some android heroes seek, like Pinocchio, to become human, as in the films "Bicentennial Man", "Hollywood", "Enthiran" and "A.I. Artificial Intelligence", or Data in "". Others, as in the film "Westworld", rebel against abuse by careless humans. Android hunter Deckard in "Do Androids Dream of Electric Sheep?" and its film adaptation "Blade Runner" discovers that his targets are, in some ways, more human than he is. Android stories, therefore, are not essentially stories "about" androids; they are stories about the human condition and what it means to be human.
One aspect of writing about the meaning of humanity is to use discrimination against androids as a mechanism for exploring racism in society, as in "Blade Runner". Perhaps the clearest example of this is John Brunner's 1968 novel "Into the Slave Nebula", where the blue-skinned android slaves are explicitly shown to be fully human. More recently, the androids Bishop and Annalee Call in the films "Aliens" and "Alien Resurrection" are used as vehicles for exploring how humans deal with the presence of an "Other".
Female androids, or "gynoids", are often seen in science fiction, and can be viewed as a continuation of the long tradition of men attempting to create the stereotypical "perfect woman". Examples include the Greek myth of "Pygmalion" and the female robot Maria in Fritz Lang's "Metropolis". Some gynoids, like Pris in "Blade Runner", are designed as sex-objects, with the intent of "pleasing men's violent sexual desires," or as submissive, servile companions, such as in "The Stepford Wives". Fiction about gynoids has therefore been described as reinforcing "essentialist ideas of femininity", although others have suggested that the treatment of androids is a way of exploring racism and misogyny in society.
The 2015 Japanese film "Sayonara", starring Geminoid F, was promoted as "the first movie to feature an android performing opposite a human actor".

</doc>
<doc id="717" url="https://en.wikipedia.org/wiki?curid=717" title="Alberta">
Alberta

Alberta () is a western province of Canada. With an estimated population of 4,196,457 as of July 1, 2015, it is Canada's fourth-most populous province and the most populous of Canada's three prairie provinces. Alberta and its neighbour Saskatchewan were districts of the Northwest Territories until they were established as provinces on September 1, 1905. The premier has been Rachel Notley since May 2015.
Alberta is bounded by the provinces of British Columbia to the west and Saskatchewan to the east, the Northwest Territories to the north, and the US state of Montana to the south. Alberta is one of three Canadian provinces and territories to border only a single US state and one of only two landlocked provinces. It has a predominantly Humid continental climate, but seasonal temperature average swings are smaller than to areas further east, with winters being warmed by occasional chinook winds bringing sudden warming which moderates average temperatures.
Alberta's capital Edmonton is near the geographic centre of the province and is the primary supply and service hub for Canada's crude oil, oil sands (Athabasca oil sands) and other northern resource industries. 
About south of the capital is Calgary, the largest city in Alberta. Calgary and Edmonton centre Alberta's two census metropolitan areas, both of which have populations exceeding one million, while the province has 16 census agglomerations. Tourist destinations in the province include Banff, Canmore, Drumheller, Jasper and Sylvan Lake.
Etymology.
Alberta is named after Princess Louise Caroline Alberta (1848–1939), the fourth daughter of Victoria, Queen of Canada, and Albert, Prince Consort. Princess Louise was the wife of the Marquess of Lorne, Governor General of Canada (1878–83). Lake Louise and Mount Alberta were also named in her honour.
Geography.
Alberta, with an area of , is the fourth largest province after Quebec, Ontario, and British Columbia. To the south, the province borders on the 49th parallel north, separating it from the US state of Montana, while on the north the 60th parallel north divides it from the Northwest Territories. To the east, the 110th meridian west separates it from the province of Saskatchewan, while on the west its boundary with British Columbia follows the 120th meridian west south from the Northwest Territories at 60°N until it reaches the Continental Divide at the Rocky Mountains, and from that point follows the line of peaks marking the Continental Divide in a generally southeasterly direction until it reaches the Montana border at 49°N.
The province extends north to south and east to west at its maximum width. Its highest point is at the summit of Mount Columbia in the Rocky Mountains along the southwest border, while its lowest point is on the Slave River in Wood Buffalo National Park in the northeast.
With the exception of the semi-arid steppe of the southeastern section, the province has adequate water resources. There are numerous rivers and lakes used for swimming, fishing and a range of water sports. There are three large lakes, Lake Claire () in Wood Buffalo National Park, Lesser Slave Lake (), and Lake Athabasca () which lies in both Alberta and Saskatchewan. The longest river in the province is the Athabasca River which travels from the Columbia Icefield in the Rocky Mountains to Lake Athabasca. The largest river is the Peace River with an average flow of 2161 m/s. The Peace River originates in the Rocky Mountains of northern British Columbia and flows through northern Alberta and into the Slave River, a tributary of the Mackenzie River.
Alberta's capital city, Edmonton, is located approximately in the geographic centre of the province. It is the most northerly major city in Canada, and serves as a gateway and hub for resource development in northern Canada. The region, with its proximity to Canada's largest oil fields, has most of western Canada's oil refinery capacity. Calgary is located approximately south of Edmonton and north of Montana, surrounded by extensive ranching country. Almost 75% of the province's population lives in the Calgary–Edmonton Corridor. The land grant policy to the railroads served as a means to populate the province in its early years.
Most of the northern half of the province is boreal forest, while the Rocky Mountains along the southwestern boundary are largely forested (see Alberta Mountain forests and Alberta-British Columbia foothills forests). The southern quarter of the province is prairie, ranging from shortgrass prairie in the southeastern corner to mixed grass prairie in an arc to the west and north of it. The central aspen parkland region extending in a broad arc between the prairies and the forests, from Calgary, north to Edmonton, and then east to Lloydminster, contains the most fertile soil in the province and most of the population. Much of the unforested part of Alberta is given over either to grain or to dairy farming, with mixed farming more common in the north and centre, while ranching and irrigated agriculture predominate in the south.
The Alberta badlands are located in southeastern Alberta, where the Red Deer River crosses the flat prairie and farmland, and features deep canyons and striking landforms. Dinosaur Provincial Park, near Brooks, Alberta, showcases the badlands terrain, desert flora, and remnants from Alberta's past when dinosaurs roamed the then lush landscape.
Climate.
Alberta has a dry continental climate with warm summers and cold winters. The province is open to cold arctic weather systems from the north, which often produce extremely cold conditions in winter. As the fronts between the air masses shift north and south across Alberta, the temperature can change rapidly. Arctic air masses in the winter produce extreme minimum temperatures varying from in northern Alberta to in southern Alberta. In the summer, continental air masses produce maximum temperatures from in the mountains to in southern Alberta.
Alberta extends for over from north to south; its climate, therefore, varies considerably. Average high temperatures in January range from in the southwest to in the far north. The climate is also influenced by the presence of the Rocky Mountains to the southwest, which disrupt the flow of the prevailing westerly winds and cause them to drop most of their moisture on the western slopes of the mountain ranges before reaching the province, casting a rain shadow over much of Alberta. The northerly location and isolation from the weather systems of the Pacific Ocean cause Alberta to have a dry climate with little moderation from the ocean. Annual precipitation ranges from in the southeast to in the north, except in the foothills of the Rocky Mountains where total precipitation including snowfall can reach annually. The province is the namesake of the Alberta clipper, a type of intense, fast-moving winter storm that generally forms over or near the province and pushed with great speed by the continental polar jetstream descends over the rest of Southern Canada and the northern tier of the United States.
In the summer, the average daytime temperatures range from around in the Rocky Mountain valleys and far north, up to around in the dry prairie of the southeast. The northern and western parts of the province experience higher rainfall and lower evaporation rates caused by cooler summer temperatures. The south and east-central portions are prone to drought-like conditions sometimes persisting for several years, although even these areas can receive heavy precipitation and sometimes resulting in flooding.
Alberta is a sunny province. Annual bright sunshine totals range between 1900 up to just under 2600 hours per year. Northern Alberta gets about 18 hours of daylight in the summer.
In southwestern Alberta, the cold winters are frequently interrupted by warm, dry chinook winds blowing from the mountains, which can propel temperatures upward from frigid conditions to well above the freezing point in a very short period. During one chinook recorded at Pincher Creek, temperatures soared from in just one hour. The region around Lethbridge has the most chinooks, averaging 30 to 35 chinook days per year. Calgary has a 56% chance of a white Christmas, while Edmonton has an 86% chance.
Northern Alberta is mostly covered by boreal forest and has a subarctic climate. The agricultural area of southern Alberta has a semi-arid steppe climate because the annual precipitation is less than the water that evaporates or is used by plants. The southeastern corner of Alberta, part of the Palliser Triangle, experiences greater summer heat and lower rainfall than the rest of the province, and as a result suffers frequent crop yield problems and occasional severe droughts. Western Alberta is protected by the mountains and enjoys the mild temperatures brought by winter chinook winds. Central and parts of northwestern Alberta in the Peace River region are largely aspen parkland, a biome transitional between prairie to the south and boreal forest to the north.
After Saskatchewan, Alberta experiences the most tornadoes in Canada with an average of 15 verified per year. Thunderstorms, some of them severe, are frequent in the summer, especially in central and southern Alberta. The region surrounding the Calgary–Edmonton Corridor is notable for having the highest frequency of hail in Canada, which is caused by orographic lifting from the nearby Rocky Mountains, enhancing the updraft/downdraft cycle necessary for the formation of hail.
Ecology.
Flora.
In central and northern Alberta the arrival of spring is marked by the early flowering of the prairie crocus anemone; this member of the buttercup family has been recorded flowering as early as March though April is the usual month for the general population. Other prairie flora known to flower early are the golden bean and wild rose. Members of the sunflower family blossom on the prairie in the summer months between July and September. The southern and east central parts of Alberta are covered by short prairie grass, which dries up as summer lengthens, to be replaced by hardy perennials such as the prairie coneflower, fleabane, and sage. Both yellow and white sweet clover can be found throughout the southern and central areas of the province.
The trees in the parkland region of the province grow in clumps and belts on the hillsides. These are largely deciduous, typically aspen, poplar, and willow. Many species of willow and other shrubs grow in virtually any terrain. On the north side of the North Saskatchewan River evergreen forests prevail for thousands of square kilometres. Aspen poplar, balsam poplar (or in some parts cottonwood), and paper birch are the primary large deciduous species. Conifers include jack pine, Rocky Mountain pine, lodgepole pine, both white and black spruce, and the deciduous conifer tamarack.
Fauna.
The four climatic regions (alpine, boreal forest, parkland, and prairie) of Alberta are home to many different species of animals. The south and central prairie was the land of the bison, commonly known as buffalo, its grasses providing pasture and breeding ground for millions of buffalo. The buffalo population was decimated during early settlement, but since then buffalo have made a comeback, living on farms and in parks all over Alberta.
Alberta is home to many large carnivores. Among them are the grizzly and black bears, which are found in the mountains and wooded regions. Smaller carnivores of the canine and feline families include coyotes, wolves, fox, lynx, bobcat and mountain lion (cougar).
Herbivorous animals are found throughout the province. Moose, mule deer, elk, and white-tail deer are found in the wooded regions, and pronghorn can be found in the prairies of southern Alberta. Bighorn sheep and mountain goats live in the Rocky Mountains. Rabbits, porcupines, skunks, squirrels and many species of rodents and reptiles live in every corner of the province. Alberta is home to only one variety of venomous snake, the prairie rattlesnake.
Central and northern Alberta and the region farther north is the nesting ground of many migratory birds. Vast numbers of ducks, geese, swans and pelicans arrive in Alberta every spring and nest on or near one of the hundreds of small lakes that dot northern Alberta. Eagles, hawks, owls and crows are plentiful, and a huge variety of smaller seed and insect-eating birds can be found. Alberta, like other temperate regions, is home to mosquitoes, flies, wasps, and bees. Rivers and lakes are populated with pike, walleye, whitefish, rainbow, speckled, brown trout, and sturgeon. Bull trout, native to the province, is the Alberta's provincial fish. Turtles are found in some water bodies in the southern part of the province. Frogs and salamanders are a few of the amphibians that make their homes in Alberta.
Alberta is the only province in Canada—as well as one of the few places in the world—that is free of Norwegian rats. Since the early 1950s, the Government of Alberta has operated a rat-control program, which has been so successful that only isolated instances of wild rat sightings are reported, usually of rats arriving in the province aboard trucks or by rail. In 2006, Alberta Agriculture reported zero findings of wild rats; the only rat interceptions have been domesticated rats that have been seized from their owners. It is illegal for individual Albertans to own or keep Norwegian rats of any description; the animals can only be kept in the province by zoos, universities and colleges, and recognized research institutions. In 2009, several rats were
found and captured, in small pockets in southern Alberta, putting Alberta's rat-free status in jeopardy. A colony of rats were subsequently found in a landfill near Medicine Hat in 2012, and again in 2014.
Paleontology.
Alberta has one of the greatest diversities and abundances of Late Cretaceous dinosaur fossils in the world. Taxa are represented by complete fossil skeletons, isolated material, microvertebrate remains, and even mass graves. At least 38 dinosaur type specimens were collected in the province. The Foremost Formation, Oldman Formation and Dinosaur Park Formations collectively comprise the Judith River Group and are the most thoroughly studied dinosaur-bearing strata in Alberta.
Dinosaur-bearing strata are distributed widely throughout Alberta. The Dinosaur Provincial Park area contains outcrops of the Dinosaur Park Formation and Oldman Formation. In the central and southern regions of Alberta are intermittent Scollard Formation outcrops. In the Drumheller Valley and Edmonton regions there are exposed Horseshoe Canyon facies. Other formations have been recorded as well, like the Milk River and Foremost Formations. However, these latter two have a lower diversity of documented dinosaurs, primarily due to their lower total fossil quantity and neglect from collectors who are hindered by the isolation and scarcity of exposed outcrops. Their dinosaur fossils are primarily teeth recovered from microvertebrate fossil sites. Additional geologic formations that have produced only few fossils are the Belly River Group and St. Mary River Formations of the southwest and the northwestern Wapiti Formation. The Wapiti Formation contains two "Pachyrhinosaurus" bone beds that break its general trend of low productivity, however. The Bearpaw Formation represents strata deposited during a marine transgression. Dinosaurs are known from this Formation, but represent specimens washed out to sea or reworked from older sediments.
History.
Paleo-Indians arrived in Alberta at least 10,000 years ago, toward the end of the last ice age. They are thought to have migrated from Siberia to Alaska on a land bridge across the Bering Strait, and then may have moved down the east side of the Rocky Mountains through Alberta to settle the Americas. Others may have migrated down the coast of British Columbia and then moved inland. Over time they differentiated into various First Nations peoples, including the Plains Indian tribes of southern Alberta such as those of the Blackfoot Confederacy and the Plains Cree, who generally lived by hunting buffalo (American bison), and the more northerly tribes such as the Woodland Cree and Chipewyan who hunted, trapped, and fished for a living.
After the British arrival in Canada, approximately half of the province of Alberta, south of the Athabasca River drainage, became part of Rupert's Land which consisted of all land drained by rivers flowing into Hudson Bay. This area was granted by Charles II of England to the Hudson's Bay Company (HBC) in 1670, and rival fur trading companies were not allowed to trade in it. After the arrival of French Canadians in the west around 1731, they settled near fur trading posts, establishing communities such as Lac La Biche and Bonnyville. Fort La Jonquière was established near what is now Calgary in 1752.
The Athabasca River and the rivers north of it were not in HBC territory because they drained into the Arctic Ocean instead of Hudson Bay, and they were prime habitat for fur-bearing animals. The first explorer of the Athabasca region was Peter Pond, who learned of the Methye Portage, which allowed travel from southern rivers into the rivers north of Rupert's Land. Fur traders formed the North West Company (NWC) of Montreal to compete with the HBC in 1779. The NWC occupied the northern part of Alberta territory. Peter Pond built Fort Athabasca on Lac la Biche in 1778. Roderick Mackenzie built Fort Chipewyan on Lake Athabasca ten years later in 1788. His cousin, Sir Alexander Mackenzie, followed the North Saskatchewan River to its northernmost point near Edmonton, then setting northward on foot, trekked to the Athabasca River, which he followed to Lake Athabasca. It was there he discovered the mighty outflow river which bears his name—the Mackenzie River—which he followed to its outlet in the Arctic Ocean. Returning to Lake Athabasca, he followed the Peace River upstream, eventually reaching the Pacific Ocean, and so he became the first European to cross the North American continent north of Mexico.
The extreme southernmost portion of Alberta was part of the French (and Spanish) territory of Louisiana, sold to the United States in 1803; in 1818, the portion of Louisiana north of the Forty-Ninth Parallel was ceded to Great Britain.
Fur trade expanded in the north, but bloody battles occurred between the rival HBC and NWC, and in 1821 the British government forced them to merge to stop the hostilities. The amalgamated Hudson's Bay Company dominated trade in Alberta until 1870, when the newly formed Canadian Government purchased Rupert's Land. Northern Alberta was included in the North-Western Territory until 1870, when it and Rupert's land became Canada's Northwest Territories.
The District of Alberta was created as part of the North-West Territories in 1882. As settlement increased, local representatives to the North-West Legislative Assembly were added. After a long campaign for autonomy, in 1905 the District of Alberta was enlarged and given provincial status, with the election of Alexander Cameron Rutherford as the first premier.
On June 21, 2013, during the 2013 Alberta floods Alberta experienced heavy rainfall that triggered catastrophic flooding throughout much of the southern half of the province along the Bow, Elbow, Highwood and Oldman rivers and tributaries. A dozen municipalities in Southern Alberta declared local states of emergency on June 21 as water levels rose and numerous communities were placed under evacuation orders.
Demographics.
The 2011 census reported Alberta had a population of 3,645,257 living in 1,390,275 of its 1,505,007 total dwellings, a 10.8% change from its 2006 population of 3,290,350. With a land area of , it had a population density of in 2011. Statistics Canada estimated the province to have a population of 4,196,457 in 2015.
Alberta has experienced a relatively high rate of growth in recent years, mainly because of its burgeoning economy. Between 2003 and 2004, the province had high birthrates (on par with some larger provinces such as British Columbia), relatively high immigration, and a high rate of interprovincial migration compared to other provinces.
About 81% of the population lives in urban areas and only about 19% in rural areas. The Calgary–Edmonton Corridor is the most urbanized area in the province and is one of the most densely populated areas of Canada. Many of Alberta's cities and towns have experienced very high rates of growth in recent history. Alberta's population rose from 73,022 in 1901 to 3,290,350 according to the 2006 census.
The 2006 census found that English, with 2,576,670 native speakers, was the most common mother tongue of Albertans, representing 79.99% of the population. The next most common mother tongues were Chinese with 97,275 native speakers (3.02%), followed by German with 84,505 native speakers (2.62%) and French with 61,225 (1.90%).
Other mother tongues include: Punjabi, with 36,320 native speakers (1.13%); Tagalog, with 29,740 (0.92%); Ukrainian, with 29,455 (0.91%); Spanish, with 29,125 (0.90%); Polish, with 21,990 (0.68%); Arabic, with 20,495 (0.64%); Dutch, with 19,980 (0.62%); and Vietnamese, with 19,350 (0.60%). The most common aboriginal language is Cree 17,215 (0.53%). Other common mother tongues include Italian with 13,095 speakers (0.41%); Urdu with 11,275 (0.35%); and Korean with 10,845 (0.33%); then Hindi 8,985 (0.28%); Persian 7,700 (0.24%); Portuguese 7,205 (0.22%); and Hungarian 6,770 (0.21%).
Alberta has considerable ethnic diversity. In line with the rest of Canada, many immigrants originated from England, Scotland, Ireland and Wales, but large numbers also came from other parts of Europe, notably Germany, France, Ukraine and Scandinavia. According to Statistics Canada, Alberta is home to the second highest proportion (two percent) of Francophones in western Canada (after Manitoba). Despite this, relatively few Albertans claim French as their mother tongue. Many of Alberta's French-speaking residents live in the central and northwestern regions of the province.
As reported in the 2001 census, the Chinese represented nearly four percent of Alberta's population, and East Indians represented more than two percent. Both Edmonton and Calgary have historic Chinatowns, and Calgary has Canada's third largest Chinese community. The Chinese presence began with workers employed in the building of the Canadian Pacific Railway in the 1880s. Aboriginal Albertans make up approximately three percent of the population.
In the 2006 Canadian census, the most commonly reported ethnic origins among Albertans were: 885,825 English (27.2%); 679,705 German (20.9%); 667,405 Canadian (20.5%); 661,265 Scottish (20.3%); 539,160 Irish (16.6%); 388,210 French (11.9%); 332,180 Ukrainian (10.2%); 172,910 Dutch (5.3%); 170,935 Polish (5.2%); 169,355 North American Indian (5.2%); 144,585 Norwegian (4.4%); and 137,600 Chinese (4.2%). (Each person could choose as many ethnicities as were applicable.)"
Amongst those of British origins, the Scots have had a particularly strong influence on place-names, with the names of many cities and towns including Calgary, Airdrie, Canmore, and Banff having Scottish origins.
Alberta is the third most diverse province in terms of visible minorities after British Columbia and Ontario with 13.9% of the population consisting of visible minorities. Nearly one-fourth of the populations of Calgary and Edmonton belong to a visible minority group.
Aboriginal Identity Peoples make up 5.8% of the population, about half of whom consist of North American Indians and the other half are Metis. There are also small number of Inuit people in Alberta. The number of Aboriginal Identity Peoples have been increasing at a rate greater than the population of Alberta.
As of the 2011 National Household Survey, the largest religious group was Roman Catholic, representing 24.3% of the population. Alberta had the second highest percentage of non-religious residents among the provinces (after British Columbia) at 31.6% of the population. Of the remainder, 7.5% of the population identified themselves as belonging to the United Church of Canada, while 3.9% were Anglican. Lutherans made up 3.3% of the population while Baptists comprised 1.9%.
The remainder belonged to a wide variety of different religious affiliations, none of which constituted more than 2% of the population. The LDS Church of Alberta reside primarily in the extreme south of the province. Alberta has a population of Hutterites, a communal Anabaptist sect similar to the Mennonites, and has a significant population of Seventh-day Adventists. Alberta is home to several Byzantine Rite Churches as part of the legacy of Eastern European immigration, including the Ukrainian Catholic Eparchy of Edmonton, and the Ukrainian Orthodox Church of Canada's Western Diocese which is based in Edmonton.
Muslims, Sikhs, Buddhists, and Hindus live in Alberta. Muslims made up 3.2% of the population, Sikhs 1.5%, Buddhists 1.2%, and Hindus 1.0%. Many of these are recent immigrants, but others have roots that go back to the first settlers of the prairies. Canada's oldest mosque, the Al-Rashid Mosque, is located in Edmonton, whereas Calgary is home to Canada's largest mosque, the Baitun Nur mosque. Jews constituted 0.3% of Alberta's population. Most of Alberta's 11,000 Jews live in Calgary (6,200) and Edmonton (3,800).
Economy.
Alberta's economy is one of the strongest in the world, supported by the burgeoning petroleum industry and to a lesser extent, agriculture and technology. In 2013 Alberta's per capita GDP exceeded that of the United States, Norway, or Switzerland, and was the highest of any province in Canada at C$84,390. This was 56% higher than the national average of C$53,870 and more than twice that of some of the Atlantic provinces. In 2006 the deviation from the national average was the largest for any province in Canadian history. According to the 2006 census, the median annual family income after taxes was $70,986 in Alberta (compared to $60,270 in Canada as a whole).
Alberta has no financial debt, with a 1.9% asset surplus over all net debt. Alberta is also the single richest jurisdiction per capita on the planet.
The Calgary-Edmonton Corridor is the most urbanized region in the province and one of the densest in Canada. The region covers a distance of roughly 400 kilometres north to south. In 2001, the population of the Calgary-Edmonton Corridor was 2.15 million (72% of Alberta's population). It is also one of the fastest growing regions in the country. A 2003 study by TD Bank Financial Group found the corridor to be the only Canadian urban centre to amass a US level of wealth while maintaining a Canadian style quality of life, offering universal health care benefits. The study found that GDP per capita in the corridor was 10% above average US metropolitan areas and 40% above other Canadian cities at that time.
The Fraser Institute states that Alberta also has very high levels of economic freedom and rates Alberta as the freest economy in Canada, and the second freest economy amongst US states and Canadian provinces. The government of Alberta has invested its earnings wisely; as of September 30, 2013, official statistics reported nearly 500 holdings.
In 2014, Merchandise exports totalled US$121.4 Billion. Energy revenues totalled $111.7 Billion and Energy resource exports totalled $90.8 Billion. Farm Cash receipts from agricultural products totalled $12.9 Billion. Shipments of forest products totalled $5.4 Billion while exports were $2.7 Billion. Manufacturing sales totaled $79.4 Billion, and Alberta's ICT industries generated over $13 Billion in revenue. In total, Alberta's 2014 GDP amassed $364.5 Billion in 2007 dollars, or $414.3 Billion in 2015 dollars. In 2015, Alberta's GDP grew despite low oil prices, however it was unstable with growth rates as high 4.4% and as low as 0.2%. Should the GDP remain at an average of 2.2% for the last two quarters of 2015, Alberta's GDP should exceed $430 Billion by the end of 2015. However, RBC Economics research predicts Alberta's real GDP growth to only average 0.6% for the last 2 quarters of 2015. This estimate predicts a real GDP growth of only 1.4% for 2015. A positive is the predicted 10.8% growth in Nominal GDP, and possibly above 11% in 2016.
Industry.
Alberta is the largest producer of conventional crude oil, synthetic crude, natural gas and gas products in Canada. Alberta is the world’s second largest exporter of natural gas and the fourth largest producer. Two of the largest producers of petrochemicals in North America are located in central and north-central Alberta. In both Red Deer and Edmonton, polyethylene and vinyl manufacturers produce products that are shipped all over the world. Edmonton's oil refineries provide the raw materials for a large petrochemical industry to the east of Edmonton.
The Athabasca oil sands surrounding Fort McMurray have estimated unconventional oil reserves approximately equal to the conventional oil reserves of the rest of the world, estimated to be 1.6 trillion barrels (254 km). Many companies employ both conventional strip mining and non-conventional in situ methods to extract the bitumen from the oil sands. As of late 2006 there were over $100 billion in oil sands projects under construction or in the planning stages in northeastern Alberta.
Another factor determining the viability of oil extraction from the oil sands is the price of oil. The oil price increases since 2003 have made it profitable to extract this oil, which in the past would give little profit or even a loss. By mid-2014 however rising costs and stabilizing oil prices were threatening the economic viability of some projects. An example of this was the shelving of the Joslyn north project in the Athabasca region in May 2014.
With concerted effort and support from the provincial government, several high-tech industries have found their birth in Alberta, notably patents related to interactive liquid crystal display systems. With a growing economy, Alberta has several financial institutions dealing with civil and private funds.
Agriculture and forestry.
Agriculture has a significant position in the province's economy. The province has over three million head of cattle, and Alberta beef has a healthy worldwide market. Nearly one half of all Canadian beef is produced in Alberta. Alberta is one of the top producers of plains buffalo (bison) for the consumer market. Sheep for wool and mutton are also raised.
Wheat and canola are primary farm crops, with Alberta leading the provinces in spring wheat production; other grains are also prominent. Much of the farming is dryland farming, often with fallow seasons interspersed with cultivation. Continuous cropping (in which there is no fallow season) is gradually becoming a more common mode of production because of increased profits and a reduction of soil erosion. Across the province, the once common grain elevator is slowly being lost as rail lines are decreasing; farmers typically truck the grain to central points.
Alberta is the leading beekeeping province of Canada, with some beekeepers wintering hives indoors in specially designed barns in southern Alberta, then migrating north during the summer into the Peace River valley where the season is short but the working days are long for honeybees to produce honey from clover and fireweed. Hybrid canola also requires bee pollination, and some beekeepers service this need.
The vast northern forest reserves of softwood allow Alberta to produce large quantities of lumber, oriented strand board (OSB) and plywood, and several plants in northern Alberta supply North America and the Pacific Rim nations with bleached wood pulp and newsprint.
Tourism.
Alberta has been a tourist destination from the early days of the twentieth century, with attractions including outdoor locales for skiing, hiking and camping, shopping locales such as West Edmonton Mall, Calgary Stampede, outdoor festivals, professional athletic events, international sporting competitions such as the Commonwealth Games and Olympic Games, as well as more eclectic attractions. There are also natural attractions like Elk Island National Park, Wood Buffalo National Park, and the Columbia Icefield.
According to Alberta Economic Development, Calgary and Edmonton both host over four million visitors annually. Banff, Jasper and the Rocky Mountains are visited by about three million people per year. Alberta tourism relies heavily on Southern Ontario tourists, as well as tourists from other parts of Canada, the United States, and many other countries.
Alberta's Rocky Mountains include well-known tourist destinations Banff National Park and Jasper National Park. The two mountain parks are connected by the scenic Icefields Parkway. Banff is located west of Calgary on Highway 1, and Jasper is located west of Edmonton on Yellowhead Highway. Five of Canada's fourteen UNESCO World heritage sites are located within the province: Canadian Rocky Mountain Parks, Waterton-Glacier International Peace Park, Wood Buffalo National Park, Dinosaur Provincial Park and Head-Smashed-In Buffalo Jump.
About 1.2 million people visit the of Calgary Stampede, a celebration of Canada's own Wild West and the cattle ranching industry. About 700,000 people enjoy Edmonton's K-Days (formerly Klondike Days and Capital EX). Edmonton was the gateway to the only all-Canadian route to the Yukon gold fields, and the only route which did not require gold-seekers to travel the exhausting and dangerous Chilkoot Pass.
Another tourist destination that draws more than 650,000 visitors each year is the Drumheller Valley, located northeast of Calgary. Drumheller, "Dinosaur Capital of The World", offers the Royal Tyrrell Museum of Palaeontology. Drumheller also had a rich mining history being one of Western Canada's largest coal producers during the war years.
Located in east-central Alberta is Alberta Prairie Railway Excursions, a popular tourist attraction operated out of Stettler, that offers train excursions into the prairie and caters to tens of thousands of visitors every year.
Alberta has numerous ski resorts most notably Sunshine Village, Lake Louise, Marmot Basin, Norquay and Nakiska.
Government and politics.
The Government of Alberta is organized as a parliamentary democracy with a unicameral legislature. Its unicameral legislature—the Legislative Assembly—consists of eighty-seven members elected first past the post (FPTP) from single-member constituencies.
Locally municipal governments and school boards are elected and operate separately. Their boundaries do not necessarily coincide. Municipalities where the same body act as both local government and school board are formally referred to as "counties" in Alberta.
As Canada's head of state, Queen Elizabeth II is the head of state for the Government of Alberta. Her duties in Alberta are carried out by Lieutenant Governor Lois Mitchell. The Queen and lieutenant governor are figureheads whose actions are highly restricted by custom and constitutional convention. The lieutenant governor handles numerous honorific duties in the name of the Queen. The government is headed by the premier. The premier is normally a member of the Legislative Assembly, and draws all the members of the Cabinet from among the members of the Legislative Assembly. The City of Edmonton is the seat of the provincial government—the capital of Alberta.
The current premier is Rachel Notley, sworn in on May 24, 2015.
The previous premier was Jim Prentice, who became the leader of the then governing Progressive Conservatives on September 6, 2014 following the resignation of Alison Redford and the interim leadership of Dave Hancock. Prentice was sworn in as the 16th Premier of Alberta on September 15, 2014. He called an early election on May 5, 2015 in which the opposition New Democratic Party (NDP) won a majority of the seats. Prentice immediately resigned his seat and leadership of the PC party, but remained premier until Notley was sworn in on May 24, 2015.
Alberta's elections have tended to yield much more conservative outcomes than those of other Canadian provinces. Since the 1960s, Alberta has had three main political parties, the Progressive Conservatives ("Conservatives" or "Tories"), the Liberals, and the social democratic New Democrats. The Wildrose Party, a more conservative party formed in early 2008, gained much support in 2012 election and became the official opposition, a role it still holds today. The strongly conservative Social Credit Party was a power in Alberta for many decades, but fell from the political map after the Progressive Conservatives came to power in 1971.
For 44 years the Progressive Conservatives governed Alberta. They lost the 2015 election to the NDP, signalling a possible shift to the left in the province, also indicated by the election of progressive mayors in both of Alberta's major cities. Since becoming a province in 1905, Alberta has seen only four changes of government - only five parties have governed Alberta: the Liberals, from 1905 to 1921; the United Farmers of Alberta, from 1921 to 1935; the Social Credit Party, from 1935 to 1971, the Progressive Conservative Party, from 1971 to 2015: and the currently governing Alberta New Democratic Party.
Alberta has had occasional surges in separatist sentiment. Even during the 1980s, when these feelings were at their strongest, there has not been enough interest in secession to initiate any major movement or referendum. Several groups are currently active promoting independence for Alberta in some form.
Taxation.
Government revenue comes mainly from royalties on non-renewable natural resources (30.4%), personal income taxes (22.3%), corporate and other taxes (19.6%), and grants from the federal government primarily for infrastructure projects (9.8%). Albertans are the lowest-taxed people in Canada, and Alberta is the only province in Canada without a provincial sales tax (but residents are still subject to the federal sales tax, the Goods and Services Tax of 5%). It is also the only Canadian province to have a flat tax for personal income taxes, which is 10% of taxable income. This is likely to change with the election of an NDP government.
The Alberta personal income tax system maintains a progressive character by granting residents personal tax exemptions of $17,787, in addition to a variety of tax deductions for persons with disabilities, students, and the aged. Alberta's municipalities and school jurisdictions have their own governments who usually work in co-operation with the provincial government.
Alberta also privatized alcohol distribution. The privatization increased outlets from 304 stores to 1,726; 1,300 jobs to 4,000 jobs; and 3,325 products to 16,495 products. Tax revenue also increased from $400 million to $700 million.
Albertan municipalities raise a significant portion of their income through levying property taxes. The value of assessed property in Alberta was approximately $727 billion in 2011. Most real property is assessed according to its market value. The exceptions to market value assessment are farmland, railways, machinery & equipment and linear property, all of which is assessed by regulated rates. Depending on the property type, property owners may appeal a property assessment to their municipal 'Local Assessment Review Board', 'Composite Assessment Review Board,' or the Alberta Municipal Government Board.
Military.
Military bases in Alberta include Canadian Forces Base (CFB) Cold Lake, CFB Edmonton, CFB Suffield and CFB Wainwright. Air force units stationed at CFB Cold Lake have access to the Cold Lake Air Weapons Range. CFB Edmonton is the headquarters for the 3rd Canadian Division. CFB Suffield hosts British troops and is the largest training facility in Canada.
Transportation.
Alberta has over of highways and roads, of which nearly are paved. The main north-south corridor is Highway 2, which begins south of Cardston at the Carway border crossing and is part of the CANAMEX Corridor. Highway 4, which effectively extends Interstate 15 into Alberta and is the busiest US gateway to the province, begins at the Coutts border crossing and ends at Lethbridge. Highway 3 joins Lethbridge to Fort Macleod and links Highway 4 to Highway 2. Highway 2 travels northward through Fort Macleod, Calgary, Red Deer, and Edmonton.
North of Edmonton, the highway continues to Athabasca, then northwesterly along the south shore of Lesser Slave Lake into High Prairie, north to Peace River, west to Fairview and finally south to Grande Prairie, where it ends at an interchange with Highway 43. The section of Highway 2 between Calgary and Edmonton has been named the Queen Elizabeth II Highway to commemorate the visit of the monarch in 2005. Highway 2 is supplemented by two more highways that run parallel to it: Highway 22, west of Highway 2, known as "Cowboy Trail", and Highway 21, east of Highway 2. Highway 43 travels northwest into Grande Prairie and the Peace River Country; Highway 63 travels northeast to Fort McMurray, the location of the Athabasca oil sands.
Alberta has two main east-west corridors. The southern corridor, part of the Trans-Canada Highway system, enters the province near Medicine Hat, runs westward through Calgary, and leaves Alberta through Banff National Park. The northern corridor, also part of the Trans-Canada network and known as the Yellowhead Highway (Highway 16), runs west from Lloydminster in eastern Alberta, through Edmonton and Jasper National Park into British Columbia. One of the most scenic drives is along the Icefields Parkway, which runs for between Jasper and Lake Louise, with mountain ranges and glaciers on either side of its entire length.
Another major corridor through central Alberta is Highway 11 (also known as the David Thompson Highway), which runs east from the Saskatchewan River Crossing in Banff National Park through Rocky Mountain House and Red Deer, connecting with Highway 12 west of Stettler. The highway connects many of the smaller towns in central Alberta with Calgary and Edmonton, as it crosses Highway 2 just west of Red Deer.
Urban stretches of Alberta's major highways and freeways are often called "trails". For example, Highway 2, the main north-south highway in the province, is called Deerfoot Trail as it passes through Calgary but becomes Calgary Trail (for southbound traffic) and Gateway Boulevard (for northbound traffic) as it enters Edmonton and then turns into St. Albert Trail as it leaves Edmonton for the City of St. Albert. Calgary, in particular, has a tradition of calling its largest urban expressways "trails" and naming many of them after prominent First Nations individuals and tribes, such as Crowchild Trail, Deerfoot Trail, and Stoney Trail.
Calgary, Edmonton, Red Deer, Medicine Hat, and Lethbridge have substantial public transit systems. In addition to buses, Calgary and Edmonton operate light rail transit (LRT) systems. Edmonton LRT, which is underground in the downtown core and on the surface outside the CBD, was the first of the modern generation of light rail systems to be built in North America, while the Calgary C-Train has one of the highest number of daily riders of any LRT system in North America.
Alberta is well-connected by air, with international airports in both Calgary and Edmonton. Calgary International Airport and Edmonton International Airport are the third and fifth busiest in Canada respectively. Calgary's airport is a hub for WestJet Airlines and a regional hub for Air Canada. Calgary's airport primarily serves the Canadian prairie provinces (Alberta, Saskatchewan and Manitoba) for connecting flights to British Columbia, eastern Canada, 15 major US centres, nine European airports, one Asian airport and four destinations in Mexico and the Caribbean. Edmonton's airport acts as a hub for the Canadian north and has connections to all major Canadian airports as well as 10 major US airports, 3 European airports and 6 Mexican and Caribbean airports.
There are more than of operating mainline railway; the Canadian Pacific Railway and Canadian National Railway companies operate railway freight across the province. Passenger trains include Via Rail's Canadian (Toronto-Vancouver) or Jasper-Prince Rupert trains, which use the CN mainline and pass through Jasper National Park and parallel the Yellowhead Highway during at least part of their routes. The Rocky Mountaineer operates two sections: one from Vancouver to Banff and Calgary over CP tracks, and a section that travels over CN tracks to Jasper.
Health care.
Alberta provides a publicly funded health care system, Alberta Health Services, for all its citizens and residents as set out by the provisions of the Canada Health Act of 1984. Alberta became Canada's second province (after Saskatchewan) to adopt a Tommy Douglas-style program in 1950, a precursor to the modern medicare system.
Alberta's health care budget is currently $17.1 billion during the 2013–2014 fiscal year (approximately 45% of all government spending), making it the best funded health care system per-capita in Canada. Every hour more than $1.9 million is spent on health care in the province.
Notable health, education, research, and resources facilities in Alberta, all of which are located within Calgary or Edmonton:
The Edmonton Clinic complex, completed in 2012, provides a similar research, education, and care environment as the Mayo Clinic in the United States.
All public health care services funded by the Government of Alberta are delivered operationally by Alberta Health Services. AHS is the province's single health authority established on July 1, 2008, which replaced nine local health authorities. AHS also funds all ground ambulance services in the province, as well as the province-wide STARS (Shock Trauma Air Rescue Society) air ambulance service.
Education.
As with any Canadian province, the Alberta Legislature has (almost) exclusive authority to make laws respecting education. Since 1905 the Legislature has used this capacity to continue the model of locally elected public and separate school boards which originated prior to 1905, as well as to create and regulate universities, colleges, technical institutions and other educational forms and institutions (public charter schools, private schools, home schooling).
Elementary schools.
There are forty-two public school jurisdictions in Alberta, and seventeen operating separate school jurisdictions. Sixteen of the operating separate school jurisdictions have a Catholic electorate, and one (St. Albert) has a Protestant electorate. In addition, one Protestant separate school district, Glen Avon, survives as a ward of the St. Paul Education Region. The City of Lloydminster straddles the Alberta/Saskatchewan border, and both the public and separate school systems in that city are counted in the above numbers: both of them operate according to Saskatchewan law.
For many years the provincial government has funded the greater part of the cost of providing K–12 education. Prior to 1994 public and separate school boards in Alberta had the legislative authority to levy a local tax on property as a supplementary support for local education. In 1994 the government of the province eliminated this right for public school boards, but not for separate school boards. Since 1994 there has continued to be a tax on property in support of K–12 education; the difference is that the mill rate is now set by the provincial government, the money is collected by the local municipal authority and remitted to the provincial government. The relevant legislation requires that all the money raised by this property tax must go to the support of K–12 education provided by school boards. The provincial government pools the property tax funds from across the province and distributes them, according to a formula, to public and separate school jurisdictions and Francophone authorities.
Public and separate school boards, charter schools, and private schools all follow the Program of Studies and the curriculum approved by the provincial department of education (Alberta Education). Homeschool tutors may choose to follow the Program of Studies or develop their own Program of Studies. Public and separate schools, charter schools, and approved private schools all employ teachers who are certificated by Alberta Education, they administer Provincial Achievement Tests and Diploma Examinations set by Alberta Education, and they may grant high school graduation certificates endorsed by Alberta Education.
Universities.
The University of Alberta, located in Edmonton and established in 1908, is Alberta's oldest and largest university. The University of Calgary, once affiliated with the University of Alberta, gained its autonomy in 1966 and is now the second largest university in Alberta. Athabasca University, which focuses on distance learning, and the University of Lethbridge are located in Athabasca and Lethbridge respectively.
In early September 2009, Mount Royal University became Calgary's second public university, and in late September 2009, a similar move made MacEwan University Edmonton's second public university. There are 15 colleges that receive direct public funding, along with two technical institutes, Northern Alberta Institute of Technology and Southern Alberta Institute of Technology.
There is also a large and active private sector of post-secondary institutions, mostly Christian Universities, bringing the total number of universities to twelve, plus a DeVry University in Calgary, the only location in Canada. Students may also receive government loans and grants while attending selected private institutions. There has been some controversy in recent years over the rising cost of post-secondary education for students (as opposed to taxpayers). In 2005, Premier Ralph Klein made a promise that he would freeze tuition and look into ways of reducing schooling costs. So far, no plan has been released by the Government of Alberta.
Culture.
Summer brings many festivals to the province of Alberta, especially in Edmonton. The Edmonton Fringe Festival is the world's second largest after the Edinburgh Festival. Both Calgary and Edmonton host a number of annual festivals and events including folk music festivals. The city's "heritage days" festival sees the participation of over 70 ethnic groups. Edmonton's Churchill Square is home to a large number of the festivals, including the large Taste of Edmonton & The Works Art & Design Festival throughout the summer months.
The City of Calgary is also famous for its Stampede, dubbed "The Greatest Outdoor Show on Earth". The Stampede is Canada's biggest rodeo festival and features various races and competitions, such as calf roping and bull riding. In line with the western tradition of rodeo are the cultural artisans that reside and create unique Alberta western heritage crafts.
The Banff Centre hosts a range of festivals and other events including the international Mountain Film Festival. These cultural events in Alberta highlight the province's cultural diversity. Most of the major cities have several performing theatre companies who entertain in venues as diverse as Edmonton's Arts Barns and the Francis Winspear Centre for Music. Both Calgary and Edmonton are home to Canadian Football League and National Hockey League teams. Soccer, rugby union and lacrosse are also played professionally in Alberta.
Friendship partners.
Alberta has relationships with several provinces, states, and other entities worldwide.

</doc>
<doc id="728" url="https://en.wikipedia.org/wiki?curid=728" title="List of anthropologists">
List of anthropologists


</doc>
<doc id="734" url="https://en.wikipedia.org/wiki?curid=734" title="Actinopterygii">
Actinopterygii

Actinopterygii , or the ray-finned fishes, constitute a class or subclass of the bony fishes.
The ray-finned fishes are so called because they possess lepidotrichia or "fin rays", their fins being webs of skin supported by bony or horny spines ("rays"), as opposed to the fleshy, lobed fins that characterize the class Sarcopterygii which also, however, possess lepidotrichia. These actinopterygian fin rays attach directly to the proximal or basal skeletal elements, the radials, which represent the link or connection between these fins and the internal skeleton (e.g., pelvic and pectoral girdles).
Numerically, actinopterygians are the dominant class of vertebrates, comprising nearly 99% of the over 30,000 species of fish. They are ubiquitous throughout freshwater and marine environments from the deep sea to the highest mountain streams. Extant species can range in size from "Paedocypris", at , to the massive ocean sunfish, at , and the long-bodied oarfish, at .
Characteristics.
Ray-finned fishes occur in many variant forms. The main features of a typical ray-finned fish are shown in the diagram at the left.
Reproduction.
In nearly all ray-finned fish, the sexes are separate, and in most species the females spawn eggs that are fertilized externally, typically with the male inseminating the eggs after they are laid. Development then proceeds with a free-swimming larval stage. However other patterns of ontogeny exist, with one of the commonest being sequential hermaphroditism. In most cases this involves protogyny, fish starting life as females and transitioning to males at some stage, triggered by some internal or external factor. This may be advantageous as females become less prolific as they age while male fecundity increases with age. Protandry, where a fish transitions from male to female, is much less common than protogyny.
Most families use external rather than internal fertilization. Of the oviparous teleosts, most (79%) do not provide parental care. Viviparity, ovoviviparity, or some form of parental care for eggs, whether by the male, the female, or both parents is seen in a significant fraction (21%) of the 422 teleost families; no care is likely the ancestral condition. Viviparity is relatively rare and is found in about 6% of teleost species; male care is far more common than female care. Male territoriality "preadapts" a species for evolving male parental care.
There are a few examples of fish that self-fertilise. The mangrove rivulus is an amphibious, simultaneous hermaphrodite, producing both eggs and spawn and having internal fertilisation. This mode of reproduction may be related to the fish's habit of spending long periods out of water in the mangrove forests it inhabits. Males are occasionally produced at temperatures below and can fertilise eggs that are then spawned by the female. This maintains genetic variability in a species that is otherwise highly inbred.
Fossil record.
The earliest known fossil actinopterygiian is "Andreolepis hedei", dating back 420 million years (Late Silurian). Remains have been found in Russia, Sweden, and Estonia.
Classification.
Actinopterygians are divided into the subclasses Chondrostei and Neopterygii. The Neopterygii, in turn, are divided into the infraclasses Holostei and Teleostei. During the Mesozoic and Cenozoic the teleosts in particular diversified widely, and as a result, 96% of all known fish species are teleosts. The cladogram shows the major groups of actinopterygians and their relationship to the terrestrial vertebrates (tetrapods) that evolved from a related group of fish. Approximate dates are from Near et al., 2012.
The polypterids (bichirs and ropefish) are the sister lineage of all other actinopterygians, The Acipenseriformes (sturgeons and paddlefishes) are the sister lineage of Neopterygii, and Holostei (bowfin and gars) are the sister lineage of teleosts. The Elopomorpha (eels and tarpons) appears to be the most basic teleosts.
The listing below follows Phylogenetic Classification of Bony Fishes with notes when this differs from Nelson, ITIS and FishBase.

</doc>
<doc id="736" url="https://en.wikipedia.org/wiki?curid=736" title="Albert Einstein">
Albert Einstein

Albert Einstein (; ; 14 March 1879 – 18 April 1955) was a German-born theoretical physicist. He developed the general theory of relativity, one of the two pillars of modern physics (alongside quantum mechanics). Einstein's work is also known for its influence on the philosophy of science. Einstein is best known in popular culture for his mass–energy equivalence formula (which has been dubbed "the world's most famous equation"). He received the 1921 Nobel Prize in Physics for his "services to theoretical physics", in particular his discovery of the law of the photoelectric effect, a pivotal step in the evolution of quantum theory. 
Near the beginning of his career, Einstein thought that Newtonian mechanics was no longer enough to reconcile the laws of classical mechanics with the laws of the electromagnetic field. This led to the development of his special theory of relativity. He realized, however, that the principle of relativity could also be extended to gravitational fields, and with his subsequent theory of gravitation in 1916, he published a paper on general relativity. He continued to deal with problems of statistical mechanics and quantum theory, which led to his explanations of particle theory and the motion of molecules. He also investigated the thermal properties of light which laid the foundation of the photon theory of light. In 1917, Einstein applied the general theory of relativity to model the large-scale structure of the universe.
He was visiting the United States when Adolf Hitler came to power in 1933 and, being Jewish, did not go back to Germany, where he had been a professor at the Berlin Academy of Sciences. He settled in the U.S., becoming an American citizen in 1940. On the eve of World War II, he endorsed a letter to President Franklin D. Roosevelt alerting him to the potential development of "extremely powerful bombs of a new type" and recommending that the U.S. begin similar research. This eventually led to what would become the Manhattan Project. Einstein supported defending the Allied forces, but largely denounced the idea of using the newly discovered nuclear fission as a weapon. Later, with the British philosopher Bertrand Russell, Einstein signed the Russell–Einstein Manifesto, which highlighted the danger of nuclear weapons. Einstein was affiliated with the Institute for Advanced Study in Princeton, New Jersey, until his death in 1955.
Einstein published more than 300 scientific papers along with over 150 non-scientific works. On 5 December 2014, universities and archives announced the release of Einstein's papers, comprising more than 30,000 unique documents. Einstein's intellectual achievements and originality have made the word "Einstein" synonymous with "genius".
Biography.
Early life and education.
Albert Einstein was born in Ulm, in the Kingdom of Württemberg in the German Empire on 14 March 1879. His parents were Hermann Einstein, a salesman and engineer, and Pauline Koch. In 1880, the family moved to Munich, where Einstein's father and his uncle Jakob founded "Elektrotechnische Fabrik J. Einstein & Cie", a company that manufactured electrical equipment based on direct current.
The Einsteins were non-observant Ashkenazi Jews, and Albert attended a Catholic elementary school from the age of 5 for three years. At the age of 8, he was transferred to the Luitpold Gymnasium (now known as the Albert Einstein Gymnasium), where he received advanced primary and secondary school education until he left Germany seven years later.
In 1894, Hermann and Jakob's company lost a bid to supply the city of Munich with electrical lighting because they lacked the capital to convert their equipment from the direct current (DC) standard to the more efficient alternating current (AC) standard. The loss forced the sale of the Munich factory. In search of business, the Einstein family moved to Italy, first to Milan and a few months later to Pavia. When the family moved to Pavia, Einstein stayed in Munich to finish his studies at the Luitpold Gymnasium. His father intended for him to pursue electrical engineering, but Einstein clashed with authorities and resented the school's regimen and teaching method. He later wrote that the spirit of learning and creative thought was lost in strict rote learning. At the end of December 1894, he travelled to Italy to join his family in Pavia, convincing the school to let him go by using a doctor's note. During his time in Italy he wrote a short essay with the title "On the Investigation of the State of the Ether in a Magnetic Field".
In 1895, at the age of 16, Einstein sat the entrance examinations for the Swiss Federal Polytechnic in Zürich (later the Eidgenössische Technische Hochschule, ETH). He failed to reach the required standard in the general part of the examination, but obtained exceptional grades in physics and mathematics. On the advice of the principal of the Polytechnic, he attended the Argovian cantonal school (gymnasium) in Aarau, Switzerland, in 1895–96 to complete his secondary schooling. While lodging with the family of Professor Jost Winteler, he fell in love with Winteler's daughter, Marie. (Albert's sister Maja later married Wintelers' son Paul.) In January 1896, with his father's approval, Einstein renounced his citizenship in the German Kingdom of Württemberg to avoid military service. In September 1896, he passed the Swiss Matura with mostly good grades, including a top grade of 6 in physics and mathematical subjects, on a scale of 1–6. Though only 17, he enrolled in the four-year mathematics and physics teaching diploma program at the Zürich Polytechnic. Marie Winteler moved to Olsberg, Switzerland, for a teaching post.
Einstein's future wife, Mileva Marić, also enrolled at the Polytechnic that year. She was the only woman among the six students in the mathematics and physics section of the teaching diploma course. Over the next few years, Einstein and Marić's friendship developed into romance, and they read books together on extra-curricular physics in which Einstein was taking an increasing interest. In 1900, Einstein was awarded the Zürich Polytechnic teaching diploma, but Marić failed the examination with a poor grade in the mathematics component, theory of functions. There have been claims that Marić collaborated with Einstein on his celebrated 1905 papers, but historians of physics who have studied the issue find no evidence that she made any substantive contributions.
Marriages and children.
The discovery and publication in 1987 of an early correspondence between Einstein and Marić revealed that they had had a daughter, called "Lieserl" in their letters, born in early 1902 in Novi Sad where Marić was staying with her parents. Marić returned to Switzerland without the child, whose real name and fate are unknown. Einstein probably never saw his daughter. The contents of his letter to Marić in September 1903 suggest that the girl was either adopted or died of scarlet fever in infancy.
Einstein and Marić married in January 1903. In May 1904, their first son, Hans Albert Einstein, was born in Bern, Switzerland. Their second son, Eduard, was born in Zürich in July 1910. In 1914, the couple separated; Einstein moved to Berlin and his wife remained in Zürich with their sons. They divorced on 14 February 1919, having lived apart for five years. Eduard, whom his father called "Tete" (for "petit"), had a breakdown at about age 20 and was diagnosed with schizophrenia. His mother cared for him and he was also committed to asylums for several periods, including full-time after her death.
In letters revealed in 2015, Einstein wrote to his early love, Marie Winteler, about his marriage and his still-strong feelings for Marie. In 1910 he wrote to her that "I think of you in heartfelt love every spare minute and am so unhappy as only a man can be" while his wife was pregnant with their second child. Einstein spoke about a "misguided love" and a "missed life" regarding his love for Marie.
Einstein married Elsa Löwenthal on 2 June 1919, after having had a relationship with her since 1912. She was a first cousin maternally and a second cousin paternally. In 1933, they emigrated to the United States. In 1935, Elsa Einstein was diagnosed with heart and kidney problems; she died in December 1936.
Patent office.
After graduating, Einstein spent almost two frustrating years searching for a teaching post. He acquired Swiss citizenship in February 1901, but was not conscripted for medical reasons. With the help of Marcel Grossmann's father, Einstein secured a job in Bern at the Federal Office for Intellectual Property, the patent office, as an assistant examiner. He evaluated patent applications for a variety of devices including a gravel sorter and an electromechanical typewriter. In 1903, Einstein's position at the Swiss Patent Office became permanent, although he was passed over for promotion until he "fully mastered machine technology".
Much of his work at the patent office related to questions about transmission of electric signals and electrical-mechanical synchronization of time, two technical problems that show up conspicuously in the thought experiments that eventually led Einstein to his radical conclusions about the nature of light and the fundamental connection between space and time.
With a few friends he had met in Bern, Einstein started a small discussion group, self-mockingly named "The Olympia Academy", which met regularly to discuss science and philosophy. Their readings included the works of Henri Poincaré, Ernst Mach, and David Hume, which influenced his scientific and philosophical outlook.
Academic career.
In 1900, Einstein's paper "Folgerungen aus den Capillaritätserscheinungen" ("Conclusions from the Capillarity Phenomena") was published in the prestigious "Annalen der Physik". On 30 April 1905, Einstein completed his thesis, with Alfred Kleiner, Professor of Experimental Physics, serving as "" advisor. As a result, Einstein was awarded a PhD by the University of Zürich, with his dissertation entitled, "A New Determination of Molecular Dimensions." That same year, which has been called Einstein's "annus mirabilis" (miracle year), he published four groundbreaking papers, on the photoelectric effect, Brownian motion, special relativity, and the equivalence of mass and energy, which were to bring him to the notice of the academic world, at the age of 26.
By 1908, he was recognized as a leading scientist and was appointed lecturer at the University of Bern. The following year, after giving a lecture on electrodynamics and the relativity principle at the University of Zurich, Alfred Kleiner recommended him to the faculty for a newly created professorship in theoretical physics. Einstein was appointed associate professor in 1909.
Einstein became a full professor at the German Charles-Ferdinand University in Prague in April 1911, accepting Austrian citizenship in the Austro-Hungarian empire to do so. During his Prague stay Einstein wrote 11 scientific works, 5 of them on radiation mathematics and on quantum theory of the solids. In July 1912 he returned to his alma mater in Zürich. From 1912 until 1914 he was professor of theoretical physics at the ETH Zurich, where he taught analytical mechanics and thermodynamics. He also studied continuum mechanics, the molecular theory of heat, and the problem of gravitation, on which he worked with mathematician and his friend Marcel Grossmann.
In 1914, he returned to the German Empire after being appointed director of the Kaiser Wilhelm Institute for Physics (1914–1932) and a professor at the Humboldt University of Berlin, but freed from most teaching obligations. He soon became a member of the Prussian Academy of Sciences, and in 1916 was appointed president of the German Physical Society (1916–1918).
Based on calculations Einstein made in 1911, about his new theory of general relativity, light from another star would be bent by the Sun's gravity. In 1919 that prediction was confirmed by Sir Arthur Eddington during the solar eclipse of 29 May 1919. Those observations were published in the international media, making Einstein world famous. On 7 November 1919, the leading British newspaper "The Times" printed a banner headline that read: "Revolution in Science – New Theory of the Universe – Newtonian Ideas Overthrown".
In 1920, he became Foreign Member of the Royal Netherlands Academy of Arts and Sciences. In 1921, Einstein was awarded the Nobel Prize in Physics "for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect". While General Theory of Relativity was still considered somewhat controversial, the citation also does not treat the cited work as an "explanation" but merely as a "discovery of the law", as the idea of photons was considered outlandish and did not receive universal acceptance until the 1924 derivation of the Planck spectrum by S. N. Bose. Einstein was elected a Foreign Member of the Royal Society (ForMemRS) in 1921. He also received the Copley Medal from the Royal Society in 1925.
1921–1922: Travels abroad.
Einstein visited New York City for the first time on 2 April 1921, where he received an official welcome by Mayor John Francis Hylan, followed by three weeks of lectures and receptions. He went on to deliver several lectures at Columbia University and Princeton University, and in Washington he accompanied representatives of the National Academy of Science on a visit to the White House. On his return to Europe he was the guest of the British statesman and philosopher Viscount Haldane in London, where he met several renowned scientific, intellectual and political figures, and delivered a lecture at King's College London. 
He also published an essay, "My First Impression of the U.S.A.," in July 1921, in which he tried briefly to describe some characteristics of Americans, much as had Alexis de Tocqueville, who published his own impressions in "Democracy in America" (1835). For some of his observations, Einstein was clearly surprised: "What strikes a visitor is the joyous, positive attitude to life . . . The American is friendly, self-confident, optimistic, and without envy."
In 1922, his travels took him to Asia and later to Palestine, as part of a six-month excursion and speaking tour, as he visited Singapore, Ceylon and Japan, where he gave a series of lectures to thousands of Japanese. After his first public lecture, he met the emperor and empress at the Imperial Palace, where thousands came to watch. In a letter to his sons, Einstein described his impression of the Japanese as being modest, intelligent, considerate, and having a true feel for art.
Because of Einstein's travels to the Far East, he was unable to personally accept the Nobel Prize for Physics at the Stockholm award ceremony in December 1922. In his place, the banquet speech was held by a German diplomat, who praised Einstein not only as a scientist but also as an international peacemaker and activist.
On his return voyage, he visited Palestine for 12 days in what would become his only visit to that region. Einstein was greeted as if he were a head of state, rather than a physicist, which included a cannon salute upon arriving at the home of the British high commissioner, Sir Herbert Samuel. During one reception, the building was stormed by people who wanted to see and hear him. In Einstein's talk to the audience, he expressed happiness that the Jewish people were beginning to be recognized as a force in the world.
1930–1931: Travel to U.S..
In December 1930, Einstein visited America for the second time, originally intended as a two-month working visit as a research fellow at the California Institute of Technology. After the national attention he received during his first trip to the U.S., he and his arrangers aimed to protect his privacy. Although swamped with telegrams and invitations to receive awards or speak publicly, he declined them all.
After arriving in New York City, Einstein was taken to various places and events, including Chinatown, a lunch with the editors of the "New York Times", and a performance of "Carmen" at the Metropolitan Opera, where he was cheered by the audience on his arrival. During the days following, he was given the keys to the city by Mayor Jimmy Walker and met the president of Columbia University, who described Einstein as "the ruling monarch of the mind." Harry Emerson Fosdick, pastor at New York's Riverside Church, gave Einstein a tour of the church and showed him a full-size statue that the church made of Einstein, standing at the entrance. Also during his stay in New York, he joined a crowd of 15,000 people at Madison Square Garden during a Hanukkah celebration.
Einstein next traveled to California where he met Caltech president and Nobel laureate, Robert A. Millikan. His friendship with Millikan was "awkward", as Millikan "had a penchant for patriotic militarism," where Einstein was a pronounced pacifist. During an address to Caltech's students, Einstein noted that science was often inclined to do more harm than good.
This aversion to war also led Einstein to befriend author Upton Sinclair and film star Charlie Chaplin, both noted for their pacifism. Carl Laemmle, head of Universal Studios, gave Einstein a tour of his studio and introduced him to Chaplin. They had an instant rapport, with Chaplin inviting Einstein and his wife, Elsa, to his home for dinner. Chaplin said Einstein's outward persona, calm and gentle, seemed to conceal a "highly emotional temperament," from which came his "extraordinary intellectual energy."
Chaplin also remembers Elsa telling him about the time Einstein conceived his theory of relativity. During breakfast one morning, he seemed lost in thought and ignored his food. She asked him if something was bothering him. He sat down at his piano and started playing. He continued playing and writing notes for half an hour, then went upstairs to his study, where he remained for two weeks, with Elsa bringing up his food. At the end of the two weeks he came downstairs with two sheets of paper bearing his theory.
Chaplin's film, "City Lights", was to premier a few days later in Hollywood, and Chaplin invited Einstein and Elsa to join him as his special guests. Walter Isaacson, Einstein's biographer, described this as "one of the most memorable scenes in the new era of celebrity." Einstein and Chaplin arrived together, in black tie, with Elsa joining them, "beaming." The audience applauded as they entered the theater. Chaplin visited Einstein at his home on a later trip to Berlin, and recalled his "modest little flat" and the piano at which he had begun writing his theory. Chaplin speculated that it was "possibly used as kindling wood by the Nazis."
1933: Emigration to the U.S..
In February 1933 while on a visit to the United States, Einstein knew he could not return to Germany with the rise to power of the Nazis under Germany's new chancellor, Adolf Hitler.
While at American universities in early 1933, he undertook his third two-month visiting professorship at the California Institute of Technology in Pasadena. He and his wife Elsa returned to Belgium by ship in March, and during the trip they learned that their cottage was raided by the Nazis and his personal sailboat confiscated. Upon landing in Antwerp on 28 March, he immediately went to the German consulate and turned in his passport, formally renouncing his German citizenship. A few years later, the Nazis sold his boat and turned his cottage into a Hitler Youth camp.
Refugee status.
In April 1933, Einstein discovered that the new German government had passed laws barring Jews from holding any official positions, including teaching at universities. Historian Gerald Holton describes how, with "virtually no audible protest being raised by their colleagues," thousands of Jewish scientists were suddenly forced to give up their university positions and their names were removed from the rolls of institutions where they were employed.
A month later, Einstein's works were among those targeted by Nazi book burnings, with Nazi propaganda minister Joseph Goebbels proclaiming, "Jewish intellectualism is dead." One German magazine included him in a list of enemies of the German regime with the phrase, "not yet hanged", offering a $5,000 bounty on his head. In a subsequent letter to physicist and friend Max Born, who had already emigrated from Germany to England, Einstein wrote, "... I must confess that the degree of their brutality and cowardice came as something of a surprise." After moving to the U.S., he described the book burnings as a "spontaneous emotional outburst" by those who "shun popular enlightenment," and "more than anything else in the world, fear the influence of men of intellectual independence."
Einstein was now without a permanent home, unsure where he would live and work, and equally worried about the fate of countless other scientists still in Germany. He rented a house in De Haan, Belgium, where he lived for a few months. In late July 1933, he went to England for about six weeks at the personal invitation of British naval officer Commander Oliver Locker-Lampson, who had become friends with Einstein in the preceding years. To protect Einstein, Locker-Lampson had two assistants watch over him at his secluded cottage outside London, with the press publishing a photo of them guarding Einstein.
Locker-Lampson took Einstein to meet Winston Churchill at his home, and later, Austen Chamberlain and former Prime Minister Lloyd George. Einstein asked them to help bring Jewish scientists out of Germany. British historian Martin Gilbert notes that Churchill responded immediately, and sent his friend, physicist Frederick Lindemann to Germany to seek out Jewish scientists and place them in British universities. Churchill later observed that as a result of Germany having driven the Jews out, they had lowered their "technical standards" and put the Allies' technology ahead of theirs.
Einstein later contacted leaders of other nations, including Turkey's Prime Minister, İsmet İnönü, to whom he wrote in September 1933 requesting placement of unemployed German-Jewish scientists. As a result of Einstein's letter, Jewish invitees to Turkey eventually totaled over "1,000 saved individuals."
Locker-Lampson also submitted a bill to parliament to extend British citizenship to Einstein, during which period Einstein made a number of public appearances describing the crisis brewing in Europe. The bill failed to become law, however, and Einstein then accepted an earlier offer from the Princeton Institute for Advanced Study, in the U.S., to become a resident scholar.
Resident scholar at the Institute for Advanced Study.
In October 1933 Einstein returned to the U.S. and took up a position at the Institute for Advanced Study (in Princeton, New Jersey), noted for having become a refuge for scientists fleeing Nazi Germany. At the time, most American universities, including Harvard, Princeton and Yale, had minimal or no Jewish faculty or students, as a result of their Jewish quota which lasted until the late 1940s.
Einstein was still undecided on his future. He had offers from several European universities, including Christ Church, Oxford where he stayed for three short periods between May 1931 and June 1933 and was offered a 5 year Studentship, but in 1935 he arrived at the decision to remain permanently in the United States and apply for citizenship.
Einstein's affiliation with the Institute for Advanced Study would last until his death in 1955.
He was one of the four first selected (two of the others being John von Neumann and Kurt Gödel) at the new Institute, where he soon developed a close friendship with Gödel. The two would take long walks together discussing their work. Bruria Kaufman, his assistant, later became a physicist. During this period, Einstein tried to develop a unified field theory and to refute the accepted interpretation of quantum physics, both unsuccessfully.
World War II and the Manhattan Project.
In 1939, a group of Hungarian scientists that included émigré physicist Leó Szilárd attempted to alert Washington to ongoing Nazi atomic bomb research. The group's warnings were discounted. Einstein and Szilárd, along with other refugees such as Edward Teller and Eugene Wigner, "regarded it as their responsibility to alert Americans to the possibility that German scientists might win the race to build an atomic bomb, and to warn that Hitler would be more than willing to resort to such a weapon."
To make certain the U.S. was aware of the danger, in July 1939, a few months before the beginning of World War II in Europe, Szilárd and Wigner visited Einstein to explain the possibility of atomic bombs, which Einstein, a pacifist, said he had never considered. He was asked to lend his support by writing a letter, with Szilárd, to President Roosevelt, recommending the U.S. pay attention and engage in its own nuclear weapons research. A secret German facility, apparently the largest of the Third Reich, covering 75 acres in an underground complex, was being re-excavated in Austria in December 2014 and may have been planned for use in nuclear research and development.
The letter is believed to be "arguably the key stimulus for the U.S. adoption of serious investigations into nuclear weapons on the eve of the U.S. entry into World War II". In addition to the letter, Einstein used his connections with the Belgian Royal Family and the Belgian queen mother to get access with a personal envoy to the White House's Oval Office. President Roosevelt could not take the risk of allowing Hitler to possess atomic bombs first. As a result of Einstein's letter and his meetings with Roosevelt, the U.S. entered the "race" to develop the bomb, drawing on its "immense material, financial, and scientific resources" to initiate the Manhattan Project. It became the only country to successfully develop an atomic bomb during World War II.
For Einstein, "war was a disease ... n he called for resistance to war." By signing the letter to Roosevelt he went against his pacifist principles. In 1954, a year before his death, Einstein said to his old friend, Linus Pauling, "I made one great mistake in my life—when I signed the letter to President Roosevelt recommending that atom bombs be made; but there was some justification—the danger that the Germans would make them ..."
U.S. citizenship.
Einstein became an American citizen in 1940. Not long after settling into his career at the Institute for Advanced Study (in Princeton, New Jersey), he expressed his appreciation of the meritocracy in American culture when compared to Europe. He recognized the "right of individuals to say and think what they pleased", without social barriers, and as a result, individuals were encouraged, he said, to be more creative, a trait he valued from his own early education.
Personal life.
Supporter of civil rights.
Einstein was a passionate, committed antiracist and joined National Association for the Advancement of Colored People (NAACP) in Princeton, where he campaigned for the civil rights of African Americans. He considered racism America's "worst disease," seeing it as "handed down from one generation to the next." As part of his involvement, he corresponded with civil rights activist W. E. B. Du Bois and was prepared to testify on his behalf during his trial in 1951. When Einstein offered to be a character witness for Du Bois, the judge decided to drop the case.
In 1946 Einstein visited Lincoln University in Pennsylvania where he was awarded an honorary degree. Lincoln was the first university in the United States to grant college degrees to blacks, including Langston Hughes and Thurgood Marshall. To its students, Einstein gave a speech about racism in America, adding, "I do not intend to be quiet about it." A resident of Princeton recalls that Einstein had once paid the college tuition for a black student, and black physicist Sylvester James Gates states that Einstein had been one of his early science heroes, later finding out about Einstein's support for civil rights.
Assisting Zionist causes.
Einstein was a figurehead leader in helping establish the Hebrew University of Jerusalem, which opened in 1925, and was among its first Board of Governors. Earlier, in 1921, he was asked by the biochemist and president of the World Zionist Organization, Chaim Weizmann, to help raise funds for the planned university. He also submitted various suggestions as to its initial programs.
Among those, he advised first creating an Institute of Agriculture in order to settle the undeveloped land. That should be followed, he suggested, by a Chemical Institute and an Institute of Microbiology, to fight the various ongoing epidemics such as malaria, which he called an "evil" that was undermining a third of the country's development. Establishing an Oriental Studies Institute, to include language courses given in both Hebrew and Arabic, for scientific exploration of the country and its historical monuments, was also important.
Chaim Weizmann later became Israel's first president. Upon his death while in office in November 1952 and at the urging of Ezriel Carlebach, Prime Minister David Ben-Gurion offered Einstein the position of President of Israel, a mostly ceremonial post. The offer was presented by Israel's ambassador in Washington, Abba Eban, who explained that the offer "embodies the deepest respect which the Jewish people can repose in any of its sons". Einstein declined, and wrote in his response that he was "deeply moved", and "at once saddened and ashamed" that he could not accept it.
Love of music.
Einstein developed an appreciation of music at an early age. His mother played the piano reasonably well and wanted her son to learn the violin, not only to instill in him a love of music but also to help him assimilate into German culture. According to conductor Leon Botstein, Einstein is said to have begun playing when he was 5, although he did not enjoy it at that age.
When he turned 13 he discovered the violin sonatas of Mozart, whereupon "Einstein fell in love" with Mozart's music and studied music more willingly. He taught himself to play without "ever practicing systematically", he said, deciding that "love is a better teacher than a sense of duty." At age 17, he was heard by a school examiner in Aarau as he played Beethoven's violin sonatas, the examiner stating afterward that his playing was "remarkable and revealing of 'great insight'." What struck the examiner, writes Botstein, was that Einstein "displayed a deep love of the music, a quality that was and remains in short supply. Music possessed an unusual meaning for this student."
Music took on a pivotal and permanent role in Einstein's life from that period on. Although the idea of becoming a professional himself was not on his mind at any time, among those with whom Einstein played chamber music were a few professionals, and he performed for private audiences and friends. Chamber music had also become a regular part of his social life while living in Bern, Zürich, and Berlin, where he played with Max Planck and his son, among others. He is sometimes erroneously credited as the editor of the 1937 edition of the Köchel catalogue of Mozart's work; that edition was actually prepared by Alfred Einstein, who may have been a distant relation.
In 1931, while engaged in research at the California Institute of Technology, he visited the Zoellner family conservatory in Los Angeles, where he played some of Beethoven and Mozart's works with members of the Zoellner Quartet. Near the end of his life, when the young Juilliard Quartet visited him in Princeton, he played his violin with them, and the quartet was "impressed by Einstein's level of coordination and intonation."
Political and religious views.
Einstein's political view was in favor of socialism and critical of capitalism, which he detailed in his essays such as "Why Socialism?". Einstein offered and was called on to give judgments and opinions on matters often unrelated to theoretical physics or mathematics. He strongly advocated the idea of a democratic global government that would check the power of nation-states in the framework of a world federation.
Einstein's views about religious belief have been collected from interviews and original writings. He called himself an agnostic, while disassociating himself from the label atheist. He said he believed in the "pantheistic" God of Baruch Spinoza, but not in a personal god, a belief he criticized. Einstein once wrote: "I do not believe in a personal God and I have never denied this but expressed it clearly".
Death.
On 17 April 1955, Albert Einstein experienced internal bleeding caused by the rupture of an abdominal aortic aneurysm, which had previously been reinforced surgically by Rudolph Nissen in 1948. He took the draft of a speech he was preparing for a television appearance commemorating the State of Israel's seventh anniversary with him to the hospital, but he did not live long enough to complete it.
Einstein refused surgery, saying: "I want to go when I want. It is tasteless to prolong life artificially. I have done my share, it is time to go. I will do it elegantly." He died in Princeton Hospital early the next morning at the age of 76, having continued to work until near the end.
During the autopsy, the pathologist of Princeton Hospital, Thomas Stoltz Harvey, removed Einstein's brain for preservation without the permission of his family, in the hope that the neuroscience of the future would be able to discover what made Einstein so intelligent. Einstein's remains were cremated and his ashes were scattered at an undisclosed location.
In his lecture at Einstein's memorial, nuclear physicist Robert Oppenheimer summarized his impression of him as a person: "He was almost wholly without sophistication and wholly without worldliness ... There was always with him a wonderful purity at once childlike and profoundly stubborn."
Scientific career.
Throughout his life, Einstein published hundreds of books and articles. He published more than 300 scientific papers and 150 non-scientific ones. On 5 December 2014, universities and archives announced the release of Einstein's papers, comprising more than 30,000 unique documents. Einstein's intellectual achievements and originality have made the word "Einstein" synonymous with "genius". In addition to the work he did by himself he also collaborated with other scientists on additional projects including the Bose–Einstein statistics, the Einstein refrigerator and others.
1905 – Annus Mirabilis papers.
The "Annus Mirabilis" papers are four articles pertaining to the photoelectric effect (which gave rise to quantum theory), Brownian motion, the special theory of relativity, and E = mc that Albert Einstein published in the "Annalen der Physik" scientific journal in 1905. These four works contributed substantially to the foundation of modern physics and changed views on space, time, and matter. The four papers are:
Thermodynamic fluctuations and statistical physics.
Albert Einstein's first paper submitted in 1900 to "Annalen der Physik" was on capillary attraction. It was published in 1901 with the title "Folgerungen aus den Capillaritätserscheinungen", which translates as "Conclusions from the capillarity phenomena". Two papers he published in 1902–1903 (thermodynamics) attempted to interpret atomic phenomena from a statistical point of view. These papers were the foundation for the 1905 paper on Brownian motion, which showed that Brownian movement can be construed as firm evidence that molecules exist. His research in 1903 and 1904 was mainly concerned with the effect of finite atomic size on diffusion phenomena.
General principles.
He articulated the principle of relativity. This was understood by Hermann Minkowski to be a generalization of rotational invariance from space to space-time. Other principles postulated by Einstein and later vindicated are the principle of equivalence and the principle of adiabatic invariance of the quantum number.
Theory of relativity and "E" = "mc"².
Einstein's "Zur Elektrodynamik bewegter Körper" ("On the Electrodynamics of Moving Bodies") was received on 30 June 1905 and published 26 September of that same year. It reconciles Maxwell's equations for electricity and magnetism with the laws of mechanics, by introducing major changes to mechanics close to the speed of light. This later became known as Einstein's special theory of relativity.
Consequences of this include the time-space frame of a moving body appearing to slow down and contract (in the direction of motion) when measured in the frame of the observer. This paper also argued that the idea of a luminiferous aether—one of the leading theoretical entities in physics at the time—was superfluous.
In his paper on mass–energy equivalence, Einstein produced "E" = "mc" from his special relativity equations. Einstein's 1905 work on relativity remained controversial for many years, but was accepted by leading physicists, starting with Max Planck.
Photons and energy quanta.
In a 1905 paper, Einstein postulated that light itself consists of localized particles ("quanta"). Einstein's light quanta were nearly universally rejected by all physicists, including Max Planck and Niels Bohr. This idea only became universally accepted in 1919, with Robert Millikan's detailed experiments on the photoelectric effect, and with the measurement of Compton scattering.
Einstein concluded that each wave of frequency "f" is associated with a collection of photons with energy "hf" each, where "h" is Planck's constant. He does not say much more, because he is not sure how the particles are related to the wave. But he does suggest that this idea would explain certain experimental results, notably the photoelectric effect.
Quantized atomic vibrations.
In 1907, Einstein proposed a model of matter where each atom in a lattice structure is an independent harmonic oscillator. In the Einstein model, each atom oscillates independently—a series of equally spaced quantized states for each oscillator. Einstein was aware that getting the frequency of the actual oscillations would be different, but he nevertheless proposed this theory because it was a particularly clear demonstration that quantum mechanics could solve the specific heat problem in classical mechanics. Peter Debye refined this model.
Adiabatic principle and action-angle variables.
Throughout the 1910s, quantum mechanics expanded in scope to cover many different systems. After Ernest Rutherford discovered the nucleus and proposed that electrons orbit like planets, Niels Bohr was able to show that the same quantum mechanical postulates introduced by Planck and developed by Einstein would explain the discrete motion of electrons in atoms, and the periodic table of the elements.
Einstein contributed to these developments by linking them with the 1898 arguments Wilhelm Wien had made. Wien had shown that the hypothesis of adiabatic invariance of a thermal equilibrium state allows all the blackbody curves at different temperature to be derived from one another by a simple shifting process. Einstein noted in 1911 that the same adiabatic principle shows that the quantity which is quantized in any mechanical motion must be an adiabatic invariant. Arnold Sommerfeld identified this adiabatic invariant as the action variable of classical mechanics.
Wave–particle duality.
Although the patent office promoted Einstein to Technical Examiner Second Class in 1906, he had not given up on academia. In 1908, he became a "Privatdozent" at the University of Bern.
In "über die Entwicklung unserer Anschauungen über das Wesen und die Konstitution der Strahlung" (""), on the quantization of light, and in an earlier 1909 paper, Einstein showed that Max Planck's energy quanta must have well-defined momenta and act in some respects as independent, point-like particles. This paper introduced the "photon" concept (although the name "photon" was introduced later by Gilbert N. Lewis in 1926) and inspired the notion of wave–particle duality in quantum mechanics. Einstein saw this wave-particle duality in radiation as concrete evidence for his conviction that physics needed a new, unified foundation.
Theory of critical opalescence.
Einstein returned to the problem of thermodynamic fluctuations, giving a treatment of the density variations in a fluid at its critical point. Ordinarily the density fluctuations are controlled by the second derivative of the free energy with respect to the density. At the critical point, this derivative is zero, leading to large fluctuations. The effect of density fluctuations is that light of all wavelengths is scattered, making the fluid look milky white. Einstein relates this to Rayleigh scattering, which is what happens when the fluctuation size is much smaller than the wavelength, and which explains why the sky is blue. Einstein quantitatively derived critical opalescence from a treatment of density fluctuations, and demonstrated how both the effect and Rayleigh scattering originate from the atomistic constitution of matter.
Zero-point energy.
In a series of works completed from 1911 to 1913, Planck reformulated his 1900 quantum theory and introduced the idea of zero-point energy in his "second quantum theory." Soon, this idea attracted the attention of Albert Einstein and his assistant Otto Stern. Assuming the energy of rotating diatomic molecules contains zero-point energy, they then compared the theoretical specific heat of hydrogen gas with the experimental data. The numbers matched nicely. However, after publishing the findings, they promptly withdrew their support, because they no longer had confidence in the correctness of the idea of zero-point energy.
General relativity and the equivalence principle.
General relativity (GR) is a theory of gravitation that was developed by Albert Einstein between 1907 and 1915. According to general relativity, the observed gravitational attraction between masses results from the warping of space and time by those masses. General relativity has developed into an essential tool in modern astrophysics. It provides the foundation for the current understanding of black holes, regions of space where gravitational attraction is so strong that not even light can escape.
As Albert Einstein later said, the reason for the development of general relativity was that the preference of inertial motions within special relativity was unsatisfactory, while a theory which from the outset prefers no state of motion (even accelerated ones) should appear more satisfactory. Consequently, in 1907 he published an article on acceleration under special relativity. In that article titled "On the Relativity Principle and the Conclusions Drawn from It", he argued that free fall is really inertial motion, and that for a free-falling observer the rules of special relativity must apply. This argument is called the equivalence principle. In the same article, Einstein also predicted the phenomena of gravitational time dilation, gravitational red shift and deflection of light.
In 1911, Einstein published another article "On the Influence of Gravitation on the Propagation of Light" expanding on the 1907 article, in which he estimated the amount of deflection of light by massive bodies. Thus, the theoretical prediction of general relativity can for the first time be tested experimentally.
Gravitational waves.
In 1916, Einstein predicted gravitational waves, ripples in the curvature of spacetime which propagate as waves, traveling outward from the source, transporting energy as gravitational radiation. The existence of gravitational waves is possible under general relativity due to its Lorentz invariance which brings the concept of a finite speed of propagation of the physical interactions of gravity with it. By contrast, gravitational waves cannot exist in the Newtonian theory of gravitation, which postulates that the physical interactions of gravity propagate at infinite speed.
The first, indirect, detection of gravitational waves came in the 1970s through observation of a pair of closely orbiting neutron stars, PSR B1913+16. The explanation of the decay in their orbital period was that they were emitting gravitational waves. Einstein's prediction was confirmed on 11 February, 2016, when researchers published direct observation, on Earth, of gravitational waves, exactly one hundred years after the prediction.
Hole argument and Entwurf theory.
While developing general relativity, Einstein became confused about the gauge invariance in the theory. He formulated an argument that led him to conclude that a general relativistic field theory is impossible. He gave up looking for fully generally covariant tensor equations, and searched for equations that would be invariant under general linear transformations only.
In June 1913, the Entwurf ("draft") theory was the result of these investigations. As its name suggests, it was a sketch of a theory, less elegant and more difficult than general relativity, with the equations of motion supplemented by additional gauge fixing conditions. After more than two years of intensive work, Einstein realized that the hole argument was mistaken and abandoned the theory in November 1915.
Cosmology.
In 1917, Einstein applied the general theory of relativity to the structure of the universe as a whole. He discovered that the general field equations predicted a universe that was dynamic, either contracting or expanding. As observational evidence for a dynamic universe was not known at the time, Einstein introduced a new term, the cosmological constant, to the field equations, in order to allow the theory to predict a static universe. The modified field equations predicted a static universe of closed curvature, in accordance with Einstein's understanding of Mach's principle in these years.
Following the discovery of the recession of the nebulae by Edwin Hubble in 1929, Einstein abandoned his static model of the universe, and proposed two dynamic models of the cosmos, the Friedman-Einstein model of 1931 and the Einstein-deSitter model of 1932. In each of these models, Einstein discarded the cosmological constant, claiming that it was "in any case theoretically unsatisfactory".
In many Einstein biographies, it is claimed that Einstein referred to the cosmological constant in later years as his "biggest blunder". The astrophysicist Mario Livio has recently cast doubt on this claim, suggesting that it may be exaggerated.
In late 2013, a team led by the Irish physicist Cormac O'Raifeartaigh discovered evidence that, shortly after learning of Hubble's observations of the recession of the nebulae, Einstein considered a steady-state model of the universe. In a hitherto overlooked manuscript, apparently written in early 1931, Einstein explored a model of the expanding universe in which the density of matter remains constant due to a continuous creation of matter, a process he associated with the cosmological constant. As he stated in the paper, "In what follows, I would like to draw attention to a solution to equation (1) that can account for Hubbel's i facts, and in which the density is constant over time"..."If one considers a physically bounded volume, particles of matter will be continually leaving it. For the density to remain constant, new particles of matter must be continually formed in the volume from space."
It thus appears that Einstein considered a Steady State model of the expanding universe many years before Hoyle, Bondi and Gold. However, Einstein's steady-state model contained a fundamental flaw and he quickly abandoned the idea.
Modern quantum theory.
Einstein was displeased with quantum theory and quantum mechanics (the very theory he helped create), despite its acceptance by other physicists, stating that God "is not playing at dice." Einstein continued to maintain his disbelief in the theory, and attempted unsuccessfully to disprove it until he died at the age of 76. In 1917, at the height of his work on relativity, Einstein published an article in "Physikalische Zeitschrift" that proposed the possibility of stimulated emission, the physical process that makes possible the maser and the laser.
This article showed that the statistics of absorption and emission of light would only be consistent with Planck's distribution law if the emission of light into a mode with n photons would be enhanced statistically compared to the emission of light into an empty mode. This paper was enormously influential in the later development of quantum mechanics, because it was the first paper to show that the statistics of atomic transitions had simple laws.
Einstein discovered Louis de Broglie's work, and supported his ideas, which were received skeptically at first. In another major paper from this era, Einstein gave a wave equation for de Broglie waves, which Einstein suggested was the Hamilton–Jacobi equation of mechanics. This paper would inspire Schrödinger's work of 1926.
Bose–Einstein statistics.
In 1924, Einstein received a description of a statistical model from Indian physicist Satyendra Nath Bose, based on a counting method that assumed that light could be understood as a gas of indistinguishable particles. Einstein noted that Bose's statistics applied to some atoms as well as to the proposed light particles, and submitted his translation of Bose's paper to the "Zeitschrift für Physik". Einstein also published his own articles describing the model and its implications, among them the Bose–Einstein condensate phenomenon that some particulates should appear at very low temperatures. It was not until 1995 that the first such condensate was produced experimentally by Eric Allin Cornell and Carl Wieman using ultra-cooling equipment built at the NIST–JILA laboratory at the University of Colorado at Boulder. Bose–Einstein statistics are now used to describe the behaviors of any assembly of bosons. Einstein's sketches for this project may be seen in the Einstein Archive in the library of the Leiden University.
Energy momentum pseudotensor.
General relativity includes a dynamical spacetime, so it is difficult to see how to identify the conserved energy and momentum. Noether's theorem allows these quantities to be determined from a Lagrangian with translation invariance, but general covariance makes translation invariance into something of a gauge symmetry. The energy and momentum derived within general relativity by Noether's presecriptions do not make a real tensor for this reason.
Einstein argued that this is true for fundamental reasons, because the gravitational field could be made to vanish by a choice of coordinates. He maintained that the non-covariant energy momentum pseudotensor was in fact the best description of the energy momentum distribution in a gravitational field. This approach has been echoed by Lev Landau and Evgeny Lifshitz, and others, and has become standard.
The use of non-covariant objects like pseudotensors was heavily criticized in 1917 by Erwin Schrödinger and others.
Unified field theory.
Following his research on general relativity, Einstein entered into a series of attempts to generalize his geometric theory of gravitation to include electromagnetism as another aspect of a single entity. In 1950, he described his "unified field theory" in a "Scientific American" article entitled "On the Generalized Theory of Gravitation". Although he continued to be lauded for his work, Einstein became increasingly isolated in his research, and his efforts were ultimately unsuccessful.
In his pursuit of a unification of the fundamental forces, Einstein ignored some mainstream developments in physics, most notably the strong and weak nuclear forces, which were not well understood until many years after his death. Mainstream physics, in turn, largely ignored Einstein's approaches to unification. Einstein's dream of unifying other laws of physics with gravity motivates modern quests for a theory of everything and in particular string theory, where geometrical fields emerge in a unified quantum-mechanical setting.
Wormholes.
Einstein collaborated with others to produce a model of a wormhole. His motivation was to model elementary particles with charge as a solution of gravitational field equations, in line with the program outlined in the paper "Do Gravitational Fields play an Important Role in the Constitution of the Elementary Particles?". These solutions cut and pasted Schwarzschild black holes to make a bridge between two patches.
If one end of a wormhole was positively charged, the other end would be negatively charged. These properties led Einstein to believe that pairs of particles and antiparticles could be described in this way.
Einstein–Cartan theory.
In order to incorporate spinning point particles into general relativity, the affine connection needed to be generalized to include an antisymmetric part, called the torsion. This modification was made by Einstein and Cartan in the 1920s.
Equations of motion.
The theory of general relativity has a fundamental law—the Einstein equations which describe how space curves, the geodesic equation which describes how particles move may be derived from the Einstein equations.
Since the equations of general relativity are non-linear, a lump of energy made out of pure gravitational fields, like a black hole, would move on a trajectory which is determined by the Einstein equations themselves, not by a new law. So Einstein proposed that the path of a singular solution, like a black hole, would be determined to be a geodesic from general relativity itself.
This was established by Einstein, Infeld, and Hoffmann for pointlike objects without angular momentum, and by Roy Kerr for spinning objects.
Other investigations.
Einstein conducted other investigations that were unsuccessful and abandoned. These pertain to force, superconductivity, gravitational waves, and other research.
Collaboration with other scientists.
In addition to longtime collaborators Leopold Infeld, Nathan Rosen, Peter Bergmann and others, Einstein also had some one-shot collaborations with various scientists.
Einstein–de Haas experiment.
Einstein and De Haas demonstrated that magnetization is due to the motion of electrons, nowadays known to be the spin. In order to show this, they reversed the magnetization in an iron bar suspended on a torsion pendulum. They confirmed that this leads the bar to rotate, because the electron's angular momentum changes as the magnetization changes. This experiment needed to be sensitive, because the angular momentum associated with electrons is small, but it definitively established that electron motion of some kind is responsible for magnetization.
Schrödinger gas model.
Einstein suggested to Erwin Schrödinger that he might be able to reproduce the statistics of a Bose–Einstein gas by considering a box. Then to each possible quantum motion of a particle in a box associate an independent harmonic oscillator. Quantizing these oscillators, each level will have an integer occupation number, which will be the number of particles in it.
This formulation is a form of second quantization, but it predates modern quantum mechanics. Erwin Schrödinger applied this to derive the thermodynamic properties of a semiclassical ideal gas. Schrödinger urged Einstein to add his name as co-author, although Einstein declined the invitation.
Einstein refrigerator.
In 1926, Einstein and his former student Leó Szilárd co-invented (and in 1930, patented) the Einstein refrigerator. This absorption refrigerator was then revolutionary for having no moving parts and using only heat as an input. On 11 November 1930, was awarded to Albert Einstein and Leó Szilárd for the refrigerator. Their invention was not immediately put into commercial production, and the most promising of their patents were acquired by the Swedish company Electrolux.
Bohr versus Einstein.
 The Bohr–Einstein debates were a series of public disputes about quantum mechanics between Albert Einstein and Niels Bohr who were two of its founders. Their debates are remembered because of their importance to the philosophy of science.
Einstein–Podolsky–Rosen paradox.
In 1935, Einstein returned to the question of quantum mechanics. He considered how a measurement on one of two entangled particles would affect the other. He noted, along with his collaborators, that by performing different measurements on the distant particle, either of position or momentum, different properties of the entangled partner could be discovered without disturbing it in any way.
He then used a hypothesis of local realism to conclude that the other particle had these properties already determined. The principle he proposed is that if it is possible to determine what the answer to a position or momentum measurement would be, without in any way disturbing the particle, then the particle actually has values of position or momentum.
This principle distilled the essence of Einstein's objection to quantum mechanics. As a physical principle, it was shown to be incorrect when the Aspect experiment of 1982 confirmed Bell's theorem, which had been promulgated in 1964.
Non-scientific legacy.
While traveling, Einstein wrote daily to his wife Elsa and adopted stepdaughters Margot and Ilse. The letters were included in the papers bequeathed to The Hebrew University. Margot Einstein permitted the personal letters to be made available to the public, but requested that it not be done until twenty years after her death (she died in 1986). Barbara Wolff, of The Hebrew University's Albert Einstein Archives, told the BBC that there are about 3,500 pages of private correspondence written between 1912 and 1955.
Corbis, successor to The Roger Richman Agency, licenses the use of his name and associated imagery, as agent for the university.
In popular culture.
In the period before World War II, "The New Yorker" published a vignette in their "The Talk of the Town" feature saying that Einstein was so well known in America that he would be stopped on the street by people wanting him to explain "that theory". He finally figured out a way to handle the incessant inquiries. He told his inquirers "Pardon me, sorry! Always I am mistaken for Professor Einstein."
Einstein has been the subject of or inspiration for many novels, films, plays, and works of music. He is a favorite model for depictions of mad scientists and absent-minded professors; his expressive face and distinctive hairstyle have been widely copied and exaggerated. "Time" magazine's Frederic Golden wrote that Einstein was "a cartoonist's dream come true".
Awards and honors.
Einstein received numerous awards and honors, including the Nobel Prize in Physics.

</doc>
<doc id="737" url="https://en.wikipedia.org/wiki?curid=737" title="Afghanistan">
Afghanistan

Afghanistan (Pashto/Dari: , "Afġānistān"), officially the Islamic Republic of Afghanistan, is a landlocked country located within South Asia and Central Asia. It has a population of approximately 32 million, making it the 42nd most populous country in the world. It is bordered by Pakistan in the south and east; Iran in the west; Turkmenistan, Uzbekistan, and Tajikistan in the north; and China in the far northeast. Its territory covers , making it the 41st largest country in the world.
Human habitation in Afghanistan dates back to the Middle Paleolithic Era, and the country's strategic location along the Silk Road connected it to the cultures of the Middle East and other parts of Asia. Through the ages the land has been home to various peoples and witnessed numerous military campaigns; notably by Alexander the Great, Muslim Arabs, Mongols, British, Soviet Russians, and in the modern-era by Western powers. The land also served as the source from which the Kushans, Hephthalites, Samanids, Saffarids, Ghaznavids, Ghorids, Khiljis, Mughals, Hotaks, Durranis, and others have risen to form major empires.
The political history of the modern state of Afghanistan began with the Hotak and Durrani dynasties in the 18th century. In the late 19th century, Afghanistan became a buffer state in the "Great Game" between British India and the Russian Empire. Following the Third Anglo-Afghan War in 1919, King Amanullah unsuccessfully attempted to modernize the country. It remained peaceful during Zahir Shah's forty years of monarchy. A series of coups in the 1970s was followed by a Soviet invasion and a series of civil wars that devastated much of Afghanistan. This was followed by the recent $100 billion nationwide rebuilding process.
Etymology.
The name "Afghānistān" (, ) is believed to be as old as the ethnonym "Afghan", which is documented in the 10th-century geography book "Hudud ul-'alam". The root name "Afghan" was used historically in reference to a member of the ethnic Pashtuns, and the suffix "-stan" means "place of" in Persian. Therefore, Afghanistan translates to "land of the Afghans" or, more specifically in a historical sense, to "land of the Pashtuns". However, the modern Constitution of Afghanistan states that "he word Afghan shall apply to every citizen of Afghanistan."
History.
Excavations of prehistoric sites by Louis Dupree and others suggest that humans were living in what is now Afghanistan at least 50,000 years ago, and that farming communities in the area were among the earliest in the world. An important site of early historical activities, many believe that Afghanistan compares to Egypt in terms of the historical value of its archaeological sites.
The country sits at a unique nexus point where numerous civilizations have interacted and often fought. It has been home to various peoples through the ages, among them the ancient Iranian peoples who established the dominant role of Indo-Iranian languages in the region. At multiple points, the land has been incorporated within large regional empires, among them the Achaemenid Empire, the Macedonian Empire, the Indian Maurya Empire, and the Islamic Empire.
Many empires and kingdoms have also risen to power in Afghanistan, such as the Greco-Bactrians, Kushans, Hephthalites, Kabul Shahis, Saffarids, Samanids, Ghaznavids, Ghurids, Khiljis, Kartids, Timurids, Mughals, and finally the Hotak and Durrani dynasties that marked the political origins of the modern state.
Pre-Islamic period.
Archaeological exploration done in the 20th century suggests that the geographical area of Afghanistan has been closely connected by culture and trade with its neighbors to the east, west, and north. Artifacts typical of the Paleolithic, Mesolithic, Neolithic, Bronze, and Iron ages have been found in Afghanistan. Urban civilization is believed to have begun as early as 3000 BCE, and the early city of Mundigak (near Kandahar in the south of the country) may have been a colony of the nearby Indus Valley Civilization. More recent findings established that the Indus Valley Civilisation stretched up towards modern-day Afghanistan, making the ancient civilisation today part of Pakistan, Afghanistan and India. In more detail, it extended from what today is northwest Pakistan to northwest India and northeast Afghanistan. An Indus Valley site has been found on the Oxus River at Shortugai in northern Afghanistan. There are several smaller IVC colonies to be found in Afghanistan as well.
After 2000 BCE, successive waves of semi-nomadic people from Central Asia began moving south into Afghanistan; among them were many Indo-European-speaking Indo-Iranians. These tribes later migrated further into South Asia, Western Asia, and toward Europe via the area north of the Caspian Sea. The region at the time was referred to as Ariana.
The religion Zoroastrianism is believed by some to have originated in what is now Afghanistan between 1800 and 800 BCE, as its founder Zoroaster is thought to have lived and died in Balkh. Ancient Eastern Iranian languages may have been spoken in the region around the time of the rise of Zoroastrianism. By the middle of the 6th century BCE, the Achaemenids overthrew the Medes and incorporated Arachosia, Aria, and Bactria within its eastern boundaries. An inscription on the tombstone of Darius I of Persia mentions the Kabul Valley in a list of the 29 countries that he had conquered.
Alexander the Great and his Macedonian forces arrived to Afghanistan in 330 BCE after defeating Darius III of Persia a year earlier in the Battle of Gaugamela. Following Alexander's brief occupation, the successor state of the Seleucid Empire controlled the region until 305 BCE, when they gave much of it to the Maurya Empire as part of an alliance treaty. The Mauryans controlled the area south of the Hindu Kush until they were overthrown in about 185 BCE. Their decline began 60 years after Ashoka's rule ended, leading to the Hellenistic reconquest by the Greco-Bactrians. Much of it soon broke away from them and became part of the Indo-Greek Kingdom. They were defeated and expelled by the Indo-Scythians in the late 2nd century BCE.
During the first century BCE, the Parthian Empire subjugated the region, but lost it to their Indo-Parthian vassals. In the mid-to-late first century CE the vast Kushan Empire, centered in Afghanistan, became great patrons of Buddhist culture, making Buddhism flourish throughout the region. The Kushans were overthrown by the Sassanids in the 3rd century CE, though the Indo-Sassanids continued to rule at least parts of the region. They were followed by the Kidarite who, in turn, were replaced by the Hephthalites. By the 6th century CE, the successors to the Kushans and Hepthalites established a small dynasty called Kabul Shahi. Much of the northeastern and southern areas of the country remained dominated by Buddhist culture.
Islamization and Mongol invasion.
Arab Muslims brought Islam to Herat and Zaranj in 642 CE and began spreading eastward; some of the native inhabitants they encountered accepted it while others revolted. The land was collectively recognized by the Arabs as al-Hind due to its cultural connection with Greater India. Before Islam was introduced, people of the region were mostly Buddhists and Zoroastrians, but there were also Surya and Nana worshipers, Jews, and others. The Zunbils and Kabul Shahi were first conquered in 870 CE by the Saffarid Muslims of Zaranj. Later, the Samanids extended their Islamic influence south of the Hindu Kush. It is reported that Muslims and non-Muslims still lived side by side in Kabul before the Ghaznavids rose to power in the 10th century.
By the 11th century, Mahmud of Ghazni defeated the remaining Hindu rulers and effectively Islamized the wider region, with the exception of Kafiristan. Afghanistan became one of the main centers in the Muslim world during this Islamic Golden Age. The Ghaznavid dynasty was overthrown by the Ghurids, who expanded and advanced the already powerful Islamic empire.
In 1219 AD, Genghis Khan and his Mongol army overran the region. His troops are said to have annihilated the Khorasanian cities of Herat and Balkh as well as Bamyan. The destruction caused by the Mongols forced many locals to return to an agrarian rural society. Mongol rule continued with the Ilkhanate in the northwest while the Khilji dynasty administered the Afghan tribal areas south of the Hindu Kush until the invasion of Timur, who established the Timurid Empire in 1370.
In the early 16th century, Babur arrived from Fergana and captured Kabul from the Arghun dynasty. In 1526, he invaded Delhi in India to replace the Lodi dynasty with the Mughal Empire. Between the 16th and 18th century, the Khanate of Bukhara, Safavids, and Mughals ruled parts of the territory. Before the 19th century, the northwestern area of Afghanistan was referred to by the regional name Khorasan. Two of the four capitals of Khorasan (Herat and Balkh) are now located in Afghanistan, while the regions of Kandahar, Zabulistan, Ghazni, Kabulistan, and Afghanistan formed the frontier between Khorasan and Hindustan.
Hotak dynasty and Durrani Empire.
In 1709, Mirwais Hotak, a local Ghilzai tribal leader, successfully rebelled against the Safavids. He defeated Gurgin Khan and made Afghanistan independent. Mirwais died of a natural cause in 1715 and was succeeded by his brother Abdul Aziz, who was soon killed by Mirwais' son Mahmud for treason. Mahmud led the Afghan army in 1722 to the Persian capital of Isfahan, captured the city after the Battle of Gulnabad and proclaimed himself King of Persia. The Afghan dynasty was ousted from Persia by Nader Shah after the 1729 Battle of Damghan.
In 1738, Nader Shah and his forces captured Kandahar, the last Hotak stronghold, from Shah Hussain Hotak, at which point the incarcerated 16-year-old Ahmad Shah Durrani was freed and made the commander of an Afghan regiment. Soon after the Persian and Afghan forces invaded India. By 1747, the Afghans chose Durrani as their head of state. Durrani and his Afghan army conquered much of present-day Afghanistan, Pakistan, the Khorasan and Kohistan provinces of Iran, and Delhi in India. He defeated the Indian Maratha Empire, and one of his biggest victories was the 1761 Battle of Panipat.
In October 1772, Durrani died of a natural cause and was buried at a site now adjacent to the Shrine of the Cloak in Kandahar. He was succeeded by his son, Timur Shah, who transferred the capital of Afghanistan from Kandahar to Kabul in 1776. After Timur's death in 1793, the Durrani throne passed down to his son Zaman Shah, followed by Mahmud Shah, Shuja Shah and others.
The Afghan Empire was under threat in the early 19th century by the Persians in the west and the British-backed Sikhs in the east. Fateh Khan, leader of the Barakzai tribe, had installed 21 of his brothers in positions of power throughout the empire. After his death, they rebelled and divided up the provinces of the empire between themselves. During this turbulent period, Afghanistan had many temporary rulers until Dost Mohammad Khan declared himself emir in 1826. The Punjab region was lost to Ranjit Singh, who invaded Khyber Pakhtunkhwa and in 1834 captured the city of Peshawar. In 1837, during the Battle of Jamrud near the Khyber Pass, Akbar Khan and the Afghan army killed Sikh Commander Hari Singh Nalwa. By this time the British were advancing from the east and the first major conflict during the "Great Game" was initiated.
Western influence.
Following the 1842 defeat of the British-Indian forces and victory of the Afghans, the British established diplomatic relations with the Afghan government and withdrew all forces from the country. They returned during the Second Anglo-Afghan War in the late 1870s for about two years to assist Abdur Rahman Khan defeat Ayub Khan. The United Kingdom began to exercise a great deal of influence after this and even controlled the state's foreign policy. In 1893, Mortimer Durand made Amir Abdur Rahman Khan sign a controversial agreement in which the ethnic Pashtun and Baloch territories were divided by the Durand Line. This was a standard divide and rule policy of the British and would lead to strained relations, especially with the later new state of Pakistan.
After the Third Anglo-Afghan War and the signing of the Treaty of Rawalpindi in 1919, King Amanullah Khan declared Afghanistan a sovereign and fully independent state. He moved to end his country's traditional isolation by establishing diplomatic relations with the international community and, following a 1927–28 tour of Europe and Turkey, introduced several reforms intended to modernize his nation. A key force behind these reforms was Mahmud Tarzi, an ardent supporter of the education of women. He fought for Article 68 of Afghanistan's 1923 constitution, which made elementary education compulsory. The institution of slavery was abolished in 1923.
Some of the reforms that were actually put in place, such as the abolition of the traditional burqa for women and the opening of a number of co-educational schools, quickly alienated many tribal and religious leaders. Faced with overwhelming armed opposition, Amanullah Khan was forced to abdicate in January 1929 after Kabul fell to rebel forces led by Habibullah Kalakani. Prince Mohammed Nadir Shah, Amanullah's cousin, in turn defeated and killed Kalakani in November 1929, and was declared King Nadir Shah. He abandoned the reforms of Amanullah Khan in favor of a more gradual approach to modernisation but was assassinated in 1933 by Abdul Khaliq, a Hazara school student.
Mohammed Zahir Shah, Nadir Shah's 19-year-old son, succeeded to the throne and reigned from 1933 to 1973. Until 1946, Zahir Shah ruled with the assistance of his uncle, who held the post of Prime Minister and continued the policies of Nadir Shah. Another of Zahir Shah's uncles, Shah Mahmud Khan, became Prime Minister in 1946 and began an experiment allowing greater political freedom, but reversed the policy when it went further than he expected. He was replaced in 1953 by Mohammed Daoud Khan, the king's cousin and brother-in-law. Daoud Khan sought a closer relationship with the Soviet Union and a more distant one towards Pakistan. Afghanistan remained neutral and was neither a participant in World War II nor aligned with either power bloc in the Cold War. However, it was a beneficiary of the latter rivalry as both the Soviet Union and the United States vied for influence by building Afghanistan's main highways, airports, and other vital infrastructure. On per capita basis, Afghanistan received more Soviet development aid than any other country. In 1973, while King Zahir Shah was on an official overseas visit, Daoud Khan launched a bloodless coup and became the first President of Afghanistan. In the meantime, Zulfikar Ali Bhutto got neighboring Pakistan involved in Afghanistan. Some experts suggest that Bhutto paved the way for the April 1978 Saur Revolution.
Marxist revolution and Soviet war.
In April 1978, the communist People's Democratic Party of Afghanistan (PDPA) seized power in Afghanistan in the Saur Revolution. Within months, opponents of the communist government launched an uprising in eastern Afghanistan that quickly expanded into a civil war waged by guerrilla mujahideen against government forces countrywide. The Pakistani government provided these rebels with covert training centers, while the Soviet Union sent thousands of military advisers to support the PDPA government. Meanwhile, increasing friction between the competing factions of the PDPA — the dominant Khalq and the more moderate Parcham — resulted in the dismissal of Parchami cabinet members and the arrest of Parchami military officers under the pretext of a Parchami coup.
In September 1979, Nur Muhammad Taraki was assassinated in a coup within the PDPA orchestrated by fellow Khalq member Hafizullah Amin, who assumed the presidency. Distrusted by the Soviets, Amin was assassinated by Soviet special forces in December 1979. A Soviet-organized government, led by Parcham's Babrak Karmal but inclusive of both factions, filled the vacuum. Soviet troops were deployed to stabilize Afghanistan under Karmal in more substantial numbers, although the Soviet government did not expect to do most of the fighting in Afghanistan. As a result, however, the Soviets were now directly involved in what had been a domestic war in Afghanistan. The PDPA prohibited usury, declared equality of the sexes, and introducing women to political life.
The United States has been supporting anti-Soviet Afghan "mujahideen" and foreign "Afghan Arab" fighters through Pakistan's ISI as early as mid-1979 (see "CIA activities in Afghanistan"). Billions in cash and weapons, which included over two thousand FIM-92 Stinger surface-to-air missiles, were provided by the United States and Saudi Arabia to Pakistan.
The Soviet war in Afghanistan resulted in the deaths of over 1 million Afghans, mostly civilians, and the creation of about 6million refugees who fled Afghanistan, mainly to Pakistan and Iran. Faced with mounting international pressure and numerous casualties, the Soviets withdrew in 1989 but continued to support Afghan President Mohammad Najibullah until 1992.
Civil war.
From 1989 until 1992, Najibullah's government tried to solve the ongoing civil war with economic and military aid, but without Soviet troops on the ground. Najibullah tried to build support for his government by portraying his government as Islamic, and in the 1990 constitution the country officially became an Islamic state and all references of communism were removed. Nevertheless, Najibullah did not win any significant support, and with the dissolution of the Soviet Union in December 1991, he was left without foreign aid. This, coupled with the internal collapse of his government, led to his ousting from power in April 1992. After the fall of Najibullah's government in 1992, the post-communist Islamic State of Afghanistan was established by the Peshawar Accord, a peace and power-sharing agreement under which all the Afghan parties were united in April 1992, except for the Pakistani supported Hezb-e Islami of Gulbuddin Hekmatyar. Hekmatyar started a bombardment campaign against the capital city Kabul, which marked the beginning of a new phase in the war.
Saudi Arabia and Iran supported different Afghan militias and instability quickly developed. The conflict between the two militias soon escalated into a full-scale war.
Due to the sudden initiation of the war, working government departments, police units, and a system of justice and accountability for the newly created Islamic State of Afghanistan did not have time to form. Atrocities were committed by individuals of the different armed factions while Kabul descended into lawlessness and chaos. Because of the chaos, some leaders increasingly had only nominal control over their (sub-)commanders. For civilians there was little security from murder, rape, and extortion. An estimated 25,000 people died during the most intense period of bombardment by Hekmatyar's Hezb-i Islami and the Junbish-i Milli forces of Abdul Rashid Dostum, who had created an alliance with Hekmatyar in 1994. Half a million people fled Afghanistan.
Southern and eastern Afghanistan were under the control of local commanders such as Gul Agha Sherzai and others. In 1994, the Taliban (a movement originating from Jamiat Ulema-e-Islam-run religious schools for Afghan refugees in Pakistan) also developed in Afghanistan as a political-religious force. The Taliban first took control of southern Afghanistan in 1994 and forced the surrender of dozens of local Pashtun leaders.
In late 1994, forces of Ahmad Shah Massoud held on to Kabul. Rabbani's government took steps to reopen courts, restore law and order, and initiate a nationwide political process with the goal of national consolidation and democratic elections. Massoud invited Taliban leaders to join the process but they refused.
Taliban Emirate and Northern Alliance.
The Taliban's early victories in late 1994 were followed by a series of defeats that resulted in heavy losses. The Taliban attempted to capture Kabul in early 1995 but were repelled by forces under Massoud. In September 1996, as the Taliban, with military support from Pakistan and financial support from Saudi Arabia, prepared for another major offensive, Massoud ordered a full retreat from Kabul. The Taliban seized Kabul in the same month and established the Islamic Emirate of Afghanistan. They imposed a strict form of Sharia, similar to that found in Saudi Arabia. According to Physicians for Human Rights (PHR), "no other regime in the world has methodically and violently forced half of its population into virtual house arrest, prohibiting them on pain of physical punishment from showing their faces, seeking medical care without a male escort, or attending school."
After the fall of Kabul to the Taliban, Massoud and Dostum formed the Northern Alliance. The Taliban defeated Dostum's forces during the Battles of Mazar-i-Sharif (1997–98). Pakistan's Chief of Army Staff, Pervez Musharraf, began sending thousands of Pakistanis to help the Taliban defeat the Northern Alliance.<ref name="Ahmed Rashid/The Telegraph"></ref> From 1996 to 2001, the al-Qaeda network of Osama bin Laden and Ayman al-Zawahiri was also operating inside Afghanistan. This and the fact that around one million Afghans were internally displaced made the United States worry. From 1990 to September 2001, around 400,000 Afghans have died in the internal mini wars.
On 9 September 2001, Massoud was assassinated by two Arab suicide attackers in Panjshir province of Afghanistan. Two days later, the September 11 attacks were carried out in the United States. The US government suspected Osama bin Laden as the perpetrator of the attacks, and demanded that the Taliban hand him over. After refusing to comply, the October 2001 Operation Enduring Freedom was launched. During the initial invasion, US and UK forces bombed al-Qaeda training camps. The United States began working with the Northern Alliance to remove the Taliban from power.
Recent history (2002–present).
In December 2001, after the Taliban government was overthrown and the new Afghan government under President Hamid Karzai was formed, the International Security Assistance Force (ISAF) was established by the UN Security Council to help assist the Karzai administration and provide basic security. Taliban forces also began regrouping inside Pakistan, while more coalition troops entered Afghanistan and began rebuilding the war-torn country.
Shortly after their fall from power, the Taliban began an insurgency to regain control of Afghanistan. Over the next decade, ISAF and Afghan troops led many offensives against the Taliban but failed to fully defeat them. Afghanistan remains one of the poorest countries in the world due to a lack of foreign investment, government corruption, and the Taliban insurgency.
Meanwhile, the Afghan government was able to build some democratic structures, and the country changed its name to the Islamic Republic of Afghanistan. Attempts were made, often with the support of foreign donor countries, to improve the country's economy, healthcare, education, transport, and agriculture. ISAF forces also began to train the Afghan National Security Forces. In the decade following 2002, over five million Afghans were repatriated, including some who were forcefully deported from Western countries.
By 2009, a Taliban-led shadow government began to form in parts of the country. In 2010, President Karzai attempted to hold peace negotiations with the Taliban leaders, but the rebel group refused to attend until mid 2015 when Taliban supreme leader finally decided to back the peace talks.
After the May 2011 death of Osama bin Laden in Pakistan, many prominent Afghan figures were assassinated. Afghanistan–Pakistan border skirmishes intensified and many large scale attacks by the Pakistan-based Haqqani Network also took place across Afghanistan. The United States blamed rogue elements within the Pakistani government for the increased attacks.
Following the 2014 presidential election President Karzai left power and Ashraf Ghani became President in September 2014. The US war in Afghanistan (America's longest war) officially ended on December 28, 2014. However, thousands of US-led NATO troops have remained in the country to train and advise Afghan government forces. The 2001–present war has resulted in over 90,000 direct war-related deaths, which includes insurgents, Afghan civilians and government forces. Over 100,000 have been injured.
Geography.
A landlocked mountainous country with plains in the north and southwest, Afghanistan is located within South Asia and Central Asia. It is part of the US-coined Greater Middle East Muslim world, which lies between latitudes and , and longitudes and . The country's highest point is Noshaq, at above sea level. It has a continental climate with harsh winters in the central highlands, the glaciated northeast (around Nuristan), and the Wakhan Corridor, where the average temperature in January is below , and hot summers in the low-lying areas of the Sistan Basin of the southwest, the Jalalabad basin in the east, and the Turkestan plains along the Amu River in the north, where temperatures average over in July.
Despite having numerous rivers and reservoirs, large parts of the country are dry. The endorheic Sistan Basin is one of the driest regions in the world. Aside from the usual rainfall, Afghanistan receives snow during the winter in the Hindu Kush and Pamir Mountains, and the melting snow in the spring season enters the rivers, lakes, and streams. However, two-thirds of the country's water flows into the neighboring countries of Iran, Pakistan, and Turkmenistan. The state needs more than to rehabilitate its irrigation systems so that the water is properly managed.
The northeastern Hindu Kush mountain range, in and around the Badakhshan Province of Afghanistan, is in a geologically active area where earthquakes may occur almost every year. They can be deadly and destructive sometimes, causing landslides in some parts or avalanches during the winter. The last strong earthquakes were in 1998, which killed about 6,000 people in Badakhshan near Tajikistan. This was followed by the 2002 Hindu Kush earthquakes in which over 150 people were killed and over 1,000 injured. A 2010 earthquake left 11 Afghans dead, over 70 injured, and more than 2,000 houses destroyed.
The country's natural resources include: coal, copper, iron ore, lithium, uranium, rare earth elements, chromite, gold, zinc, talc, barites, sulfur, lead, marble, precious and semi-precious stones, natural gas, and petroleum, among other things. In 2010, US and Afghan government officials estimated that untapped mineral deposits located in 2007 by the US Geological Survey are worth between and .
At , Afghanistan is the world's 41st largest country, slightly bigger than France and smaller than Burma, about the size of Texas in the United States. It borders Pakistan in the south and east; Iran in the west; Turkmenistan, Uzbekistan, and Tajikistan in the north; and China in the far east.
Demographics.
, the population of Afghanistan is around 32,564,342, which includes the roughly 2.7 million Afghan refugees still living in Pakistan and Iran. In 1979, the population was reported to be about 15.5 million.
The only city with over a million residents is its capital, Kabul. Other large cities in the country are, in order of population size, Kandahar, Herat, Mazar-i-Sharif, Jalalabad, Lashkar Gah, Taloqan, Khost, Sheberghan, and Ghazni. Urban areas are experiencing rapid population growth following the return of over 5 million expatriates. According to the Population Reference Bureau, the Afghan population is estimated to increase to 82 million by 2050.
Ethnic groups.
Afghanistan is a multiethnic society, and its historical status as a crossroads has contributed significantly to its diverse ethnic makeup. The population of the country is divided into a wide variety of ethnolinguistic groups. Because a systematic census has not been held in the nation in decades, exact figures about the size and composition of the various ethnic groups are unavailable. An approximate distribution of the ethnic groups is shown in the chart below:
Languages.
Pashto and Dari are the official languages of Afghanistan; bilingualism is very common. Both are Indo-European languages from the Iranian languages sub-family. Dari (Afghan Persian) has long been the prestige language and a lingua franca for inter-ethnic communication. It is the native tongue of the Tajiks, Hazaras, Aimaks, and Kizilbash. Pashto is the native tongue of the Pashtuns, although many Pashtuns often use Dari and some non-Pashtuns are fluent in Pashto.
Other languages, including Uzbek, Arabic, Turkmen, Balochi, Pashayi, and Nuristani languages (Ashkunu, Kamkata-viri, Vasi-vari, Tregami, and Kalasha-ala), are the native tongues of minority groups across the country and have official status in the regions where they are widely spoken. Minor languages also include Pamiri (Shughni, Munji, Ishkashimi, and Wakhi), Brahui, Hindko, and Kyrgyz. A small percentage of Afghans are also fluent in Urdu, English, and other languages.
Religions.
Over 99% of the Afghan population is Muslim; up to 90% are from the Sunni branch, 7–19% are Shia.
Until the 1890s, the region around Nuristan was known as Kafiristan (land of the kafirs (unbelievers)) because of its non-Muslim inhabitants, the Nuristanis, an ethnically distinct people whose religious practices included animism, polytheism, and shamanism. Thousands of Afghan Sikhs and Hindus are also found in the major cities. There was a small Jewish community in Afghanistan who had emigrated to Israel and the United States by the end of the twentieth century; only one Jew, Zablon Simintov, remained by 2005.
Governance.
Afghanistan is an Islamic republic consisting of three branches, the executive, legislative, and judicial. The nation is led by President Ashraf Ghani with Abdul Rashid Dostum and Sarwar Danish as vice presidents. Abdullah Abdullah serves as the chief executive officer (CEO). The National Assembly is the legislature, a bicameral body having two chambers, the House of the People and the House of Elders. The Supreme Court is led by Chief Justice Said Yusuf Halem, the former Deputy Minister of Justice for Legal Affairs.
A January 2010 report published by the United Nations Office on Drugs and Crime revealed that bribery consumed an amount equal to 23% of the GDP of the nation. A number of government ministries are believed to be rife with corruption, and while President Karzai vowed to tackle the problem in late 2009 by stating that "individuals who are involved in corruption will have no place in the government", top government officials were stealing and misusing hundreds of millions of dollars through the Kabul Bank. According to Transparency International's 2014 corruption perceptions index results, Afghanistan was ranked as the fourth most corrupt country in the world.
Elections and parties.
The 2004 Afghan presidential election was relatively peaceful, in which Hamid Karzai won in the first round with 55.4% of the votes. However, the 2009 presidential election was characterized by lack of security, low voter turnout, and widespread electoral fraud. The vote, along with elections for 420 provincial council seats, took place in August 2009, but remained unresolved during a lengthy period of vote counting and fraud investigation.
Two months later, under international pressure, a second round run-off vote between Karzai and remaining challenger Abdullah was announced, but a few days later Abdullah announced that he would not participate in the 7 November run-off because his demands for changes in the electoral commission had not been met. The next day, officials of the election commission cancelled the run-off and declared Hamid Karzai as President for another five-year term.
In the 2005 parliamentary election, among the elected officials were former mujahideen, Islamic fundamentalists, warlords, communists, reformists, and several Taliban associates. In the same period, Afghanistan reached to the 30th highest nation in terms of female representation in parliament. The last parliamentary election was held in September 2010, but due to disputes and investigation of fraud, the swearing-in ceremony took place in late January 2011. The 2014 presidential election ended with Ashraf Ghani winning by 56.44% votes.
Administrative divisions.
Afghanistan is administratively divided into 34 provinces ("wilayats"), with each province having its own capital and a provincial administration. The provinces are further divided into about 398 smaller provincial districts, each of which normally covers a city or a number of villages. Each district is represented by a district governor.
The provincial governors are appointed by the President of Afghanistan and the district governors are selected by the provincial governors. The provincial governors are representatives of the central government in Kabul and are responsible for all administrative and formal issues within their provinces. There are also provincial councils that are elected through direct and general elections for a period of four years. The functions of provincial councils are to take part in provincial development planning and to participate in the monitoring and appraisal of other provincial governance institutions.
According to article 140 of the constitution and the presidential decree on electoral law, mayors of cities should be elected through free and direct elections for a four-year term. However, due to huge election costs, mayoral and municipal elections have never been held. Instead, mayors have been appointed by the government. In the capital city of Kabul, the mayor is appointed by the President of Afghanistan.
The following is a list of all the 34 provinces in alphabetical order:
Foreign relations and military.
The Afghan Ministry of Foreign Affairs is in charge of maintaining the foreign relations of Afghanistan. The state has been a member of the United Nations since 1946. It enjoys strong economic relations with a number of NATO and allied states, particularly the United States, United Kingdom, Germany and Turkey. In 2012, the United States designated Afghanistan as a major non-NATO ally and created the U.S.–Afghanistan Strategic Partnership Agreement. Afghanistan also has friendly diplomatic relations with neighboring Pakistan, Iran, Turkmenistan, Uzbekistan, Tajikistan, and China, and with regional states such as India, Bangladesh, Nepal, Kazakhstan, Russia, the UAE, Saudi Arabia, Iraq, Egypt, Japan, and South Korea. It continues to develop diplomatic relations with other countries around the world.
United Nations Assistance Mission in Afghanistan (UNAMA) was established in 2002 under United Nations Security Council Resolution 1401 in order to help the country recover from decades of war. Today, a number of NATO member states deploy about 38,000 troops in Afghanistan as part of the International Security Assistance Force (ISAF). Its main purpose is to train the Afghan National Security Forces (ANSF). The Afghan Armed Forces are under the Ministry of Defense, which includes the Afghan National Army (ANA) and the Afghan Air Force (AAF). The ANA is divided into 7 major Corps, with the 201st Selab ("Flood") in Kabul followed by the 203rd in Gardez, 205th Atul ("Hero") in Kandahar, 207th in Herat, 209th in Mazar-i-Sharif, and the 215th in Lashkar Gah. The ANA also has a commando brigade, which was established in 2007. The Afghan Defense University (ADU) houses various educational establishments for the Afghan Armed Forces, including the National Military Academy of Afghanistan.
Law enforcement.
The National Directorate of Security (NDS) is the nation's domestic intelligence agency, which operates similar to that of the United States Department of Homeland Security (DHS) and has between 15,000 to 30,000 employees. The nation also has about 126,000 national police officers, with plans to recruit more so that the total number can reach 160,000. The Afghan National Police (ANP) is under the Ministry of the Interior and serves as a single law enforcement agency all across the country. The Afghan National Civil Order Police is the main branch of the ANP, which is divided into five Brigades, each commanded by a Brigadier General. These brigades are stationed in Kabul, Gardez, Kandahar, Herat, and Mazar-i-Sharif. Every province has an appointed provincial Chief of Police who is responsible for law enforcement throughout the province.
The police receive most of their training from Western forces under the NATO Training Mission-Afghanistan. According to a 2009 news report, a large proportion of police officers were illiterate and accused of demanding bribes. Jack Kem, deputy to the commander of NATO Training Mission Afghanistan and Combined Security Transition Command Afghanistan, stated that the literacy rate in the ANP would rise to over 50% by January 2012. What began as a voluntary literacy program became mandatory for basic police training in early 2011. Approximately 17% of them tested positive for illegal drug use. In 2009, President Karzai created two anti-corruption units within the Interior Ministry. Former Interior Minister Hanif Atmar said that security officials from the US (FBI), Britain (Scotland Yard), and the European Union will train prosecutors in the unit.
All parts of Afghanistan are considered dangerous due to militant activities. Hundreds of Afghan police are killed in the line of duty each year. Kidnapping and robberies are also reported. The Afghan Border Police (ABP) are responsible for protecting the nation's airports and borders, especially the disputed Durand Line border, which is often used by members of criminal organizations and terrorists for their illegal activities. A report in 2011 suggested that up to 3 million people were involved in the illegal drug business in Afghanistan. Attacks on government employees may be ordered by powerful mafia groups who reside inside and outside the country. Drugs from Afghanistan are exported to neighboring countries and then to other countries. The Afghan Ministry of Counter Narcotics is tasked to deal with these issues by bringing to justice major drug traffickers.
Economy.
Afghanistan is an impoverished least developed country, one of the world's poorest because of decades of war and lack of foreign investment. , the nation's GDP stands at about $60.58 billion with an exchange rate of $20.31 billion, and the GDP per capita is $1,900. The country's exports totaled $2.7 billion in 2012. Its unemployment rate was reported in 2008 at about 35%. According to a 2009 report, about 42% of the population lives on less than $1 a day. The nation has less than $1.5 billion in external debt.
The Afghan economy has been growing at about 10% per year in the last decade, which is due to the infusion of over $50 billion in international aid and remittances from Afghan expats. It is also due to improvements made to the transportation system and agricultural production, which is the backbone of the nation's economy. The country is known for producing some of the finest pomegranates, grapes, apricots, melons, and several other fresh and dry fruits, including nuts. Many sources indicate that as much as 11% or more of Afghanistan's economy is derived from the cultivation and sale of opium, and Afghanistan is widely considered the world's largest producer of opium despite Afghan government and international efforts to eradicate the crop.
While the nation's current account deficit is largely financed with donor money, only a small portion is provided directly to the government budget. The rest is provided to non-budgetary expenditure and donor-designated projects through the United Nations system and non-governmental organizations. The Afghan Ministry of Finance is focusing on improved revenue collection and public sector expenditure discipline. For example, government revenues increased 31% to $1.7 billion from March 2010 to March 2011.
Da Afghanistan Bank serves as the central bank of the nation and the "Afghani" (AFN) is the national currency, with an exchange rate of about 47 Afghanis to 1 US dollar. Since 2003, over 16 new banks have opened in the country, including Afghanistan International Bank, Kabul Bank, Azizi Bank, Pashtany Bank, Standard Chartered Bank, and First Micro Finance Bank.
One of the main drivers for the current economic recovery is the return of over 5 million expatriates, who brought with them fresh energy, entrepreneurship and wealth-creating skills as well as much needed funds to start up businesses. For the first time since the 1970s, Afghans have involved themselves in construction, one of the largest industries in the country. Some of the major national construction projects include the "New Kabul City" next to the capital, the "Ghazi Amanullah Khan City" near Jalalabad, and the "Aino Mena" in Kandahar. Similar development projects have also begun in Herat, Mazar-e-Sharif, and other cities.
In addition, a number of companies and small factories began operating in different parts of the country, which not only provide revenues to the government but also create new jobs. Improvements to the business environment have resulted in more than $1.5 billion in telecom investment and created more than 100,000 jobs since 2003. Afghan rugs are becoming popular again, allowing many carpet dealers around the country to hire more workers.
Afghanistan is a member of WTO, SAARC, ECO, and OIC. It holds an observer status in SCO. Foreign Minister Zalmai Rassoul told the media in 2011 that his nation's "goal is to achieve an Afghan economy whose growth is based on trade, private enterprise and investment". Experts believe that this will revolutionize the economy of the region. Opium production in Afghanistan soared to a record in 2007 with about 3 million people reported to be involved in the business, but then declined significantly in the years following. The government started programs to help reduce poppy cultivation, and by 2010 it was reported that 24 out of the 34 provinces were free from poppy growing. In June 2012, India advocated for private investments in the resource rich country and the creation of a suitable environment therefor.
Mining.
Michael E. O'Hanlon of the Brookings Institution estimated that if Afghanistan generates about $10 bn per year from its mineral deposits, its gross national product would double and provide long-term funding for Afghan security forces and other critical needs. The United States Geological Survey (USGS) estimated in 2006 that northern Afghanistan has an average (bbl) of crude oil, 15.7 trillion cubic feet ( bn m) of natural gas, and of natural gas liquids. In 2011, Afghanistan signed an oil exploration contract with China National Petroleum Corporation (CNPC) for the development of three oil fields along the Amu Darya river in the north.
The country has significant amounts of lithium, copper, gold, coal, iron ore, and other minerals. The Khanashin carbonatite in Helmand Province contains of rare earth elements. In 2007, a 30-year lease was granted for the Aynak copper mine to the China Metallurgical Group for $3 billion, making it the biggest foreign investment and private business venture in Afghanistan's history. The state-run Steel Authority of India won the mining rights to develop the huge Hajigak iron ore deposit in central Afghanistan. Government officials estimate that 30% of the country's untapped mineral deposits are worth between and . One official asserted that "this will become the backbone of the Afghan economy" and a Pentagon memo stated that Afghanistan could become the "Saudi Arabia of lithium". In a 2011 news story, the "CSM" reported, "The United States and other Western nations that have borne the brunt of the cost of the Afghan war have been conspicuously absent from the bidding process on Afghanistan's mineral deposits, leaving it mostly to regional powers."
Transport.
Air.
Air transport in Afghanistan is provided by the national carrier, Ariana Afghan Airlines (AAA), and by private companies such as Afghan Jet International, East Horizon Airlines, Kam Air, Pamir Airways, and Safi Airways. Airlines from a number of countries also provide flights in and out of the country. These include Air India, Emirates, Gulf Air, Iran Aseman Airlines, Pakistan International Airlines, and Turkish Airlines.
The country has four international airports: Herat International Airport, Hamid Karzai International Airport (formerly Kabul International Airport), Kandahar International Airport, and Mazar-e Sharif International Airport. There are also around a dozen domestic airports with flights to Kabul or Herat.
Rail.
, the country has only two rail links, one a 75 km line from Kheyrabad to the Uzbekistan border and the other a 10 km long line from Toraghundi to the Turkmenistan border. Both lines are used for freight only and there is no passenger service as of yet. There are various proposals for the construction of additional rail lines in the country. In 2013, the presidents of Afghanistan, Turkmenistan, and Uzbekistan attended the groundbreaking ceremony for a 225 km line between Turkmenistan-Andkhvoy-Mazar-i-Sharif-Kheyrabad. The line will link at Kheyrabad with the existing line to the Uzbekistan border. Plans exist for a rail line from Kabul to the eastern border town of Torkham, where it will connect with Pakistan Railways. There are also plans to finish a rail line between Khaf, Iran and Herat, Afghanistan.
Roads.
Traveling by bus in Afghanistan remains dangerous due to careless and intoxicated bus drivers as well as militant activities. The buses are usually older model Mercedes-Benz and owned by private companies. Serious traffic accidents are common on Afghan roads and highways, particularly on the Kabul–Kandahar and the Kabul–Jalalabad Road.
Newer automobiles have recently become more widely available after the rebuilding of roads and highways. They are imported from the United Arab Emirates through Pakistan and Iran. , vehicles more than 10 years old are banned from being imported into the country. The development of the nation's road network is a major boost for the economy due to trade with neighboring countries. Postal services in Afghanistan are provided by the publicly owned Afghan Post and private companies such as FedEx, DHL, and others.
Communication.
Telecommunication services in the country are provided by Afghan Wireless, Etisalat, Roshan, MTN Group, and Afghan Telecom. In 2006, the Afghan Ministry of Communications signed a $64.5 million agreement with ZTE for the establishment of a countrywide optical fiber cable network. , Afghanistan had around 17 million GSM phone subscribers and over 1 million internet users, but only had about 75,000 fixed telephone lines and a little over 190,000 CDMA subscribers. 3G services are provided by Etisalat and MTN Group. In 2014, Afghanistan leased a space satellite from Eutelsat, called AFGHANSAT 1.
Health.
According to the Human Development Index, Afghanistan is the 15th least developed country in the world. The average life expectancy is estimated to be around 60 years for both sexes. The country has one of the highest maternal mortality rate in the world as well as the highest infant mortality rate in the world (deaths of babies under one year), estimated in 2015 to be 115.08 deaths/1,000 live births. The Ministry of Public Health plans to cut the infant mortality rate to 400 for every 100,000 live births before 2020. The country currently has more than 3,000 midwives, with an additional 300 to 400 being trained each year.
A number of hospitals and clinics have been built over the last decade, with the most advanced treatments being available in Kabul. The French Medical Institute for Children and Indira Gandhi Childrens Hospital in Kabul are the leading children's hospitals in the country. Some of the other main hospitals in Kabul include the 350-bed Jamhuriat Hospital and the Jinnah Hospital, which is still under construction. There are also a number of well-equipped military-controlled hospitals in different regions of the country.
It was reported in 2006 that nearly 60% of the population lives within a two-hour walk of the nearest health facility, up from 9% in 2002. The latest surveys show that 57% of Afghans say they have good or very good access to clinics or hospitals. The nation has one of the highest incidences of people with disabilities, with around a million people affected. About 80,000 people are missing limbs; most of these were injured by landmines. Non-governmental charities such as Save the Children and Mahboba's Promise assist orphans in association with governmental structures. Demographic and Health Surveys is working with the Indian Institute of Health Management Research and others to conduct a survey in Afghanistan focusing on maternal death, among other things.
Education.
Education in the country includes K–12 and higher education, which is supervised by the Ministry of Education and the Ministry of Higher Education. The nation's education system was destroyed due to the decades of war, but it began reviving after the Karzai administration came to power in late 2001. More than 5,000 schools were built or renovated in the last decade, with more than 100,000 teachers being trained and recruited. More than seven million male and female students are enrolled in schools, with about 100,000 being enrolled in different universities around the country; at least 35% of these students are female. , there are 16,000 schools across Afghanistan. Education Minister Ghulam Farooq Wardak stated that another 8,000 schools are required to be constructed for the remaining 3 million children who are deprived of education.
Kabul University reopened in 2002 to both male and female students. In 2006, the American University of Afghanistan was established in Kabul, with the aim of providing a world-class, English-language, co-educational learning environment in Afghanistan. The capital of Kabul serves as the learning center of Afghanistan, with many of the best educational institutions being based there. Major universities outside of Kabul include Kandahar University in the south, Herat University in the northwest, Balkh University in the north, Nangarhar University and Khost University in the east. The National Military Academy of Afghanistan, modeled after the United States Military Academy at West Point, is a four-year military development institution dedicated to graduating officers for the Afghan Armed Forces. The $200 million Afghan Defense University is under construction near Qargha in Kabul. The United States is building six faculties of education and five provincial teacher training colleges around the country, two large secondary schools in Kabul, and one school in Jalalabad.
The literacy rate of the entire population has been very low but is now rising because more students go to schools. In 2010, the United States began establishing a number of Lincoln learning centers in Afghanistan. They are set up to serve as programming platforms offering English language classes, library facilities, programming venues, Internet connectivity, and educational and other counseling services. A goal of the program is to reach at least 4,000 Afghan citizens per month per location. The Afghan National Security Forces are provided with mandatory literacy courses. In addition to this, Baghch-e-Simsim (based on the American Sesame Street) was launched in late 2011 to help young Afghan children learn.
In 2009 and 2010, a 5,000 OLPC – One Laptop Per Child schools deployment took place in Kandahar with funding from an anonymous foundation. The OLPC team seeks local support to undertake larger deployment.
Culture.
The Afghan culture has been around for over two millennia, tracing back to at least the time of the Achaemenid Empire in 500 BCE. It is mostly a nomadic and tribal society, with different regions of the country having their own traditions, reflecting the multi-cultural and multi-lingual character of the nation. In the southern and eastern region the people live according to the Pashtun culture by following Pashtunwali, which is an ancient way of life that is still preserved. The remainder of the country is culturally Persian and Turkic. Some non-Pashtuns who live in proximity with Pashtuns have adopted Pashtunwali in a process called Pashtunization (or "Afghanization"), while some Pashtuns have been Persianized. Millions of Afghans who have been living in Pakistan and Iran over the last 30 years have been influenced by the cultures of those neighboring nations.
Afghans display pride in their culture, nation, ancestry, and above all, their religion and independence. Like other highlanders, they are regarded with mingled apprehension and condescension, for their high regard for personal honor, for their tribe loyalty and for their readiness to use force to settle disputes. As tribal warfare and internecine feuding has been one of their chief occupations since time immemorial, this individualistic trait has made it difficult for foreigners to conquer them. Tony Heathcote considers the tribal system to be the best way of organizing large groups of people in a country that is geographically difficult, and in a society that, from a materialistic point of view, has an uncomplicated lifestyle. There are an estimated 60 major Pashtun tribes, and the Afghan nomads are estimated at about 2–3 million.
The nation has a complex history that has survived either in its current cultures or in the form of various languages and monuments. However, many of its historic monuments have been damaged in recent wars. The two famous Buddhas of Bamiyan were destroyed by the Taliban, who regarded them as idolatrous. Despite that, archaeologists are still finding Buddhist relics in different parts of the country, some of them dating back to the 2nd century. This indicates that Buddhism was widespread in Afghanistan. Other historical places include the cities of Herat, Kandahar, Ghazni, Mazar-i-Sharif, and Zarang. The Minaret of Jam in the Hari River valley is a UNESCO World Heritage site. A cloak reputedly worn by Islam's prophet Muhammad is kept inside the Shrine of the Cloak in Kandahar, a city founded by Alexander and the first capital of Afghanistan. The citadel of Alexander in the western city of Herat has been renovated in recent years and is a popular attraction for tourists. In the north of the country is the Shrine of Hazrat Ali, believed by many to be the location where Ali was buried. The Afghan Ministry of Information and Culture is renovating 42 historic sites in Ghazni until 2013, when the province will be declared as the capital of Islamic civilization. The National Museum of Afghanistan is located in Kabul.
Although literacy is low, classic Persian and Pashto poetry plays an important role in the Afghan culture. Poetry has always been one of the major educational pillars in the region, to the level that it has integrated itself into culture. Some notable poets include Rumi, Rabi'a Balkhi, Sanai, Jami, Khushal Khan Khattak, Rahman Baba, Khalilullah Khalili, and Parween Pazhwak.
Media and entertainment.
The Afghan mass media began in the early 20th century, with the first newspaper published in 1906. By the 1920s, Radio Kabul was broadcasting local radio services. Afghanistan National Television was launched in 1974 but was closed in 1996 when the media was tightly controlled by the Taliban. Since 2002, press restrictions have been gradually relaxed and private media diversified. Freedom of expression and the press is promoted in the 2004 constitution and censorship is banned, although defaming individuals or producing material contrary to the principles of Islam is prohibited. In 2008, Reporters Without Borders ranked the media environment as 156 out of 173 countries, with the 1st being the most free. Around 400 publications were registered, at least 15 local Afghan television channels, and 60 radio stations. Foreign radio stations, such as Voice of America, BBC World Service, and Radio Free Europe/Radio Liberty (RFE/RL) broadcast into the country.
The city of Kabul has been home to many musicians who were masters of both traditional and modern Afghan music. Traditional music is especially popular during the Nowruz (New Year) and National Independence Day celebrations. Ahmad Zahir, Nashenas, Ustad Sarahang, Sarban, Ubaidullah Jan, Farhad Darya, and Naghma are some of the notable Afghan musicians, but there are many others. Most Afghans are accustomed to watching Indian Bollywood films and listening to its filmi hit songs. Many major Bollywood film stars have roots in Afghanistan, including Salman Khan, Saif Ali Khan, Shah Rukh Khan (SRK), Aamir Khan, Feroz Khan, Kader Khan, Naseeruddin Shah, Zarine Khan and Celina Jaitly. In addition, several Bollywood films, such as "Dharmatma", "Khuda Gawah", "Escape from Taliban", and "Kabul Express" have been shot inside Afghanistan.
Sports.
In recent years, Afghan sports teams have increasingly celebrated titles at international events. Afghanistan's basketball team won the first team sports title at the 2010 South Asian Games. Later that year, the country's cricket team followed as it won the 2010 ICC Intercontinental Cup. In 2012, the country's 3x3 basketball team won the gold medal at the 2012 Asian Beach Games, in 2013, Afghanistan's football team followed as it won the SAFF Championship.
Cricket is the country's most popular sport, followed by association football. The Afghan national cricket team, which was formed in the last decade, participated in the 2009 ICC World Cup Qualifier, 2010 ICC World Cricket League Division One and the 2010 ICC World Twenty20. It won the ACC Twenty20 Cup in 2007, 2009, 2011 and 2013. The team eventually made it to play in the 2015 Cricket World Cup. The Afghanistan Cricket Board (ACB) is the official governing body of the sport and is headquartered in Kabul. The Ghazi Amanullah Khan International Cricket Stadium serves as the nation's main cricket stadium, followed by the Kabul National Cricket Stadium. Several other stadiums are under construction. Domestically, cricket is played between teams from different provinces.
The Afghanistan national football team has been competing in international football since 1941. The national team plays its home games at the Ghazi Stadium in Kabul, while football in Afghanistan is governed by the Afghanistan Football Federation. The national team has never competed or qualified for the FIFA World Cup, but has recently won an international football trophy in 2013. The country also has a national team in the sport of futsal, a 5-a-side variation of football.
Other popular sports in Afghanistan include basketball, volleyball, taekwondo, and bodybuilding. Buzkashi is a traditional sport, mainly among the northern Afghans. It is similar to polo, played by horsemen in two teams, each trying to grab and hold a goat carcass. The Afghan Hound (a type of running dog) originated in Afghanistan and was originally used in hunting.

</doc>
